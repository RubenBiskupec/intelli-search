Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 :
https://doi.org/10.1186/s13638-020-01782-6 EU RAS | P J ourna | on Wire | ess
Communications and Networking

RESEARCH Open Access

An adversarial learning approach for ®
discovering social relations in
human-centered information networks

Shicheng Cui! ©, Qianmu Li!??" and Shu-Ching Chen*

 

 

sianmuenjusteducn Abstract

"School of Computer Science and The analytics on graph-structured data in cyber spaces has advanced many

Sees and techy ot ebe!, human-centered computing technologies. However, if only utilizing the structural
Nanjing, China ) properties, we might be prohibited from unraveling unknown social relations of nodes
*School of Cyber Science and especially in the structureless networked systems. Up-to-date ways to unfold latent
Sees and techy ot ebe!, relationships from graph-structured data are network representation learning (NRL)
Nanjing, China techniques, but it is difficult for most existing ones to deal with the

Full list of author information is network-structureless situations due to the fact that they largely depend on the

available at the end of the article observed connections. With the ever-broader spectrum of human-centered networked
systems, large quantities of textual information have been generated and collected
from social and physical spaces, which may provide the clues of hidden social relations.
In order to discover latent social relations from the accompanied text resources, this
paper attempts to bridge the gap between text data and graph-structured data so that
the textual information can be encoded to substitute for those incomplete structural
information. Generative adversarial networks (GANs) are employed in the cross-modal
framework to make the transformed data indistinguishable in graph-domain space and
also capable of depicting structure-aware relationships with network homophily.
Experiments conducted on three text-based network benchmarks demonstrate that
our approach can reveal more realistic social relations from text-domain information
compared against the state-of-the-art baselines.

Keywords: Human-centered networked systems, Social relations,
Network-structureless, Textual information, Generative adversarial networks

 

1 Introduction

Human-centered networked systems, such as cyber-physical systems (CPS) or Internet of
Things (IoT) [1-4], have increasingly accumulated a large amount of multimedia informa-
tion, which includes texts, images, and videos. Such heterogeneous and inter-networked
environments present the data in the format of graph-structured networks. Traditional
human-centered computing methods to tackle such cyberspace data largely depend on
the analytics of structural properties. However, they have difficulties in detecting the
updated or hidden social relations due to the fact that the plain graphs with only graph

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
GQ) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
— credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 2 of 19

topologies (including nodes and edges) sometimes limit the scope of analyzing the mod-
ern social and physical networks. For example, we might be prohibited from inferring the
social behaviors of an unknown node or predicting the potential targets that a newly com-
ing node would communicate with especially in the structureless networks. Thus, it is
crucial to incorporate useful multimedia resources to discover unknown social relations.

Generally, the problem of predicting unknown relationships in networks is called “link
prediction” [5, 6]. Researchers mostly concentrate on inferring the behavior of linking for-
mation process through the observed connections in current networks. In recent years,
network representation learning (NRL) [7, 8] has been proposed to support subsequent
network processing and improve the performance of relation inference. It aims at learning
the embedding space that can preserve the relationships for network reconstruction and
support network inference effectively. Many NRL methods [9-12] have indeed advanced
graph pattern discovery, analysis, and prediction. However, there exists a “blind zone”
where few explicit connections can be observed in the network-structureless situations.
We cannot expect that the complex network environments of CPS or IoT [13-15] would
always be well-structured and exactly take the whole picture of the networked systems.
For instance, there might be nodes in the system which have been isolated from the main
graph; thus, some valuable information might be blocked for the lack of explicit struc-
tural properties; or several newly arrived nodes without any topology information are
waiting to be added to the main system or to connect with others. Thus, if a network
is only analyzed from the perspective of the currently observed structural information,
some potential social relations cannot be discovered since those hidden but vital relation-
ships of nodes may not be preserved in the embedding space. The incomplete network
structure forces us to neglect such implicit information. To the best of our knowledge, the
existing NRL methods fail to handle structureless networks due to the fact that the inner
core that drives those methods to work is the “currently observed connections.” Admit-
tedly, this task seems intractable if the graph structure is the only feature that we can
utilize. Therefore, it is difficult to construct those missing parts of the original network
or to infer the undiscovered relationships of nodes. To be specific, if omitting the text
data in Fig. 1, we will be confused about identifying the topic of the yellow node and the
links it may have. Also, it will be unclear how to construct the potential relations like
friendships or partnerships amongst a bunch of nodes in the network-structureless set.
However, since modern networked systems have generated and collected a large amount
of multimedia data, which can provide related clues for discovering unknown social rela-
tions. As illustrated in Fig. 1, if considering such textual information, we can infer that
the yellow node would have a higher probability being linked to the blue node labeled
with “network analysis” rather than the orange one labeled with “image processing,’ and
we can also establish meaningful connections for those black nodes due to their textual
information.

Some researchers [16-18] try to utilize textual information, but they concentrate on
integrating and balancing graph-structured data and text data in network embedding to
improve the performance of relation inference. Hence, they still struggle with the afore-
mentioned problem that the latent social relations still cannot be detected from the
incomplete networks. Under the circumstance that the structural information of needed-
to-be-analyzed graph data is missing, we also consider text data as the accompanied
resource. However, we attempt to bridge the gap between text data and graph-structured
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 3 of 19

 

Original Network Structureless Network

I am focusing on graph
| mining. (text, )

 

 

|
LY |

 

 

I am doing the research ) ©

       
     
   

 

\ | on link prediction in L Cae @( text. )
social networks. @ (text) @ (text)
Oe)

{-tmaging processing helps
me with facial
recognition.

AIS

Ol) Ole)

Fig. 1 Illustration of using text data to construct social relations. Color blue represents the topic of “network
analysis,” while color orange represents the topic of “image processing.” The gray nodes consist of parts of
the graph-structured data. The black nodes are in the structureless node set. The solid lines denote the
existing links, while the dotted lines denote the potential links

 

 

 

 

data instead so that the “observed texts” can be encoded to substitute for those incom-
plete structural information. As a consequence, we can predict missed or future social
relations based on text-domain information. Hence, we think of applying deep domain
adaptation (DDA) techniques to map the two modalities. DDA techniques embed domain
adaptation in the deep learning frameworks for learning more transferable representa-
tions [19]. Since our task demands for generating simulated samples that are similar to the
target samples and preserve the source domain information, we consider using generative
adversarial networks (GANs) [20] to address the aforementioned problem due to the fact
that the techniques applied in the existing structure-based NRL methods have difficulties
in making valid predictions of social relations by the non-graph-structured data. Inspired
by image-to-image translation [21—23], we propose social relation GAN (SRGAN) for
cross-domain knowledge translation. Two GANSs are employed in our framework. One
(namely t-GAN) aims to learn content-to-structure mapping by adversarially training a
discriminator and a generator. Specifically, the discriminator tries to distinguish the real
network embeddings from the fake embeddings transformed from the text domain, and
the generator tries to fool the discriminator to make the fake embeddings look like the
ones just learned from the graph domain. The other (namely g-GAN) learns structure-
to-content mapping by inverting the task of t-GAN. In addition, as shown in [21-23],
the data reconstruction of the source or target samples can be helpful for improving the
performance of domain adaptation. Thus, we also apply reconstruction techniques into
our adversarial training process. Social relation is one of the essential characteristics in
social networks. The “sociality” in this task is derived from two aspects: one is the orig-
inal network where the structural information is mostly preserved, and the other is the
structureless network where the text-domain information can be encoded to substitute
for those incomplete structural information. SRGAN tries to make the transformed data
reflect the sociability and the tendency to associate in or to form social connections of
the graph-structured data. Experimental results on three real-world datasets show that
translating meaningful social relations from the text-domain information is challenging,
while SRGAN outperforms the baseline methods.
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 4 of 19

Overall, our main contributions lie in three aspects:

1 Our approach is a remedy for most existing structure-based NRL techniques which
have difficulties in handling such text-based network-structureless problems;

2 We bridge the gap between graph-structured data and text data using GANs in the
networked systems;

3 Meaningful social relations can be translated from text-domain information by our
proposed approach.

The rest of the paper is organized as follows. In Section 2, related work is briefly
introduced. In Section 3, we present the approach of bridging the gap between graph-
structured data and text data using GANs. After that, the proposed SRGAN is evaluated
over several baselines and the detailed experiments are given in Section 4. Finally, we

conclude our work and point out the future work in Section 5.

2 Related work

Human-centered techniques have achieved a great success in many real-world appli-
cations [24-31]. As mentioned in [7], there are two goals for NRL. First, the learned
embedding space can reconstruct the original network. The network relationships are
reflected as the relative distance of any two nodes in the embedding space. If there is an
edge between two nodes, then the distance of these two nodes should be relatively small.
Second, network inference can be supported by the learned embedding space, such as link
prediction, node identification, and label inference.

Hence, there are large amounts of structure-based methods proposed for learning
network embedding spaces from the network topology. Inspired by natural language
processing, Perozzi et al. [9] treated nodes as words, while paths generated by the
random walk model over a network were regarded as sentences which were fed into
word2vec framework [32] aiming at preserving the neighborhood structure. In [10, 33],
they improved the network exploration strategy trying to capture more meaningful node
sequences. In order to handle very large scale information networks, LINE [11] was pro-
posed to preserve local and global network structures by utilizing the information of
local pairwise proximity to learn half of the dimensions over neighbors of nodes and con-
straining the sampled nodes at a two-hop distance from the sources to learn the rest.
To effectively capture the highly non-linear network structure, SDNE [34] exploited the
first-order proximity and second-order proximity jointly to preserve the global and local
structures. Similar to the image-based convolutional networks, Niepert et al. [35] pro-
posed a framework for learning convolutional neural networks for arbitrary graphs. The
graph convolutional network [36] used a localized first-order approximation of spectral
graph convolutions for semi-supervised learning on graph-structured data. To enhance
the robustness of network embeddings, Dai et al. [37] proposed an adversarial network
embedding (ANE) framework, which utilized GANs to capture latent graph features.
Gao et al. [38] generated proximities via GAN framework to discover the relationships
between nodes. HeGAN [39] was developed for capturing the rich semantics on het-
erogeneous information networks. GraphRNA [40] was composed of a collaborative
walking mechanism and a tailored deep embedding architecture, where the jointed ran-
dom walks on attributed networks were utilized to boost the process of learning node

representations.
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 5 of 19

In addition to the structure-based networks, there are some networks accompanied
with rich external text-based information, such as content attributes or text profiles. Tu
et al. [41] embedded nodes and edges into the same vector space based on the accompa-
nied semantic information. Yang et al. [17] incorporated textual information of the nodes
into NRL under a matrix factorization framework. CENE [18] jointly leveraged the net-
work structure and the content information for enhancing the network representation.
MMDW [42] tried to learn discriminative network representations by utilizing the label-
ing information of the nodes. CANE [16] was proposed for modeling the relationships
between nodes given rich external information. He et al. [43] fused both structural and
content information in a generative manner.

As is known to all, textual information is useful in learning network embedding
spaces due to the reason that it can provide related clues for constructing relation-
ships among network nodes. However, most existing text-enhanced NRL techniques
concentrate on integrating and balancing graph-structured data and text data in net-
work embedding, which means they still need structural information of the unseen
nodes. While in some real-world scenarios (as discussed in Section 1), we might not
know such information beforehand. Therefore, we apply DDA methods to learn trans-
ferable representations for mapping the two modalities. Since the task needs to make
the transformed data similar to the target one and preserve the source domain informa-
tion. Thus, similar to the adversarial-based and reconstruction-based DDA approaches
(21-23, 44, 45], we introduce GANs to address the aforementioned problem. The
main difference between our proposed approach and previous NRL work is that we
focus on the “blind zone,’ trying to infer meaningful social relations in structureless
network environments from the accompanied text resources. Our approach can be
regarded as a remedy for most existing NRL techniques to analyze text-based networked

systems.

3 SRGAN

3.1 Generative adversarial networks

In order to capture the “sociality,’ GANs [20] are employed in our framework. The
necessity of using GANs lies in that the adversarial process can break the barriers in
multi-modal data so that the potential social connections in some structureless situa-
tions can be inferred by the non-graph-structured data. The basic idea behind GANs is
to set up a game between a generator and a discriminator [46]. The generator tries to
mimic the distribution of the training data, i-e., it generates fake samples that are intended
to be indistinguishable from the real ones as much as possible; while the discriminator

determines whether a sample is fake or real using supervised learning techniques.

3.2 Text-based network definitions

Let G = (V,E) bea graph, where V denotes the set of vertices (nodes) and E c (V x V)
denotes the set of edges. V’ C V denotes the set of vertices lacking structural informa-
tion, while V” = V \ V’ denotes the rest with known structures. Let T be the set of textual
information of V. For the purpose of translating social relations from text-domain infor-
mation, without loss of generality, assume there exists an injective mapping f : Tt» V
between the two types of data, ie., S = {(¢,v)|v = f(t), t € T,v € V}.

We also present two important concepts as follows.
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 6 of 19

Real embeddings are the mathematical embeddings in a continuous vector space
learned by domain-specific representation learning techniques.

Fake embeddings are created by the generators trying to make discriminators inca-
pable of separating them from the real embeddings.

3.3. Cross-domain knowledge translation
Our task can be categorized to the heterogeneous domain adaptation setting [19]. It is
defined as transforming text-modal data to graph-modal data by cross-modal mapping
knowledge learned from two domain information. Suppose Xy € R'Y'*4v denotes the
embedding space of graph-modal data, and 4’ € R'"!*4r denotes the embedding space
of text-modal data. Vy 4 X7. dy and d7 are the small numbers of latent dimensions. The
purpose is to learn the cross-modal mapping g(-) with the following characteristics:
Indistinguishability. For x; ¢ Vr, g(x;) € Vy. The transformed data g(x;) can exactly
map the form of graph-modal data in the embedding space 1. Meanwhile, it should be
hard for a domain discriminator to differ the transformed one from the original one.
Structure awareness. For (t,v) € S, g(x;) should play the role of x, in the embedding
space Xy to some extent. In other words, the transformed data g(x;) should be able to
hold the structural relationships of x, with network homophily [47]. Hence, the empirical

structure-preserving objective is as follows.

min ) /IP(gO%) xu) — POsbxu)|
ueV (1)
+ Y|PRulg(xt)) — Peau),

uEeV

where |-| denotes the symbol of absolute value and P(.|-) denotes the conditional proba-
bility defined in [11, 16]. For example, suppose P(x,|x,,) is the conditional probability of
node v generated by node u:

exp (x! X,)

P(x,|x,) = =. ,
) Dizev EXP (x} - x)

(2)
where exp(-) stands for the exponential function and x! is the transpose of xy.

3.4 Pretrained domain embedding space
As the graph-modal data and text-modal data are totally heterogeneous in their original
forms, there are two demands for representing each one of them:

1 The domain-specific embedding space should preserve meaningful relationships or
semantics of the modality;

2 The form of the modality representations should be easy to translate from one to
the other.

Hence, before learning the cross-modal mapping knowledge, we first apply skip-gram
framework [32, 48], one of the most popular techniques in deep learning [49, 50], to pre-
train the embedding spaces for each domain. The skip-gram is one of the frameworks in
word2vec that tries to represent each word as a vector in a continuous low-dimensional
space, where similar words are close to each other. The objective under skip-gram is to
maximize the conditional probability of each word and its context.
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 7 of 19

Graph domain. In the graph domain, we adopt the random walk strategy to generate
the node sequences. Same as [9, 10, 33], we then apply the skip-gram framework with
negative sampling to map nodes to a continuous vector space. We maximize the likelihood
of node sequences to learn the structure regularities in the networks.

Text domain. In this domain, we also utilize skip-gram to train word vectors. And
then, we obtain the average of word vectors as the text embedding, which has shown its
effectiveness in text representation tasks [18, 51].

3.5 Cross-modal framework
The overall framework of SRGAN is illustrated in Fig. 2. In this particular DDA
task, we first use domain-specific encoders (Ey and Evy) to respectively pretrain text
embeddings and network embeddings. Two GANs are then employed to deal with
the data in set S, where x, ~ Py(x,) and x,» ~ Pr(x;) are the two modal data
distributions. To be more precise, Gr : Xr t Av encodes text embeddings into
fake network embeddings, while Dy tries to distinguish them from the real ones
in the graph domain. To the contrary, Gy : Yy t Ar and Dr invert the pro-
cess to regulate t-GAN and prevent model collapse. During the adversarial training
processes, reconstruction and construction losses are produced to update the param-
eters in GANs for the purpose of making the transformed data indistinguishable in
the target domain and also capable of holding structural relationships with network
homophily.

Content-to-structure. In this framework, t-GAN learns content-to-structure mapping
knowledge, the process of which transforms text-modal data to graph-modal data. It aims
at reflecting the structural style of a node by its textual information.

 

r oo oe cee, a eee oe sey

Graph Domain

Bie

      
 

       

Text Domain
bee Construction
a Loss Loss
text

— Reconstructio Construction
; Loss

i eg

 

Fig. 2 Illustration of the SRGAN framework. EF; and Ey are the domain encoders to pretrain the embedding
spaces. sy and sy are the scores which evaluate the likelinood of the transformed data to be the target one.
The light blue and light green embeddings are the reconstructed data, while the dark ones are the
transformed data. The blue arrows denote the text-node-text data flows, while the green ones denote the
node-text-node data flows

 

 

 

 
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172

Structure-to-content. Unlike image-to-image translation, we interpret the intention
of g-GAN as learning the structure-to-content mapping knowledge which provides back-
ward cycle consistency [21] to regulate t-GAN and prevent model collapse. The necessity
has been thoroughly discussed in [22].

The least-squares adversarial loss [52] is applied for both GANs to match the distri-
bution of the generated modal data to the data distribution in the target domain. The

objective functions are as follows.

1]

Le = 5 Ex~Prow) [(Dv(Gr(x)) — 1)7], (3)
1]

Ley = 5Ex~Pvom) [(Dr(Gv(x)) — 1’), (4)

1
Lor = 5 Ex~Pr x) [(Dr(x) — 17]

1 (5)
2
+ 3 Bxy~Pv Cw) [(Dr(Gv(x%))) |,
1]
Lpy =5Ex)~Pye%) [(Dv~%) — 1)”]
(6)

1 2
+ 5 Bxi~Pr (xs) | Dv (Gr))) |;

where “1” denotes the real label, and “O” (omitted in Eqs. (5) and (6)) denotes the fake label.
We minimize Lg, and £p,, to make the generated modal data to be indistinguishable
in the graph-domain embedding space. Meanwhile, we also minimize Cc, and Lp, to
enable Gy(x,) € Xr.

Since the purpose of cross-modal mapping is not only to make the transformed modal
data indistinguishable in the target domain, it should hold the network structure of the
target node as well. Therefore, besides the adversarial losses, we also apply reconstruction
losses and construction losses to optimize both GANs. We adopt the cycle consistency
loss (reconstruction loss) [21—23] to compute the reconstruction error, the idea of which
is to use transitivity to induce the generators to be consistent with each other. The
reconstruction loss measures how well the original data is reconstructed after a tran-
sit generative sequence. Meanwhile, the construction loss is also needed to measure the
similarity of relationships between the transformed data and the original one.

: = Gr (Xz), (7)
x; = Gy (xy),
x* = Gy(x*), @)

/
x, = Gr(x;),

where x* and x* in Eq. (7) are the transformed data, and x* and x* in Eq. (8) are the
reconstructed data. Thus, the cycle consistency loss is defined as follows.

Loe = Exy~Prox) [IX — Xelln | + Exy~Py ou) | [XS — xvi], (9)

where /; norm is applied in the loss, and we push Gy (Gr (x;)) © x; and G7r(Gy(xy)) © x,
in the cycle by minimizing Ley.
The construction loss is defined in Eq. (10):

Loon = Ex,~Pr(x;) [x7 _ Xz l|1 | + Ex, ~Py (xy) [ Ix? _ Xyll1 | . (10)

Page 8 of 19
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 9 of 19

To satisfy Eq. (1), we minimize L,o,,. Let
Lyov = |P(Gr(xt)|Xu) — PO |xu)|
oc Jexp (G(X) » Xu) — exp (x) - Xx)!
= exp (xj - Xu) - lexp ((G7-(x1) — xj) -Xu) — 1
x lexp ((G(x:) — x}) xu) — 1],
if ||x> — xy||1 — 0, then Ly, —> 0.

Besides, minimizing |P(x,|Gr(x:)) — P(x,|x,)| is equal to minimizing
|P~* (xulGr (xz) — P~! (xy|xy)|. Thus, let

Lysu = |PO* (Xul|Gr (x) — P7* (xy|xv)|
= > exp ((x} — x,,) Gr(x;))

zeV
— S exp ((x — x;,) Xy)|
zeV
= > exp ((x} — x,,) Xy)
zeV

- [exp (xt — xj) (Gre) — xv)) — 1]1,
if ||x> — xy||1 — 0, then Ly_,, > 0.
The full objective of SRGAN is:

L= Lor + Ley + Lpr + Lpy + aLeyc + BLeon: (11)

where the hyper-parameters a and # are the factors controlling the contributions of the

reconstruction and construction losses, respectively.

3.6 Edge representation learning

Hadamard product [53] is adopted for learning the edge representations from content-to-
structure knowledge, which experimentally shows its effectiveness in [10]. For example,
given two nodes v,u with textual information, the edge representation is defined as
Xe(v,u) = Hadamard(x*, x7), where Xeiy,,) denotes the linking relationship of v, u trans-
lated from the corresponding texts.

4 Experiments

In this section, several experiments are conducted to validate the performances of
SRGAN and baseline methods.

4.1 Datasets
The following three real-world network datasets! are used in the experiments, and the
detailed statistics of these datasets are listed in Table 1.

CitNet”. This dataset was extracted by Tang et al. [54] where each paper is regarded as
a node, and every directed edge between two nodes denotes a citation. We obtain 132,033
papers with abstract contents after filtering and split them into training and testing sets
by a ratio of 70:30.

 

1For the validation purpose, the original linking information of the nodes in the testing sets is hidden during the training
stages.
*https://cn.aminer.org/citation
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 10 of 19

Table 1 Statistics of the datasets

 

 

CitNet Cora HepTh
#Nodes 217,335 225,026 1038
#Edges 632,751 714,266 1990
#Texts 132,033 30,305 1038

 

Cora’. A typical benchmark [55] for text-based social network analysis. All of the arti-
cles are divided into 10 root categories. Also, 70% of the articles are chosen to be the
training data, and the rest is for testing.

HepTh’. A filtered graph dataset [16, 56] extracted from the e-print arXiv, where
directed edges denote the citation relationships. Following the same rule, nodes with

textual information are split into training and testing sets as well.

4.2 Baselines

The DDA methods (standard GAN and LSGAN), structure-based NRL methods (Deep-
Walk, node2vec, and AIDW), and text similarity methods (Jaccard and CosSim) are
employed to demonstrate the effectiveness of SRGAN. Subsection 4.4 will present the
transformation quality of each DDA method in neighborhood preserving. To evaluate
the performance of translating social relations from text-domain information, in sub-
section 4.5, we also apply Hadamard product for all DDA methods and structure-based
NRL methods. Text similarity methods measure the similarity of texts straightforward.
Detailed descriptions of all baselines are as follows.

GAN. A classic GAN [20] (standard GAN) learns two competing mappings: a discrim-
inator and a generator, both of which are modeled as deep neural networks. They play
a min-max game where the discriminator tries to identify the fake network embeddings
and the generator tries to produce the examples as real as possible.

LSGAN. Least squares GAN [52] is able to generate samples that are closer to real
data. It adopts the least squares loss function for the discriminator to move the fake sam-
ples toward the decision boundary, which also performs more stable during the learning
process.

DeepWalk. This online learning approach learns low-dimensional latent representa-
tions of nodes from the samples yielded by short random walks [9], which is scalable to
build incremental results. Note that, for nodes without structural information, we estab-
lish self-links to learn the network representations due to the reason that intuitively a
node should have the closest relationship with itself. We switch hierarchical softmax to
negative sampling for improving the efficiency [10].

node2vec. It simulates breadth-first sampling and depth-first sampling by tunable
parameters p and q. According to the parameter sensitivity experiments [10], we set
Pp = q = 0.5 for balancing outward exploration and a proper distance from the start
vertex. Same as DeepWalk, we establish self-links for those nodes without structural
information both in the training and testing stages.

AIDW. ANE with inductive DeepWalk (AIDW) [37] unifies a structure-preserving
component and an adversarial-learning component alternatively to train the generator.

The former component aims at encoding structural properties, while the latter acts as a

 

$https://people.cs.umass.edu/~mccallum/data.html
*https://snap.stanford.edu/data/cit- HepTh.html
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 11 of 19

regularizer for learning more stable and robust representations based on the adversarial
learning principle.

Jaccard. Jaccard similarity coefficient [57], also known as Intersection over Union, is
used for measuring text similarity between finite sentence sample sets. It is defined as the
size of the word intersection divided by the size of the word union of the sample sets.

CosSim. Cosine Similarity measures the cosine of the angle between two text embed-
dings. It was adopted to build the document network dataset by Wang et al. [34]. Same as
SRGAN, each text is also represented by the average of the word vectors.

4.3 Experiment settings

The embedding size for both modalities is set to 300, which is the same size as Google
pretrained word vectors’. In random-walk procedure, we empirically set the length of
a random path to 30 and the iteration time for each node to 20. Following the settings
[37], AIDW initializes the input data with the pretrained node embeddings by DeepWalk
and applies one layer for the generator and 512-512-1 layer structure for the discrimi-
nator. Due to the pretrained data forms, all DDA methods apply deep neural networks
for linear transformation of the input data, and Adam [58] is employed as the opti-
mization algorithm for the neural networks. Instead of using ReLU and Leaky ReLU
[59-61] in image-to-image translation, we adopt hyperbolic tangent (tanh) as the activa-
tion function, and so do the other DDA methods. For regularization, we employ dropout
(rate = 0.3) [62] for the generators of GANs. Deep neural networks composed of an input
layer, several hidden layers and an output layer are employed for both generators (300-
600-300-300) and discriminators (300-600-300-300-1). The weights of neural networks
are initialized from a Gaussian distribution with mean 0 and standard deviation 0.02.
The best-performed hyper-parameters a and # in the space of {0.1, 0.3, 0.5, 0.7, 0.9} are
selected by applying the grid search strategy.

4.4 Neighborhood preserving quality
In this subsection, we evaluate the transformation performance of each DDA method. Q;,
and Qj), are the metrics used to validate the quality of the transformed data.
Qn = Evry [Eo~nay|IlXy — Xolla — [Xv — Xollal], (12)
Qn = Ey~v'[Eo~n IX) — Xoll2 — [Xv — Xollal],
where N(v) denotes the neighbors of node v. Q;, applies /; norm and Q,, applies /2 norm.
As discussed in Section 3.3, for v € V’, we expect that the transformed data x* could
hold the structural relationships of x, with network homophily in the graph-domain
space. Thus, the idea of the proposed metrics in Eq. (12) is to measure the similarity of
the neighborhood-preserving distance. If Q;, and Q), are relatively smaller, then we can
conclude that the transformed data preserves much more similar relationships with the
original one in its neighborhood. Figures 3 and 4 show the results of the transformation
quality in neighborhood preserving. Apparently, SRGAN outperforms the standard GAN
and LSGAN on all three datasets. Despite the different adversarial learning processes
between the two state-of-the-art GAN methods, the performances in neighborhood pre-
serving are almost the same. However, SRGAN achieves obviously smaller Q;, and Q),

 

°https://code.google.com/archive/p/word2vec/
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 12 of 19

) CitNet dataset ( (a =0.1, e= = 0. 1). (b) Cora dataset (a= = 0.3, B= “0. 1). c) HepTh dataset (a= = 0.9, B= = 0. 1).

 

re 3 Results of Q;, distance on the three datasets in neighborhood preserving over the DDA methods

 

 

 

distances. We reduce more than 18% of Q;, and Q), distances on CitNet, 5% on HepTh,
and almost a half on Cora.

The state-of-the-art GAN methods only consider the adversarial loss in modality
transformation. They just try to make the generated data indistinguishable in the graph-
domain space but neglect to preserve structural relationships in networks. However, as
the experimental results present, SRGAN can narrow down the difference between the
transformed data and the original one in the relationships with neighbors. We think this
is mainly because of the framework of SRGAN, where the cycle learning process with
reconstruction and construction losses provides the cross-modal mapping knowledge
that helps diminish Q;, and Qj), distances.

4.5 Relation inference from text-domain information

To validate the effectiveness of all methods in translating social relations, we conduct link
prediction experiments based on text-domain information. We construct positive sam-
ples labeled with “1” by selecting all edges (v, u) € E, and negative ones labeled with “O” by
randomly generating node pairs (v,u) ¢ E. We employ /2-regularized logistic regression
[63, 64] implemented using scikit-learn © to train the classifiers based on the original net-
work structures for evaluating the methods. We aim to predict whether there exists a link
between two given nodes and thus following [65], the metrics adopted for performance
evaluation include Micro-F, and Macro-F,. Micro-F, sums up the individual true posi-
tives, false positives, and false negatives of the dataset for different classes, while Macro-F}
calculates the average of the precision and recall values of the dataset on different classes
and finds their unweighted mean.

Intuitively, when facing with no explicit structures of the text-based networks, it would
be easy to consider measuring the similarity of two nodes by their textual information.
Therefore, we employ two commonly used text similarity methods, the Jaccard similarity
coefficient and cosine similarity, to show the baseline results of relation inference based
on texts. The threshold is set to 0.5, which is the same as in the /2-regularized logistic
regression classifiers. If Jaccard similarity coefficient or cosine similarity score between
two texts is larger than 0.5, then we infer that there exists a link between the corre-
sponding nodes. Besides, due to the scalability of DeepWalk, node2vec, and AIDW, we
also investigate the effectiveness of the three state-of-the-art NRL methods when dealing
with the situation of missing structures. Tables 2, 3, and 4 show the performances of all
methods, and the numbers in bold represent the best results.

 

Shttp://scikit-learn.org/stable/
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 13 of 19

RGAN

) CitNet dataset ( (a=0.1, 6 =0.1). wc Cora dataset (o= = 0.3, B= = 0. 1). c) HepTh dataset ‘(a= = 0.9, a= = 0. 1).

 

 

 

 

re 4 Results of Q;, distance on the three datasets in neighborhood preserving over the DDA methods

 

Table 2 demonstrates the superiority of SRGAN in comparison against the other seven
methods. First, we can see that the DDA methods achieve a significant improvement in
relation inference based on textual information. The best Micro-F] and Macro-F] scores
on the CitNet dataset produced by SRGAN are 0.8340 and 0.8265, respectively. Mean-
while, we find that standard GAN, LSGAN, and SRGAN perform stable when varying
the percentages of the training edges. Second, we can conclude that the structure-based
NRL methods (DeepWalk, node2vec, and AIDW) are unsuitable in the situation of miss-
ing structures. To our surprise, the edge representations generated by DeepWalk and
node2vec make the classifiers predict all potential links negative regardless of the dif-
ferent percentages of the training edges. We think the reason is that the online training
process barely learns meaningful structural information for those data in the testing sets.
Therefore, the testing nodes might be mapped to the position where they are incapable
of holding the proper structural relationships. Though AIDW performs slightly better
than DeepWalk and node2vec, it still suffers from unknown structures when generating
new embeddings. Third, the naive thought of inferring potential relations by measuring
the similarity of textual information cannot achieve expected results. Cosine similar-
ity performs the worst and Jaccard is just slightly better than DeepWalk, node2vec, and
AIDW.

As the results presented in Table 3, SRGAN increases more than 5% in Micro-F; and
Macro-F] scores, respectively, compared with the other two DDA methods. The stan-
dard GAN and LSGAN are considered indistinguishable on the Cora dataset. We find
that DeepWalk and node2vec fail to produce valid results because they make the classi-
fiers predict all relations nonexistent again. AIDW performs much better than the other
two NRL methods, but it cannot predict meaningful relationships even though the node
representations are enhanced. Jaccard is better than cosine similarity on Cora dataset, but

Table 2 Micro-F; and Macro-F; scores on CitNet dataset in link prediction (@ = 0.1, B = 0.1)

 

 

 

%Training edges 10% 30% 50% 70% 90%
Micro-F, Macro-F, Micro-F; Macro-F, Micro-F; Macro-F; Micro-F, Macro-Fy; Micro-F, Macro-Fy

GAN 0.7734 0.7722 0.7714 0.7703 0.7725 0.7713 0.7715 0.7704 =0.7718 0.7707
LSGAN 0.8124 0.8085 0.8130 0.8092 0.8109 0.8071 0.8120 0.8083 0.8114 0.8076
DeepWalk 0.6041 0.3766 0.6041 0.3766 0.6041 0.3766 0.6041 0.3766 0.6041 0.3766
node2vec 0.6041 0.3766 0.6041 0.3766 0.6041 0.3766 0.6041 0.3766 0.6041 0.3766
AIDW 0.6043 0.3775 0.6043 0.3775 0.6043 0.3776 0.6043 0.3774 0.6043 0.3775
Jaccard 0.6050 0.3792 0.6050 0.3792 0.6050 0.3792 0.6050 0.3792 0.6050 0.3792
CosSim 0.3990 0.2897 0.3990 0.2897 0.3990 0.2897 0.3990 0.2897 0.3990 0.2897

SRGAN 0.8328 0.8248 0.8325 0.8248 0.8335 0.8259 0.8337 0.8261 0.8340 0.8265

 
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 14 of 19

Table 3 Micro-F; and Macro-F; scores on Cora dataset in link prediction (a = 0.3, B = 0.1)

 

 

 

%Training edges 10% 30% 50% 70% 90%

Micro-F; Macro-F,; Micro-F,; Macro-F, Micro-F; Macro-F,; Micro-F,; Macro-F, Micro-F; Macro-F
GAN 0.7306 0.7301 0.7337 0.7333 = 0.7309 0.7305 = 0.7327 0.7323 0.7309 0.7305
LSGAN 0.7335 0.7335 0.7336 0.7335 = 0.7328 0.7328 0.7344 0.7344 0.7359 0.7359
DeepWalk 0.5519 0.3556 0.5519 0.3556 0.5519 0.3556 0.5519 0.3556 0.5519 0.3556
node2vec 0.5519 0.3556 0.5519 0.3556 0.5519 0.3556 0.5519 0.3556 0.5519 0.3556
AIDW 0.5717) 04123 05717 04135 05714 04113 05716 04125 05715 04120
Jaccard 0.5682 0.3945 0.5682 0.3945 0.5682 0.3945 0.5682 0.3945 0.5682 0.3945
CosSim 0.4687 0.3745 04687 0.3745 04687 0.3745 04687 0.3745 04687 0.3745
SRGAN 0.7894 0.7876 0.7895 0.7877 0.7881 0.7863 0.7893 0.7874 0.7905 0.7887

 

still it cannot infer convincing social relations. SRGAN outperforms all baseline methods
and achieves the highest Micro-F; (0.7905) and Macro-F; (0.7887) scores involving 90%
of training edges. Still, the DDA methods perform steady when varying the percentage of
training edges from 10 to 90%.

Table 4 shows the results on HepTh dataset. SRGAN also produces the best Micro-F}
(0.8004) and Macro-F (0.7973) scores giving 50% of training edges. LSGAN turns out
to be competitive since in most cases, it shows a better performance compared with the
standard GAN. DeepWalk and node2vec seem to be unfit for inferring social relations in
the network-structureless situation. We think the two structure-based methods cannot
take the benefit from pretrained embedding models, which leads to generate meaning-
less edge representations. As a consequence, all edges in the testing sets are predicted
as invalid connections. Though AIDW makes positive predictions, few of them are cor-
rect. Jaccard similarity coefficient and cosine similarity are still uncompetitive with our

proposed approach.

4.6 Sensitivity of hyper-parameters
We evaluate the sensitivity of hyper-parameters a and § in the space of
{0.1, 0.3, 0.5, 0.7, 0.9} in this subsection. Figures 5 and 6 present the results.

With the increase of a and f, the performance of SRGAN gradually approaches to
DeepWalk and node2vec on the CitNet and Cora datasets. It seems to be inappropriate
to set the hyper-parameters too large in SRGAN. When a = 0.1 and 6 = 0.1, SRGAN
achieves its best results on CitNet. For the Cora dataset, SRGAN produces the highest
results when a increases to 0.3. Different from the two datasets, there is a slight fluctua-
tion approximately around 0.77 in both metrics on HepTh. SRGAN performs competitive

Table 4 Micro-F; and Macro-F; scores on HepTh dataset in link prediction (a = 0.9, B = 0.1)

 

 

 

%Training edges 10% 30% 50% 70% 90%
Micro-F, Macro-F; Micro-F; Macro-F,; Micro-F, Macro-F, Micro-F, Macro-F,; Micro-F; Macro-Fy

GAN 0.7590 0.7553 0.7541 0.7507 0.7472 0.7439 = 0.7555) 0.7525. 0.7535 0.7497
LSGAN 0.7714 0.7692 0.7686 0.7666 0.7728 0.7711 0.7452 0.7433 0.7562 0.7546
DeepWalk 0.6098 0.3788 0.6098 0.3788 0.6098 0.3788 0.6098 0.3788 0.6098 0.3788
node2vec 0.6098 0.3788 0.6098 0.3788 0.6098 0.3788 0.6098 0.3788 0.6098 0.3788
AIDW 0.6098 0.3805 0.6091 0.3785 0.6105 0.3840 0.6098 0.3837 0.6105 0.3840
Jaccard 0.6126 0.3865 0.6126 0.3865 0.6126 0.3865 0.6126 0.3865 0.6126 0.3865
CosSim 0.3909 0.2847 0.3909 0.2847 0.3909 0.2847 0.3909 0.2847 0.3909 0.2847

SRGAN 0.7887 0.7844 0.7901 0.7861 0.8004 0.7973 0.7859 0.7820 0.7887 0.7861

 
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172

 

bo me io i me
0.75 0.750 I 70 0725 0.780
0.725 0.778
0.700 0.776
0.675 0.774
0.772

0.625 0.770

   

(b) Cora dataset. (c) HepTh dataset.

 

 

Fig. 5 Micro-F; scores on the three datasets in hyper-parameter sensitivity evaluation

 

compared against other methods and the best results are achieved at (a = 0.9, 8 = 0.1).
Hence, we think that in most cases, it would be more effective for SRGAN to use a
relatively smaller 8. Depending on the applications, a still needs to be fine-tuned.

4.7 Efficiency of SRGAN

To demonstrate the efficiency of SRGAN, we conduct the experiments on the speed
of SRGAN during the training stage. For each dataset, the batch size is set to 64. Two
graphics processing units (GPUs, NVIDIA Tesla K80 ’) are deployed to accelerate model
training.

Table 5 shows the efficiency of SRGAN on the three datasets, where T,); denotes the
total time for training SRGAN per iteration (epoch), T; denotes the time that SRGAN
completes the first batch loop, and Tz), denotes the average time for the rest per batch.
Due to the GPU accelerators, the speed of training SRGAN on the three datasets is fast.
Tavg is only around 0.05s for all datasets. Even dealing with the large CitNet network, we
can still finish training SRGAN within 87s per iteration.

4.8 Discussions

Overall, the performance of SRGAN is superior to the state-of-the-art DDA methods
(standard GAN and LSGAN), structure-based NRL methods (DeepWalk, node2vec, and
AIDW), and text similarity methods (Jaccard similarity coefficient and cosine similar-
ity). SRGAN makes the transformed data preserve much more similar relationships with
the original one in its neighborhood. Meanwhile, the results of relation inference pro-
duced by SRGAN show that even if lacking some of the structural information, we can
still make valid prediction according to text-domain information. Thus, SRGAN outper-
forms the baseline methods in making the transformed data reflect the sociability and
the tendency to associate in or to form social connections of the graph-structured data.
The standard GAN and LSGAN only consider adversarial losses in modality transforma-
tion, which cannot well-preserve the network structures. It is difficult for DeepWalk and
node2vec to learn meaningful embeddings without explicit structures, since the online
learning process cannot transfer valid information of the pretrained network relation-
ships to those unseen nodes. AIDW enhances the robustness of node representations, but
it still struggles with the network-structureless situations. Jaccard similarity coefficient
and cosine similarity methods infer the relationships only based on text resources, which
in our experiments proves that the complex real-world social relations cannot be simply
inferred according to the similarity of their text data.

 

7https://www.nvidia.com/en- gb/data-center/tesla-k80/

Page 15 of 19
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 16 of 19

 

0.65 0.70
0.65 0.775 0.776

] ]
0.785
oy 0.75 0.75 f
0.75 0.75 0.70 0.780 0.778
0.774
0.772

0.45 0.50 0.770

 

0.768
0.766

 

(a) CitNet dataset. (b) Cora dataset. (c) HepTh dataset.

Fig. 6 Macro-F; scores on the three datasets in hyper-parameter sensitivity evaluation

 

 

 

Compared with the aforementioned baselines, the effectiveness of SRGAN can be

explained as follows.

1 Inour cross-modal mapping framework, not only do we use the adversarial losses
to deceive domain discriminators, but also reconstruction and construction losses
are applied to learn textual and topological styles, which depict the structure-aware
relationships with the network homophily;

2 SRGAN incorporates the knowledge from the text domain which remedies the
network-structureless situation where structure-based NRL methods cannot be
well-performed;

3 Unlike the text similarity methods that only consider text resources, in social
relation translation, SRGAN also takes the advantages of the original graph data.
The cross-modal mapping knowledge we learn bridges the text-modal data and

graph-modal data, which helps infer meaningful social relations.

However, this task is still challenging that demands for more research efforts. In the
experiments, we find that the generated network data cannot perfectly imitate the “man-
ners” of real one in the original network space (i.e., Q;, and Q;, distances still have some
rooms to be reduced). We think SRGAN can preserve “relative” relationships (social
connections) based on textual information, but it is challenging to locate the “absolute”
position of a node in the graph-domain space. The reason may lie in that the networks
might be generated from diverse social information, where the utilized textual infor-
mation might just be a part of the key components that lead to construct some of the
topologies or interactions. Therefore, it would be hard to accurately locate an unseen

node in the original network space only by such textual information.

5 Conclusion and future work

In this paper, we propose social relation GAN (SRGAN) which tries to rem-
edy for most of the existing structure-based NRL techniques that have difficulties
in dealing with text-based network-structureless problems. The cross-modal map-
ping framework bridges the gap between the graph-modal data and _ text-modal

Table 5 Efficiency of SRGAN on the three datasets

 

 

Dataset Tail(S) T;(s) Tavg(s)
CitNet 86.7461 2.6436 0.0583
Cora 20.7628 2.5968 0.0552

HepTh 3.0681 2.5363 0.0532

 
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 17 of 19

data, which helps learn meaningful relations from the text-domain information in
networked systems. Experimental results on three text-based network benchmarks
show that SRGAN can translate more realistic social relations compared against the
baselines.

In future work, we will consider incorporating other multimedia data like images,
videos, etc., to analyze such a network-structureless situation. Also, we believe it is pos-
sible to generate meaningful text-based profiles from the graph-modal data, which could
provide more information for some human-centered applications such as recommenda-

tions and detections.

Abbreviations

NRL: Network representation learning; GANs: Generative adversarial networks; CPS: Cyber physical systems; loT: Internet
of Things; DDA: Deep domain adaptation; SRGAN: Social relation GAN; LSGAN: Least squares GAN; ANE: Adversarial
network embedding; AIDW: ANE with inductive DeepWalk; GPUs: Graphics processing units

Acknowledgements
Not applicable.

Authors’ contributions
Shicheng Cui is the principal contributor. In a supervising role, Dr. Qianmu Li and Dr. Shu-Ching Chen formulated the
research problem and contributed to the discussion of results. The authors read and approved the final manuscript.

Funding

This work was supported in part by the China Scholarship Council (No. 201706840112), Fundamental Research Funds for
the Central Universities (No. 30918012204), Jiangsu province key research and development program (BE2017739), the
Ath project “Research on the Key Technology of Endogenous Security Switches” (2020YFB1804604) of the National Key
R&D Program “New Network Equipment Based on Independent Programmable Chips" (2020YFB1804600), 2018 Jiangsu
Province Major Technical Research Project “Information Security Simulation System” (BE2017100), Military Common
Information System Equipment Pre-research Special Technical Project (315075701), and Industrial Internet Innovation
and Development Project in 2019 - Industrial Internet Security On-Site Emergency Detection Tool Project.

Availability of data and materials
The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable
request.

Competing interests
The authors declare that they have no competing interests.

Author details

'School of Computer Science and Engineering, Nanjing University of Science and Technology, 210094 Nanjing, China.
School of Cyber Science and Engineering, Nanjing University of Science and Technology, 210094 Nanjing, China.
3Intelligent Manufacturing Department, Wuyi University, 529020, Jiangmen, China. *School of Computing and
Information Sciences, Florida International University, 33199 Miami, USA.

Received: 7 January 2020 Accepted: 14 August 2020
Published online: 07 September 2020

References

1. X. Xu, R. Mo, F. Dai, W. Lin, S. Wan, W. Dou, Dynamic resource provisioning with fault tolerance for data-intensive
meteorological workflows in cloud. IEEE Trans. Ind. Inform. (2019). https://doi.org/10.1109/TII.2019.2959258

2. J.Li, T. Cai, K. Deng, X. Wang, T. Sellis, F. Xia, Community-diversified influence maximization in social networks. Inf.
Syst. 92, 1-12 (2020)

3. L.Qi,Q. He, F. Chen, X. Zhang, W. Dou, Q. Ni, Data-driven web APIs recommendation for building web applications.
IEEE Trans. Big Data (2020). https://doi.org/10.1109/TBDATA.2020.2975587

4. X. Xu, X. Liu, Z. Xu, C. Wang, S. Wan, X. Yang, Joint optimization of resource utilization and load balance with privacy
preservation for edge services in 5G networks. Mob. Netw. Appl. (2019). https://doi.org/10.1007/s11036-019-01448-8

5. \V.Martinez, F. Berzal, J-C. Cubero, A survey of link prediction in complex networks. ACM Comput. Surv. 49(4), 69
(2017)

6. H.Liu, H. Kou, C. Yan, L. Qi, Link prediction in paper citation network to construct paper correlation graph. J. Wirel.
Commun. Netw. EURASIP. 2019, 233 (2019)

7. P. Cui, X. Wang, J. Pei, W. Zhu, A survey on network embedding. arXiv preprint arXiv:1711.08752 (2017)

8. W.L. Hamilton, R. Ying, J. Leskovec, Representation learning on graphs: methods and applications. arXiv preprint
arXiv:1709.05584 (2017)

9. B. Perozzi, R. Al-Rfou, S. Skiena, in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, Deepwalk: online learning of social representations (ACM, 2014), pp. 701-710. https://doi.org/10.
1145/2623330.2623732
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 18 of 19

20.

21,

22.

23.

24,

25.

26.

2/.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

39.

40.

A. Grover, J. Leskovec, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, node2vec: scalable feature learning for networks (ACM, 2016), pp. 855-864. https://doi.org/10.1145/
2939672.2939754

. J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, Q. Mei, in Proceedings of the 24th International Conference on World Wide

Web, Line: large-scale information network embedding (ACM, 2015), pp. 1067-1077. https://doi.org/10.1145/
2736277,.2741093

S. Cui, T. Li, S-C. Chen, M.-L. Shyu, Q. Li, H. Zhang, DISL: deep isomorphic substructure learning for network
representations. Knowl.-Based Syst. 189, 105086 (2020). https://doi.org/10.1016/j.knosys.2019.105086

X. Xu, X. Zhang, H. Gao, Y. Xue, L. Qi, W. Dou, Become: blockchain-enabled computation offloading for loT in mobile
edge computing. IEEE Trans. Ind. Inform. 16(6), 4187-4195 (2020)

Y. Chen, N. Zhang, Y. Zhang, X. Chen, W. Wu, X. S. Shen, Energy efficient dynamic offloading in mobile edge
computing for internet of things. IEEE Trans. Cloud Comput. (2019). https://doi.org/10.1109/TCC.2019.2898657

X. Xu, C. He, Z. Xu, L. Qi, S. Wan, M. Z. A. Bhuiyan, Joint optimization of offloading utility and privacy for edge
computing enabled iot. IEEE Internet Things J. 7(4), 2622-2629 (2020)

C. Tu, H. Liu, Z. Liu, M. Sun, in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,
vol 1, Cane: context-aware network embedding for relation modeling, (2017), pp. 1722-1731. https://doi.org/10.
18653/v1/P17-1158

C. Yang, Z. Liu, D. Zhao, M. Sun, E. Y. Chang, in Proceedings of the 24th International Joint Conference on Artificial
Intelligence, Network representation learning with rich text information (AAAI Press, 2015), pp. 2111-2117

X. Sun, J. Guo, X. Ding, T. Liu, A general framework for content-enhanced network representation learning. arXiv
preprint arXiv:1610.02906 (2016)

M. Wang, W. Deng, Deep visual domain adaptation: a survey. Neurocomputing (2018). https://doi.org/10.1016/j.
neucom.2018.05.083

|. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, in Advances in Neural
Information Processing Systems, Generative adversarial nets, (2014), pp. 2672-2680

J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, in 2017 IEEE International Conference on Computer Vision, Unpaired
image-to-image translation using cycle-consistent adversarial networks (IEEE, 2017), pp. 2242-2251

T. Kim, M. Cha, H. Kim, J. K. Lee, J. Kim, in International Conference on Machine Learning, Learning to discover
cross-domain relations with generative adversarial networks, (2017), pp. 1857-1865

Z. Yi, H. Zhang, P. Tan, M. Gong, in 2017 IEEE International Conference on Computer Vision, DualGAN: unsupervised
dual learning for image-to-image translation (IEEE, 2017), pp. 2868-2876

T. Mukherjee, P. Kumar, D. Pati, E. Blasch, E. Pasiliao, L. Xu, LOSI: large scale location inference through FM signal
integration and estimation. Big Data Min. Analytics. 2(4), 319-348 (2019)

B. S. Jena, C. Khan, R. Sunderraman, High performance frequent subgraph mining on transaction datasets: a survey
and performance comparison. Big Data Min. Analytics. 2(3), 159-180 (2019)

L. Qi, X. Zhang, S. Li, S. Wan, Y. Wen, W. Gong, Spatial-temporal data-driven service recommendation with
privacy-preservation. Inf. Sci. 515, 91-102 (2020)

M. Bouazizi, T. Ohtsuki, Multi-class sentiment analysis on twitter: classification performance and challenges. Big Data
Min. Analytics. 2(3), 181-194 (2019)

W. Zhong, X. Yin, X. Zhang, S. Li, W. Dou, R. Wang, L. Qi, Multi-dimensional quality-driven service recommendation
with privacy-preservation in mobile edge environment. Comput. Commun. 157, 116-123 (2020)

X. Xu, Y. Chen, X. Zhang, Q. Liu, X. Liu, L. Qi, A blockchain-based computation offloading method for edge
computing in 5G networks. Softw. Pract. Experience (2019). https://doi.org/10.1002/spe.2749

C. Zhou, A. Li, A. Hou, Z. Zhang, Z. Zhang, P. Dai, F. Wang, Modeling methodology for early warning of chronic heart
failure based on real medical big data. Expert Syst. Appl., 113361 (2020). https://doi.org/10.1016/j.eswa.2020.113361
T. Cai, J. Li, A. Mian, R.-H. Li, T. Sellis, J. X. Yu, Target-aware holistic influence maximization in spatial social networks.
IEEE Trans. Knowl. Data Eng. (2020). https://doi.org/10.1109/TKDE.2020.3003047

T. Mikolov, |. Sutskever, K. Chen, G. S. Corrado, J. Dean, in Advances in Neural Information Processing Systems,
Distributed representations of words and phrases and their compositionality, (2013), pp. 3111-3119

S. Cui, B. Xia, T. Li, M. Wu, D. Li, Q. Li, H. Zhang, in 2017 12th International Conference on Intelligent Systems and
Knowledge Engineering, SimWalk: learning network latent representations with social relation similarity (IEEE, 2017),
pp. 1-6. https://doi.org/10.1109/ISKE.201 7.8258804

D. Wang, P. Cui, W. Zhu, in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, Structural deep network embedding (ACM, 2016), pp. 1225-1234. https://doi.org/10.1145/2939672.
2939753

M. Niepert, M. Ahmed, K. Kutzkov, in International Conference on Machine Learning, Learning convolutional neural
networks for graphs, (2016), pp. 2014-2023

T.N. Kipf, M. Welling, in /nternational Conference on Learning Representations, Semi-supervised classification with
graph convolutional networks, (2017)

Q. Dai, Q. Li, J. Tang, D. Wang, in The 32nd AAAI Conference on Artificial Intelligence, Adversarial network embedding,
(2018)

H. Gao, J. Pei, H. Huang, in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, ProGAN: network embedding via proximity generative adversarial network (ACM, 2019),

pp. 1308-1316. https://doi.org/10.1145/3292500.3330866

B. Hu, Y. Fang, C. Shi, in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Adversarial learning on heterogeneous information networks (ACM, 2019), pp. 120-129. https://doi.org/10.
1145/3292500.3330970

X. Huang, Q. Song, Y. Li, X. Hu, in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, Graph recurrent networks with attributed random walks (ACM, 2019), pp. 732-740. https://doi.org/
10.1145/3292500.3330941

 
Cui et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:172 Page 19 of 19

 

Ad,

 

52.

53.

54,

55,

56.

5/.

58.
59.

60.

61.

62.

63.

64.

65,

C. Tu, Z. Zhang, Z. Liu, M. Sun, in Proceedings of the 26th International Joint Conference on Artificial Intelligence, Transnet:
translation-based network representation learning for social relation extraction (AAAI Press, 2017), pp. 2864-2870
C. Tu, W. Zhang, Z. Liu, M. Sun, in Proceedings of the 25th International Joint Conference on Artificial Intelligence,
Max-margin deepwalk: discriminative learning of network representation (AAAI Press, 2016), pp. 3889-3895

Z. He, J. Liu, N. Li, ¥. Huang, in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, Learning network-to-network model for content-rich network embedding (ACM, 2019),

pp. 1037-1045. https://doi.org/10.1145/3292500.3330924

K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, D. Krishnan, in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, Unsupervised pixel-level domain adaptation with generative adversarial networks, (2017),
pp. 3722-3731

P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
Image-to-image translation with conditional adversarial networks, (2017), pp. 1125-1134

|. Goodfellow, Nips 2016 tutorial: generative adversarial networks. arXiv preprint arXiv:1701.00160 (2016)

M. McPherson, L. Smith-Lovin, J. M. Cook, Birds of a feather: homophily in social networks. Annu. Rev. Sociol. 27(1),
415-444 (2001)

 

. T.Mikolov, K. Chen, G. Corrado, J. Dean, Efficient estimation of word representations in vector space. arXiv preprint

arXiv:1301.3781 (2013)

Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature. 521(7553), 436-444 (2015)

|. Goodfellow, Y. Bengio, A. Courville, Deep learning. (MIT Press, 2016). https://www.deeplearningbook.org/

A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, in Proceedings of the 15th Conference of the European Chapter of the
Association for Computational Linguistics, vol 2, Bag of tricks for efficient text classification, (2017), pp. 427-431

X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, S. P. Smolley, in 2017 IEEE International Conference on Computer Vision, Least
squares generative adversarial networks (IEEE, 2017), pp. 2813-2821

C. Davis, The norm of the Schur product operation. Numer. Math. 4(1), 343-344 (1962)

J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, Z. Su, in Proceedings of the 14th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, ArnetMiner: extraction and mining of academic social networks (ACM, 2008),
pp. 990-998. https://doi.org/10.1145/1401890.1402008

A. K. McCallum, K. Nigam, J. Rennie, K. Seymore, Automating the construction of internet portals with machine
learning. Inf. Retr. 3(2), 127-163 (2000)

J. Leskovec, J. Kleinberg, C. Faloutsos, in Proceedings of the 11th ACM SIGKDD International Conference on Knowledge
Discovery in Data Mining, Graphs over time: densification laws, shrinking diameters and possible explanations (ACM,
2005), pp. 177-187. https://doi.org/10.1145/1081870.1081893

P. Jaccard, Distribution de la flore alpine dans le bassin des dranses et dans quelques régions voisines. Bull Soc.
Vaudoise Sci. Nat. 37, 241-272 (1901)

D. P. Kingma, J. Ba, Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)

X. Glorot, A. Bordes, Y. Bengio, in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,
Deep sparse rectifier neural networks, (2011), pp. 315-323

A. L. Maas, A. Y. Hannun, A. Y. Ng, in International Conference on Machine Learning, vol 30, Rectifier nonlinearities
improve neural network acoustic models, (2013), p. 3

B. Xu, N. Wang, T. Chen, M. Li, Empirical evaluation of rectified activations in convolutional network. arXiv preprint
arXiv:1505.00853 (2015)

N. Srivastava, G. Hinton, A. Krizhevsky, |. Sutskever, R. Salakhutdinov, Dropout: a simple way to prevent neural
networks from overfitting. J. Mach. Learn. Res. 15(1), 1929-1958 (2014)

R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, C.-J. Lin, LIBLINEAR: a library for large linear classification. J. Mach. Learn.
Res. 9, 1871-1874 (2008)

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V.
Dubourg, et al, scikit-learn: machine learning in python. J. Mach. Learn. Res. 12, 2825-2830 (2011)

H. Wang, J. Wang, J. Wang, M. Zhao, W. Zhang, F. Zhang, X. Xie, M. Guo, in The 32nd AAAI Conference on Artificial
Intelligence, GraphGAN: graph representation learning with generative adversarial nets, (2018)

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
