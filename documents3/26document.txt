Soft Computing (2020) 24:17589-17600
https://doi.org/10.1007/s00500-020-05220-z

FOCUS

®

Check for
updates

On the use of the Infinity Computer architecture to set up a dynamic

precision floating-point arithmetic

Pierluigi Amodio! - Luigi Brugnano? - Felice lavernaro'® - Francesca Mazzia

Published online: 6 August 2020
© The Author(s) 2020

Abstract

3

We devise a variable precision floating-point arithmetic by exploiting the framework provided by the Infinity Computer. This
is a computational platform implementing the Infinity Arithmetic system, a positional numeral system which can handle
both infinite and infinitesimal quantities expressed using the positive and negative finite or infinite powers of the radix @.
The computational features offered by the Infinity Computer allow us to dynamically change the accuracy of representation
and floating-point operations during the flow of a computation. When suitably implemented, this possibility turns out to be
particularly advantageous when solving ill-conditioned problems. In fact, compared with a standard multi-precision arithmetic,
here the accuracy is improved only when needed, thus not affecting that much the overall computational effort. An illustrative
example about the solution of a nonlinear equation is also presented.

Keywords Infinity Computer - Dynamic precision floating-point arithmetic - Conditioning

1 Introduction

The Arithmetic of Infinity was introduced by Y.D. Serge-
yev with the aim of devising a new coherent computational
environment able to handle finite, infinite and infinitesimal
quantities, and to execute arithmetical operations with them.
It is based on a positional numeral system with the infinite
radix ®, called grossone and representing, by definition, the
number of elements of the set of natural numbers N (see,
for example, Sergeyev (2008, 2009) and the survey paper
Sergeyev (2017)). Similar to the standard positional notation

Communicated by Yaroslav D. Sergeyev.
EX] Felice Iavernaro
felice.iavernaro @uniba.it

Pierluigi Amodio
pierluigi.amodio @uniba.it

Luigi Brugnano
luigi.brugnano @unifi.it

Francesca Mazzia
francesca.mazzia @ uniba.it
Dipartimento di Matematica, Universita di Bari, Bari, Italy

Dipartimento di Matematica e Informatica “U. Dini’,
Universita di Firenze, Firenze, Italy

Dipartimento di Informatica, Universita di Bari, Bari, Italy

for finite real numbers, a number in this system is recorded
as

Cm DP™ 22.0 DP cg @Pe_, OP! .. cp @P-*,

with the obvious meaning

Cm DPm H- ee +e,@DP! + cgDP?? +e_, OP! +--+» +0e_, ODP.
(1)

The coefficients c;, called grossdigits, are real numbers while
the grosspowers p;, sorted in decreasing order
Pm >-+++ > Pi > po=O0> p-1 >-:++ > pk,
may be finite, infinite or infinitesimal even though, for our
purposes, only finite integer grosspowers will be considered.
Notice that, since @° = 1 by definition, the set of real
numbers and the related operations are naturally included
in this new system. In this respect, the Arithmetic of Infinity
should be perceived as a more powerful tool that improves the
ability of observing and describing mathematical outcomes
that the standard numeral system could not properly handle.
In particular, the new system allows us to better inspect the
nature of the infinite objects we are dealing with. For exam-
ple, while co + 1 = o in the standard thinking, if we are

Q) Springer
17590

in the position to specify as, say ®, the kind of infinity we
are observing using the new methodology, such an equality
could be better replaced with ® + 1 > @. According to the
principle that the part is less than the whole, this novel per-
ception of the infinite dimensionality has proved successful
in resolving a number of paradoxes involving infinities and
infinitesimals, the most famous being Hilbert’s paradox of
the Grand Hotel (see Sergeyev (2008, 2009)).

The Arithmetic of Infinity paradigm is rooted in three
methodological postulates and its consistency has been rigor-
ously recognized in Lolli (2015). Its theoretical and practical
implications are formidable also considering that the final
goal is to make the new computing system available through
a dedicated processing unit. The computational device that
implements the Infinity Arithmetic has been called Infinity
Computer and is patented in EU, USA, and Russia (see, for
example, Sergeyev (2010)). Itis worth emphasizing that the
Infinity Computer methodology is not related to non-standard
analysis (see Sergeyev (2019)).

Among the many fields of research this new methodol-
ogy has been successfully applied, we mention numerical
differentiation and optimization (Cococcioni et al. 2020; De
Cosmis and Leone 2012; Sergeyev 2011; Zilinskas 2012),
numerical solution of differential equations (Amodio et al.
2016; Iavernaro et al. 2019; Mazzia et al. 2016; Sergeyev
2013; Sergeyev et al. 2016), models for percolation and bio-
logical processes (Iudin et al. 2012; Vita et al. 2012), cellular
automata (D’ Alotto 2015; Iudin et al. 2015). Finally, in Fal-
cone et al. (2020) the authors introduce a Simulink-based
simulator of the Infinity Computer, especially suitable for
visual programming in control theory. For further references
and applications see the survey (Sergeyev 2017).

The aim of the present study is to devise a dynamic
precision floating-point arithmetic by exploiting the com-
putational platform provided by the Infinity Computer. In
contrast with standard variable precision arithmetics, here
not only may the accuracy be dynamically changed during
the execution of a given algorithm, but variables stored with
different accuracies may be combined through the usual alge-
braic operations. This strategy is explored and addressed to
the accurate solution of ill-conditioned/unstable problems
(Brugnano et al. 2011; Iavernaro et al. 2006).

One interesting application is the possibility of handling
ill-conditioned problems or even of implementing algorithms
which are labeled as unstable in standard floating-point arith-
metic.! One example in this direction has been illustrated in
Amodio et al. (2020). It consists in the use of the iterative
refinement to improve the accuracy of a computed solution
to an ill-conditioned linear system until a prescribed input
accuracy is achieved.

' First results on handling ill-conditioning using the Infinity Computer
may be found in Gaudioso et al. (2018) and Sergeyev et al. (2018).

Q) Springer

P. Amodio et al.

Observe that the new architectures for high-performance
computing allow the use of low precision floating point arith-
metic available in hardware devices like, for example, GPUs.
Consequently, a new generation of numerical algorithms is
going to be developed, that work with mixed precision (see
Dongarra et al. (2020) for a review). In this direction, (Car-
son and Higham 2018) illustrates an example of the use of
iterative refinement with the aid of three different machine
precisions.

The paper is organized as follows. In the next section we
highlight those features of the Infinity Computer that play
a key role to set up the variable-precision arithmetic. This
latter is discussed in Sect. 3 together with a few illustrative
examples. As an application in Numerical Analysis, in Sect.
4 we consider the problem of finding the zero of a nonlinear
function affected by ill-conditioning issues. Finally, some
conclusions are drawn in Sect. 5.

2 Background

As is the case with the standard floating-point arithmetic,
the Infinity Computer handles both numbers and operations
numerically (not symbolically). Consequently, it is prone to
efficiently afford the massive amount of computation needed
while solving a wide variety of real-life problems. On the
other hand, a roundoff error proportional to the machine
accuracy is generated during representation of data (1.e., the
coefficients c; and p; in (1)) and execution of the basic oper-
ations. We will give a more detailed description about how
the representation of grossdigits and the floating-point oper-
ations should be carried out in the next section. Here, for sake
of simplicity, we will neglect these sources of errors.

The grossnumbers that will be considered in the sequel
are those that admit an expansion in terms of integer powers
of ©! and, thus, take the form

T
X= Soci O!, (2)
j=0

where T denotes the maximum order of infinitesimal appear-
ing in X. For this special set, the arithmetic operations on the
Infinity Computer follow the same rules defined for the poly-
nomial ring. For example, given the two grossnumbers

xX = xo @? + x1O7!, Y= yo@” + yO! + y2D~?, (3)

we get

X+Y = (x0 + yo)@? + (4) + yO} + pO”?,
On the use of the Infinity Computer architecture to set up a dynamic precision floating-point arithmetic

X -¥ =xgyo®® + (xoy1 + x1 90) 07!
+ (xoy2 + xy) O77? +2120,

and analogously for the division X/Y. Notice that, on the
Infinity Computer, variables may coexist with different stor-
age requirements. Taking aside the (negative) powers of ®
that, as we will see, need not to be stored in our usage,
the variable Y displays infinitesimals quantities up to the
order 2, thus requiring one extra record to store the gross-
digit y2, if compared with the variable X that only contains
a first-order infinitesimal. This circumstance also influences
the computational complexity associated with each single
floating-point operation. As a consequence of the different
amount of memory allocated for storing grossnumbers, the
global computational complexity associated with a given
algorithm performed on the Infinity Computer, cannot be
merely estimated in terms of how many flops are executed,
but should also take into account how many grossdigits are
involved in each operation.

If X is chosen as in (2), we denote by X its section
obtained by neglecting, in the sum, all the infinitesimals of
order greater than q, that is

q
x =) ocjO7. (4)

j=0

For example, choosing g = O and X and Y as in (3), we see
that XO + YO = xp + yo and X 0). yO — xoyo would
resemble the floating-point addition and multiplication in
standard arithmetic, respectively, while additional effort is
needed if other powers of @~! are successively involved.
More precisely, the computational cost associated with a sin-
gle operation of two grossnumbers will depend on how many
infinitesimal are considered. Assuming g < p and denoting
by d; the grossdigits associated with Y, for the two sections
X@ and Y), the addition

q P
XD LYM =S ep +d)Ol+ Yo ajo! (5)
j=0 J=qtl1

requires g + | additions of grossdigits, while the multiplica-
tion

q J Po
XD .YP =S\7 YS cidj iO! + S- \) cidj iD

j=0 i=0 j=qt+1 i=0
pt+q q
+ Yo YS cidj-iO7!
J=pt+l i=j—p
qt+p min{q,j}
=)

jJ=0 i=max{0,j—p}

cidj iO, (6)

17591

amounts to (¢-+1)(p+1) multiplications and gp—gq(q—1)/2
additions/subtractions of grossdigits.* It is worth noticing
that, since in both operations all the coefficients of O~/ may
be independently calculated, there is room for a huge par-
allelization. We will not consider this aspect in detail in the
present study.

3 A variable-precision representation of
floating-point numbers on the Infinity
Computer

Grossnumbers of the form (2) and their Sect. (4) form the
basis of the new floating-point arithmetic where numbers
with a different accuracy may be simultaneously represented
and combined. The idea is to let ©! and its powers act as
machine infinitesimal quantities when related to the classical
floating-point system. These infinitesimal entities, if suitably
activated or deactivated, may be conveniently exploited to
increase or decrease the required accuracy during the flow of
a given computation. This strategy may be used to automati-
cally detect ill-conditioning issues during the execution of a
code that solves a given problem, and to change the accuracy
accordingly, in order to optimize the overall computational
effort under the constrain that the resulting error in the output
solution should fit a given input tolerance. A formal intro-
duction of the new dynamic precision arithmetic is discussed
hereafter.

3.1 Machine numbers and their storage in the
Infinity Computer

Let t and T be two given non-negative integers and N =
(T + 1)(t + 1) — 1. The set of machine numbers we are
interested in is given by

N
p= {xer|x= 267 ae} ve, (7)

i=0

where 6 > 2 denotes the base of the numeral system, the
integer p is the exponent ranging in a given finite interval,
and 0 < d; < 6 — 1 are the significant digits, with dy) 4 0
(normalization condition). Starting from do, we group the
digits d; in T + 1 adjacent strings each of length rt + 1:

X = +B? do.d,--- dj dy41 +++ dar4i-:-
er §
t+1 t+1
djity-: A441 °° drs) ++ Aer 4ya4)-1
—-_——.-_—_——— —— oe
t+] t+1

* The division algorithm is described in Sect. 3 and therefore is not
discussed here.

Q) Springer
17592

T 1
= +B? S- pieey S- dja+e+iB (8)

j=0 i=0

The representation of the numbers X as in (7), under the
shape (8), suggests an interesting application of the Infin-
ity Computer. Introducing the new symbol @, called dark
grossone, as

Qe — prt, (9)

and setting
f .
cj = So djesisiB (10)
i=0

the number X in (8) may be rewritten as

T
X=+f") cj@7/. (11)
j=0

Its section of index q is then given by

q
X@ = +p? Yc O/. (12)
j=0

In analogy with the arithmetic of grossone, we continue refer-
ring to the coefficients c; as grossdigits. We assume that a
real number x is represented by a floating-point number X
in the form (11) by truncating or rounding it to the nearest
even, after the digit dy. This is the most attainable accu-
racy during the data representation phase but, in general, a
lower accuracy (and hence faster execution times) will be
required while processing the data, which will be achieved
by involving sections of X of suitable indices qg during the
computations. In this respect, forg = 0,1,2,...,7, we
can interpret the section X (7) in (12) as the single, double,
triple and, in general, multiple precision approximation of
x, respectively. These attributes are specific for the variable-
precision arithmetic epitomized by the sections X‘% defined
in (12) and should not be confused with the corresponding
meaning in the IEEE-754 standard for binary floating-point
arithmetic: only for the particular choice 6 = 2 and t = 23
may the two contexts exhibit some similarities.

Echoing the symbol ©, the new symbol @ emphasizes
the formal analogy between a machine number and a gross-
number (compare (11) with (2) and (12) with (4)). This
correspondence suggests that the computational platform
provided by the Infinity Computer may be conveniently
exploited to host the set F defined at (7) and to execute
operations on its elements using a novel methodology. This
is accomplished by formally identifying the two symbols,

Q) Springer

P. Amodio et al.

which means that, though they refer to two different defi-
nitions, they are treated in the same way in relation to the
storage and execution of the basic operations. In accord with
the features outlined in Sect. 2, the Infinity Computer will
then be able to:

(a) store floating-point numbers at different accuracy levels,
by involving different infinitesimal terms, according to
the need;

(b) easily access to sections of floating-point numbers as
defined in (12);

(c) perform computations involving numbers stored with
different accuracies.

The affinity between the meaning of the two symbols goes
even beyond what has been stated above. We have already
observed that the case g = O in (12) resembles the standard
set of floating-point numbers with ¢ + | significant figures.
This means that when the Infinity Computer works with
numbers of the form X) it precisely matches the structure
designed following the principles of the [EEE 754 standard.
In this mode, the operational accuracy is set at its mini-
mum value and the upper bound on the relative error due to
rounding (unit roundoff) is @~ |. Inother words, @~! will be
perceived as an infinitesimal entity which cannot be handled
unless we let numbers in the form X“) come into play. This
argument can then be iterated to involve o-,” i=2,...,T.
Mimicking the same concept expressed by the use of ®, neg-
ative powers of @ act like lenses to observe and combine
numbers using different accuracy levels.

Remark 1 What about the role of @ as an infinite-like quan-
tity? Consider again the basic operational mode with numbers
in the form X). If we ask the computer to count integer
numbers according to the scheme

n=0

while n+1>n
n=n+1

end

it would stop at n = @, yielding a further similarity with the
definition of © in the Arithmetic of Infinity. Again, involving
sections of higher index, the counting process could be safely
continued. Anyway, to avoid possible sources of confusion
that might arise from similarities such as the one highlighted
above, we stress that the roles of @ and ®, as well as their
methodological and philosophical meanings, are completely
different. For example, while the dark grossone is mainly
addressed to extend the capabilities of the standard floating-
point arithmetic, the grossone methodology covers a wider
spectrum of applications and, in particular, may successfully
cope with situations where the infinite or infinitesimal nature
of an object has to be explored, e.g., measuring infinite or
infinitesimal sets.
On the use of the Infinity Computer architecture to set up a dynamic precision floating-point arithmetic

In conclusion, the role of @ could be interpreted as an
inherent feature of the machine architecture which, con-
sistently with the Infinity Arithmetic methodology, could
activate suitable powers of @ to get, when needed, a better
perception of numbers. The examples included in the sequel
further elucidate this aspect.

3.2 Floating-point operations

We have seen that, through the formal identification of ®
with @, it is possible to store the elements of F as if they were
grossnumbers and, consequently, to take advantages of the
facilities provided by the Infinity Computer in accessing their
sections and performing the four basic operations on them,
according to the rules described in Sect. 2 (see, for example,
(5) and (6)). For these reasons, in the sequel, we shall use ©
in place of ® when working on the Infinity Computer, even
though, due to the finite nature of @, the result of a given
operation may not be in the form (12), so that a normalization
procedure has to be considered. Hereafter, we report a few
examples in order to elucidate this aspect. For all cases, a
binary base has been adopted for data representation.
Addition Set t = 3 and T = 2 (three grossdigits each with
four significant digits), and consider the sum of the two
floating-point normalized numbers:

X = 2°. 1.11010101110,
Y = 2-3. 1.11111001011.

Table 1 summarizes the procedure by a sequence of com-
mented steps. First of all, the two numbers are stored in
memory by distributing their digits along the powers @”,
@~! and @~? (step (a)). Before summing the two num-
bers, an alignment is performed to make the two exponents
equal (step (b)). Notice that shifting to the right the digits

Table 1 Scheme of the addition of two positive floating-point numbers

@? @! @~? @-3

(a) Data acquisition 29 1.110 1.010 1.110

2-7 1.111 «1.100 OO.111
(b) Alignment 2° 1.110 1.010 1.110

2° 0.001 1.111 1.000 1.110
(c) Sum 2° 1.111 11.001 10.110 1.110
(d) Redistribution 29 1.111

2° 0.001 1.001

2° 0.001 0.110 1.110
(e) Sum 2° 10.000 1.010 ~—0.110 1.110
(f) Normalization 2! 1.000 0.101 0.011 0.111
(g) Rounding 2! 1.000 0.101 0.011

17593

of the second number causes a redistribution of the digits
along the three mantissas. Step (c) performs a polynomial-
like sum of the two numbers. The contribution of each term
has to be consistently redistributed (step (d)), in order to
take into account possible carry bits, and the three mantissas
accordingly updated (step (e)). Steps (f) and (g) conclude the
computation by normalizing and rounding the result.
Subtraction As usual, floating-point subtraction between two
numbers sharing the same sign is performed by inverting the
sign bit of the second number, converting to 2’s complement
its mantissa, and then performing the addition as outlined
above. It is well-known that subtracting two close numbers
may lead to cancellation issues. We consider an example
where the accuracy may be dynamically changed in order to
overcome ill-conditioning issues. We assume to work with
the arithmetic resulting by setting t = 7 and T = 3 (four
grossdigits each consisting of eight binary digits, i1.e., a byte)
with truncation.

Loss of accuracy, resulting from a subtraction between
two numbers having the same sign, will be detected during
the normalization phase, and estimated when shifting the
mantissa by a given number of bits. For example, if X and
Y are the floating-point representations of two close posi-
tive real numbers x and y, and we see that X (0) _ yO) —
0.0---Odx---d;, then normalization requires shifting the
decimal point k places to the right. On the other hand,
when the operation is performed in single, double, triple and
quadruple precision, the relative errors in the result

(q) (xX _ Y@) —(x—y)|
EXD? —

, g=O0,...,3,
Ix — y|

for the four considered sections of X and Y are estimated as

EY ~ BOOTDETK g= 0, a 3.

Consequently, the size k of the shift required to normalize
the result gives us information about the size of the relative
error.

Consider the simple problem of evaluating the function
f(x, y,z) =x + y+z that computes the sum of three real
numbers, and assume that the user requires a single precision
accuracy in the result. In the examples below, we discuss three
different situations.

Example 1 The three real numbers

x =27!- 1,0001100000010111111001001110110--- ,
y = 2° -1.0010101010110010110101001101011--- ,
z= 2° .1.1011011010111011011011010111001--- ,

are represented on the Infinity Computer as

Q) Springer
17594

X = 27!. (@°1.0001100 + ®~'0.0001011
+ ®~0.1101010 + © 30.1101011),

Y = 2°. (©°1.0010101 + ®~!0.1011001
+ ®~71.0110110 + ®30.1101011),

Z = 2°. (@°1.1011011 + © '0.1011101
+ ®~71.0110110 + ©~71.0111001).

Since we are adding positive numbers, no control on the
accuracy is needed here, and the result is yielded as

f(X,Y,Z) = X% 4 YO 4 ZO = 2! .1.1011011,

with a relative error E ~ 1.1-27!9, as is expected in single
precision.

Example 2 Given the three real numbers defined in the pre-
vious example, we want now to evaluate f(x, y, —z) again
requiring an eight-bit accuracy in the result. Table 2 shows
the sequence of steps performed to achieve the desired result.

The computation in single precision, as in the previous
example, is described in step (a): it leads to a clear can-
cellation phenomenon and, once detected, the accuracy is
improved by letting the @~! terms enter into play (step (b)).
However, the relative error remains higher than the prescribed
tolerance, and accuracy needs to be improved by also con-
sidering the @®~* terms. The computation is then repeated at
step (c) and the correct result is finally achieved. Notice that,
in performing steps (b) and (c), one can evidently exploit the
work already carried out in the previous step. The overall

P. Amodio et al.

procedure thus requires 6 additions/subtractions of grossdig-
its, the same that would be needed by directly working with
a 24-bit register which, for this case, is the minimum accu-
racy requirement to obtain eight correct bit in the result. This
means that no extra effort is introduced during the steps. As a
further remark, we stress again that a parallelization through
the steps is also possible, even though we will not discuss
this issue here.

Example 3 We want to evaluate f(x, —y, —z) requiring an
eight-bit accurate result, now choosing

x = 2° - 1,0010101101010111111001001110110--- ,
y = 2° - 1,0010100010110010110101001101011--- ,
z = 277 - 1.0101000011011110010010110001010-.-.- .

Table 3 shows the sequence of steps performed to achieve
the desired result for this case.

When working in single precision, an accuracy improve-
ment is already needed when subtracting the first two terms
X® and Y and, consequently, step (a) is stopped. At step
(b), the difference x — y is evaluated in double precision
which, on balance, assures an eight-bit accuracy in the result.
However, a new cancellation issue emerges when subtract-
ing Z from X — Y), suggesting that the two terms
need to be represented more accurately. This is done in step
(c), evaluating x — y in triple precision and representing z
in double precision. The overall procedure requires 5 addi-
tions/subtractions of grossdigits. This example, compared
with the previous one, reveals the coexistence of variables
combined with different precisions.

Table 2 Avoiding cancellation issues when evaluating the function f(x, y, z) =x + y+ z for the input data in Example 2

Steps Error Action
(a) SO = x9 +4 yO = 29. @°1.1011011 <2-° Accept the result

SO ZO =0 1 Improve the accuracy
(b) SO = XY 4 yO = 29. @°1.1011011 + © !0.1011110) < 27/6 Accept the result

s® — ZO = 2°. ®~!0,0000001 = 2~! - ©°1.0000000 > 2? Improve the accuracy
(c) S® = xX 4+ y@ = 29. (@°1.1011011 + © '0.1011111 + ©~701100011) < 2716 Accept the result

(2) __ 72) — 90. (@-! ~2
S Z® = 29. (®~!0.0000001 + ©~71.0101101) ok Final result

= 2-15. (®°1.1010110 + ®!1.000000)

Table 3 Avoiding cancellation issues when evaluating the function f(x, y, z) =x + y+ z for the input data in Example 3

Steps
(a) s® := x — yO — 2-7. ©°1,0000000
(b) s® = xD — yD — 2-7. @°1.0101001

sY — Z© — 2-14. © 1,0000000

(c) S® := x@ — y2) = 2-7. (@°1.0101001 + ©—!0.1000100)
s2) — ZY) — 2-7. @~!1.1010101 = 27! - ©°1.1010101

Q) Springer

Error Action

2-7 Improve the accuracy
< 27° Accept the result
>27! Improve the accuracy
< 2770 Accept the result

< 2719 Final result
On the use of the Infinity Computer architecture to set up a dynamic precision floating-point arithmetic

Summarizing the three examples above, we observe how
the accuracy of representation and combination of variables
may be dynamically changed, in order to overcome possi-
ble loss of significant figures in the result when evaluating a
function. Of course, for this strategy to work, it is necessary
that the input data are stored with high precision and a tech-
nique to detect the loss of accuracy be available. In Sect. 4 we
will illustrate this procedure applied to the accurate determi-
nation of zeros of functions [a further example may be found
in Amodio et al. (2020)].

Concerning the computational complexity, it should be
noticed that Example | reflects the normal situation where the
use of the standard precision is enough to produce a correct
result, while Examples 2 and 3 highlight less frequent events.
Multiplication Set t = 3 and T = 2 (three grossdigits each
with four significant digits). Consider the product of the two
floating-point normalized numbers

X = 2°. 1.01101111100,
Y = 2°. 1.10111111101.

Table 4 summarizes the procedure by a sequence of com-
mented steps.

After expanding the input data along the negative pow-
ers of ® for data storage (step (a)), the convolution product
described in (6) is performed (step (b)). At step (c), the con-
tribution of each term is redistributed, and a sum is then
needed to update the mantissas (step (d)). Steps (e) and (f)
conclude the computation by normalizing and rounding the
result. Notice that step (e) may be carried out by applying the
rules for the addition described in Table 1. Again, we stress
that the terms in the convolution product, as well as in the
subsequent sum, may be computed in parallel.

Division The division of two floating-point numbers X and
Y has been switched to the multiplication of X by the recip-
rocal of Y. This latter, in turn, is obtained with the aid of

17595

the Newton—Raphson method applied to find the zero of the
function f(Z) = 1/Z — Y. Hereafter, without loss of gener-
ality, we assume Y > OQ. Starting from a suitable initial guess
Zo, the Newton iteration then reads

Ze41 = Ze + ZC — YZx). (13)

The relative error

_ 1/Y — Zr

Ex:
7 1/Y

=—-1-YZ,

satisfies
Exy1 =1—Y Zep) = 1—2Y Ze + (V Ze)? = EE, (14)

which means that, as is expected in presence of simple zeros,
the sequence Z; eventually converges quadratically to 1/Y,
and the number of correct figures doubles at each iteration.
This feature makes the division procedure extremely effi-
cient in our context, since the required accuracy may be
easily increased to an arbitrary level. In order to obtain such
a good convergence rate starting from the very beginning of
the sequence, the numerator X and denominator Y are scaled
by a suitable factor 6° so that Y:= B*Y lies in the interval
[0.5, 1]. In the literature, the minmax polynomial approxi-
mation is often used to estimate the reciprocal of Y (see, for
example, Habegger et al. (2010)). For the linear approximat-
ing polynomial, the resulting initial guess is

48 32~

Zo = — — ~2Y,
O77

which assures an initial error Eg < 1/17. Taking into account
the equality (14), the relative error at step k decreases as

gk
Qk 1

Table 4 Scheme of the multiplication of two floating-point numbers

@! @ @!
(a) Data acquisition 29 1.011 0.111
2° 1.101 1.111
(b) Convolution product 29 10.001111 100.0
(c) Redistribution 20 0.001 0.001 1.110
2° 0.010 0.000
2° 0.011
99
90
(d) Sum with redistribution 2° 0.001 0.100 0.001
(e) Normalization 2! 1.41010 0.000
2! 1.010 0.000

@-2 @-3 @-4 @-5
1.100
1.101

00000 110.010100 100.001111 10.011100
0.000
0.010 1.000
0.010 0.001 1.110

0.001 0.011 1.000

0.100 1.011 0.001 1.000
1.010 0.101 1.000 1.100
1.010

(f) Rounding

Q) Springer
17596

Table 5 Newton iteration to

0
compute the reciprocal of Sequence ®
_ 40.
Y = 2” - 1010 on the Infinity Zo 50 "ol
Computer
41 291.101
22 291.100
43 29 1.100
Z4 291.100

P. Amodio et al.

@7! @-? @-? @-4 @- @~6 @-/

0.000 0.000 0.000 0.000 0.000 0.000 0.000
0.100 1.101 0.100 1.101 0.100 1.101 0.100
1.100 0.111 1.100 0.010 1.011 1.101 1.100
1.100 1.100 1.100 1.010 1.101 0.000 1.110
1.100 1.100 1.100 1.100 1.100 1.100 1.101

and consequently, assuming $8 = 2, a g-bit accurate approx-
imation is obtained by setting

_ oe: oa |

log, 17
where [-| denotes the ceiling function. As an example, four
iterations suffice to get an approximation with at least 32
correct digits. Table 5 shows the sequence generated from
the scheme above applied to find, on the Infinity Computer,
the reciprocal of the binary number Y = (1010)2 (1/10 in

decimal base), under the choice t = 3 and T = 7 (eight
grossdigits each with four significant figures).

3.3 Implementation details

We have developed a MATLAB prototype emulating the
Infinity Computer environment interfaced with a module that
performs the suitable carrying, normalization and rounding
processes, needed to ensure proper functioning of the result-
ing dynamic floating-point arithmetic.

The emulator represents input real numbers using a set of
binary grossdigits, whose length and number are defined by
the two input parameters ¢ and 7. This latter parameter is
used to define the maximum available accuracy for storing
variables. In accord with formulae such as (5) and (6), the
actual accuracy used to execute a single operation will depend
on the accuracy of the two operands but cannot exceed T.

At the moment, the emulator implements the four basic
operations following the strategies described above, plus
some simple functions. The vectorization issue, to speed
up the execution time associated with each floating-point
operation, has not yet been addressed, so that all operations
between grossdigits are executed sequentially.

All computations reported in the present paper, including
the results presented in the next section, have been carried
out on an Intel 15 quad-core computer with 16GB of memory,
running MATLAB R2019b.

4 Anumerical illustration

As an application highlighting the potentialities of the
dynamic precision arithmetic introduced above, we consider

Q) Springer

the problem of determining accurate approximations of the
zeros of a function f : [a,b] — R, in the case where this
problem suffers from ill-conditioning issues.

The finite arithmetic representation of the function f
introduces perturbation terms of different nature: analytical
errors, errors in the coefficients or parameters involved in
the definition of the function, or roundoff errors introduced
during its evaluation.

From a theoretical point of view, these sources of errors
may be accounted for by introducing a perturbation function
g(x) and analyzing its effects on the zeros of the perturbed
function f (x) := f(x) + €g(x) where the factor ¢ has the
size of the unit roundoff. Under regularity assumptions on f,
if a € (a,b) is a zero with multiplicity d > 0, it turns out
that f(x) admits a perturbed zero a + 6a, with the perturbing
term da satisfying, in first approximation,

 

 

1/d
sa] x el/dgy| 8 (15)
| fO@)
As an example, consider the polynomial
p(x) =x — 5x? + 10x? — 10x? +. 5x —1 (16)

that admits ~ = | as unique root with multiplicity d = 5
(indeed p(x) = (x — 1)°). For this problem, from formula
(15) we get

da

a

= |Sa| ~ 6/4) ey /4, (17)

 

 

Working with 64-bit IEEE arithmetic, 1.e., with a roundoff
unit uw = 2—>», we expect a breakdown of the relative error
proportional to u!/> ~ 6.4- 1074 > 0.5 - 107%, so that,
assuming |g(1)|'/4 ~ 1, the approximation of the zero a
only contains 3 + 4 correct figures.

This is confirmed by the two plots in Fig. 1. They display
the relative error |x, —a|/|a| = |x, — 1| of the approxima-
tions to aw generated by applying the Newton method to the
problem p(x) = 0, choosing xo = 2 as initial guess:

P(Xxk)
p' (xk)

 

Xk+1 = Xk - (18)
On the use of the Infinity Computer architecture to set up a dynamic precision floating-point arithmetic

relative error

 

 

 

steps

Fig.1 Relative error related to the sequence of approximations gener-
ated by the Newton method applied to the polynomial (16). Solid line:
implementation on the Infinity Computer with t = 52 and T = 0.
Dashed line: implementation in MATLAB double precision arithmetic

 

10°
oH
5
oH
oH
oO
© 190°%°
3
©
—
oO
oH

=

oO
a
or

 

 

 

=

2.
NO
o

20 40 60 80 100 120 140 160 180 200
steps

Fig.2 Relative error corresponding to the sequence of approximations
generated by the Newton method applied to the polynomial (16) on
the Infinity Computer. Solid line: dynamic precision implementation.
Dashed line: fixed precision implementation, for different accuracies

The solid line refers to the implementation of the iteration on
the Infinity Computer using t = 52 and T = 0. This choice
mimics the default double precision arithmetic in MATLAB,
which uses a register of 64 bit to store a normalized binary
number, 52 bit being dedicated to the (fractional part of the)
mantissa. As a matter of fact, the dashed line, coming out
from the implementation of the scheme using the standard
MATLAB arithmetic, precisely overlaps with the solid line
as long as the error decreases, while the two lines slightly
depart from each other when they reach the saturation level
right below 10~, namely starting from step 32.

We want now to improve the accuracy of the approxi-
mation to the zero a = | of (16) by exploiting the new
computational platform. Hereafter, the 53-bit precision used
above will be referred to as single precision. The dashed lines
in Fig. 2 show the relative error reduction when the Newton
method is implemented on the Infinity Computer by work-
ing with multiple fixed precision. From top to bottom, we
can see the five saturation levels corresponding to the stag-
nation of the error at E; ~ 6.8 - 10~* in single precision,

17597

Ey © 3.7- 107’ in double precision, E3 ~ 2.0 - 107!° in
triple precision, E, ~ 1.5 - 107! in quadruple precision,
and E; ~ 6.7- 107!8 in quintuple precision. These satura-
tion values are consistently predicted by formula (17), after
replacing ¢ with 2~>**, fork = 1,...,5.

Now suppose we want 53 correct binary digits in the
approximation (1.e., about 15 + 16 correct decimal digits).
From the discussion above, it turns out that we have to activate
the quintuple precision, thus setting t = 52 and T = 4 (five
grossdigits, each consisting of a 53-bit register). However,
the computational effort may be significantly reduced if we
increase the accuracy by involving new negative grosspowers
only when they are really needed. In a dynamic usage of the
accuracy, starting from x9, we can initially activate the single
precision mode until we reach the first saturation level and,
thereafter, switch to double precision until the second satu-
ration level is reached, and so forth until we get the desired
accuracy in the approximation. Denoting by

Xk — Xk-1
Nk

err(k) =

 

 

the estimated error at step k, and by prec the current pre-
cision, initially set equal to 1, the points where an increase
of the accuracy is needed may be automatically detected by
employing a simple control scheme such as

if err (k)>=s*err(k-1)
prec=prect+l
end

and prec <=T

where s < | is a positive safety factor that we have set equal
to 1. Notice that we have here considered the estimated
(rather than the exact) relative error err(k) to reflect the more
general context, where the value of a is indeed unknown.

The solid line in Fig. 2 shows the corresponding reduc-
tion of the error and we can see that the change of precision
scheme described above works quite well for this example,
since all saturation levels are correctly detected and over-
come. At step 162 the error reaches its minimum value of
2.2-107!® and the iteration could be stopped by the standard
criterion err(k) < 1075 even though, for clarity, we have
generated additional points to reveal the last saturation level
corresponding to prec= 7 +1=5.

Now, let us compare the computational cost of the dynamic
implementation versus the fixed quintuple precision one,
considering that to reach the highest precision each mode
requires 162 Newton iterations (see Fig. 2). On the basis of
the formula reported right below (6), the dynamic implemen-
tation would take about 2.4 - 10° grossdigits multiplications
while the fixed quintuple precision implementation requires

Q) Springer
17598

P. Amodio et al.

Table6 The Horner method for evaluating p(x) in (16) at x = 2° - 1.0000000000000000000000000000000000000000000001000010

p=1 2° @?
p=p:x-5 —2!
p=p-x+10 2?

eee eo
oy 8 6

p=p-x-10 —2!

ee oe
NO —

p=p:x+5 27!

QS

|

v

es

N

Oo
eee Coe ee °e
LLeEL eau

 

9-230

©
o

p=p-l

2.0 - 10* grossdigits multiplications. It follows that the for-
mer mode would reduce the execution times of a factor at least
eight with respect to the latter. Actually, it does much better:
the dynamic usage of variables and operations, understood as
the ability of handling variables with different accuracy and
executing operations on them, makes the resulting arithmetic
definitely much more efficient than what emerged from the
comparison above.

In carrying out the computation above, for the dynamic
precision mode we have assumed that all floating-point oper-
ations were executed with the current selected precision. For
example, under this assumption, the computational effort per
step of the two modes would become equivalent starting from
step 139 onwards since, at that step, the dynamic mode acti-
vates the quintuple precision to overcome the threshold level
E4 in Fig. 2.

There is, however, one fundamental aspect that we have
not yet considered. In fact, to overcome the ill-conditioning
of the problem, the higher precision is only needed during
the evaluation of p(x,) and p’(x,x) in (18), while the single
53-bit precision is enough to handle the sequence x;. In other
words, to minimize the overall computational effort, we may
improve the accuracy only in the part of the code that imple-
ments the Horner rule to evaluate the polynomial p(x) and
its derivative.

Interestingly, we do not have to instruct the Infinity Com-
puter to switch between single and quintuple precision: all

> For simplicity, we do not consider additions/subtractions in the com-
putation, since their contribution would not alter the final result.

Q) Springer

1 .CODDDDDODDDDODDDDD00000000000000000000000000000000000
LLDUDDDLUD DDL D D100 000011001011111111111111111111011111
L.OLLLL12 00 101011011111111111111111111111111111111001110
1 .CODDDDDDDDDDODD00000000000000000000000000 1000 100000 10
L.LDUDDD00000011011111111111111111111111111111110011101
0.00000000000000000000000000000000000000 1000 10000001 11
1.12 01111111111111111111111111111101110011100111110000
L.LDUDDDD 00 0000001011011111111111111111111111101111100
0.0000000000000000000000000000000000000 100010000001 111
1.1111111111111111111111111111110111001110011111000000
0.00000000000000000000000 100 10000 1 1000 10000001 00000000
1 .CODDDDDODDDDODDDDD00000000000000000000000000000000000
0.0000000000000000000000000000000000000000000000000000
0.0000000000000000000000000000000000000000000000000000
0.0000000000000000000000000000000000000000000000000000
0.00000000000000000 100 1010101001010001010000 1000000000
1.001010101001010001010000 1000000000000000000000000000

is done automatically and naturally and, more importantly,
even during the evaluation of p(x;,) and p’(x;), the transi-
tion from single to quintuple precision is gradual, in that all
the intermediate precisions are actually involved only when
really needed, which makes the whole machinery much more
efficient.

To better elucidate this aspect, we illustrate the sequence
produced by the Horner rule to evaluate p(x;) at step
k = 145, where the quintuple precision is activated. The
first column in Table 6 reports the five steps of the Horner
method applied to evaluate the polynomial p(x) in (16)
at the floating-point single precision number x = x145
(its value is in the caption of the table). The variable p
is initialized with the leading coefficient of the polyno-
mial, but is allowed to store five grossdigits, each 53-bit
long, to host floating-point numbers up to quintuple preci-
sion. From the table we see that, as the iteration scheme
proceeds, new negative grosspowers appear in the values
taken by the variable p. More precisely, at step k the vari-
able p stores a k-fold precision floating-point number, for
k=1,...,5.

The increase in the precision of one unit at each step
evidently arises from the product p - x, since x remains
a single-precision variable and no rounding occurs. Let us
better examine what happens at the last step. The product
px generates a quintuple-precision number whose expan-
sion along negative grosspowers matches the number | up
to @?, Consequently, the last operation p — 1 only con-
tains significant digits in the coefficient of ®* so that,
after normalization, p will store again a single-precision
On the use of the Infinity Computer architecture to set up a dynamic precision floating-point arithmetic

number that can be consistently combined inside formula
(18).

In conclusion, the Horner procedure, though being enabled
to operate in quintuple precision, actually involves lower
precision numbers, except at the very last step. The five
steps reported in Table 6 require 15 multiplications of gross-
digits, with a clear saving of time, if we consider that the
fixed quintuple-precision mode would require 125 multi-
plications of grossdigits. Comparing the execution times in
MATLAB over 162 steps, we found out that the dynamic-
precision implementation is about 1.75 times slower than
the single-precision implementation (which however stag-
nates at level E;) and about 19 times faster than the
quintuple precision mode, thus confirming the expected effi-
ciency.

5 Conclusions

We have proposed a variable precision floating-point arith-
metic able to simultaneously storing numbers and execute
operations with different accuracies. This feature allows one
to dynamically change the accuracy during the execution of
a code, in order to prevent inherent ill-conditioning issues
associated with a given problem. In this context, the Infinity
Computer has been recognized as a natural computational
environment that can easily host such an arithmetic. The
assumption that makes this paradigm work is the mutual
replacement of the two symbols ® (grossone) and @ (dark
grossone) during the flow of a given computation. The latter,
defined as ® = f't!, is evidently a finite quantity for our
numeral system but, in many respects, its reciprocal behaves
as an infinitesimal-like entity in the numeral system induced
by a floating-point arithmetic operating with ¢ + | significant
figures. In the same spirit of the Infinity Computer, it turns
out that negative powers of @ may be used as “lenses” to
increase and decrease the accuracy when needed. An emu-
lator of this dynamic precision floating-point arithmetic has
been developed in MATLAB, and an application to the accu-
rate solution of (possibly ill-conditioned) scalar nonlinear
equations has been discussed.

Acknowledgements Open access funding provided by Universita degli
Studi di Bari Aldo Moro within the CRUI-CARE Agreement. This work
was funded by the INCAM-GNCS 2018 Research Project “Numerical
methods in optimization and ODEs” (the authors are members of the
INdAM Research group GNCS).

Compliance with ethical standards

Conflicts of interest The authors declare that they have no conflict of
interest.

17599

Human and animal rights This article does not contain any studies with
human participants or animals performed by any of the authors.

Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing, adap-
tation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indi-
cate if changes were made. The images or other third party material
in this article are included in the article’s Creative Commons licence,
unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copy-
right holder. To view a copy of this licence, visit http://creativecomm
ons.org/licenses/by/4.0/.

References

Amodio P, Iavernaro F, Mazzia F, Mukhametzhanov MS, Sergeyev YD
(2016) A generalized Taylor method of order three for the solution
of initial value problems in standard and infinity floating-point
arithmetic. Math. Comput Simul 141:24—-39

Amodio P, Brugnano L, Iavernaro F, Mazzia F (2020) A dynamic pre-
cision floating-point arithmetic based on the Infinity Computer
framework. Lect Notes Comput Sci 11974:289-297. https://doi.
org/10.1007/978-3-030-40616-5_22

Brugnano L, Mazzia F, Trigiante D (2011) Fifty years of stiffness.
Recent Advances in Computational and Applied Mathematics.
Springer, Dordrecht, pp 1-21. https://doi.org/10.1007/978-90-
481-9981-5_1

Carson E, Higham N (2018) Accelerating the solution of linear systems
by iterative refinement in three precisions. SIAM J Sci Comput
40(2):A817—A847

Cococcioni M, Cudazzo A, Pappalardo M, Sergeyev Y (2020) Solving
the lexicographic multi-objective mixed-integer linear program-
ming problem using branch-and-bound and grossone methodol-
ogy. Commun Nonlinear Sci Numer Simul 84:105177. https://doi.
org/10.1016/j.cnsns.2020.105177

D’Alotto L (2015) A classification of one-dimensional cellular
automata using infinite computations. App! Math Comput 255: 15—
24

De Cosmis S, Leone RD (2012) The use of grossone in mathemat-
ical programming and operations research. Appl] Math Comput
218(16):8029-8038

Dongarra J, Grigori L, Higham N (2020) Numerical algorithms for high-
performance computational science. Philos Trans R Soc A Math
Phys Eng Sci 378(2166):20190066

Falcone A, Garro A, Mukhametzhanov M, Sergeyev Y (2020) A
simulink-based infinity computer simulator and some applications.
Lect Notes Comput Sci 11974:362—369. https://doi.org/10.1007/
978-3-030-40616-5_31

Gaudioso M, Giallombardo G, Mukhametzhanov MS (2018) Numerical
infinitesimals in a variable metric method for convex nonsmooth
optimization. App] Math Comput 318:312—320

Habegger A, Jacomet M, Stahel A, Goette J (2010) An efficient hard-
ware implementation for a reciprocal unit. IEEE Computer Society,
Los Alamitos, pp 183-187

Iavernaro F, Mazzia F, Trigiante D (2006) Stability and conditioning in
numerical analysis. J Numer Anal Ind Appl Math 1(1):91-112

Iavernaro F, Mazzia F, Mukhametzhanov MS, Sergeyev YD (2019)
Conjugate-symplecticity properties of Euler-Maclaurin methods
and their implementation on the Infinity Computer. App] Numer
Math. https://doi.org/10.1016/j.apnum.2019.06.011

Q) Springer
17600

Iudin D, Sergeyev Y, Hayakawa M (2012) Interpretation of perco-
lation in terms of infinity computations. Appl Math Comput
218(16):8099-8111

Iudin D, Sergeyev Y, Hayakawa M (2015) Infinity computations in cel-
lular automaton forest-fire model. Commun Nonlinear Sci Numer
Simul 20(3):861—870

Lolli G (2015) Metamathematical investigations on the theory of
grossone. Appl Math Comput 255:3-14

Mazzia F, Sergeyev Y, Iavernaro F, Amodio P, Mukhametzhanov M
(2016) Numerical methods for solving ODEs on the Infinity Com-
puter. In: 2nd International conference on numerical computations:
theory and algorithms, NUMTA 2016, vol 1776, pp 090033 (2016)

Sergeyev YD (2010) Computer system for storing infinite, infinitesimal,
and finite quantities and executing arithmetical operations with
them. USA patent 7,860,914

Sergeyev Y (2008) A new applied approach for executing computations
with infinite and infinitesimal quantities. Informatica 19(4):567—
596

Sergeyev YD (2009) Numerical computations and mathematical mod-
elling with infinite and infinitesimal numbers. J App! Math Comput
29(1-2):177-195

Sergeyev Y (2011) Higher order numerical differentiation on the Infinity
Computer. Optim Lett 5(4):575-585

Sergeyev YD (2013) Solving ordinary differential equations by working
with infinitesimals numerically on the Infinity Computer. Appl
Math Comput 219(22):10668—1068 1

Q) Springer

P. Amodio et al.

Sergeyev Y (2017) Numerical infinities and infinitesimals: Method-
ology, applications, and repercussions on two Hilbert problems.
EMS Surv Math Sci 4(2):219-320

Sergeyev YD (2019) Independence of the grossone-based infinity
methodology from non-standard analysis and comments upon
logical fallacies in some texts asserting the opposite. Found Sci
24(1):153-170

Sergeyev YD, Mukhametzhanov MS, Mazzia F, Iavernaro F, Amodio
P (2016) Numerical methods for solving initial value problems on
the Infinity Computer. Int J Unconv Comput 12(1):3—23

Sergeyev Y, Kvasov D, Mukhametzhanov MS (2018) On strong homo-
geneity of a class of global optimization algorithms working with
infinite and infinitesimal scales. Commun Nonlinear Sci Numer
Simul 59:3 19-330

Vita M, De Bartolo S, Fallico C, Veltri M (2012) Usage of infinitesimals
in the Menger’s Sponge model of porosity. App! Math Comput
218(16):8187-8 196

Zilinskas A (2012) On strong homogeneity of two global optimization
algorithms based on statistical models of multimodal objective
functions. App] Math Comput 218(16):8131-—8136

Publisher’s Note Springer Nature remains neutral with regard to juris-
dictional claims in published maps and institutional affiliations.
