Measurement 165 (2020) 108116

 

Contents lists available at ScienceDirect

Measurement

     

ELSEVIER

journal homepage: www.elsevier.com/locate/measurement

 

 

Convolutional neural network architecture for beam instabilities
identification in Synchrotron Radiation Systems as an anomaly detection | Ss
problem

 

Michat Piekarski*”, Joanna Jaworek-Korjakowska>*, Adriana I. Wawrzyniak*, Marek Gorgon”

* National Synchrotron Radiation Centre Solaris, Jagiellonian University, ul. Czerwone Maki 98, 30-392 Krakow, Poland
> AGH University of Science and Technology, Department of Automatic Control and Robotics, Al. Adama Mickiewicza 30, 30-059 Krakow, Poland

 

ARTICLE INFO ABSTRACT

 

 

Article history:

Received 31 March 2020

Received in revised form 11 May 2020
Accepted 11 June 2020

Available online 23 June 2020

Solaris National Synchrotron Radiation Centre is a research facility that provides high quality synchrotron
light. To control such a complex system it is necessary to monitor signals from various devices and sub-
systems. Despite the high demand for solutions to monitor the operation of centres, little work has con-
centrated on automatic analysis and fault detection. Anomaly detection prevents from financial loss,
unplanned downtimes and in extreme cases cause damage. To address the problem a convolutional neu-
ral network (CNN) for fault detection in time series data has been proposed. The aim of the system is to
identify abnormal status of sensors in certain time steps. In this study, we deploy transfer learning by
examining pre-trained VGG-16, VGG-19, InceptionV3 and Xception CNN models with an adjusted
densely-connected classifiers. Our database contains 336 h of signals in total which have been divided
into 6300 time windows of 3 min length. The proposed solution, based on the VGG-16 architecture,
detects anomalies in diagnostics signals with 92% accuracy and 85.5% precision what is a state-of-the-
art result.

© 2020 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http: //

 

Keywords:

Anomaly detection

Signal analysis

Fault detection

Synchrotron radiation system
Convolutional neural network
Transfer learning

creativecommons.org/licenses/by/4.0/).

 

1. Introduction

The most important for all research institutes including Solaris
National Synchrotron Radiation Centre is to ensure proper and
stable working conditions for scientists and users. Due to the com-
plexity of the synchrotron control system and furthermore, the
physical phenomena which occurs in such infrastructures it is dif-
ficult to determine the reason of beam instabilities or its lost. Cur-
rently, just slow-changing anomalies are detected only by
experienced operators but even those faults are recognized far
too late. In particular in cases when there is no previous alarm or
interlock triggered.

Anomaly detection systems in such facilities are vital, as they
prevent from financial loss, unplanned downtimes, and ensure
measurement stability allowing scientists to conduct their experi-
ments successfully and achieve high quality results. Despite the
fact that this problem is so significant only few research groups
have proposed anomaly detection techniques for diagnostic data

* Corresponding author.
E-mail addresses: michal.piekarski@uj.edu.pl (M. Piekarski), jaworek@agh.edu.pl
J. Jaworek-Korjakowska), adriana.wawrzyniak@uj.edu.p!| (A.J. Wawrzyniak),
mago@agh.edu.pl (M. Gorgon).

https://doi.org/10.1016/j.measurement.2020.108116
0263-2241/© 2020 The Authors. Published by Elsevier Ltd.

in accelerators (see Section 1.3) |19,37,12,11], however deep learn-
ing methods or sophisticated machine learning solutions have not
been widely used yet, furthermore no detailed information about
Statistical results have been presented. To the best of our knowl-
edge this is the first paper describing the approach of deploying
deep neural networks with transfer learning to the detection of
anomalies in signals in a synchrotron facility.

Deep learning is currently one of the most popular branch of
artificial intelligence and contains a set of advanced and sophisti-
cated architectures which are widely used for image segmentation,
classification, signal analysis, and data investigation. Deep Neural
Networks (DNN) are state-of-the-art algorithms for challenges that
haven’t been solved yet [1]. Progress in the hardware, software,
architectures and availability of huge datasets allowed to employ
high accuracy image recognition systems. A systematic review on
the use of deep neural networks in anomaly detection of various
signals can be found in [27,28,32,7].

Therefore, in this paper we present a new and first approach to
the anomaly detection in Synchrotron Radiation Centres with data-
base preparation and specification, methodology description and
Statistical analysis. We examined the possibility to use pre-
trained convolutional neural network models including VGG-16,
VGG-19, InceptionV3, Xception, which are competition winning

This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
2 M. Piekarski et al. / Measurement 165 (2020) 108116

models combined with the CNN transfer learning from image clas-
sification to signal analysis domains | 35,9]. We firstly register, pre-
pare and extract the data, secondly due to the limited and
imbalanced data we generate synthetic samples employing the
Synthetic Minority Over-sampling Technique (SMOTE) [8]. Subse-
quently, we analyse the CNN models and choose the most accurate
pre-trained ConvNet architecture (VGG-16) which is a common
and highly effective approach to deep learning on small image
datasets [42]. The network architecture classifier is adapted to
the specification of our problem including the densely-connected
layer which is a fully connected neural network layer where every
input is connected to each output by a weight. We also deploy the
dropout layers as a regularization technique preventing overfitting
[10].

We wish to stress that the main aim of our research and imple-
mented system is to detect abnormal status of sensors and thereby
identify beam instability. The novelty of our proposed approach
can be summarized as:

e we propose a system to classify different kind of signals in
terms of anomaly occurrence in a given time window,
e the proposed system enables the detection of anomalies within
3 min while currently only some of the anomalies are detected
by the operator due to the experience and careful observation of
diagnostic signals, but even then it is often too late to react,
we present a CNN architecture with a determined pre-trained
model and fitted densely connected classifier for diagnostic sig-
nal classification, mainly anomaly detection,
our method proved sufficient effectiveness of the VGG-16
model for anomalies detection, which not only increased the
accuracy, but also reduced the detection time itself,
thanks to transfer learning method we were able to adapt pre-
trained models despite a limited and imbalanced amount of
data.

This paper is organized in five sections as follows. Section 1
Introduction presents Solaris synchrotron and covers background
information about importance of anomaly detection in machine
subsystems. Section 2 Vacuum subsystem equipment describes
equipment used to keep stable vacuum and gather pressure read-
ings. Section 3 Proposed beam stability analysis system for anomaly
detection shows in detail the methodology used in this research
including database preparation and specification, the transfer
learning method for pre-trained CNN architectures to detect
anomalies in vacuum signals, as well as training and classification

    
  

stages. Section 4 Results describes testing conditions as well as con-
ducted tests and results compared to related works. Section 5 Con-
clusions covers summary of research work and suggests possible
further study directions.

1.1. Synchrotron radiation source

Solaris is a third generation light source built at the Jagiellonian
University in Krakow, Poland [40] in close cooperation with MAX
IV Laboratory held by Lund University, Sweden [2]. Both facilities
have similar technical solutions according to injector, transfer line
and 1.5 GeV storage ring. The Solaris linear accelerator (linac) is the
first section in beam injection process (Fig. 1).

It is composed of Radio Frequency (RF) thermionic gun (with
BaO cathode as electrons source), six accelerating sections operat-
ing at the maximum energy of 600 MeV and the dog-leg vertical
transfer line. The RF power to the linac is supplied by 4 RF stations.
Quadrupole magnets are placed to focus the beam. The last four are
used as a matching cell to the transfer line. Detailed description of
the Solaris linac design can be found in [39]. The transfer line is
designed to be symmetric with the total bend angle of 27 degrees
and is ended by the Lambertson DC septum magnet which con-
nects the injector with the storage ring (Fig. 2).

The heart of the Solaris facility is a storage ring with a diameter
of 96 meters and 1.5 GeV maximum energy (Fig. 3).

Electrons accelerated in linac are injected into storage ring at
the energy of 536 MeV and further accelerated to the energy of
1.5 GeV. They travel at 99.999% of the light speed on a circular orbit
controlled by bending magnets, that are the actual sources of syn-
chrotron radiation [18]. Solaris storage ring consists of 12 electro-
magnet blocks called DBA (Double-Bend Achromat) cells. One DBA
contains a set of magnets that focus the beam, bend its trajectory,
and are used in orbit correction. Due to energy loss electron beam
can circulate in the storage ring for limited number of hours.
Therefore it is essential to design the storage ring with long life-
time. The synchrotron radiation, which is very bright, polarized
and covers the entire electromagnetic spectrum from the infrared
region through the visible, ultraviolet up to the X-rays [17], is pro-
duced from bending magnets or insertion devices installed in the
straight sections in Solaris storage ring.

1.2. Beam stability

The synchrotron radiation, which is characterized by the high
brightness, polarization and wide electromagnetic spectrum from

Transfer line

Accelerating sections

Fig. 1. The Solaris linear accelerator (linac) is composed of thermionic gun six accelerating sections and the dog-leg vertical transfer line.
M. Piekarski et al. / Measurement 165 (2020) 108116 3

 

. septum magnet
dipole magnet

 

Fig. 2. Transfer line in Solaris is optically mirror symmetric and provides beam deflection in the vertical plane at an angle of 27°.

 

Fig. 3. Storage ring in Solaris with a diameter of 96 meters and 1.5 GeV maximum
energy. Its construction and applied technologies are similar to those used in MAX
IV.

the infrared region through the visible, ultraviolet up to the X-rays
results in synchrotron light with high brightness. The most impor-
tant goal of the Solaris synchrotron is to provide a stable electron
beam of high quality. To guarantee users the highest quality of
experiments, it is necessary to properly tune, control and monitor
many parameters. Furthermore, only stable work of all advanced
and sensorized subsystems, which have been shortly stated below,
can result in a high beam stability. The most significant include:

1. magnets subsystem: responsible for the electron beam focusing
and keeping at the design orbit,

2. RF (radio frequency) subsystem: used for beam energy ramping,
electrons bunches creation and energy loss compensation,

3. vacuum subsystem: necessary to maintain stable ultra-high vac-
uum (UHV) environment,

4. cooling subsystem: important for keeping stable temperature
conditions appropriate for each section,

5. diagnostics subsystem: used to monitor beam parameteres, i.e.
current, tune, emittance, position, etc. To control the position
mainly beam position monitors (BPMs) and correctors are used
for the slow orbit correction feedback.

Any anomalies in these systems are quickly transferred to the
deterioration of the beam quality, hence their early detection can
significantly reduce unwanted side effects. Primarily, pressure
readings, temperature and voltage signals have been analysed as
a possible input to CNN anomaly detection system. Finally, based
on the expertise of operators the vacuum subsystem has been
selected to study dynamics and impact on the discussed issue. Fur-
thermore, operators confirm that the signal readings observation
provides additional information on the current state of the entire
system and may highly indicate possible unwanted events.

1.3. Related works

Artificial intelligence techniques including machine learning
and deep neural networks have become state-of-the-art solutions
for anomaly detection tasks which are one of the most challenging
in data analysis. Machine learning algorithms have a wide variety
of methods and algorithms to solve the task of anomaly detection
both for supervised and unsupervised tasks. The most common
supervised methods include k-nearest neighbors, supervised neu-
ral networks, support vector machine, Bayesian networks and deci-
sion trees. Worth mentioning examples are the isolation of outliers
by the Isolation Forests method or assigning to each sample a
degree of being an outlier by the Local Outlier Factor (LOF) algo-
rithm, Generative Adversarial Active Learning (GAAL), Extreme
Gradient Boosting Outlier Detection (XGBOD) as well as Locally
Selective Combination in Parallel Outlier Ensembles (LSCP) meth-
ods [21,5,22,43,44]. An interesting solution is also the use of
DBSCAN clustering algorithm to find the outliers. Some machine
learning techniques for multivariate outlier detection were also
widely applied to detect anomalies in multivariate time series,
although they treat data points independently and neglect tempo-
ral relationships [5,41 |. In recent years many researchers have pro-
ven that DNN and especially CNN can serve as better methods for
outlier detection. A comprehensive overview of research methods
in deep learning-based anomaly detection can be found in [6].
The most commonly used deep architectures for anomaly detec-
tion include Generative Adversarial Networks (GAN), Autoen-
coders, Support Vector Machines (SVM), Long Short Term
Memory Networks (LSTM), Spatial Transformer Networks (STN),
Convolution Neural Networks (CNN) and Recurrent Neural Net-
works (RNN) [6]. The implementation of sophisticated solutions
is also observed in the accelerator field where new methods are
developed, validated and deployed to solve existing problems
regarding the operation of radiation systems. Classic neural net-
works have been proposed to correct the beam orbit in the Aus-
tralian Synchrotron [24]. This model consists of two neural
networks that have been trained on archived data of beam position
in the storage ring in order to determine the appropriate correction
to minimize the proposed cost function. To the best of our knowl-
edge, only one work that has been published so far is dealing with
the problem of anomaly detection in the accelerator control sys-
tems. This topic has been raised at the ICALEPCS Conference in
2017 by the European Council for Nuclear Research (CERN) [37].

The Authors proposed three different mathematical approaches
that have been designed and developed to detect anomalies. Those
methods are unsupervised detection systems for finding anomalies
in multivariate data signals. The proposed methodologies involved
4 M. Piekarski et al. / Measurement 165 (2020) 108116

sensors values monitoring over time to build model of normal
operation. Any deviations from it were considered as anomalies.
Proposed methods are classic, unsupervised machine learning
approaches, including: sensors correlation and conditional nearest
neighbours classification, stochastic clustering and finally anomaly
detection based on experts’ knowledge (mathematical conditional
equations). The Authors were able to identify errors initially not
foreseen by the expert but there is no detailed information about
statistical results [37].

Since there is a high demand on a precise and reliable anomaly
detection system we propose to deploy the latest architectures of
convolutional neural networks additionally with transfer learning
to perform an accurate detection of faults. In very similar research
issues from the field of medicine it has been proven that deep
learning approaches are far more accurate and effective than the
widely known and used signal processing methods combined with
machine learning algorithms [27,28,32,38,31,34,6].

2. Vacuum subsystem equipment
2.1. Technical background

Vacuum subsystem plays crucial role in all accelerator facilities.
Its main task is to keep stable ultra-high vacuum conditions in the
beam pipes. In Solaris synchrotron three different kind of vacuum
pumps are deployed: Non Evaporable Getter (NEG) strips ST707
(Zr-V-Fe), differential diode Sputter lon Pumps (SIP) and Titanium
Sublimation Pumps (TSP) which have been presented in Fig. 4 [40].
The way that sputter ion pumps work is the process of removing
gases from the vacuum chamber by creating high magnetic field,
ionizing gas molecules and capturing gas ions.

Titanium Sublimation Pump is another type of vacuum pump
used to remove residual gas in ultra high vacuum systems. It con-
sists of a titanium filament through which a high current (typically
around 40 amps) is passed (Fig. 4b). This current causes the fila-
ment to reach the sublimation temperature of titanium. As a result,
the chamber walls become coated with a thin layer of clean tita-
nium. Since clean titanium is very reactive, components of the
residual gas in the chamber are likely to react and to form a stable,
solid product and the gas pressure in the chamber is reduced. NEG

 

 

(b)

strips are used together with pump in a chamber to improve the
performance by sorbing gas molecules that remain within a partial
vacuum. They are mainly applied to spaces that are narrow and
hard to pump out, which makes it very useful in storage rings
[3]. In Solaris storage ring there are installed in total 65 SIPs, 24
NEG strips and 12 TSPs [23].

2.2. Pumps controllers

For each type of pump an appropriate controller model is used
to control and measure the essential parameters. For Sputter Ion
Pumps there are SPC (Small Pump Controller) and MPC (Multiple
Pump Controller) models installed whereas Titanium Sublimation
Pumps use MPC or LPC (Large Pump Controller) models (Fig. 5).

All types of controllers allow user not only to set a wide variety
of parameters (see Table 1) but also to read actual pressure in the
corresponding chamber. There is a wide variety of SIP subtypes and
sizes which have been installed in the Solaris systems. However,
their capabilities for obtaining pressure are similar and in the

range from 10° mbar as a starting point to less than 10°-'° mbar
as a UHV goal.

In this research, pressure readings from 64 SIPs located around
the storage ring have been selected for further analysis as an input
for CNN based anomaly detection system.

3. Proposed beam stability analysis system for anomaly
detection

In this section we describe in detail the database preparation,
most popular and deployed pre-trained architectures with transfer
learning method as well as the training parameters and evaluation
process. At the very end we present the CNN layers outcomes as a
visualization of the trained network. Fig. 6 presents the main parts
and steps of the proposed solution. The pre-processing step
includes the pressure registration, data augmentation and image
database generation, afterwards we implement the dedicated clas-
sifier and connect it with different pre-trained CNN architectures.
At the end, we adjust the hyperparameters, train and validate the
model to find the best solution for our classification task.

 

Fig. 4. We present two types of vacuum pumps used in Solaris. The proper work of vacuum pumps is decisive to keep ultra-high vacuum stable which is vital for all
accelerator facilities: a) Titanium Sublimation Pump (filament) mounted inside a vacuum chamber, b) Titanium filament of a TSP is placed in a special, long chamber which is

presented in a) [13], c) Sputter Jon Pump mounted on one of the Solaris’ beamlines.
M. Piekarski et al. / Measurement 165 (2020) 108116 5

e ‘Oe ons
(— eo a See @

peel

 

Fig. 5. Presentation of vacuum pumps controllers used in Solaris. Their task is to set and maintain given pressure as well as to read actual one in chambers: a) Multiple Pump
Controller is able to control more than one vacuum pump. It is mostly handled in places where two pumps are needed or where additionally TSP has been used, b) Large Pump
Controller is used to control the biggest pumps where highest accuracy of ultra high vacuum is enforced, c) Small Pump Controller is a basic vacuum pump controller and

handles at the vast majority of all vacuum pumps.

Most important parameters and readouts used in the process of UHV creation | 14].

Outputs

Maximum power[W] Resolution [nA] Readouts

—7000, —3000, 3000, 7000 1

Table 1
Controller Output current [mA] Output voltage [VDC]
SPC 0-50
LPC 100 —7000, —5600, 5600, 7000 1
MPC 100 or 500

50 1 pressure, voltage, current,
200 programmable options
1000 1 or 100

—7000, —5600, 5600, 7000 2

3.1. The transfer learning phenomenon in deep neural networks

Deep neural network models require not only computational
resources but also huge amount of training, validation and testing
data in order to tune all parameters well which is the main limita-
tion in building deep networks from scratch Moreover, sometimes
an experiment cannot be repeated or data gathering can be danger-
ous or very expensive so often scientists end up with a limited
amount of data. While observing the ability of DNN to generalize
on big datasets one of the possibilities is to use transfer learning.
Its main idea is to share base model of a neural network, build only
classifier on top of it and by this architecture solve similar issue
[35].

There are several transfer learning strategies which have been
presented in Fig. 7, where the two most popular include [25]:

e Feature Extraction: quite straightforward strategy, where pre-
trained base model is used with trained classifier on top. All lay-
ers except for the classifier are frozen and do not change

weights during training process. It is usually applied in cases
where the amount of training data is most limited. Feature
extraction method is quite fast and shows very good
effectiveness.

e Fine-tuning: strategy applied in cases where the number of data
is big but still too small to train deep model from scratch. The
main idea is to unlock some of the layers of base model and
train both new classifier on top and those few layers (original
weights are used as a starting point). This method can improve
models’ performance by tuning architecture to its new task.

The decision of choosing the transfer-learning strategy has been
done based on the amount of data and data similarity between
ImageNet and our dataset. In our case where dataset is imbalanced
and still limited we have decided to use the first strategy. The con-
volutional base has been frozen, which means the weights
remained the same in their original form and served as a feature
extraction mechanism. Only the implemented classifier has been
trained and adjusted on the new database. Taking into account that
6 M. Piekarski et al. / Measurement 165 (2020) 108116

Flow-chart of the anomaly detection system

   

Pressure readings

Pre-trained CNN architectures

INPUT

Sh
N
>
&
Oo
O

Conv 5-1
Conv 5-2
Conv 5-3
OUTPUT

N
>
c
9

O

Conv 1-1
Conv 1-2
Dense-ReLU

 

VGG-16

Filter
concatenation

1x1 convolutions 5x5 convolutions 1x1 convolutions

3x3 convolutions

3x3 max poling

Previous layer

InceptionV3 module

INPUT

 

Anomaly detection model Anomaly detection

Conv 2-1
Conv 2-2
Conv 5-1
Conv 5-2
Conv 5-3
OUTPUT

nv =
' oO
> e
9 2
Oo o
a

  

VGG-19

INPUT
First layers

Module 3
Module n
CLASSIFICATION

InceptionV3 architecture

Output channels

Xception module

 

Xception architecture

Fig. 6. Anomaly detection diagram for beam faults including following steps: data processing, data augmentation, anomaly detection model selection from the following
VGG-16, VGG-19, Xception, InceptionV3, on the basis of the training process of the classification layer, and validation phase.

 

 

 

 

 

 

 

 

 

 

 

 

 

a oe ad
ay Pretrained
2 convolutional
: 2 base
¥ Pretrained 5 Pretrained 3 oe
5 convolutional S convolutional 3 -
® base % base E Fine-tuned
os = > convolutional
2 > ¢ base
£ = FREEZE a TRAIN
Bs 8 Be & eS
alt
Pretrained g New classifier New FC layers
FC layers i TRAIN TRAIN
we we -o
Output Output
(a) (b) (c)

Fig. 7. Diagrams of transfer learning strategies including: a) both base and classifier
use frozen, unchanged weights for the new classification problem, b) a dedicated
classifier is proposed and trained on the new data examples, c) part of the base has
unfrozen weights as well as a dedicated classifier are trained on the new dataset.

the pre-trained model has been well trained on a huge database we
kept a small learning rate (only for the classifier) to ensure that we
don’t distort the CNN weights too much.

3.2. Deep neural network architectures including VGG-16, VGG-19,
Xception, InceptionV3

The proposed deep neural network system consists of two parts
namely, the basis which is an already trained architecture with
fixed weights, and an implemented classifier added on top of the
basis. Based on the phenomen that for traditional machine learning
algorithms performance grows according to a power law and then
reaches a plateau, while deep learning performance scales with
increasing data size, we have investigated few pre-trained models
including VGG-16, VGG-19, Xception, and InceptionV3 [10,9]. The
specified architectures have been trained on one of the largest
visual databases called ImageNet, which currently has 14,197,122
images from 21841 different categories [30]. The database is
widely used in visual object recognition and computer vision.
M. Piekarski et al. / Measurement 165 (2020) 108116 7

In 2014 researchers from Visual Geometry Group (University of
Oxford) presented two similar network architectures: VGG-16 and
VGG-19 and took the first and second places in the localisation and
classification tracks in the ImageNet challenge, respectively [33].
The novelty of these models was based on a combination of small
convolution filters (3 x 3) which had the same effective receptive
field as the bigger once and furthermore, introduced more non-
linearities which in combination with the power of the GPU clus-
ter, allowed to increase the depth of the network up to 16 and
19 layers. Nowadays, VGG models are considered to be one of
the best for transfer learning in image recognition tasks because
of its simple architecture and high generalization ability. The
VGG-16 model has roughly 134 million parameters and contains
16 trainable layers including convolutional as well as fully con-
nected, max pooling, and dropout layers. The VGG-19 version has
144 million parameters and 19 trainable layers. The architectures
have been presented in Fig. 6.

In 2014 Szegedy et al. has introduced GoogleLeNet (also called
Inception) which is a deeper network with computational effi-
ciency. The Authors have introduced a Inception module which is
a good local network topology consisting of multiple receptive field
sizes for convolution, pooling operation and at the end the module
concatenates all filter outputs. The novelty of this network (Incep-
tionV3) lies not only in the module but also in the 1 x 1 convolu-
tion which helps reducing the feature-map dimension and global
average pooling [20]. As a result, the main idea behind the Incep-
tion model is to connect several layers parallelly in a kind of block
instead of stacking up one on another (Fig. 6). The main goal (and
the most obvious way to improve the network’s performance) was
to increase its size, both in depth and width. However this simple
solution came with major drawbacks including a risk of overfitting
or, more importantly, dramatic increase in computational
resources needed. It was assumed that a network utilizing such
an approach will choose the most useful layers rising its weights,
while decreasing useless layers at the same time (based on the
Hebbian principle) [36].

Extreme Inception (Xception) has been introduced by Chollet
et al. in 2016 who has studied different versions of Inception -
based models, like GoogLeNet, InceptionV1, InceptionV3, and
Inception-ResNet in order to increase the performance not by
capacity but rather by more efficient use of model parameters
[9]. Novelty of this approach was to introduce depthwise separable
convolution layers to the underlying Inception model (Fig. 6).
Separable convolution in deep learning frameworks, consists in a
depthwise convolution, i.e. a spatial convolution performed inde-
pendently over each channel of an input [9]. The Xception architec-
ture has 36 convolutional layers grouped in 14 modules whereas
data flow consists of three steps: entry flow, middle flow (repeated
8 times) and finally exit flow. It is robust, stronger than the Incep-
tion module, and can operate correlations of cross-channels and
spatial relations with maps fully decoupled.

While VGG-16 and VGG-19 models have similar architectures
that differ only in the number of layers, introducing InceptionV3
and its extreme version (Xception) allowed to increase the number
of layers in CNNs. Basing on the differences in the architecture of
the pre-trained models it is not possible to state in advance which
of them will perform best for a particular task without completing
experiments.

In the proposed system, we have implemented the classifier
specifically for the binary classification problem: distinguishing
between anomaly and correct signal. The classifier consists of three
layers. The first one is the Fully-Connected (FC) layer with the recti-
fied linear unit (ReLU) activation function (f(x) ), which is currently
the most popular nonlinear activation function defined as the posi-
tive part of its argument where x is the input to a neuron [15]:

f(x) = max(0,x) (1)

The FC layer is followed by the dropout layer which randomly
deactivates neurons during the training phase. It forces the net-
work to be redundant and helps to alleviate the overfitting prob-
lem. Finally, the classifier is completed with the FC output layer
and the Softmax activation function for classification purpose.
For each of the outputs we receive the probability whether the
input image is an anomaly or a normal signal.

3.3. Database preparation and specification

Data preparation is one of the most important and crucial steps
in artificial intelligence and moreover, the accuracy of the pro-
posed CNN architecture mostly depends on the data quality. As
we are facing the problem of limited and imbalanced data as well
as specific format of the input data to the CNNs we have to under-
take following steps to guarantee the correctness of the dataset:
data standarization, preparation of the signal images which is fol-
lowed by the generation of synthetic samples employing the Syn-
thetic Minority Over-sampling Technique (SMOTE) [8,10].

Our initial database contains 336 h of signals in total that have
been divided into 6300 time windows of 3 min length which has
been chosen on the basis of our expertise. The signals have been
extracted from the central, SQL-based archiving system of the
Solaris synchrotron, which periodically stores readings from all rel-
evant subsystems. The time windows have been labelled into two
classes containing the following number of examples: 5771 correct
and 529 anomaly signal time windows (Fig. 9a-c, Fig. 9d-f, respec-
tively). Based on the recommendations from operators and scien-
tist the classification task has been reduced to a_ binary
classification issue. As the most needed the engineers have indi-
cated an anomaly signaling system. However, in the future we pro-
pose to extend this implementation by detecting different
anomalies and specifying their origin. All the time window images
have been assessed manually in such a way that as soon as the
anomalous disturbance was visible in the window, it was classified
as positive.

As we can observe the number of examples is highly uneven
which may cause to classify all examples into one class achieving
high statistical scores which is the main problem of anomaly
detection systems likewise highly imbalanced multi-class classifi-
cation problems. In SMOTE method, new examples are created
by averaging a few representatives of the same minority classes
which are nearest neighbors in the feature space. We have per-
formed the upsampling by adding generated images to the under-
represented class of anomalies to make the quantity of each class
equal [8]. In both sets, the classes have been balanced receiving
5771 samples, for each class respectively.

In the first stage of database generation the aggregation of 64
consecutive pressure signals from 64 vacuum pumps which are
located around the storage ring have been registered. Signals are
read from devices every 3 s and saved, which means that one win-
dow is about three minutes long. As CNN architectures have been
designed to analyse images the obtained signals have been stored
as square windows and serve as the input data for the proposed
model. In the second stage, we had to standardize our measure-
ments as samples were collected from different storage ring seg-
ments. We have performed the data standardization to have a
mean of zero and a standard deviation of one with the z-score
method | 16]:

X—X
ca 2)
where, xX is the mean of the sample, and S is the standard deviation
of the sample. Finally, windows have been transformed in such a
8 M. Piekarski et al. / Measurement 165 (2020) 108116

way that three layers were created by duplication, obtaining images
of 64 x 64 x 3 dimension. All input data has also been subjected to
pre-processing appropriate for each of the tested architectures
(Fig. 8).

Finally, the dataset has been divided into two parts containing
9898 training samples and 1644 test samples, where the data are
balanced and each class is equally represented. In addition, for
the purposes of optimizing model parameters the validation set
was created by randomly choosing 20% of samples from training
dataset. As they come from entirely various periods of time we
ensure the reliability of the results obtained in tests.

3.4. Training and validation stages

One of the the toughest challenges in the correct implementa-
tion and deployment of deep learning solutions is model optimiza-
tion. It has been widely observed that hyperparameters are one of

Pressure [mbar]

the most critical components which determine the effectiveness of
the solution [26]. Hyperparameters are variables that determine
the network structure and need to be set before training. Hyperpa-
rameters presented in Table 2 have been chosen experimentally
and set before training.

As we are dealing with two classes classification problem the
binary cross-entropy loss function has been applied [29]. It consists
of Softmax activation which is applied before the cross-entropy
loss that calculates the error rate between the predicted value
and the original value. The Binary Cross-Entropy (CE) is given by:

CE =~ “tylog (f(s) 3)

where f; is the ground truth and s; is the output score for each class,
wheres f(s;) refers to the Softmax activation function.
For a given class s;, the Softmax function is calculated as [4]:

 

(b)

Fig. 8. Comparison between raw image and its pre-processed form which is used to generate the feature maps and heatmaps presented in the following figures: a) example of
an input image with an anomaly showing the impact of beam instability in vacuum signals, b) pre-processed image which is used as the input data to the proposed deep

learning network.

 

 

 

 

 

w

 

Pressure [mbar]
4
Pressure [mbar]

 

N

Pressure [mbar]

 

 

-
0

23:32:00 23:32:30 23:33:00 23:33:30 23:34:00 23:34:30 23:35:00
Time

(a)

wu a ~

Pressure [mbar]
Pressure [mbar]
>

w

 

0
06:41:00 06:41:30 06:42:00 06:42:30 06:43:00 06:43:30
Time

(d)

 

07:32:00 07:32:30 07:33:00 07:33:30 07:34:00 07:34:30 07:35:00

Time

(b)

 

06:31:30 06:32:00 06:32:30 06:33:00 06:33:30 06:34:00
Time

(e)

 

07:58:00 07:58:30 07:59:00 07:59:30 08:00:00 08:00:30
Time

(c)

le—7

eS
wu
4

Pressure [mbar]

=
°o
4

Ss
wu

 

2
°o

 

06:54:00 06:54:30 06:55:00 06:55:30 06:56:00 06:56:30
Time

(f)

Fig. 9. Signal time windows examples. Each one contains 64 samples of signal which is about three minutes of operation. Figures a-c are examples of correct signal whereas

d-f contain or predict forthcoming anomalies.
M. Piekarski et al. / Measurement 165 (2020) 108116

Table 2
Overview of the hyperparameters settings for the proposed CNN models.
Parameters InceptionV3
Number of layers 48
Input shape 128, 128, 3
Dropout rate 0.6
Epochs 1
Loss function
Metrics
Optimizer
e°i

 

f(s); (4)

~ ¢
se
J

where s; are the scores inferred by the net for each class in C. The
ground truth vector t is a one-hot vector with a positive class and
C — 1 negative classes (zeros), SO we can write:

e’p

C (3)

de
j

where S, is the score for the positive class.

For each model, we have chosen the Adam optimization algo-
rithm that is an extension of the stochastic gradient descent
(SGD). The optimizer is responsible for finding the optimum

 

CE = —log

input_1

(a)

block2_conv2

 

(c)

 

Xception VGG-16 VGG-19

71 16 19
128, 128, 3 64, 64, 3 64, 64, 3

0.6 0.6 0.5

1 2 2
Categorical cross-entropy
Accuracy
Adam

weights, minimizing error and maximizing accuracy while updat-
ing the weights of the neurons via backpropagation. It calculates
the derivative of the loss function with respect to each weight
and subtracts it from the weight. Adam optimization technique is
computationally efficient, invariant to diagonal rescale of the gra-
dient, and appropriate for problems with very noisy gradients.

3.5. Visualization of the convolutional layers of the anomaly detection
system

To better interpet and evaluate the CNN model architectures,
we visualized the intermediate convolutional layers outputs (inter-
mediate activations), filters and heatmap for the input image
shown in Fig. 8. This helped us to better understand how to extract
and present the learning representations as well as confirm the
rightness of the architecture. As shown in Fig. 10 three convolu-
tional layers have been chosen to present the changes in the inter-

block1_conv1

 

(b)

block3_conv1

 

(d)

Fig. 10. Visualization of the intermediate activation maps for the convolutional layers: a) activation output for the input data, b) intermediate activation for the first
convolutional layer, c) intermediate activation for the second convolutional layer, d) intermediate activation for the third convolutional layer.
10 M. Piekarski et al. / Measurement 165 (2020) 108116

 

(a)

(c)

Fig. 11. Visualization of the filters for the first convolutional layers and the heatmap used for the classification stage: a) filters from the first convolutional layer in the first
convolutional block of the network. Most of the filters have encoded some type of direction or color, b) filters from the first convolutional layer in the second convolutional
block of the network. Visualizations have become more complex, c) heatmap indicating which part of the image is used for the classification process.

mediate activations (feature maps). The Intermediate activation for
the first convolutional layer with 64 channels identified the loca-
tion of each signal and shows straight lines. In the following layers
(128 and 256) we observe various edge detectors and pattern
recognition, respectively. As the depth of the layers increased,
the activations became abstract and less interpretable.

In Fig. 11 we can observe what maximizes each filter in each
layer, giving us a precise visualization of the convnet’s modular-
hierarchical decomposition of its visual space. In the first layer
the filters basically encode direction (horizontal and vertical) as
well as color. In following blocks we can observe that these direc-
tion and color filters get combined into basic grid, textures, and
finally complex patterns. At the end (Fig. 11c) we present the heat-
map that indicates which part of the image are used for the classi-
fication process.

4. Results
4.1. Statistical analysis

The analysis of proposed techniques of anomaly detection was
carried out for diagnostic signals from ultra-high vacuum subsys-
tem. Time windows have been classified into anomaly and non-
anomaly classes by four different base models namely InceptionV3,
Xception, VGG-16 and VGG-19 with identical, proposed and imple-
mented classifier.

In Fig. 12 we have presented the training and validation accu-
racy as well as loss for each epoch, respectively. The presented
plots confirm that the model has the ability to converge and gen-
eralize the dataset. Training loss decreases with each epoch
(Fig. 12c) and the training accuracy increases which each epoch
(Fig. 12a), but this is not the case for validation set (Fig. 12d and
b, respectively). It indicates whether to stop training at certain step
in order to prevent model from overfitting which may result in
obtaining increasingly worse performance on testing data. Further-
more, to evaluate the obtained results for each of the proposed
models several metrics have been introduced. In Table 4 we pre-
sent the analysis of differences in classification between the classi-
fication carried out by the classifier and the actual classification
(ground truth), summarized in a confusion matrix for each of the
evaluated models. In this matrix, each row refers to actual classes,
and each column refers to classes as predicted by the classifier. It is
a matrix where the rows correspond to the correct decision classes
and the columns to those predicted by the classifier. In confusion

matrix we specify: TP (true positive), FN (false negative), TN (true
negative), FP (false positive) values.

To measure the diagnostic performance we use a pair of statis-
tical parameters including accuracy (ACC), precision (PPV, positive
predicted value), sensitivity (SE, recall), and specificity (SPC, true
negative ratio).

Accuracy which measures the statistical bias and systematic
error refers to closeness of the measurements to a specific value
and can be expressed as:

Acc= iP +IN (6)
TP +FN+FP+ TN

Precision refers to random errors, and is a measure of statistical

variability describes the closeness of the measurements to each

other and can be written as:

TP
~ TP + FP (7)

Sensitivity measures the proportion of actual positives that are
correctly identified as such is defined as:

TP
~ TP + FN (8)

Specificity measures the proportion of actual negatives that are
correctly identified as such and is calculated as:

TN
~ TN + FP (9)

Fl-score (also F-score) considers both the precision and the
recall of the test to compute the final score and is a measure of a
test’s accuracy. The F-score can be expressed as:

_ 2-PPV-SE
1 "PPV +SE

From results presented in Table 3 we can conclude that all mod-
els were able to recognize anomalies in diagnostic signals with
high accuracy. The best results for both accuracy and sensitivity
metrics have been obtained by VGG-16 model with the adjusted
classifier. Despite the fact that precision of the model was not
the highest among other discussed architectures, its ability to
detect positive time windows (sensitivity measure) resulted in
the highest final score. It is also worth mentioning that not detect-
ing anomalies may result in downtimes or even failures, further-
more cause damage to the infrastructure. The sensitivity of the
system is much more important than specificity in a balanced
way. From this point of view, also VGG-16 achieved the best results

PPV

SE

SPC

(10)
M. Piekarski et al. / Measurement 165 (2020) 108116 11

Model accuracy

0.96

 

 

0.94
~
=
= 0.92
UU
Vv
©
mo
£ 0.90
Cc
©
Kb
0.88
InceptionV3
0.86 Xception
VGG19
VGG16
T T T T T
0 2 + 6 8
Epoch
(a)
Model loss
0.9
InceptionV3
0.8 Xception
VGG19
0.7 VGG16
0.6
Ww
w
2
> 0.5
Cc
c
id
a 0.4
0.3
0.2
0.1

 

(c)

Model accuracy

0.95

0.90

° °
© co
° ul

Validation accuracy
Oo
a |
ui

 

 

0.70 \
0.65

—— Inceptionv3
0.6047 Xception

—— VGGi9 i
oss4.— VGG16

T T T T T

0 2 + 6 8

Epoch
Model loss

InceptionV3
Xception
VGG19
VGG16

Validation loss

 

(d)

Fig. 12. Line plot showing learning curves of loss and accuracy of the training and validation data for each of the analysed models: a) training accuracy, b) validation accuracy,
c) training loss, d) validation loss. Monitoring performance on the training and testing data is crucial as CNN may start to under- or overfit. Validation accuracy and loss clearly
show that Xception and InceptionV3 models, in contrast to the VGG ones, do not converge. As VGG-16 obtained better results then VGG-19, for both training and validation

process, it has been chosen as the most suitable for this application.

Table 3
The performance of anomaly detection classification models

Models Accuracy (ACC) [%] Precision (PPV) [%]
InceptionV3 90.1 85.4
Xception 80.2 98.1
VGG-16 92.0 85.5
VGG-19 91.4 84.8
Table 4

Confusion matrix in the validation process for each of the analyzed CNN based
models. Each row of the matrix represents the instances in the predicted class while
each column represents the instances in the actual class.

InceptionV3 Xception
702 120 806 16
43 779 308 504
VGG-16 VGG-19
703 119 697 125
12 810 20 802

by committing the least errors considered as dangerous for this
application (missing an anomaly - false negative). It also generated
the least false positive type errors (false alarm) for comparatively
good false negative results (the Xception model due to the type
of errors made is not suitable for this application).

Sensitivity (SE) [%] Specificity (SPC) [%] F-score (F,) [%]

94.2 86.6 89.6
72.4 96.9 83.3
98.3 87.2 91.5
97.2 86.5 90.6

Comparison with related works. Results presented in Table 3 con-
firm that the proposed solution can be used to detect beam insta-
bility. Obtained outcome is state-of-the-art solution for anomaly
detection in diagnostics signals with the use of artificial intelli-
gence tools in accelerator facilities. Only in [37] Authors present
a prototype version of an anomaly detection system. However,
direct comparison with this study is not possible due to the lack
of statistical results and detailed information.

5. Conclusions

In this paper the authors proposed design of the anomaly detec-
tion system in the diagnostic, multivariate signals of the Solaris
synchrotron. The presented solution is based on deep, convolu-
tional neural networks InceptionV3, Xception, VGG-16 and VGG-
19. It has been shown that a set of measurements in a certain time
window can be treated as an image and classified into two classes
12 M. Piekarski et al. / Measurement 165 (2020) 108116

with high accuracy. VGG-16 - based model allowed classifying
anomalies with accuracy at the level of 92%. To the best of our
knowledge, this is the first experiment that shows that deep learn-
ing architectures previously trained with ImageNet may be used
for anomaly detection in diagnostic signals in accelerators.

We believe that proposed model can find applications not only
in the described field but also serve as a solution to the problems of
anomaly detection in other areas. Starting with applications in pro-
duction plants where it is important to constantly monitor the pro-
duction process, ending with medical applications where testing is
based on the detection of disturbances or outliers. It is enough to
mention widely analysed ECG or EEG signals, where the classifica-
tion and finding anomalies in time windows is crucial for the right
diagnosis and examination evaluation. It has been observed in the
medical field that DNN served better for anomaly detection than
regular methods based on thresholding and machine learning
algorithms.

In the future, authors plan to analyse other types of diagnostic
signals such as temperatures or beam position. Next step will be
operation in real-time so aggregation of the measurements into
time windows and their classification. Furthermore, modifications
may include an extention of number of classes to include the
Solaris’ state machine and better adapt to the specifics of the
machine operation.

CRediT authorship contribution statement

Michat Piekarski: Supervision, Conceptualization, Methodol-
ogy, Writing - original draft, Writing - review & editing. Joanna
Jaworek-Korjakowska: Data curation, Software, Writing - original
draft, Writing - review & editing. Adriana I. Wawrzyniak: Data
curation, Conceptualization, Writing - original draft. Marek Gor-
gon: Conceptualization, Writing - review & editing.

Declaration of Competing Interest

The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared
to influence the work reported in this paper.

Acknowledgments

The presented work has been achieved in collaboration
between AGH University of Science and Technology and National
Synchrotron Radiation Centre Solaris. We would like to thank to
Solaris team for providing an insight and an expertise that greatly
assisted the research.

References

[1] CC. Aggarwal, in: Neural Networks and Deep Learning, Springer International
Publishing, 2018.

[2] M. Bartosik, C. Bocchetta, P. Borowiec, P. Goryl, R. Nietubyc, M. Stankiewicz, P.
Tracz, L. Walczak, A. Wawrzyniak, K. Wawrzyniak, J. Wiechecki, M. Zajac, U.
Zytniak, Solaris — National Synchrotron Radiation Centre, project progress,
May 2012, Radiat. Phys. Chem. 93 (2013) 4-8, https://doi.org/10.1016/j.
radphyschem.2013.03.036.

[3] C. Benvenuti, et al., Non-evaporable getters: from pumping strips to thin film
coatings, in: Proceedings of the fourth European Particle Accelerator
Conference, paper THZO2A, Stockholm, Sweden, 1998.

[4] C.M. Bishop, N.M. Nasrabadi, Pattern recognition and machine learning, J.
Electronic Imaging 16 (2007) 049901.

[5] M.M. Breunig, H.P. Kriegel, R.T. Ng, J. Sander, Lof: Identifying density-based
local outliers, in: Proceedings of the 2000 ACM SIGMOD International
Conference on Management of Data, Association for Computing Machinery,
New York, NY, USA. 2000, pp. 93-104. doi:10.1145/342009.335388. URL
https: //doi.org/10.1145/342009.335388.

[6] R. Chalapathy, S. Chawla, Deep learning for anomaly detection: A survey, 2019,
ArXiv abs/1901.03407.

[7] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM
Comput. Surv. 41 (2009) 15:1-15:58.

[8] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer, Smote: Synthetic
minority over-sampling technique, J. Artif. Intell. Res. 16 (2002) 321-357.

[9] F. Chollet, Xception: Deep learning with depthwise separable convolutions,
2016. CORR abs/1610.02357. URL _ http://arxiv.org/abs/1610.02357,
arXiv: 1610.02357.

[10] F. Chollet, Deep learning with python, 2017.

[11] R. Fliller, et al., Application of Machine Learning to Minimize Long Term Drifts
in the NSLS-II Linac, in: Proc. 9th International Particle Accelerator Conference
(IPAC’18), Vancouver, BC, Canada, 29 April-O04 May 2018, JACoW Publishing,
Geneva, Switzerland, 2018, pp. 1867-1869. doi:doi:10.18429/JACoW-
IPAC2018-WEPAFO022. https://doi.org/10.18429/JACoW-IPAC2018-WEPAFO22.
http://jacow.org/ipac2018/papers/wepaf022.pdf.

[12] E. Fol, J.C. de Portugal, R. Tomas, Application of Machine Learning to Beam
Diagnostics, in: Proc. 7th International Beam Instrumentation Conference
(IBIC’18), Shanghai, China, 9-13 September 2018, JACoW Publishing, Geneva,
Switzerland, 2018, pp. 169-176, https://doi.org/10.18429/JACoW-IBIC2018-
TUOAO2, URL http://jacow.org/ibic2018/papers/tuoa02.pdf.

[13] Gamma Vacuum, GammaVacuum product page, 2020a, URL https://www.
gammavacuum.com/index.php/product?id=23.

[14] Gamma Vacuum, SPCe, MPCe and LPCe specification sheets, 2020b, https://
www.gammavacuum.com/index.php/downloads.

[15] X. Glorot, A. Bordes, Y. Bengio, Deep sparse rectifier neural networks, in:
AISTATS, 2011.

[16] J. Han, M. Kamber, Data mining: Concepts and techniques, 2000.

[17] S. Hulbert, G. Williams, 1 - synchrotron radiation sources, in: J. Samson, D.
Ederer (Eds.), Vacuum Ultraviolet Spectroscopy, Academic Press, Burlington,
2000, pp. 1-25, https://doi.org/10.1016/B978-012617560-8/50002-5, URL
http://www.sciencedirect.com/science/article/pii/B97801261 75608500025.

[18] M.F. L’Annunziata, Chapter 8 - electromagnetic radiation: Photons, in: M.F.
L’Annunziata (Ed.), Radioactivity (Second Edition). second edition ed. Elsevier,
Boston, 2016, pp. 269-302. doi: 10.1016/B978-0-444-63489-4.00008-3.
http://www.sciencedirect.com/science/article/pii/B9780444634894000083.

[19] S.C. Leemann, S. Liu, A. Hexemer, M.A. Marcus, C.N. Melton, H. Nishimura, C.
Sun, Demonstration of machine learning-based model-independent
stabilization of source properties in synchrotron light sources, Phys. Rev.
Lett. 123 (2019) 194801, https://doi.org/10.1103/PhysRevLett.123.194801,
URL https://link.aps.org/doi/10.1103/PhysRevLett.123.194801.

[20] M. Lin, Q. Chen, S. Yan, Network in network, in: International Conference on
Learning Representations, 2014.

[21] F.T. Liu, K.M. Ting, Z. Zhou, solation forest, in: 2008 Eighth IEEE International
Conference on Data Mining, 2008, pp. 413-422.

[22] Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, X. He, Generative adversarial
active learning for unsupervised outlier detection, IEEE Trans. Knowl. Data
Eng. (2019) 1.

[23] A. Marendziak, et al., Performance of the Vacuum System for the Solaris 1.5
GeV Electron Storage Ring, in: Proc. of International Particle Accelerator
Conference (IPAC’16), Busan, Korea, May 8-13, 2016, JACoW, Geneva,
Switzerland, 2016, pp. 2898-2901. doi:doi:10.18429/JACoW-IPAC2016-
WEPOWO031. doi:10.18429/JACoW-IPAC2016-WEPOW031._ http://jacow.org/
ipac2016/papers/wepow031.pdf.

[24] E. Meier, Orbit correction studies using neural networks, in: Proceedings of
IPAC2012 New Orleans, Louisiana, USA, 2012.

[25] R. Mormont, P. Geurts, R. Maree, Comparison of deep transfer learning
strategies for digital pathology, in: The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops, 2018.

[26] F. Pedregosa, Hyperparameter optimization with approximate gradient, in:
ICML, 2016.

[27] M. Porumb, S. Stranges, A. Pescapé, L. Pecchia, Precision medicine and artificial
intelligence: A pilot study on deep learning for hypoglycemic events detection
based on ecg. Scientific Reports (2020) 10. https://www.scopus.com/inward/
record.uri? eid=2-s2.0-85077785778&doi=10.1038%2fs4 1598-01 9-56927-
5&partnerID=40&md5=e08e49f684b63bfle13e4e0d71d219ec, doi:10.1038/
$41598-019-56927-5. cited By 0.

[28] S. Raghu, N. Sriraam, Y. Temel, S. Rao, P. Kubben, Eeg based multi-class seizure
type classification using convolutional neural network and transfer learning.
Neural Networks 124 (2020) 202-212. https://www.scopus.com/inward/
record.uri?eid=2-s2.0-85078697690&doi=10.1016%2fj.neunet.2020.01.017&
partnerID=40&md5=8c61 151e90503fd34b1a59617fe4350a, doi:10.1016/j.
neunet.2020.01.017. cited By 0.

[29] R.Y. Rubinstein, D.P. Kroese, The cross-entropy method, in, in: Information
Science and Statistics, 2004.

[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.
Karpathy, A. Khosla, M. Bernstein, A.C. Berg, L. Fei-Fei, ImageNet Large Scale
Visual Recognition Challenge, International Journal of Computer Vision (IJCV)
115 (2015) 211-252, https://doi.org/10.1007/s11263-015-0816-y.

[31] M. Sabokrou, M. Fayyaz, M. Fathy, R. Klette, Fully convolutional neural
network for fast anomaly detection in crowded scenes, Comput. Vis. Image
Underst. 172 (2016) 88-97.

[32] M. Salem, S. Taheri, J. Yuan, Ecg arrhythmia classification using transfer
learning from 2- dimensional deep cnn features, in: 2018 IEEE Biomedical
M. Piekarski et al. / Measurement 165 (2020) 108116 13

Circuits and Systems Conference (BioCAS), 2019, pp. 1-4, https://doi.org/
10.1109/BIOCAS.2018.8584808.

[33] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale
image recognition, in: International Conference on Learning Representations,
2015.

[34] B. Staar, M. Liitjen, M. Freitag, Anomaly detection with convolutional neural
networks for industrial surface inspection, 2019.

[35] P. Hao Su, Y. Li, Transfer learning, in: Encyclopedia of Machine Learning and
Data Mining, 2017.

[36] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S.E. Reed, D. Anguelov, D. Erhan, V.
Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: IEEE
Conference on Computer Vision and Pattern Recognition, 2014.

[37] F. Tilaro, B. Bradu, M. Gonzalez-Berges, Model learning algorithms for anomaly
detection in cern control systems, in: Proceedings of ICALEPCS’17, paper
TUCPA04, Barcelona, Spain, 2017.

[38] M. Wang, W. Tong, S. Liu, Fault detection for power line based on convolution
neural network, in: ICDLT ’17, 2017.

[39] A. Wawrzyniak, CJ. Bocchetta, S.C. Leemann, S. Thorin, Injector layout and
beam injection into solaris, in: Proceedings, 2nd International Conference,

IPAC 2011, San Sebastian, Spain, September 4-9, 2011, JACoW, Geneva,
Switzerland, 2011, pp. 3173-3175.

[40] A. Wawrzyniak, A. Marendziak, A. Kisiel, P. Borowiec, R. Nietubyc, J. Wiechecki,
K. Karas, K. Szamota-Leandersson, M. Zajac, C. Bocchetta, M. Stankiewicz,
Solaris a new class of low energy and high brightness light source, Nucl.
Instrum. Methods Phys. Res., Sect. B (2017), https://doi.org/10.1016/j.
nimb.2016.12.046.

[41] T. Wen, R. Keyes, Time series anomaly detection using convolutional neural
networks and transfer learning, 2019, arXiv:1905.13628.

[42] W. Yu, Y. Bai, Visualizing and comparing alexnet and vgg_ using
deconvolutional layers, 2016.

[43] Y. Zhao, M.K. Hryniewicki, Xgbod: Improving supervised outlier detection with
unsupervised representation learning, in: 2018 International Joint Conference
on Neural Networks (IJCNN), 2018, pp. 1-8.

[44] Y. Zhao, Z. Nasrullah, M.K. Hryniewicki, Z. Li, Lscp: Locally selective
combination in parallel outlier ensembles, in: Proceedings of the 2019 SIAM
International Conference on Data Mining, 2019, pp. 585-593, https://doi.org/
10.1137/1.9781611975673.66.
