Berend EPJ Data Science (2020) 9:23 o E PJ D ata S Ci e Nn ce

https://doi.org/10.1140/epjds/s13688-020-00240-z a Spri ngerOpen Journal

EP] °'s

REGULAR ARTICLE Open Access

. . ®
Efficient algorithm to compute Markov ame

transitional probabilities for a desired
PageRank

Gabor Berend'*’*@®

 

“Correspondence:
berendg@inf.u-szeged.hu Abstract

"Institute of Informatics, University , , wo veg
of Szeged, Arpad tér 2, Szeged, We propose an efficient algorithm to learn the transition probabilities of a Markov

Hungary chain in a way that its weighted PageRank scores meet some predefined target
*SZTE-MTA Research Group on values. Our algorithm does not require any additional information about the nodes
Artificial Intelligence, Tisza Lajos dthe ed in the f ff Lo lol id h k | f
krt, 103, Szeged, Hungary and the edges in the form of features, i.e., it solely considers the network topology for
calibrating the transition probabilities of the Markov chain for obtaining the desired
PageRank scores. Our experiments reveal that we can reliably and efficiently

approximate the probabilities of the transition matrix, resulting in the weighted
PageRank scores of the nodes to closely match some target distribution. We
demonstrate our findings on both quantitative and qualitative evaluations by
reporting experimental results on web traffic (the English Wikipedia and a Hungarian
news portal) and the bicycle sharing network of New York City.

Keywords: Markov chains; PageRank scores; Random walk; Wikipedia

 

1 Introduction

Complex networks provide a ubiquitous and natural way to model real-world processes
and phenomena including but not limited to social interactions [1—6], natural language
[7-9] or biomedicine [10, 11], inter alia. In the steadily evolving area of network science,
PageRank [12] and its vast number of variants, including [8, 11, 13, 14], admittedly belong
to the most popular approaches of network analysis.

Given a network with its topology, PageRank algorithm determines a probability distri-
bution over its nodes by calculating the stationary distribution of a random walk which
is assumed to make the decision about which node to visit next based on local vicinity
information of nodes in a uniform manner, i.e., inversely proportional to the number of its
neighboring nodes.

From a real-world web browsing perspective, this assumption implies that users are be-
lieved to click on hyperlinks located at some website with equal probability, which most
likely is not the case in reality. Substantial research has been conducted on biasing the
transition probabilities of random walk models in an application-specific manner in order

to obtain modified PageRank scores for the nodes of certain types of networks [15-21].

© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use,
sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original
author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other
third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line
to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by

e
Gy) Sp ringer statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a

copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Berend EPJ Data Science (2020) 9:23 Page 2 of 26

In this work, we take a different approach as our goal is not to devise such transition
probabilities which yield useful PageRank scores for a particular application. Our goal
instead is to set the weights of the transition matrix such that it converges to a predefined
PageRank vector provided as input. This latter problem has been in the focus of [22, 23]. In
this paper, we propose an efficient algorithm—being linear in the number of edges of the
input graph—for the above task, which we shall refer as the reverse PageRank problem.

From an application-oriented point of view, finding such transition probabilities which
results in a predefined PageRank distribution for some Markov chain can help in designing
better routing protocols. For instance, one might want to spread around some resource
over a communication or social network with the additional (soft) constraint that all the
nodes should have an even (or similar) amount of access to the resource in the long run. In
such a scenario, we are interested in predicting the transition probabilities of the network
such that the desired distribution of the PageRank scores over the nodes is as uniform
as possible, i.e., the random walk visits nodes independently of their centrality within the
graph.

Another potential use case for the investigated problem is to gain insights into the un-
derlying principles of some partially observable Markov chain. Placing the problem in the
original context of the PageRank algorithm, even if we have information about the rela-
tive popularity of the individual websites, it is not necessary that we also have access to
the actual flow of network traffic between pairs of websites (hence partial observability).
We might, however, want to still be able to approximate click-through rates between web-
sites in the absence of access to web server logs with referral statistics. In such a scenario,
solving for the reverse PageRank problem, we can approximate the transition probabili-
ties between websites such that the stationary distribution over them closely matches our
initial beliefs for the relative popularity of the websites. Our large scale experiment on En-
glish Wikipedia is precisely of this form, i.e., we trained our model such that it attempts to
predict click-through rates between pairs of Wikipedia articles. In order to do so, we were
merely relying on the overall popularity of the articles.

The main contributions of this paper can be summarized in the followings:

¢ we propose an efficient algorithm to calculate the approximate gradient for our

problem, which can be calculated in linear time as a function of the number of
possible transitions of the Markov chain,

¢ we illustrate the effectiveness of the proposed algorithm via rigorous quantitative and

qualitative experiments on three real-world complex networks, i.e., the English
Wikipedia, a Hungarian news portal and the bicycle sharing network of New York
City,

¢ we release our source code at https://github.com/begab/reversePageRank in order to

foster reproducibility and additional experimentation.

2 Illustration of the problem

This section provides a small example which demonstrates the problem and its difficulty.
Suppose that we have a graph G = (V,E) as displayed in Fig. 1. Let us further assume
that there is some resource traversing the network, and this resource should be present at
vertices A through F with probabilities proportional to the radii of the solid circles marking
them, i.e., 0.26, 0.24, 0.12, 0.09, 0.16 and 0.13, respectively. Now the question that we face
is the following: how should we guide the resource, so that its expected presence over the
nodes (marked by dashed circles) match the desired input distribution (solid circles)?
Berend EPJ Data Science (2020) 9:23 Page 3 of 26

 

 

(a) Stationary distribution of the random walk when routing performed uniformly (dashed
radii) versus the desired distribution (solid radii).

9.413

 

(b) Stationary distribution of the random walk when routing performed proportionally to the
desired distribution (dashed radii) versus the desired distribution (solid radii).

 

(c) Stationary distribution of the random walk according to our model (dashed radii) versus
the desired distribution (solid radii).

Figure 1 Illustration of the problem of finding transition probabilities for some desired stationary distribution.

The radii of the nodes is proportional to their share from the stationary distribution
Ne J

 

 

Figure 1(a) illustrates that letting the resource to choose its upcoming destination uni-
formly at random from the directly accessible neighboring states—as it is done in vanilla
PageRank algorithm—can provide some states with excess importance (e.g., states C and
D) and leave some of the states under-represented at the same time (cf. states A and E).

An alternative to the uniform routing scheme could be if we chose succeeding states
proportional to the desired stationary distribution of the neighboring states. As Fig. 1(b)
depicts, however, such a strategy might not provide any real benefit over uniform routing.

Figure 1(c) demonstrates the behavior of our proposed algorithm. Note that in this fig-
ure the radii of the dashed and solid circles practically concur as opposed to Fig. 1(a) and
Fig. 1(b). This means that performing resource routing according to the proposed tran-
sition probabilities would result in a stationary distribution closely matching the desired
one.

For this previous motivating example our algorithm is able to find a close to perfect
solution, however, we should note that in the general case it is possible that for a certain
network topology and expected PageRank scores no perfect solution exists even in theory.

We deal with the general solvability of our problem in Sect. 5.3.
Berend EPJ Data Science (2020) 9:23 Page 4 of 26

3 Related work

There have also been previous work with similar motivation to ours [22, 23], i-e., to deter-
mine transition weights in some complex network. The core motivation in [22] and [23]
are identical to ours, however, they employ different approaches.

Inspired by Luce’s choice theory [22-24] learn weights for the vertices of some network
in an iterative fashion and infer transitional probabilities for the edges based on them. The
reliance on Luce’s choice theory lets [22, 23] to learn only O(|V|) parameters for some
network G = (V,E) and use those parameters for inferring the O(|E|) > O(|V]|) transition
probabilities. Note that the classical Metropolis—Hastings algorithm [25, 26] is also appli-
cable to obtain state transition probabilities of a Markov chain for some desired stationary
distribution, however, as stated in [22], “very little is known about it from a formal point
of view, especially, its mixing time and rate of convergence”. ChoiceRank [23] is based on
and involves an iterative EM-type inference algorithm for trying to recover the marginal
popularity of the nodes in the network provided as an input to the algorithm. Unlike [22],
the authors of [23] do not require the input network to be strongly connected.

There exist further work on the estimation of transition probabilities in Markov chains
that assume temporal access to observing transitions from the network [27, 28]. [29] pro-
posed the inverse Markov chain problem, in which the authors try to predict the number of
transitions in a transportation network in a partially observable setting. The problem for-
mulation in [29] is similar to ours, but also differs in important aspects. [29] assumes that
the visit frequencies for a subset of verticies V, C V are observed, whereas our algorithm
requires a distribution (the PageRank vector) defined for all the vertices. Due to the differ-
ent setting, the loss function employed in [29] also differs from the one we utilize. Partial
observability in [29] also apply to the edge transition frequencies over V, x Vo, ie., the
algorithm has an additional (and optional) term in the loss function regarding the actual
edge transition frequencies over a subset of the edges in the network. Another difference
is that the objective function applied in [29] was optimized using the natural gradient,
whereas we used a quasi-Newton optimization technique, which suites better problems
of the scale our experiments involved. Perhaps most importantly, the calculation of the
gradient of the objective in [29] is quadratic in the number of (observable) vertices, which
makes its application prohibitive even for moderate sized networks.

The Supervised Random Walk approach [2] aims at learning an appropriate weighting
function over the edges of complex networks by minimizing the sum of margin-based soft
losses between positively and negatively labeled nodes. The margin-based loss employed
in [2] includes the stationary distribution of random walks with restarts from multiple
source nodes. [2] assumes that each edge (u,v) can be described by a low dimensional
feature vector W,,,, whereas our model does not require access to any additional meta-data
in order to calculate edge features. Since we do not regard edge weights as a parameterized
function of feature values, our proposed approach is also applicable in scenarios when edge
features, such as edge age—an example feature used in [2]—are not accessible.

Various studies have been conducted recently related to Wikipedia about trying to pre-
dict missing links [30, 31], improving and analyzing navigability [32-35]. More generally,
the field of network tomography [36] focuses on statistical methods that can be used for
making inferences about unknown edge properties of (typically computer) networks that
could be based on node-level aggregated information. Network tomography applications
vary vastly, i.e., the measurements may be collected passively by monitoring natural traffic
Berend EPJ Data Science (2020) 9:23 Page 5 of 26

flows or actively by generating probe traffic. Our setting belongs to the former category of
passive approaches. Typical network tomography approaches, such as [37, 38] for recover-
ing the unobserved random variables operate via Expectation Maximization and Markov
Chain Monte Carlo Methods.

4 Notation and background

In this section, we briefly describe the original PageRank algorithm and also introduce our
notation for the rest of the paper. Markov chains can be used to define stochastic processes
by providing the state space S = {S),...,S,,} and a row stochastic transition matrix P €
IR”*", the pj; element of which can be interpreted as the probability of observing state S;
in time ¢ given that state S; was observed at time ¢t — 1. The stationary distribution of a
Markov chain characterized by transition matrix P can be determined by calculating the
left principal eigenvector of P.

Given a graph G = (V,E), the PageRank algorithm determines a distribution of impor-
tance over the nodes in the network that we denote by z. We use subscripts for indexing
a particular element of a vector. That is, 2, denotes the PageRank score for node v.

The PageRank model assumes that the prestige of any node depends on the importance

of those nodes which link to it in a recursive manner. Formally, 7, can be defined as

Ty = S PuvTus (1)

(u,v)EE

where p,, denotes the probability of traversing from state u to v. For some node 4, py, is
uniformly set to Z , d, denoting the outdegree of node u. According to the matrix formula-
tion, the PageRank vector is the stationary distribution of the Markov chain parameterized
by P, which is guaranteed to uniquely exist for ergodic Markov chains. In order to handle
non-ergodic networks as well, Page et al. [12] suggests the following calculation for the
PageRank scores:

_&B 1
Ty = + (1 - p) de 5 Tw (2)

where the damping factor £ is typically chosen from the interval [0.1, 0.2]. This extension
ensures the Markov chain to be aperiodic and irreducible as a non-zero probability (pro-
portional to the damping factor) is reserved for direct transitioning between any pair of
nodes in the network. Equation (2) reveals that all the outgoing edges of a particular node
are still weighted in a uniform manner, i.e., proportional to the outdegree of that node. In
the following, we introduce our model for learning edge-specific transition weights for a
given target distribution over the network.

5 Reverse PageRank algorithm

We next formulate the model utilized for learning the transition weights of Markov pro-
cesses with predefined PageRank scores. We coin our approach as the Reverse PageRank
algorithm that we shall refer as REVERSEPR for short. Throughout the rest of the paper,
we denote the desired PageRank vector we wish our weighted random walk with restarts

to converge by z™*.
Berend EPJ Data Science (2020) 9:23 Page 6 of 26

In the following, we denote the parameters we optimize for and the particular PageRank
scores the nodes receive after convergence by © and z(@), respectively. The edge weight
associated for a particular edge (u,v) € E is going to be proportional to exp(6,,). More
concretely, the probability of directly traversing between any pair of nodes (u,v) € V x V
is given by

exp(Ouv) :
a —, if(u,v) €E,
Puv(®) = D(wk)eE EXP Ouk) (3)

0, otherwise.

As the above formulation is invariant to a constant additive factor, that is p,,(©) = pyy(@ +
c) for any c € R, infinitely many © can yield the same p,, values. In order to overcome
this and to decrease the number of variables in the model by O(|V]|), we ensure that for
each node u, there is one parameter in 6,,,, i.e, among the parameters controlling for the
transition probabilities starting from node u, which is constantly kept at the value of zero
(cf. the comment in line 11 of Algorithm 1).

Our overall goal is thus to find a feasible value of © for which 2(@) lies as close to *
as possible. More formally, given a graph G = (V,E), the objective function we seek to
minimize is

n*

in KL(zx* || 2(@)) = mi ; log ——, 4
min (xc || 2c ( )) min Dm, 08 (6) (4)

ie., the Kullback—Leibler (KL) divergence between distributions z* and 2(@).
As KL(z* || 7(@)) = H(a*,2(@©)) — H(zt*) and the second term—being the entropy of
the target PageRank vector—does not depend on @, we alternatively opt for the equivalent

optimization problem

max -H (x*,2(0)) = max ) ms log z,(), (5)
aS

with H(z*,2(@)) denoting the cross entropy between z* and 1(@).

Algorithm 1 Efficient computation of the approximate gradient of Eq. (5)

 

1: function CALCUATEAPPROXIMATEGRADIENT(G, @, B)
2: for all (u,v) ¢ Edo

3: gl(u,v)] <0 > initialization
4: mw <- WEIGHTEDPAGERANK(G, @, B) > 6 denotes the damping factor
5: for all uc V do
6: T <0
7: for all v ¢ ADJACENTVERTICES(G, 1) do
8: T< T + Pw(@)=
9: i<O
10: for all v ¢ ADJACENTVERTICES(G, 1) do
11: if i> 0 then > we keep one of the 6,,,. values as 0
12: glu, v)] — muPw(O)(= - T)
13: i<xit+l

return g
Berend EPJ Data Science (2020) 9:23 Page 7 of 26

In case of Eq. (5), the partial derivative with respect to a single parameter 6, has the form

d—-H(x*,7(O)) _ 1, d7,(O)
“2 50) 00% 0)

 

 

revealing that we need to sum over all the vertices in order to calculate a single par-
tial derivative. Furthermore, since 2,(@) is recursively entangled with 6;, the partial
derivative within the summation would require the application of a PageRank-style it-
erative approach [2, 39]. As a consequence, the gradient of Eq. (5) can be calculated in
O(\E| - (|E| + |V])) time, implying that the exact calculation of the gradient is beyond prac-
tical applicability even for moderate sized graphs.

In order to circumvent the difficulty of calculating the gradient of Eq. (5), we propose
an alternative approximate gradient which can be computed in a highly effective O(|E|)
manner. As stated earlier, the main bottleneck during the computation of the gradient of
Eq. (5) arises from the presence of a in Eq. (6). Due to the recursive formulation of
PageRank, (©) can be expressed as uy) <ePu(@)x,(@), which means that

 

 

 

dmy(@) OPuv(®) dm4()

= Ty(O + Dyy(O . 7
Fa oe HOV + PO) (7)
(u,v)EE
In our approximation, we make the simplifying assumption that the z,, values are not
dependent on ©, canceling this way out the (otherwise computationally still expensive)
second term from Eq. (7). Together with the fact that the transition probabilities parame-

terized by © have the partial derivative

—py(©)-py(O)+py(©), ifu=iandv=j,
= ) —pi(@) - pir(@), ifu=iandv ¥j, (8)

0, otherwise.

OPuv(O)
06;

 

the partial derivative from Eq. (6) can be conveniently approximated as

me Pwl®) a). (@)( 7 __ DP (0)
2 morn dG; = 7(0)W0)( 5 Loy) (9)

 

 

 

The nice property of Eq. (9) is that its value can be calculated in O(1) amortized time.
This is due to the fact that the last sum in Eq. (9) can be reused for the calculation of mul-
tiple partial derivatives—i.e., for any of the variables 6;,.. This observation implies that the
entire gradient of the objective function can be calculated in O(|E]|) time as also illustrated

in Algorithm 1.

5.1 Details on the optimization

A key property of optimization algorithms is their time needed for convergence. As op-
posed to first order methods, second order methods are known to have the ability of con-
verging faster to an optimum. The application of second order methods for optimizing Eq.

(5), however, would be strictly prohibitive. This is because second order methods rely on
Berend EPJ Data Science (2020) 9:23

the inverse of the Hessian of the objective function which has O(|E|) parameters accord-
ing to our problem formulation—with |E| potentially being in the range of 107s for large
real-world networks.

In order our algorithm to be applicable for large-scale networks as well, we applied the
limited-memory Broyden—Fletcher—Goldfarb—Shanno (L-BFGS) [40] quasi- Newton op-
timization technique. The usage of L-BFGS is advantageous since it tries to mimic the be-
havior and the fast convergence of second order methods, while only requiring the com-
putation and storage of the gradients of the objective function up to a fixed number of
previous iterations.

Similar to first-order optimization methods, L-BFGS also has hyperparameters, such as
the step size for standard gradient descent, however, we did not investigate tuning these
hyperparameters for obtaining better results. Instead, we relied on the default hyperpa-
rameter settings in this regard of the MALLET package that we used for performing the
optimization.

Confirmed by our preliminary experiments, relying on the calculation of the proposed
approximate gradient not only sped up the optimization process by orders of magnitude,
but we also managed to obtain better solution in terms of our objective function intro-
duced in (5). Our conjecture for the latter phenomenon is that the proposed approximate
gradient is strictly smaller than the actual gradient, thus missing good local optima during
the optimization of the non-convex objective function was less likely.

5.2 The non-convexity of the objective
For an illustration of the non-convexity of the objective function already with a single
parameter, see Fig. 2. Note that the non-convexity of the objective implies that different
initializations for © can result in convergence to different local optima. According to our
empirical evidences—that we discuss in more details in Sect. 6.3.2—the quality of the local
optima according to multiple evaluation criteria did not differ substantially.
Furthermore, we have experienced that initializing © homogeneously with all zeros can
be regarded as a generally robust way to initialize model parameters. We should also add
that the optimization procedure we employ is deterministic in the sense that once initial-
ized identically, it always converges to the same (local) optimum when applying the same
hyperparameters for L-BFGS (that we kept constant at their default values throughout our

 

 

   
   

  

 

 

 

experiments).
(— >)
Pac
0.018 0.119 0.500 0.881
0.3754
0.350;
a
E 0.325]
‘r
exp(@) = 0.300
PAC = 1 0 x
Fexp(9) 0.275}
0.250, :
1.0 —4 —2 0 2
0
(a) Example graph with 2* = (0.7,0.2,0.1) re- (b) Corresponding objective function plotted
flected by the radii of the nodes. over the interval [—4, 2]
Figure 2 Illustration of the non-convex nature of the objective function already with a single variable

 

 

\ J

Page 8 of 26
Berend EPJ Data Science (2020) 9:23 Page 9 of 26

 

Objective value for varying Pgc

 

10)

 

 

 

és
0 J, =
a 0 ‘n 41
Qy | s
lL} JS ~ 91
re
01 Be eee eo nn ee ee
(B) pBe 00 O02 04 06 O8 10
Psc
(a) Example graph with 7* = (0.1,0.1,0.8) re- (b) The objective function (blue
flected by the radii of the nodes. curve) cannot reach zero (in orange).

 

 

Figure 3 |llustration of a problem for which no perfect solution exists
X /

5.3 The general solvability of the problem

We finally comment on the existence and the general solvability for the proposed prob-

lem formulation. A trivial solution always exists—i.e., choosing p,,, proportional to 7—

for fully connected networks with z* obeying the inequality Fi < minycy 77. Note that

when the inequality is violated, then the node corresponding to the smallest value in z*

already receives a larger then desired amount of distribution based on its share from the
B

damping factor (iA) hence a perfect solution cannot be given. On the other hand, for any

fully connected network obeying the above inequality, it suffices to set all the rows of its
corresponding transition matrix identically proportional to 2* — vi in order to obtain an
objective value of zero.

In the more common scenario, when we are not given a fully connected network, an op-
timal solution with a KL-divergence of zero might not exist for certain input distributions.
It can be easily verified that whenever node u has a single outgoing edge towards node v,
1y(@) > 2,(@) should always follow. If the desired distribution behaves contradictory for
such a pair of vertices, that is 7; < 17, then a perfect solution in terms of KL-divergence
cannot be given. Figure 3 illustrates a further example for which no perfect solution can

be determined, even though it obeys the previously stated necessity condition.

5.4 Comparison to existing algorithms

REVERSEPR differs from [22, 23] in that it treats the transition probability of every edge
in the network as a separately tuneable parameter. As the number of edges are typically
larger than the number of vertices, it makes the problem formulation of REVERSEPR un-
derdetermined. [22] and [23] follows a different route, as they learn a separate parameter
for each node in the network and they infer the edge transition probabilities based on the
node-level parameters they learn.

Another difference between ChoiceRank and REVERSEPR is that the former receives the
aggregate incoming traffic and outgoing traffic of each individual node as input, whereas
for the REVERSEPR algorithm it suffices to have access solely to 2*. Unlike [29], ChoiceR-
ank and REVERSEPR share the common trait that they both rely on the full topology, ice.,
the entire edge structure are assumed to be observable (without the actual traffic flowing
over them).

Finally, the algorithm in [22] assumed that the input network is strongly connected. Note
that REVERSEPR does not require such an assumption as it conveniently turns any input
network into ergodic by employing a damping factor similar to the PageRank algorithm.
Berend EPJ Data Science (2020) 9:23 Page 10 of 26

6 Experiments

This section contains our experimental results conducted over the English Wikipedia, a
Hungarian news portal and the bicycle sharing network of New York City. We used the
machine learning package MALLET [41] to perform the L-BFGS optimization over our
objective function.

As already mentioned in Sect. 4, PageRank implementations often employ a damping
factor 6 to overcome difficulties arising with non-ergodic networks. Unless stated other-
wise, we conducted all our experiments using the default damping factor of 6 = 0.01. We
purposely chose such a small damping factor in order not to severely alter the random
walk applied over the original network, while ensuring ergodicity.

6.1 Baselines

In order to measure the effectiveness of the proposed algorithm, we compare it to sev-
eral strong baselines. As our algorithm makes use of the topology of the network (E) and
the expected PageRank scores (*), we derived various baselines relying on these sources
of information. The different approaches utilized in our experiments are summarized in
Table 1.

The Random strategy is the most simplistic of all as it assigns transition probabilities in
an uniform manner when determining it for some node. The Indegree strategy assigns the
transition probabilities based on the number of indegrees some node has. As the number
of incoming edges can be a good proxy to the hubness of the nodes, we anticipated this
kind of heuristic to work particularly for transportation networks in particular, such as
the Citibike network.

The heuristic dubbed Jaccard takes into consideration the extent to which the neigh-
borhood of some potential target node v overlaps with the source node u when providing
predictions for p,,. Note that this strategy can only assign some nonzero probability be-
tween pairs of nodes which have at least one neighbor in common. Another property of
the Jaccard heuristic is that it assigns higher values for pairs of nodes for which there is
little difference in their direct neighbors. This strategy seems less useful for transportation
networks, i.e., from a real world perspective one would expect low traffic between airports
from which the directly accessible destinations are highly similar. For networks, where se-
mantics matter more, such as the Wikipedia network, the Jaccard baseline is expected to
provide more accurate predictions, since high similarity in the neighborhood of nodes can
be an indication towards high topical relatedness.

The Traffic baseline simply chooses the next node to visit based on the marginal popu-
larity of the nodes, proportional to z*. Obviously, the choice which is the most popular in

general is not necessarily the most popular one in every situation.

Table 1 Overview of the different approaches for approximating Puy

Approach
Random Duy & 1
Indegree Duv & |{k|(k,v) © E}|
I{k|(uk)eEA(v,k) EF} |
Jaccard Puv & Wea eevee
Traffic Puy K70F
(unweighted) PageRank Duy X Toy
ChoiceRank [23] Duy = <2 (VA; € Rs) [24]

Luv )eE Ay!
Reverse PageRank Duy & exp(Oyy)
Berend EPJ Data Science (2020) 9:23 Page 11 of 26

Predictions for the PageRank approach are determined based on the PageRank scores
of such a random surfer, which performs random walk in an unweighted manner, ie., fol-
lowing the next node to visit uniformly at random.

Our baselines also include ChoiceRank [23], a recent algorithm building upon Luce’s
choice theorem [24] for identifying network traffic preferences. We relied on the pub-
licly available open-source implementation of the algorithm’ during our experiments.
ChoiceRank operates on the marginal indegree and outdegree sequence of the nodes and
tries to recover them by calibrating a score for each node in the network. As our proposed
algorithm has sole access to a desired distribution over the nodes, we treated the indegree
and outdegree of some node uw as Diy, yee 7) and )\y,,,ye¢ 7)» respectively.

Although the model proposed in [22] is also intended to solve our task, we did not con-
sider it as a potential baseline, since its assumption on the strong connectedness of the
input network does not generally hold for the networks used in our experiments.

6.2 Evaluation metrics
For some node u, we measure the quality of the predicted distribution over its outgoing
edges by comparing it to the ground truth transition probability distribution from that
node. We rely on four distinct evaluation measures that we describe below.

Following [23], we apply the normalized link displacement scores and KL-divergence
between the ground truth and the predicted distributions for evaluation. Normalized link
placement for some node u is calculated as

1

vin eBye iaweE\P dX |ou(v)" — 0,(v)

u,v)EE

 

; (10)

where o,,(v)* and o,,(v) returns the ordered rank of node v within the neighbors of node u
according to the ground truth and the predicted transition probabilities, respectively. The
KL-divergence that we use for evaluating the quality of the learned transition probabilities
gets calculated for some node u in the network as

y~ pi, log Se. (11)

{v:(u,v) EE} uv

An important property of (11) is that it is only defined for such cases when p*,, > 0 also
implies p,, > 0 to be true. That is, we cannot measure the KL-divergence when there is a
non-zero ground truth transition probability, but a model assigns zero probability for it.
This is hence a major shortcoming for the Jaccard similarity based heuristic that it can
only assign a non-zero transition probability between pairs of nodes which have at least
one neighbor in common. Since this criterion was not fully met for all the nodes involved
in our experiments, we were not able to calculate the KL-divergence based evaluation for
the Jaccard baseline.

Besides the aforementioned scores, we also measure the quality of the predicted transi-
tion probability distributions by quantifying their root mean squared error (RMSE) as in
[22]. For some node u, the RMSE of the predicted transitions is calculated according to

 

1 . ;
Iv: (uv) €E} » (pi, — Pw) .

v:(u,v)EE}
Berend EPJ Data Science (2020) 9:23 Page 12 of 26

Additionally, we devise a fourth evaluation criterion, namely mean reciprocal rank
(MRR) having its roots in the information retrieval community. This criterion is intended
to measure the extent to which a model has the ability of ranking those transitions high
that are actually the most frequent ones according to the ground truth data. That is, for a

given node u, we calculate the value

1

Oy (arg Max 1.7, y) EE} Piy)

with o(-) denoting the same function as in (10), i-e., it returns the rank of the node in its
argument relative to the predicted transition probabilities with respect to u. MRR is then
obtained by averaging the individual scores for the different nodes.

6.3 Experiments on the link structure of Wikipedia

For these experiments, we take the hyperlink structure of the English Wikipedia and try
to estimate its click-through rates solely based on the distribution that we derive from the
page visit statistics of the individual articles.

We conducted our experiments on the 20160305 dump of the English Wikipedia,”
which consist of more than 1.23 « 10’ articles and 3.87 « 10° hyperlinks. Figure 4 illustrates
the typical long tail degree distribution of the Wikipedia network we had access to.

As an additional resource in our evaluation, we relied on the publicly available raw page
visit statistics© for calculating 2*. That is, we derived the desired distribution over the
pages based on the individual page visit statistics of the Wikipedia articles.

The running time of REVERSEPR was approximately 40 minutes on a 2.00 GHz Intel
Xeon E7-4820 processor for the above described Wikipedia network. For ease of repro-
ducibility, we make the Wikipedia network as well as the z* vector that we based our
experiments on readily accessible.‘

6.3.1 Predicting the most likely cross-article clicks

For evaluation purposes, we utilize the Wikipedia Clickstream dataset [42] which includes
referral statistics for March 2016 over the entire English Wikipedia. This valuable source
of information makes it possible to calculate p*.,, i.e., the ground truth click-through rate
between a pair of Wikipedia articles (u, v).

 

C ~
0.012 LO
o
0.010 €
Sos
o S
5 0.008 £
S 2 0.6
£ 0.006 a
v 2
> 0.4
‘3 0.004 $
Z 6
0.002 = 0.2
5
O
0.000 0.0
10° 101 102 103 104 10° 101 102 103 104
Out-degree Out-degree
(a) Outdegree (b) Cumulated outdegree

Figure 4 Outdegree distribution of Wikipedia articles. Articles with a moderate number of hyperlinks are the

most typical, as 50% of the articles have an outdegree at most 61
Ne J

 

 
Berend EPJ Data Science (2020) 9:23

 

  

  

Figure 5 Cumulated evaluation metrics for the Wikipedia network. Scores are reported as a function of the
outdegree of Wikipedia articles

 

(— >)
c Random —>- PageRank
= Indegree === ChoiceRank
a — Jaccard —e= ReversePR
UO “ .
& oO Traffic
Q
a £
oO &
= 3
L Random =r PageRank =
< Indegree —= ChoiceRank O
& Jaccard —e= ReversePR
O Traffic
10! 10? 103 104 10! 10? 103 104

Out-degree

Random =r PageRank =r PageRank
Ww Indegree —m ChoiceRank = Indegree —m ChoiceRank
2 Jaccard =e ReversePR c =< Jaccard ee ReversePR
a Traffic = v= Traffic

Oo
oO
g 2
© &
Ss 3
€ £
5 5
O O
10 102 103 104 10! 102 103 104
Out-degree Out-degree

 

X /

Obviously, predicting the most likely article to visit next is much easier (purely by
chance) for articles with a low outdegree as opposed to articles with a large number—
potentially thousands—of neighboring articles to choose from. For this reason, we report
our performance metrics as a function of the outdegree of the articles.

Results are reported in Fig. 5 according to the metrics already introduced in Sect. 6.2.
It can be seen in Fig. 5, that the rankings produced by the REVERSEPR algorithm is domi-
nantly better compared to the alternative baselines.

The baseline which relies on the popularity of articles can only produce reliable predic-
tions for articles with extremely low outdegree and mostly for those evaluations that focus
more on the ranking capabilities of the methods instead of the accurateness of the actual
transition probabilities.

In particular, the popularity-based Traffic baseline delivers the best performance in
terms of its replacement and MRR scores for articles not having more than five outgoing
links. Its performance, however fades out quickly for articles containing more hyperlinks.
Inspecting Fig. 4 reveals that only a minority of Wikipedia articles belong to the category
on which the popularity baseline could give reliable predictions.

It is the Jaccard similarity-based approach alone which shows a similar overall ranking
ability to REVERSEPR. When approximating p*.,, the Jaccard baseline compares the extent
to which the set of hyperlinks to be found on Wikipedia article u overlap with those in-
cluded on article v. As a consequence, the Jaccard similarity-based approach makes much
less reliable predictions for Wikipedia articles with a small number of outgoing links. The
inability of the Jaccard similarity-based heuristic to give accurate predictions for nodes
with low degree is most pronouncedly showcased by the RMSE results. The fact that the
KL-divergence evaluation cannot be calculated for the Jaccard baseline illustrates another

Page 13 of 26
Berend EPJ Data Science (2020) 9:23 Page 14 of 26

weakness of that baseline, i.e., it sometimes assigns zero transition probabilities to transi-
tions for which there is a nonzero ground truth transition probability in reality. Figure 5
demonstrates that even random guessing (i.e., the Random baseline) can outperform the
Jaccard approach for articles with an extremely low number of outgoing links. We can no-
tice that a large fraction of the Wikipedia articles fall into the category for which Jaccard
baseline performs poorly, i.e., half of the Wikipedia articles had an outdegree of at most
61 for the investigated snapshot of Wikipedia (cf. Fig. 4).

6.3.2 Sensitivity analysis of REVERSEPR

Our algorithm is deterministic in the sense, that when initialized by the same parameters,
it converges to the same solution. There could nonetheless still be some variability in the
results depending on how we initialize our parameters ©. This potential variability comes
from non-convexity of our objective (cf. Sect. 5.2).

As such we cannot guarantee that every initialization of © would converge to the same—
or even to an equally good—solution. Our default edge transition initialization strategy—
that we applied in all our previously reported experiments—is to set the node transition
distributions according to the vanilla PageRank algorithm, i.e., we deterministically set all
edge probabilities uniformly. This means that initial transition probabilities for some node
with outdegree d were initialized to have a value s. Practically, this means that all param-
eters of © got initialized to zero by our default strategy (that we denote as Initialization O
hereon).

In order to assess the sensitivity of REVERSEPR towards the initial choice for ©, we
repeated our experiments by applying random initializations for © as well. This strategy
simply set the parameters in © independently to random values sampled uniformly from
[O, 1].

Besides the previously mentioned default initialization (Initialization 0), we executed
REVERSEPR nine additional times, having © randomly initialized as described above. Fig-
ure 6 contains the evaluation scores for all the initializations, suggesting that random ini-
tialization does not play a crucial role in the performance of REVERSEPR in terms of RMSE
and KL-divergence. Furthermore, we can see practically no variability in the results for any
of the evaluations between the different random initializations of 0.

Even though the default initialization performs similarly to random initializations for the
KL-divergence and RMSE evaluations, they dominantly perform better for the remaining
two ranking-oriented metrics, i.e., we can see a clear benefit from initializing © according
to the all-zeros default strategy in terms of the displacement and MRR scores. Table 2 in-
cludes the aggregated numeric metrics for the default initialization strategy as well as the
summary statistics (mean and standard deviation) of the metrics for 9 random initializa-
tions.

To summarize, random initialization of the parameters we optimize for does not account
for a large variability in the quality of the solution REVERSEPR converges to. Furthermore,
our deterministic strategy to initialize all parameters to zero seems to provide as good or
even better quality in all evaluation aspects, for which reason we return to the usage of

our default initialization strategy for all our upcoming experiments.

6.3.3 The effects of damping factor
Another aspect of REVERSEPR which could potentially affect the quality of the solution it
converges to is the choice of the damping factor 6. We have mentioned earlier that initially,
Berend EPJ Data Science (2020) 9:23 Page 15 of 26

 

  

  

 

( >)
+ 0.32
ov
£ 0.30
o Z
© 0.28 oS
2 126 Initialization ° Initialization
s ‘ . — 3S oo. —
£ 0.24 1 - E 1 °
= O
5 2 2
= 0.22
3 3 3
10! 10? 103 104 10! 10? 103 104
Out-degree Out-degree
0.30 Initialization Initialization
‘ —~e 0 ——-~— oOo —*
7 = —— °
= 0.25 = ~+
oS 3 + —_~
ov v
# 0.20 & > —_——
Ss >
€ £
Ss =}
O 0.15 O
0.10
10+ 102 103 104 101 102 103 104
Out-degree Out-degree
Figure 6 Cumulated evaluation metrics on the Wikipedia network for various initializations of @. 0
corresponds to our default deterministic strategy for setting all initial values of @ to zero. Values for @ were
chosen randomly from the interval [0, 1] for the remaining 9 initializations

 

XX /

Table 2 Comparison of the default initialization strategy and alternative random initializations on
the final overall evaluation metrics

Default Avg. of 9 random initializations Std. of 9 random initializations
displacement 0.3075 0.3314 44e-05
KL 2.8656 2.8673 4.0e-05
RMSE 0.0997 0.0999 6.1e-06
MRR 0.2573 0.1204 8.6e-05

we set the damping factor to 0.01 so that the random walk does not deviate considerably
from the true topology of the network. Here we report our results for REVERSEPR when
evaluated using four substantially different damping factors, 6B € {0.01, 0.05, 0.1, 0.2}.

Figure 7 illustrates the results that we recorded for our experiments when we applying
different damping factors. We can draw similar conclusions for we did earlier for the effects
of random initializations in Sect. 6.3.2. That is, the choice of the damping factor does
not seem to have a decisive impact on the results obtained, REVERSEPR behaves rather
robustly for the different 6 values.

We can nonetheless observe some small differences again for the two metrics which
focus on the quality of the rankings and not the actual transition probabilities, i.e., the
displacement and the MRR scores. Choices for larger values of £ tend to slightly favor low
degree nodes, whereas higher degree nodes benefit more from the application of smaller
values of 6. This can be explained by the fact that nodes with large degree are more likely
to be a core member of the network, whereas lower degree nodes tend to be located at
the periphery of the network. These are the nodes on the periphery which benefit more
from a more frequent restart of the random walk, which corresponds to a higher value
Berend EPJ Data Science (2020) 9:23 Page 16 of 26

 

  

  

(— >)
5 0.30
2 0.
© —
8 =
a. 0.25 g
3 &
TO >
£ 0.20 E
©
s O
5
Oo 0.15
10+ 10? 103 104 10+ 10 103 104
Out-degree Out-degree
wD &
ow =
g g
® &
sS 5
= =
5 5
O O
10+ 102 10 104 10+ 102 103 104
Out-degree Out-degree
Figure 7 Cumulated evaluation metrics on the Wikipedia network for varying damping factor B. Larger
values of B tend to slightly favor low degree nodes for the displacement and MRR metrics

 

 

X

Table 3 The effects of applying different damping factors B

B =0.01 B =0.05 B=0.1 B =0.2
displacement 0.3075 0.3116 0.3158 0.3229
KL 2.8656 2.8656 2.8656 2.8656
RMSE 0.0997 0.0997 0.0997 0.0997

MRR 0.2573 0.2558 0.2507 0.2406

of 6. We report the aggregated metrics that we obtained for the different choices of 6 in
Table 3, corroborating that smaller values of 6 seem to perform better for the ranking-
based metrics and that the other two metrics (KL and RMSE) behaved mostly insensitive
to the different choices of f.

6.3.4 Qualitative assessment of the different approaches

In this section, we evaluate the different approaches from a qualitative perspective. While
these examples are not intended for drawing far-reaching conclusions, they can comple-
ment the more rigorous quantitative evaluation and possibly provide additional insights
into the behavior of the different approaches.

Since the Jaccard similarity-based approach is heavily relying on the local vicinity of
articles, it makes the Jaccard approach less capable for adapting to the changes in tempo-
ral user preferences compared to REVERSEPR. This phenomenon is illustrated in Table 4
which lists the titles of Wikipedia articles when following the most likely transitions up to
two hops, starting out from the Wikipedia article about Machine learning, Stephen Hawk-
ing and Brie Larson.

The articles predicted by REverseEPR—starting at the article Machine learning and
traversing through Deep learning—illustrates its ability to reflect temporal dynamics of
Berend EPJ Data Science

(2020) 9:23

Table 4 Articles most likely to be visited in a 2-hop session and their respective in starting from three
different Wikipedia articles according to the various approaches

(a) Starting from the article entitled ‘Machine Learning’

 

Page 17 of 26

Hop #1 article Indegree Hop #2 Indegree
Ground truth Artificial neural network 597 Backpropagation 88
Random Databricks 9 O'Reilly Media 725
Indegree International Standard Book Number 574,335 Integrated Authority File 200,124
Jaccard Random forest 108 Out-of-bag error 4
Traffic Stanford University 12,462 United States 547,915
PageRank International Standard Book Number 574,335 Digital object identifier 574,335
ChoiceRank International Standard Book Number 574,335 Canada 167,535
REVERSEPR Deep learning 154 AlphaGo 13
(b) Starting from the article entitled ‘Stephen Hawking’
Hop #1 Indegree Hop #2 Indegree
Ground truth Jane Hawking 19 Lucy Hawking 37
Random National Health Service 2268 — Gibraltar Health Authority 11
Indegree International Standard BookNumber 574,335 Integrated Authority File 200,124
Jaccard Roger Penrose 637 Alexander Friedmann 251
Traffic Facebook 20,403 ~—_ United States 547,915
PageRank International Standard Book Number 574,335 Digital Object Identifier 169
ChoiceRank World War II 122,312 United States 547,915
REVERSEPR The Big Bang Theory 1125 The Big Bang Theory (season 9) 60
(c) Starting from the article entitled ‘Brie Larson’
Hop #1 Indegree Hop #2 Indegree
Ground truth Alex Greenwald 57 Phantom Planet 215
Random Meryl Streep 2060 Tracey Ullman 590
Indegree United States 54,7915 Geographic coordinate system 962,772
Jaccard Greer Garson 409 Jennifer Jones 312
Traffic United States 547,915 Ronald Reagan 9079
PageRank United States 547,915 International Standard Book Number 547,335
ChoiceRank United States 547,915 Daylight saving time 204,750
REVERSEPR Room (2015 film) 142 List of accolades received by Room 2

page visits since the deep learning—empowered AlphaGo versus Lee Sedol match took
place exactly during the investigated time period (March 2016). This increased temporal
interest towards AlphaGo is also reflected in the prediction of REVERSEPR.

The most likely transition provided by REvERSEPR starting from the article about
Stephen Hawking can also be explained by temporal dynamics, as Stephen Hawking had
a cameo in the U.S. sitcom The Big Bang Theory aired on February 25, 2016, which likely
increased the respective click-through rate during March 2016. Similarly, the most likely
two-hop session predicted by REVERSEPR starting from the Wikipedia article on Brie Lar-
son reflects the fact that Brie Larson was awarded as the Best Actress on the Academy
Awards on February 28, 2016 for her role in the 2015 movie called Room.

Further inspection of Table 4 reveals that multiple baseline approaches favor transi-
tions towards nodes with high indegree centrality, such as International Standard Book
Number. Not pruning such articles from the network was a deliberate choice in order to
preserve the original topology of the network—instead of applying any pragmatic (but
somewhat arbitrary) removal of vertices from the network. Table 4 also suggests that RE-
VERSEPR seems to treat nodes with high indegree centrality but low interest better than

the baseline approaches.
Berend EPJ Data Science (2020) 9:23 Page 18 of 26

6.3.5 Experiments on the Clickstream subnetwork of Wikipedia

In our experiments reported so far, we were relying on the entire Wikipedia network from
March 2016. Our motivation for not performing any filtering of the vertices or the hyper-
link structure was to preserve the “natural topology” of Wikipedia.

The Wikipedia network used in [23] covers the same period of time (March 2016), how-
ever, it contains only that subnetwork of the entire Wikipedia that is included in the Click-
stream dataset [42]. As a consequence, the Wikipedia network included in [23] is approxi-
mately one order of magnitude smaller compared to the network we have reported our ex-
periments earlier. The subnetwork that is purely based on the Clickstream dataset consists
of approximately 2.32 * 10° nodes and 1.32 « 10’ hyperlinks, whereas the entire Wikipedia
network from the same period consists of 1.23 « 10’ articles and 3.87 « 10° hyperlinks,
respectively.

For the sake of better comparability with the experimental protocol included in [23],
we additionally report another set of experiments on the English Wikipedia. This time we
restricted our (sub)network to be the one that is included in the Clickstream dataset in
the exact same manner how it was done in [23].

The gold standard transition probabilities for our previous experiments also relied on
the same Clickstream data, however, the network itself could also include such nodes and
edges that were not necessarily part of the Clickstream dataset. For our Clickstream exper-
iments, we determined z* proportional to the traffic that was included in the Clickstream
dataset.

We include the results according to the different metrics obtained for the Clickstream
network in Fig. 8. Comparing Fig. 8 with Fig. 5—which includes the same metrics over
the large Wikipedia network—we can see similar trends. That is, we can observe that RE-

 

  

ia >
= == Random w= PageRank
wo (= Indegree —® ChoiceRank
5 4 =~ Jaccard ~=@ ReversePR
8 ~
Q
n £
oS &
oO S
@ £
oD —@ Random == PageRank 3
S == Indegree = ChoiceRank
S = Jaccard ~—®@= ReversePR
O =—w— Traffic
101 102 103 10! 102 10
Out-degree Out-degree

= Random w= PageRank

== Indegree —® ChoiceRank
et Jaccard ~=@ ReversePR
ewe Traffic

= Random =e PageRank

=~ Indegree —#® ChoiceRank
=~ Jaccard ~~ ReversePR
—w— Traffic

Cumulated RMSE
Cumulated MRR

  

10+ 102 103 10+ 102 10
Out-degree Out-degree

Figure 8 Cumulated evaluation metrics for the Clickstream click-stream dataset. Scores are reported as a

function of the outdegree of the network
Ne /

 

 
Berend EPJ Data Science (2020) 9:23 Page 19 of 26

VERSEPR either performs better than alternative approaches by a fair margin or it is com-
petitive to the rest of the approaches. The metrics are in general better compared to the
ones received for the entire Wikipedia network, which can be explained by the fact that
the vertices in the Clickstream network have fewer connections on average and making

predictions for a smaller-sized neighborhood is an easier task in general.

6.4 Experiments on the Kosarak dataset

The Kosarak dataset® includes anonymized click-stream data of a Hungarian on-line news
portal including 41,001 vertices and 974,560 edges based on 7,029,013 clicks. Kosarak is
similar to Wikipedia in the sense that it also contains web traffic data, but it also differs
from the Wikipedia in important aspects.

On the one hand, the size of the network induced from the Kosarak dataset is substan-
tially smaller from the hyperlink network of Wikipedia. Additionally, in the case of Kosarak
dataset we see a much higher density of edges in contrast to the Wikipedia network, i.e.,
the fraction of edges and the number of potential edges in the network is 1 « 10~° and
4 10° for Kosarak and Wikipedia, respectively.

Figure 9 contains a comparison of all the evaluated approaches regarding their perfor-
mances on the various evaluation metrics on the Kosarak dataset. Figure 5 contains the
same evaluation metrics for the Wikipedia network, which lets us compare the two results.

Besides the resemblances, we can also spot differences between Fig. 5 and Fig. 9. We
can see for instance that the Traffic baseline achieves the best MRR score on the Kosarak
dataset. In contrast, the Traffic baseline was only performant up to articles with medium

outdegree for Wikipedia. Another difference is that, even though the Jaccard baseline was

 

  

  

( >
< =—® Random = PageRank
o =~ Indegree = ChoiceRank
E 0.35 5 et Jaccard ~—@e ReversePR
v Yv —w— Traffi
oe 0.30 3
2 2
so &
oO =}
% 0.25 a =
© =@= Random =e PageRank O
SD 0.20 =~ Indegree = ChoiceRank
L = Jaccard =e ReversePR
O —w<— Traffic
0.15
10? 102 103 104 10? 102 103 104
Out-degree Out-degree
=—® Random =e PageRank =—® Random PageRank
~~ Indegree =a ChoiceRank =~ Indegree ChoiceRank
HW =~ Jaccard ~=@ = ReversePR <os =~ Jaccard ReversePR
= —w— Traffic = =—w— Traffic
ao
To
o 2
WW
© £ 0.6
Ss =}
€ E
5 3
O O
0.4
10+ 102 10 104 101 102 103 104
Out-degree Out-degree
Figure 9 Cumulated evaluation metrics for the Kosarak click-stream dataset. Scores are reported as a function
of the outdegree of the network

 

 

X /
Berend EPJ Data Science (2020) 9:23 Page 20 of 26

among one of the best performing baselines for Wikipedia, it fails to deliver reasonable
performance on the Kosarak network.

The differences can potentially be explained by the different nature of the two networks,
ie., Wikipedia has a different link structure due to its encyclopedic nature compared to the
Kosarak network which was obtained from a news portal. A further important difference
is that the Kosarak network contains only such edges that were included in at least one
browsing session. In contrast when experimenting with Wikipedia, we could exploit the
entire network structure (including those links which received no actual traffic). To this
end, we regard the Wikipedia-based evaluation more meaningful and realistic.

Interestingly, the performance of the Jaccard approach is similarly good for the Kosarak
dataset when evaluated in terms of displacement. What it means that the transition prob-
abilities inferred from the Jaccard heuristic is generally applicable, even though it is not so
reliable for determining the single most likely transition compared to other approaches.

The Traffic baseline behaves oppositely on the Kosarak network, i.e., it has the highest
displacement scores even though it was performing best in terms of MRR. Again, what
it means, that the setting the transition probabilities could help us find the most likely
transition, but in general the ranking it provides might be quite dissimilar from reality.

ChoiceRank provides a steady performance on the Kosarak dataset regarding those eval-
uations which solely focus on the ranking of the neighboring vertices, as it obtains one
of the highest MRR and a medium displacement score. REVERSEPR has lower MRR and
higher displacement values, however, it outperforms all alternative approaches in terms
of the KL-divergence and RMSE metrics by a fair margin. These metrics can be viewed
more informative for the Kosarak dataset, where providing a good ranking is much easier
in most of the cases, since the median outdegree in this network is only 4. This peculiarity
of the dataset explains the good performance of the Random baseline as well.

6.5 Experiments on Citibike network

We additionally considered the dataset of the bicycle sharing system of New York City .'
The dataset contains sessions of bike rentals in the form of pick-up and drop-off station
pairs with additional meta-data such as the date and duration for the rides.

We conducted our evaluation on the same network as in [23]. That is, we relied on those
source—target destination pairs from year 2015 for which the duration of the ride was no
more than one hour and which had an average daily ride frequency of at least one. In the
end, we were dealing with a network of 489 nodes and 5209 edges from 3.38 million bike
rentals in total.

We performed the same quality assessment on the Citibike network as for the previ-
ously analyzed networks. The result of our evaluation is depicted in Fig. 10. Note that
REVERSEPR refers to our algorithm using its default version, i.e., when © gets initialized
to all zeros and the damping factor f equals 0.01.

Figure 10 illustrates that REveRSEPR scored the best for all evaluation metrics when
evaluated on the Citibike network. ChoiceRank obtains the second best results for all
evaluation criteria, whereas the Jaccard baseline was hardly able to beat even the Ran-
dom baseline for this network. This observation verifies our anticipation for the Jaccard
baseline being less suitable for transportation networks (cf. Sect. 6.1).

By looking at Fig. 10, we can additionally notice that the simple strategy of predicting
transition probabilities proportional to the indegree of the neighboring vertices provides
Berend EPJ Data Science (2020) 9:23 Page 21 of 26

 

  

  

 

( >)
= =—@= Random == PageRank
Vv =~ Indegree = ChoiceRank
5 4 = Jaccard w= ReversePR
8 s
Q
a g
Oo &
5 5
@ &
© == Random PageRank 5
S == Indegree —®= ChoiceRank v
= = Jaccard ~e ReversePR
O —w=— Traffic
10! 102 10! 102
Out-degree
=—@- Random we PageRank we PageRank
=~ Indegree —#® ChoiceRank =~ Indegree —#® ChoiceRank
i =~t= Jaccard == ReversePR co = Jaccard ~~ ReversePR
= S =< Traffic = Traffic
ac
Oo
o 2
WwW
© &
Ss 3
3
e £
Ss 5
O UO
10! 102 10! 102
Out-degree Out-degree
Figure 10 Cumulated evaluation metrics for the Citibike bike sharing network. Scores are reported as a
function of the outdegree of the network

 

XN S

a strong baseline for the Citibike data. Recall that the same baseline had one of the poorest
performances when evaluated on the Wikipedia network (cf. Fig. 5).

We can thus conclude that the dynamics driving the bike rental network makes different
baselines perform better in predicting the transition probabilities compared to web traffic
networks.

In summary, the relative performance of the individual alternative baseline approaches
vary remarkably based on the peculiarities of the analyzed networks, whereas REVERSEPR
always performed close to or better than the best performing baseline approach in all of

our experiments.

6.6 Summary of the experimental results

We finally provide a thorough comparison between REVERSEPR and ChoiceRank in Ta-
ble 5 both in terms of their speed and overall performance on the three notably different
network structures we conducted our experiments on. The median outdegree was 61 for
the Wikipedia network and 4 in the case of Kosarak and Citibike datasets.

Table 5(a) summarizes the most important statistics of the different input networks
alongside with the amount of time needed for the different approaches to approximate
the transition probabilities. We can see that the size and density of the three input net-
works differ substantially, and that REVERSEPR managed to find a solution at least 1.5-
2 times faster than ChoiceRank. We find it remarkable—even if part of the speedup
could be accounted for the usage of different programming languages for implementa-
tion (Python for ChoiceRank and Java for REVERSEPR)—since the optimization problem
solved by ChoiceRank involves only |V| parameters, whereas REVERSEPR optimizes over
O(|E|) >> |V| parameters.
Berend EPJ Data Science (2020) 9:23 Page 22 of 26

Table 5 A comparative summary of ChoiceRank and REVERSEPR (abbreviated as CR and RPR,
respectively)

(a) Network properties and the running times of the algorithms

[V| |E| Density (ip) CR running time RPR running time
Wikipedia 1 * 107 ~ 4 108 3% 10° 5500 seconds 2400 seconds
Clickstream ~ 2x 10° = 1* 107 3% 10° 240 seconds 30 seconds
Kosarak ~ 4x 104 = 1 * 10° 6 «10+ 4 seconds 2.5 seconds
Citibike ~5 * 102 ~5x* 10° 2* 107 0.3 seconds 0.05 seconds

(b) The mean values of the different evaluation criteria for the different networks. Bold means better result and
significant differences (p <« 0.01) are marked with +

Displacement KL divergence RMSE MRR

CR RPR CR RPR CR RPR CR RPR
Wikipedia 0.325 0.307' 3.005 2.866" 0.101 0.099" 0.094 0.257"
Clickstream 0.299 0.276' 0.242 0.2417 0.0979 0.0978" 0.447 0.517'
Kosarak 0.332' 0.359 0.885 0.4807 0.150 0.074" 0.554" 0.431

Citibike 0.267 0.264 0.073 0.070 0.047 0.045 0.439 0.447

(c) The median values of the different evaluation criteria for the different networks. Bold means better result and
significant differences (p <« 0.01) are marked with +

Displacement KL divergence RMSE MRR

CR RPR CR RPR CR RPR CR RPR
Wikipedia 0.324 0.3097 2.946 2.776! 0.074 0.073" 0.045 0.1117
Clickstream 0.326 0.2817 0.1433 0.1431 0.076 0.076 0.333 0.5'
Kosarak 0.344' 0.378 0.703 0.210 0.116 0.046" 0.5! 0.333

Citibike 0.283 0.267 0.059 0.058 0.030 0.028 0.333 0.333

Table 5(b) and Table 5(c) compares the mean and median of the evaluation measures
that were calculated over all the vertices of the different networks with an out-degree larger
than one. We can see that other than for the displacement and MRR scores on the Kosarak
dataset, the performance of REVERSEPR is as good as that of ChoiceRank for every com-
bination of evaluation criteria and input network.

We additionally performed paired t-test and Mood’s median test to see whether the
differences in the mean and median scores of the two approaches differ statistically sig-
nificantly. We indicate it in Table 5(b) and Table 5(c) when the difference was found to
be significant (with p < 0.01). Using the non-parametric Wilcoxon sign test for checking
whether the differences of the means over the vertices differ significantly also yielded the
same results. We can see from Table 5(b) and Table 5(c) that all differences are signifi-
cant for the click-stream networks, i.e., the Wikipedia, Clickstream and Kosarak datasets.
ChoiceRank performed significantly better for the displacement and MRR metrics in the
case of the Kosarak, whereas the metrics obtained by REVERSEPR were significantly better
in all the other cases. For the smallest network, the mean and median evaluation metrics
did not differ significantly between the two algorithms, which was likely due to the small
size of the network.

As a final comparative assessment, we provide the distribution of the per vertex evalua-
tion scores for all the datasets as a box plot in Fig. 11. We can observe that REVERSEPR is
indeed among the best performing models for all networks and evaluation metrics, both
in terms of its median and average scores. The advantage of REVERSEPR is the most pro-
nounced for the KL-divergence and RMSE metrics, i.e., in those cases when the actual
transitional probabilities are also considered not just the ranking they specify.
Berend EPJ Data Science (2020) 9:23

 

0.320f —— TT 0.5

ahs | ss mate: “dhs;

oe carte caffi aank cant et om 2 cat, caffe ear ork 2% catty caffe ack ert 0 cee cal, offi an
aor Sse pacer a8 «goto otae gots "se joc 18 eae pace aor med — aa «oho cet gan) des yar 1 pane

»
°

9 “Placement
o
KL divergence

oN & DO

OK oP
een =

(a) Displacement for Wikipedia (b) KL divergence for Wikipedia (c) RMSE for Wikipedia (d) MRR for Wikipedia

mo ss nbs

displacement
KL divergence
or rYF NON Ow
ou ow oO
RMSE

uu

oM ree aro, Ac nk nk oP OM ret card, affic nk nk oe oO cel avo, AC nk nk pRB on ree ard, «fic ak nk op
ane? ded" acc?" 11? pase noi Revere gore? ded vere we pad enol cover ers8 poe?| ged vere °e p20 noi cover es paTer aed face*N 112 pavehs ciceRever™
(e) Displacement for Clickstream (f) KL divergence for Clickstream (g) RMSE for Clickstream (h) MRR for Clickstream
0.5 6 1.0
0.4 ° 0.8
€ 24
£03 Fy w 0:6
o £3 Lo «
u o =0
& 2 « &
6.0.2 = 0.4
3 z?
0.1 1 0.2
0.0 0 0.0
ane’, mes" ec?" Scaife 08 athe ale ora tate ce cok ane, media a8 otha" ae onal gnats oh corey ve sere aane? al ceo oho se cok ane’, mes xc atte 08 Ratha ale ora gate coy Ne coer
(i) Displacement for Kosarak (j) KL divergence for Kosarak (k) RMSE for Kosarak (1) MRR for Kosarak
08 0.30 1.0
0.25 os
x o 0.6
g° ¥ 0.20
E g w a)
© 0.3 2 vn
3 $04 $0.15 a
z.. 3 0.4
gy Z 0.10
0.2
0.05 0.2
0.0 | 0.00 0.0
oKfic R R attic 3 ffic
gard gea"{acc" ow ach anceRy Ne se qardrge "face" Or castie panera cetayesse™ gard gece" 1a och cetNerserh gare geo" accra paseo Ne cer
(m) Displacement for Citibike (n) KL divergence for Citibike (o) RMSE for Citibike (p) MRR for Citibike

Figure 11 Box plots of the different evaluation metrics aggregated over the vertices of various datasets. The

 

 

triangles mark average scores
L J

The primarily benefit of REVERSEPR resides in its efficiency. As also highlighted in Ta-
ble 5(a), REVERSEPR delivered a 2—8-fold speedup compared to ChoiceRank, with its eval-
uation metrics being competitive or even better than those of ChoiceRank. We achieved
the above—mentioned speedup despite of the fact that our approach has O(|E|) parame-
ters, whereas ChoiceRank operates with O(|V|) < O(|E|) parameters.

As our model involves O(| V|) constraints, i.e., one for each element of 2*, having O(|E})
variables makes our problem formulation overparameterized. This overparameterization
allows us more flexibility in determining the p,,, transitional probabilities, however, it also
means that multiple configuration of edge weights could yield the same objective value.
A potential solution for mitigating this phenomenon could be to incorporate various reg-
ularization terms into the objective for encouraging the transitional probabilities to have
a high entropy, or to the contrarily incentivize finding highly skewed transitional proba-
bilities. The kind of inductive bias conveyed by the regularizers could be determined in
an application specific manner. We treat the incorporation of various regularizes as a po-

tential future extension of our proposed algorithm.

7 Conclusion
We introduced an efficient algorithm for estimating the transition probabilities between
the nodes of complex networks given some desired PageRank distribution over its vertices.

We managed to solve the problem by formulating an approximate gradient of our objective

Page 23 of 26
Berend EPJ Data Science (2020) 9:23 Page 24 of 26

function which can be calculated in linear time relative to the number of edges in the
graph.

We illustrated the effectiveness of our proposed approach by evaluating it towards dif-
ferent clickstream datasets (the entire English Wikipedia, its filtered version according to
the Clickstream dataset and a Hungarian news portal) and the bicycle sharing network of
New York City.

Our proposed algorithm can be naturally extended to other networks, such as citation
and collaboration networks, as it does not require any additional meta-data behind the

network topology and a desired PageRank distribution.

Acknowledgements
The author is grateful for the funding agencies. The author is also grateful for Gyula Kovacs (funded by
EFOP-3.6.3-VEKOP-16-2017-0002) for providing code snippets for processing Wikipedia dumps.

Funding

This work was supported by the National Research, Development and Innovation Office of Hungary through the Artificial
Intelligence National Excellence Program (grant no.: 2018-1.2.1-NKP-2018-00008). This research was supported by the
project “Integrated program for training new generation of scientists in the fields of computer science’, no
EFOP-3.6.3-VEKOP-16-2017-0002. The project has been supported by the European Union and co-funded by the
European Social Fund. This research was supported by grant TUDFO/47138-1/2019-ITM of the Ministry for Innovation and
Technology, Hungary. Publication costs were covered by the University of Szeged Open Access Fund (application

id. 4100).

Abbreviations
REVERSEPR, Reverse PageRank; KL-divergence, Kullback—Leibler divergence; L-BFGS, limited-memory
Broyden-Fletcher—Goldfarb-Shanno; RMSE, Root Mean Squead Error; MRR, Mean Reciprocal Rank.

Availability of data and materials

The source code and the datasets including the Wikipedia dump, Wikipedia Clickstream dataset, the Kosarak dataset and
the Wikipedia network we were using, as well as the Citibike data used in our experiments are available on-line and also
from the author upon request.

Competing interests
The authors declare that they have no competing interests.

Authors’ contributions
GB designed the study and experiments, implemented the proposed algorithm and wrote the paper. All authors read
and approved the final manuscript.

Endnotes
2 https://github.com/lucasmaystre/choix

https://archive.org/details/enwiki- 20160305
https://dumps.wikimedia.org/other/pagecounts-raw/
http://rgail .inf'u-szeged.hu/~berend/wikipedia_data/
http://Aimi.uantwerpen.be/data/

The dataset is available fromhttps://www.citibikenyc.com/system-data

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Received: 22 July 2019 Accepted: 14 July 2020 Published online: 29 July 2020

References

1. Matakos A, Terzi E, Tsaparas P (2017) Measuring and moderating opinion polarization in social networks. Data Min
Knowl Discov 31(5):1480-1505. https://doi.org/10.1007/s10618-017-0527-9

2. Backstrom L, Leskovec J (2011) Supervised random walks: predicting and recommending links in social networks. In:
Proceedings of the fourth ACM international conference on web search and data mining. WSDM '11. ACM, New York,
pp 635-644. https://doi.org/10.1145/1935826.1935914

3. Leskovec J, Huttenlocher D, Kleinberg J (2010) Predicting positive and negative links in online social networks. In:
Proceedings of the 19th international conference on world wide web. WWW ‘10. ACM, New York, pp 641-650.
https://doi.org/10.1145/1772690.1772756

4. Newman MEJ (2001) The structure of scientific collaboration networks. Proc Natl Acad Sci USA 98(2):404—409,
http://www.pnas.org/content/98/2/404 fullodf+html. https://doi.org/10.1073/pnas.98.2.404

 
Berend EPJ Data Science (2020) 9:23 Page 25 of 26

20.

21.

22.

23.

24.

25.

26.

2/.
28.
29,

30.

31.

32.

33.

34.

Hasan MA, Chaoji V, Salem S, Zaki M (2006) Link prediction using supervised learning. In: Proc. of SDM 06 workshop
on link analysis, counterterrorism and security

Tasnadi E, Berend G (2015) Supervised prediction of social network links using implicit sources of information. In:
Proceedings of the 24th international conference on world wide web. WWW '15 companion. International World
Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, pp 1117-1122.
https://doi.org/10.1145/2740908.2743037

Erkan G, Radev DR (2004) LexRank: graph-based lexical centrality as salience in text summarization. J Artif Intell Res
22(1):457-479

Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Lin D, Wu D (eds) Proceedings of EMNLP 2004.
Association for Computational Linguistics, Barcelona, pp 404-411. http://www.aclweb.org/anthology/W04-3252
Toutanova K, Manning CD, Ng AY (2004) Learning random walk models for inducing word dependency distributions.
In: Brodley CE (Ed) ICML. ACM international conference proceeding series, vol 69. ACM, New York

Ji M, He Q, Han J, Spangler S (2015) Mining strong relevance between heterogeneous entities from unstructured
biomedical data. Data Min Knowl Discov 29(4):976-998. https://doi.org/10.1007/s10618-014-0396-4

. WuG, Xu W, Zhang Y, Wei Y (2013) A preconditioned conjugate gradient algorithm for GeneRank with application to

microarray data mining. Data Min Knowl Discov 26(1):27—-56. https://doi.org/10.1007/s10618-011-0245-7
Page L, Brin S, Motwani R, Winograd T (1998) The PageRank citation ranking: bringing order to the web. In:
Proceedings of the 7th international world wide web conference, Brisbane, Australia, pp 161-172.
citeseer.nj.nec.com/page98pagerank.htm|

. Gydngyi Z, Garcia-Molina H, Pedersen J (2004) Combating web spam with Trustrank. In: Proceedings of the thirtieth

international conference on very large data bases, VLDB '04, vol 30. VLDB Endowment, Toronto, pp 576-587.
http://dl.acm.org/citation.cfm?id=1316689.1316740

Jeh G, Widom J (2002) Simrank: a measure of structural-context similarity. In: Proceedings of the eighth ACM SIGKDD
international conference on knowledge discovery and data mining. KDD ‘02. ACM, New York, pp 538-543.
https://doi.org/10.1145/775047.7751 26

. Richardson M, Domingos P (2002) The intelligent surfer: probabilistic combination of link and content information in

PageRank. Advances in neural information processing systems, vol 14. MIT Press, Vancouver

Hwang H, Hristidis V, Papakonstantinou Y (2006) ObjectRank: a system for authority-based search on databases. In:
Proceedings of the ACM SIGMOD international conference on management of data, Chicago, Illinois, USA, June
27-29, 2006, pp 796-798. https://doi.org/10.1145/1142473.1142593

Nemirovsky D, Avrachenkov K (2008) Weighted PageRank: cluster-related weights. In: Voorhees EM, Buckland LP (eds)
TREC, vol. Special publication 500-277, National Institute of Standards and Technology (NIST), Gaithersburg.
http://dblp.uni-trier.de/db/conf/trec/trec2008.html#NemirovskyA08

Tsoi AC, Morini G, Scarselli F, Hagenbuchner M, Maggini M (2003) Adaptive ranking of web pages. In: Proceedings of
the twelfth international world wide web conference, WWW 2003, Budapest, Hungary, May 20-24, 2003,

pp 356-365. https://doi.org/10.1145/775152.775203

Haveliwala TH (2002) Topic-sensitive PageRank. In: Proceedings of the 11th international conference on world wide
web. WWW ‘02. ACM, New York, pp 517-526. https://doi.org/10.1145/511446.511513

Ding Y (2011) Applying weighted PageRank to author citation networks. J Am Soc Inf Sci Technol 62(2):236-245.
https://doi.org/10.1002/asi.21452

Xing W, Ghorbani A (2004) Weighted PageRank algorithm. In: Communication networks and services research, 2004.
Proceedings. Second annual conference on, pp 305-314. https://doi.org/10.1109/DNSR.2004.1344743.
http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1344743&tag=1

Kumar R, Tomkins A, Vassilvitskii S, Vee E (2015) Inverting a steady-state. In: Proceedings of the eighth ACM
international conference on web search and data mining. WSDM '15. ACM, New York, pp 359-368.
https://doi.org/10.1145/2684822.2685310

Maystre L, Grossglauser M (2017) Choicerank: identifying preferences from node traffic in networks. In: Proceedings
of the 34th international conference on machine learning. ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,

pp 2354-2362. http://proceedings.mlr.press/v70/maystrel 7b.html

Luce RD (1977) The choice axiom after twenty years. J Math Psychol 15(3):215-233

Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E (1953) Equation of state calculations by fast
computing machines. J Chem Phys 21(6):1087-1092. https://doi.org/10.1063/1.16991 14

Hastings WK (1970) Monte Carlo sampling methods using Markov chains and their applications. Biometrika
57(1):97-109. http://biomet.oxfordjournals.org/cgi/reprint/57/1/97.pdf. https://doi.org/10.1093/biomet/57.1.97

Bai DS (1975) Efficient estimation of transition probabilities in a Markov chain. Ann Stat, 3:1305-1317

Barsotti F, De Castro Y, Espinasse T, Rochet P (2014) Estimating the transition matrix of a Markov chain observed at
random times. Stat Probab Lett 94:98-105

Morimura T, Osogami T, Ide T (2013) Solving inverse problem of Markov chain with partial observations. In: Burges
CJC, Bottou L, Welling M, Ghahramani Z, Weinberger KQ (eds) Advances in neural information processing systems,
vol 26. Curran Associates, Red Hook, pp 1655-1663

Adafre SF, de Rijke M (2005) Discovering missing links in Wikipedia. In: Proceedings of the 3rd international workshop
on link discovery. LinkKDD ‘05. ACM, New York, pp 90-97. https://doi.org/10.1145/1134271.1134284

Paranjape A, West R, Zia L, Leskovec J (2016) Improving website hyperlink structure using server logs. In: Proceedings
of the ninth ACM international conference on web search and data mining. WSDM '16. ACM, New York, pp 615-624.
https://doi.org/10.1145/2835776.2835832

Lamprecht D, Dimitrov D, Helic D, Strohmaier M (2016) Evaluating and improving navigability of Wikipedia: a
comparative study of eight language editions. In: Proceedings of the 12th international symposium on open
collaboration. OpenSym ‘16. ACM, New York, Article no 17, pp 1-10. https://doi.org/10.1145/2957792.2957813
Singer P Lemmerich F, West R, Zia L, Wulczyn E, Strohmaier M, Leskovec J (2017) Why we read Wikipedia. In:
Proceedings of the 26th international conference on world wide web, pp 1591-1600. International World Wide Web
Conferences Steering Committee

West R, Leskovec J (2012) Human wayfinding in information networks. In: Proceedings of the 21st international
conference on world wide web. WWW ‘12. ACM, New York, pp 619-628. https://doi.org/10.1 145/2187836.2187920

 
Berend EPJ Data Science (2020) 9:23 Page 26 of 26

35.

36.

37.

38.

39.

40.

41.
42.

West R, Pineau J, Precup D (2009) Wikispeedia: an online game for inferring semantic distances between concepts. In:
Proceedings of the 21st international joint conference on artificial intelligence. UCAI’09. Morgan Kaufmann, San
Francisco, pp 1598-1603. http://dl.acm.org/citation.cfm?id=1661445.1661702

Lawrence E, Michailidis G, Nair V, Xi B (2006). Network tomography: a review and recent developments

Vardi Y (1996) Network tomography: estimating source-destination traffic intensities from link data. J Am Stat Assoc
91(433):365-377

Castro R, Coates M, Liang G, Nowak R, Yu B (2004) Network tomography: recent developments. Stat Sci
19(3):499-517. https://doi.org/10.1214/088342304000000422

Andrew AL (1978) Convergence of an iterative method for derivatives of eigensystems. J Comput Phys
26(1):107-112. https://doi.org/10.1016/0021-9991(78)90102-X

Liu DC, Nocedal J (1989) On the limited memory bfgs method for large scale optimization. Math Program
45(3):503-5 28. https://doi.org/10.1007/BF01589116

McCallum AK (2002) MALLET: a machine learning for language toolkit. http://www.cs.umass.edu/~mccallum/mallet
Wulezyn E, Taraborelli D (2017) Wikipedia Clickstream. https://doi.org/10.6084/m9.figshare.1305770.v22.
https://figshare.com/articles/Wikipedia_Clickstream/1 305770

 

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
