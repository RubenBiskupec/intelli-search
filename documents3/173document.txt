Kosarwal et al. BMC Bioinformatics (2020) 21:515

https://doi.org/10.1186/s12859-020-03668-2 R M| C B l O l nfo rm ati CS

METHODOLOGY ARTICLE Oy else =e

Novel domain expansion methods to ®
improve the computational efficiency of ~
the Chemical Master Equation solution for

large biological networks

Rahul Kosarwal'*, Don Kulasiri'* and Sandhya Samarasinghe'”

 

 

* Correspondence: Don.Kulasiri@
lincoln.ac.nz

"Centre for Advanced . , , , ,
Computational Solutions (C-fACS), Background: Numerical solutions of the chemical master equation (CME) are important

Lincoln University, Lincoln, for understanding the stochasticity of biochemical systems. However, solving CMEs is a

Christchurch, New Zealand formidable task. This task is complicated due to the nonlinear nature of the reactions and

“Complex Systems, Big Data, and the size of the networks which result in different realizations. Most importantly, the

Informatics Initiative (CSBII), Lincoln ‘ ’

University, Lincoln, Christchurch, exponential growth of the size of the state-space, with respect to the number of different

New Zealand species in the system makes this a challenging assignment. When the biochemical system
has a large number of variables, the CME solution becomes intractable. We introduce the
intelligent state projection (/SP) method to use in the stochastic analysis of these systems.
For any biochemical reaction network, it is important to capture more than one moment:
this allows one to describe the system's dynamic behaviour. /SP is based on a state-space
search and the data structure standards of artificial intelligence (Al). It can be used to
explore and update the states of a biochemical system. To support the expansion in /SP, we
also develop a Bayesian likelihood node projection (BLNP) function to predict the likelihood
of the states.

Abstract

Results: To demonstrate the acceptability and effectiveness of our method, we apply the
ISP method to several biological models discussed in prior literature. The results of our
computational experiments reveal that the /SP method is effective both in terms of the
speed and accuracy of the expansion, and the accuracy of the solution. This method also
provides a better understanding of the state-space of the system in terms of blueprint
patterns.

Conclusions: The /SP is the de-novo method which addresses both accuracy and
performance problems for CME solutions. It systematically expands the projection space
based on predefined inputs. This ensures accuracy in the approximation and an exact
analytical solution for the time of interest. The /SP was more effective both in predicting the
behavior of the state-space of the system and in performance management, which is a vital
step towards modeling large biochemical systems.

Keywords: Biochemical reaction networks, chemical master equation, stochastic, intelligent
state projection, Bayesian likelinood node projection

© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which
permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to
the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The

images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise
in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright
holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain
Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless
otherwise stated in a credit line to the data.

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 2 of 42

Background

In systems biology, it is crucial to understand the dynamics of large and complicated
biochemical reaction networks. Recent advances in computing and mathematical tech-
niques mean it is easier for biologists to deal with enormous amounts of experimental
data, right down to the level of a single molecule of a species. Such information reveals
the presence of a high level of stochasticity in the networks of biochemical reactions. In
biochemical reaction networks, stochastic models have made significant contributions
to the fields of systems biology [1, 2], neuroscience [3], and drug modeling [4].

In a complex system, biochemical reactions are often modeled as reaction rate equa-
tions (RREs) using ordinary differential equations (ODEs). Examples of this kind of
work include the biochemical networks of Alzheimer's disease (AD) [5]; the pathways
in the fungal pathogen Candida albicans [6]; and the COVID-19 coronavirus pathogen
network [7]. In each of these examples, the behavior of different pathways is still largely
unknown. All these models only contain species with small copy numbers and widely
different reaction rates; the probabilistic descriptions of time evolution of molecular
concentrations (or numbers) are more suited for understanding the dynamics of such
systems. One probabilistic approach for modeling a biochemical reaction network is to
deduce a set of integro-differential equations known as chemical master equations
(CMEs) [8, 9]. CMEs describe the evolution of the probability distribution over the en-
tire state-space of a biochemical system that jumps from one set of states to another
set of states in continuous time: they are a continuous time version of Markov chains
(CTMCs) [8, 10] with discrete states. By defining the Markov chain [10, 11], we can
consider the joint and marginal probability densities of the species in a system that
changes over time [12].

In such cases, the development of RREs with molecular numbers becomes very im-

portant. The biochemical reaction network can be defined in terms of the discrete state
X = (x, +2)
>1. (X(t): t< K; p} defines a stochastic process, where K is the indexing scheme and 9 is

T LG ; ae -
vector of non-negative integers “x for the given conditions, where N

the sample space. Following the derivation in [9], for every reaction, there exists a reac-
tion channel, Ry, which determines the unique reaction in the system with a propensity
function kj,. The specific combinations of the reactant species in Ry, will react during
an infinitesimal [¢, ¢ + dt) time interval. The average probability a,(X(¢))dt of a particular
Ry fires within [t,¢+ dt) is the multiplication of the numbers of reactant species, de-
noted by square brackets, by ky. For example,

Ric “SA ay:ky ({C])
Rx:A+E“3 T, ay:ky(\A].[E})
R3:T 3 C, a3:ks((T])

k
In the case where the reactants are of the same type, for example A +A-—> T,, then

dgtky(Al4—V)), The set consisting of all the reaction channels, Ry, is the union of sets
of fast reactions and slow reactions [12]. They are categorized into sets of Rag) and
Ryysr) reactions, respectively, based on their propensity values. Therefore,

Ry = Rus) Ruisr)- (1)
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 3 of 42

A reaction is faster than others if its propensity is of several orders of magnitude lar-
ger than the other propensity values (see the list of abbreviations and notations at the
end).

Chemical master equation
In this paper, we consider a network of biochemical reactions at a constant volume.

The network consists of N >1 different species {5j, Sey} They are spatially homoge-

neous and interact through M =1 reaction channels in thermal equilibrium. The num-
ber of counts of each different species defines the state of the system. If all the species

are bounded by S, then the approximate number of states in the system would be sN
[13]. Each state X = («1, ee
species. For every state, X, the probability satisfies the following CME [8],

= Dy aulX— vs) POC (X - v4) - Yai (2)

where P(X) = the probability function, representing the time-evolution of the system,
given that f> fo and the initial probability is, P“) (Xo),
M = elementary chemical reaction channels Rj, ... .. Rag

“xp denotes the number of molecules (counts) of each

a, = chemical reaction propensity of channel y = {1, 2,.... M}, and

v, = the stoichiometric vector that represents a change in the molecular population

ue
of the chemical species due to the occurrence of one Ry reaction. The system transi-
tions to a new state: X + v, records the changes in the number of counts of different
species when the reactions occur.

We note that a,(X - v,)dt is the probability for state (X — v,) to transition to state X
through chemical reaction, Ry, during [t,t + dt), and 14 (X)dt is the probability
for the system to shift from state X as a result of any reaction during dt. If X; = {X),

beveees xX x is the ordered set of possible states of the system indexed by {1, 2...K} hav-
s

ing SN elements, then Eq. (2) represents the set of ordinary differential equations
(ODEs) that determines the changes in probability density P= (P(X), ... POX wy):
S

Once X;, is selected, the matrix-vector form of Eq. (2) is described by an ODE:

ap
—_ = A pl) 3
ot (3)

where the transition rate matrix is A = [q;,]. If each reaction leads to a different state,

X,, then the elements in submatrix A;; are given as:

Las ),ifi=j
A, = (a
Ay (X Cit X; =Xit+tvy
0, otherwise

This equation represents the infinitesimal generator of the Markov process [10, 14,
15]. Rows and columns are ordered in lowercase letters, i and j respectively. The entry
of a;; of the matrix determines the propensity for the chemical system to transition
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 4 of 42

from one state to another state, given that i ~j, are non-negative. The diagonal terms of

the matrix are defined by a;, when i=j and the matrix has a zero-column sum, so its
probability is conserved. From Eq. (3) we can derive the P“1) probability vector at the

final time, f, of interest given an initial density of pl);

 

pltr) = exp(trA).P, (5)
where the matrix exponential function is defined by the convergent Taylor series as
[16, 17]

“(t fA "
exp(trA) =I + S- ( 4) . (6)

n=1

However, algorithms, such as in [13, 18-20] truncate Eq. (6) infinite summation to
approximate Eq. (3) at the cost of a truncation error.

Initial value problem

If v, or vy, for w or M={1,2,.... M} are the stoichiometric vectors for Ry reaction
channels, then we will define the stoichiometric matrix for the system by V, or
Vas = [V3 Vo} eweee Vul ’ If is the sample space and Xj€@ is the initial state of the
system, X, denotes the only set of states in . To solve P(X) in Eq. (2) for Xeq,
we define the P® vector as(P(X))xeg or (PY (X))yex, for a finite set of states,

then ape is defined as a vector (2B) xe’ Solving the CME involves finding the solu-

tion of the initial value problem over a time period using the differential equation
Eq. (3) when f>0, whereas, P) is the initial distribution at t=0. Here, the sample
space ¢ can be infinite for large biochemical systems. Finding the solution for Eq. (3) for
the given parameters with a finite set of states X; is a major problem for CME’s because in

large biochemical systems the size of A will be extremely large.
For example, consider an enzymatic reaction network [13] described by reactions R:

S+E Ss, C, RoC Ns + E, R3:C Sp + E. This network of reactions involves four spe-
cies: namely, S— substrate, E- enzyme, C-— complex and P- product molecules. The
X = (x1, X2, %3, Xa)" = (S, E, C, P)" represents any state of the system, with Xo = (So, Eo, Co,
Po) given as the initial state. The stoichiometric vectors are given by v, = (-1,-1, 1,0),

vy =(1,1,-1,0), v3=(0,1,-1,1). Therefore, for (%1, Xo, x3, x4) the propensity

“N = 4’
functions are:
Ry : ay (loi), [x2], [3], [va]) = ki x a1 (£) x x2(E)

Ro : a1 ([o1], [%2], [x3], [ea]) = ko x x3 (F)
R3: ay({xy], [x2 [x3], ~4]) — k3 x x3(t)

ue
\e

The set of states reachable from Xp is finite in number. With multiple explosions of
the number of states in a large model, the size of A increases exponentially.

As seen in Eq. (5), solving Eq. (2) becomes a problem when the model’s dimensions
grow due to the increase of species present in the system. This is particularly true for

large biochemical models. The approximate estimate of SN’ shows how the size of the
problem increases. This explosion in size is known as the curse of dimensionality [9,
13]. The CME solution given in Eq. (5) has two major parts: (a) the expansion of the
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 5 of 42

state-space, and (b) the approximation of the series. For the expansion of state-space,
Finite State Projection (FSP) [21] and Sliding Windows (SW) [18] are used to find the
domain. Methods like Krylov subspace [13] and Runge Kutta [22] are commonly used
for approximation (of the series) of the CME Eq. (5).

Although CME has been employed and solved explicitly for relatively small biological
systems [13, 18-20, 23, 24], computationally complaisant but accurate solutions are
still unknown for most significant systems and for large systems which have an
infinite (or a very large) number of states. This lack of closed-form solutions has
driven the system biology research towards Monte-carlo Algorithms (MC) [25] to
capture dynamics. One algorithm, the Stochastic Simulation Algorithm (SSA) by
Gillespie [9], has been used in the CME. Although the original FSP state-space
expansion has been used in research [21, 26], it has some drawbacks [21]. The FSP
[21] and its variants [20, 24, 26, 27] are based on r-step reachability [26]. While
SW [18] is also a FSP based method, it employs a stochastic simulation algorithm
(SSA) to find the domain. This is more effective than FSP and suitable for stiff
problems. Add-on weighting functions like GORDE [28] and likelihoods [24]
methods are used to improve the expansion. FSP GORDE [28] removes the states
with small probabilities before the calculation of Eq. (5). This practice saves com-
putational time, meaning that FSP GORDE performs faster than conventional FSP
r-step reachability. However, removing the probabilities before the calculation of
Eq. (5) increases the steps error and affects the accuracy of the final solution at ty
regardless of whether the state-space is small or large. If one is interested in solv-
ing stiff and/or large systems, it will greatly affect the solution.

The FSP variant, Optimal Finite State Projection (OFSP), [20] based on r-step reach-
ability, performs better in terms of producing optimal order domain. It is faster than
both FSP and FSP GORDE. However, it is infeasible to use SW for large CME problems
because creating hyper-rectangles is a very difficult task. At least four-times the number
of SSA simulations are required to minimize the error by half, because of very low con-
vergence rates of routines in MC . The original SSA takes a long time, because one
simulation may have several different Rj. Recently, the SSA’s efficiency has been greatly
enhanced by researchers through various schemes such as T leaps (adaptive) [29, 30].
Thus, we compare the OFSP and SSA (rt leaps adaptive) against the JSP in terms of
finding the domain, accuracy and computational efficiency. Key to solving the CME
remains in finding the right projection size (domain) for large models which would
then ensure efficient approximation.

In this paper, we focus primarily on developing the expansion strategy, namely
the Intelligent State Projection (ISP) method, to mitigate several problems: the ac-
curacy of the solution, the performance of the method and projection size. The
ISP has two variants: the Latitudinal Search (LAS) and the Longitudinal-Latitu-
dinal Search (LOLAS). It treats the Markov chain of a biochemical system as a
Markov chain tree structure and states as objects of class node. Based on the di-
mension of the system, search is performed in a latitudinal way for different model
sizes using the JSP LAS method. Whereas, bidirectional search is applied using
ISP LOLAS, which quickly expands the state-space up to a specified bound limit.
To support the expansion strategy, we also develop the Bayesian Likelihood Node
Projection (BLNP) function, based on Bayes’ theorem [31, 32]. It is adjoined with
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 6 of 42

the JSP variants to determine the likelihood of events at any interval at the mo-
lecular population level. BLNP provides confidence to the expansion strategy by
assigning probability values to the occurrence of future reactions and prioritizing
the direction of expansion. The JSP embedding BLNP function inductively expands
the multiple states with the likelihood of occurrence of fast and slow reactions
[12]. It also defines the complexity of the system by predicting the pattern of
state-space updation, and the depth of the end state from the initial state. When
used for any size of biological networks, LAS’ memory usage is proportional to the
entire width of expansion; it is less than JSP LOLAS. Both methods are feasible
and differentiated for various types of biological networks. However, the computa-
tional time for both variants depend on the nature of the model and the size of
the time step used. At any point, the amount of memory in use is directly propor-
tional to the neighboring states reachable through a single Ry, reaction. ISP LOLAS
uses considerably less memory, even when it retracts to the initial node to track
new reactions, then revisiting the depth many times.

Results

Having discussed the CME solution, we now discuss the modeling and integration
of the biochemical reaction systems for the JSP methods, as well as the assumptions
underlying these methods. Using ISP, we tested its ability to reproduce the model
to measure dynamics of the key parameters in the models. The JSP method is a
novel, easy-to-use, technique for modeling and expanding the state-space of bio-
chemical systems. It features several improvements in modeling and computational
efficiency.

The computational experiment (initializing and solving the model) was conducted on
the carbon-neutral platform of Amazon’ Web Service Elastic Computing (EC2), in-
stance type large (m5a), running on HVM (hardware virtual environment) virtualization
with variable ECUs. We used multicore environment 16vCPU @ 2.2GHz, AMD EPYC
7571 running Ubuntu 16.04.1 with relevant dependencies, and 64GB memory with 8GB
Elastic Block Storage (EBS) type General Purpose SSD (GP2) formatted with Elastic File
System (EFS). The performance mode was set to General Purpose with input-output
per second (IOPS = 100/3000). We used the type bursting throughput mode (see Sup-
plementary Information (SI) 1).

Intelligent state projection

The main aim of the proposed algorithm is to expand the Xx iteratively, such that Xx
contains a minimum number of states carrying the maximum probability mass of the
system. To create the sample space for JSP, a Markov chain tree KX [33] was used to
visualize a biochemical system to exhibit the transition matrix as directed trees [10, 11]
of its associated graph. Additionally, the Markov chain tree bX generates sample space
of the system to represent Markov processes associated with the Markov chain and the
transition matrices of biochemical reaction networks. In following section, we visualize
the Markov chain of the biochemical system as a Markov chain graph (tree) for JSP
compatibility.
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 7 of 42

Markov chain as a Markov chain tree

We define the Markov chain tree, bX, [33] as infinite and locally finite. It is a special
type of graph with a prominent vertex called a parent node without loops or cycles. If
graph G,,, is a state-space of the finite state Markov chain with {P(X;,X,) | Xi,X;¢
Gmc} transition probabilities meeting the condition Ss” P(X;,X,) = 1, then the induced

xX,

Markov chain tree is a combination of valued G,,,, random variables with the distribu-
tions inductively defined from P(X;,X,) with an initial state, X;¢G,,,.. That being the
case, it is easy to expand this class of Markov random field through a Markov chain
tree structure for biochemical systems. Furthermore the Markov chain tree and the
Markov processes can be equated as explained in [34] for the stochastic analysis.

Since we are interested in aperiodic states in the expansion of state-space, we shall
assume the reducibility or simplification of the G,,.; namely for each X;,Xj€Ginc
through bx. Therefore, let us concentrate on the case where G,,,, is considered as a locally
finite connected graph. The transition probabilities of each state are not equal due to the
propensities and parameters of different reactions in the biochemical system. Consequently,
a Markov chain tree, b¥, can be used to visualize a biochemical system process to exhibit
a transition matrix as directed trees of its associated graph [10, 11]. It can also be used to
generate a sample space for the system to represent the Markov processes and the transi-
tion matrices of biochemical reaction networks. We discuss the details needed to represent
Markov models on trees and working with graphs for state-space later.

If X; is the finite set of cardinality (1, 2....... K} of a Markov chain ®,, then A is the
transition probability matrix associated with X;. A state-space is, substantially, a class of a
set of states containing the unique state of the system. The arcs between the states repre-
sent the transitions from the initial state to the end state. This transition is defined as tran-
sient and communicating class in graphs. When all the transitions are combined, every
state-space takes the form of a graph and creates the state-space of the system, as shown
in Fig. 1 below.

We can now associate chain ®, with the directed graph G,,,,=(X,, V,), where V,,=[v1;
V2} ecco V,]. Vy defines the transition from state X; to X, and is denoted as v, = {(X;,X/)
; aij > O}. For every transition (X;, X;)<X,, then weight w(X;,X,) is a; ;.

Suppose G,,,- has a cycle, which starts and terminates at some state, X; € X;. If there is
a transition from X; to X,, we add a unique transition by creating a cycle from X; back
to itself and then consider the original transition from X; to X,. This contradicts the
uniqueness of the walk in tree [35]. In terms of the CTMC of a biochemical system
process, the change in molecular population is defined by a stoichiometric vector, so,
in Gyo there must be at least one intermediate state that will send the system back to
the previous state to create the cycle. This process categorizes the forward and back-
ward reactions given the initial state, Xo, of the system. The transient class of the tran-
sition leads the system to a unique state that defines the forward reaction in the
system. In contrast, the communicating class of a transition defines the reversible reac-
tion in the system. We define such systems as transient class systems and communicat-
ing class systems. Large biochemical systems are usually a combination of both classes.

A biochemical system is visualized as a tree KK [33] to enable the expansion of the

state-space. A tree, KK, is a special form of graph in data structure constituting a set of
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 8 of 42

 

   

 

Fig. 1 Markov chain graph showing forward and reversible reactions through four different states
S/S

nodes and a collection of edges (or arcs), each of which connects to an ordered pair of
nodes. G,,,, 1s considered a directed tree, LK. It is rooted with No = (Xo, dz) if it contains a
unique walk to N;=(X;,d;+ 1) and does not contain any cycles. Meanwhile, if X; ©

X;\{Xo} has exactly one outgoing transition away from Xp it is called an arborescence. If
it makes its transition towards Nop = (Xo, d;) it is called an anti-arborescence. An arbores-
cence is a subset CV, that has one edge out of every node that contains no cycles and has
maximum cardinality. For example, if set U= {5,7,8, 10} contains 4 elements, then the
cardinality of | U| is 4.

If X; and X,are the states other than the initial Xo state, there is a transition from X;
to X,, so X; has at least one transition. Now, suppose X; has two walks, (Xj, X;,1) and
(X;, X; 42). Concatenating these walks to the walks (X;,1, X/) and (X;,2, X,), respect-
ively, we have two distinct changes in state from X; to X, in G,,,-. However, in EX, this
concatenation is not considered, which makes them Directed Acyclic Graphs (DAG) (see
Fig. 2). Most of the biochemical models G,,, can be visualized as DAGs irrespective of
the nature of the reactions present in the model. Figure 2 shows the equivalent G,,,,tree of
shown in Fig. 1. The trees are less complex as they have no cycles, no self-loops. Yet they
are still connected, meaning they can depict the state-space.

The weight of the tree containing all e edges is defined by w(K) = [[oczx w(e),
where w(e) = @(X;,X;) = 4; is the weight of an edge starting from X; and ending at
X, when e € b% [36]. For systems which have both forward and backward reactions,
if ny is the total number of nodes indexed by {1, 2...K} the same as states, then nx is the
set of nodes carrying Xx, and n;, is the set of nodes carrying Xx given No root node of

the tree bX, then the walk from one node to another node is given by:
Uf (Ni Ny), f(NyNi) | No feXy, (7)

EX is formed by superimposing the forward transitions between the states X; and X,,
with the reverse orientation. Where X, and X;, indicate backward reactions, these
are graphically denoted as an individual edge from N; = (X;,d;) to N; = (X;,d;+ 11)
to N; =(X;,d;+ 2) in a tree. The N; of d;+2 can be renamed as a new node. N; ,1;
remains as it is at a different depth from the N; of d; but contains the same state
X;. In the expansion, repeated states are not considered in the domain; therefore,
any node which carries a similar state is considered the same, regardless of the
level and indexing. Consideration of trees for the state-space expansion in JSP not
only helps to reduce the complexity but also improves the accuracy of the solution
of Eq. (5) by identifying nodes which carry probable states. If the Markov chain
graph starts in state X;¢X,, then the mean number of transits to any state X, con-

. _ . . .’\th
verging to Gy.x, is given by the (i,i )" value of
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 9 of 42

 

   

 
  

depth 1 depth 2 ,deptn 3 ‘depth 4

¥ v

Fig. 2 Equivalent tree of a Markov chain graph, as shown in Fig. 1. This is a special form of graph which
has no cycle and no self-loops. It depicts the state-space of the system in the form of a tree (DAG)

 

_ 1 n-1l
A = lim ()) SA‘. (8)
moe MAL 7

U is the set of all arborescences. Let Uy, x, is the set of all arborescences which have
a transition from X; to X, and ||Ux,,x, || is the sum of the weights of the arborescences

in Ux, x, then according to the Markov chain tree theorem [33],

tx,
axx, =! 9
Mx = Ta °)

 

 

adx,x, is probabilistic in nature. This nature is not only restricted to the systems

which have irreducible Markov chains, in which graph G,,,, is strongly connected while
carrying probable state-spaces, but also for the systems that can be simplified by con-
verting to a Markov chain tree and then by reducing that tree by ignoring the states
which have low probabilities in space 9.

Expansion criterion for state space

As previously mentioned, the states are indexed using {1, 2....... kK} in the domain de-
noted by set X;. To derive the time, based on the state-space expansion conditions, the
probability exponential form of the CME Eq. (5) is evaluated for approximation up to
the desired final time f in steps. To focus on the probable states that contribute most
to the probability mass in the domain, we first define the set of non-probable states
(those which have the least probability mass) as Xx which are to be bunked. The num-
ber of states will usually be infinite, without selecting probable states for the domain.
By doing this we can avoid recalculating the probabilities and decrease the computa-
tional efforts by keeping the domain small. This bunking can also be applied to the
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 10 of 42

initial distribution of the system at fo. If submatrix A, contains the non-probable set

X,, of states, then the probability of set will be,
po) (x, ) = exp (.4)) P(X). (10)

The criterion for defining the non-probable states is determined by the 1,, tolerance
value. A, will only be considered to have non-probable states if,
nonprobable states, j¢ pt) (Xk < Tm

A, = else,

; (11)
probable states, if PO (xi) 2Tm

Similarly, submatrix A; has a probable set Xx of states if P(X) = T,, otherwise, the
states from Xx are bunked to Xx if P(X) < T,. For any iteration, if Pp) (X;) >T, then

(from Eq. (11)) some states from Xx return to Xx in the next iteration to increase the
accuracy of the approximate solution (42). The column sum of the approximate solution
(42) of these states is defined as:

A =I" exp(tpA;).P (Xo), (12)
where, I = (1,...1)’ is of an appropriate length. Declaring some states as non-
probable and removing them before calculating the probabilities as seen in [28] will de-

crease the accuracy of A with the cumulative step errors. This can be validated from
the state probabilities that have been ignored in the domain:

B=1-P (Xi). (13)

We define the step error in terms of the probabilities bunked. If e,7, « pt?) (Xx) then,
Crror = 1-1" exp(tpA;).P\ (Xo) (14)
Cnror = 1- E (15)

Every expansion step explores at least one new state and change {Xx} but not neces-

sarily {X,} if:
PO(X)> Iq > PO (Xi.), (16)
is satisfied. For ideal systems with a given initial probability of P'°)(Xq), the {X;,}

should be null and so P\1)(X,,) = 0. For such systems {Xx}, {X,} € {X,} for final pro-

jection and,
PU) (X;) = PL) (Xx) + PO) (Xe), (17)
Pl) (x,) = PC) (Xx) £0. (18)

ptr) (X;) in Eq. (18) is the solution of Eq. (3) after the state-space is expanded to Xx.
However, for large biochemical systems, Eq. (18) may not hold completely true, due to
the nature ((fast (Ray) and slow (Rys,))) of some reactions present in the system;
therefore, the condition in Eq. (11) will pass the states from Xx to Xx. The states with

the lowest probabilities will be bunked when:
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 11 of 42

po) (Xx) << Plt) (Xx), (19)

This improves the solution. Removing without calculating the probabilities of some
states is one of the lags of the current methods [18, 20, 21, 24, 26-28]; it is a result of
achieving the truncated domain and saving computation time. To address this, we set a

P“ (X,,) leakage point based on:
PO (Xp) > tm(leak) > P (Xi) . (20)
where, T,,(leak) for systems will reform Eq. (16) as

PO (XK) 21m *0.4 > PO (Xi), (21)

which would then zip the Xx further by leaking the highest probabilities to Xx so that
the probability sum is no longer conserved. The motivation of setting this ration is to
reconsider (up to 40% of X,,) the bunked states to improve the A solution and decrease
the expansion step error. While modeling the biochemical system, if the slow and fast reac-
tion [12] criterion is considered during expansion, then 1,,,(leak) will be defined as,

f Ruisr
_ {no.0f Ruuisr)) if no.of Ryysy) < no.of Ry)

* (no. of Rup) )’
else,
= __{no. of Ruri) ) (22)
if no.of Rays) > no.of Rug
™ (no.of Rugs) of Ru «) ))’ lf no.o M(sr) no.o M (fs)
else,
Tm*0.4, if no.of Ry) = no.of Rysr)-

We consider Eq. (21) criterion for all the computational experiments in this study.
The conditions in Eqs. (21) and (22) will lead to an optimal set of states as,

Xx—Xx — Xx, (23)

at fz; in the domain. When Xx is updated at every f,:-, before reaching f, this creates
several intermediate domains which we define as Bound. At to, the domain only has the
initial state of the system; therefore, we define the Bound as:

Bound tower = {domain, a), } (24)

After a single f,,., of expansion, if Xx is updated with a new state or set of states, it
creates:

Bound upper = {domain, a;} (25)

at ty. Here, d; denotes the depth level of the latest state or set of states that has been
added in the domain to form Bound yyyer. This Bound ypyey is re-labeled and considered
as Bound ower for the next tse, of the expansion. If the expansion is to be limited in the
number of Bounds, then every count(Byjmiz) leads to:

count (Biimit) = Biimit, (26)
where, 6j;,;: is the bound limit. For example, if 6j;,,;, = 2, then the count(By,,;,) will be

t t . . .
from 0—> 1-2. If the CoUNt(Byiniz) is increased to By; for I," iterations, then Bound ypper
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 12 of 42

in the current iteration will be Bound), for the next iteration. Every Bound ower state will

be the strict subset of every consecutive Bound, pyyer Ziven as:

Bound jower(Z)CBOUNA ypper(Z). (27)
and the upper bound as:
Bound ypper(Z) = { Domain at Z"" iteration, d,}, (28)

where Z is the number of Bounds (or intermediate domains). The 2D pyramid domain
in Fig. 3 graphically illustrates the increase in the population of states in the domain with
the increase in iterations (J,,). The apex of the pyramid represents the initial state Xo of
the system at Bound )oy.(1) at tp, whereas the base represents the deepest level where the
system ends with the final domain carrying set Xx with the maximum probability mass.

For large biochemical systems, the number of created Bounds are based on J. They
have million/billions of states. Expansion can be terminated by defining time f at which
the solution is required. To have an auto break-off point in the expansion, it is first ne-
cessary to define the criteria that limits J,,or when no more new states can be searched.
Therefore, we define this criterion in the following section. This criterion also applies

to biochemical systems which have fast and slow reactions [12].

Cease of criterion after updating
In every expansion step, the domain is validated by Eq. (21) and new states are added

in Xx as long as:
1-1" exp(t,A;) P! (X9) >t, (29)

is satisfied for probable states and stops if Eq. (29) is not satisfied. This leads to a

point at t where @,,.,<T,,, but the expansion can be extended to meet accuracy re-

quirements by re-considering the criteria as:

 
   
 
 
 
 

Bound jower(1) = Xo
, Boundypper(1) = domain up to Biimit

Bound jower (2) — Bound yyyer (1)

Increase in size o
Z Bound ypyer (2) = domain up to Dyimit

the domain with the
increase in
states population

        
 

Bound ower (3) — Bound ypper (2)
Bound yyper (3) = domain up to Bim it

   
  
  
 
 

Bound jower(Z) — Boundypper(Z — 1)
Bound ypper(Z) = domain up to Byimit

Fig. 3 General framework of 2D pyramid domain showing increases in domain size concomitant with the
increase in state with an increase in the bounds. Boundjwer(1) represents the initial condition, whereas
Boundupper(Z) represents the final domain which carries the explored set of states of the system

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 13 of 42

“or | ( (no.of Rays) )
1-I* exp(tA;).P\? (Xo) = tm* > (no.of Ruy)’ (30)
“or \ ple i (no. of Rup) )
1-I* exp(tA;).P\ (Xo) = tm ———_——+ (no.0f Ruan) (31)
1-1" exp(tpA;).P (Xo) =Tm (leak), (32)

before steps to t However, the size of Xx obtained through Eqs. (30), (31) and (32)
at t will be greater compared to the size of Xx obtained by Eq. (29) at ty as the latter
will have fewer states. In Eqs. (30), (31) and (32), with the increase in the size of Aj;, the
value of the left-hand side will also increase, resulting in an improvement in AZ. When
considering any Markov process of a biochemical system of any size in which the prob-

ability density expands according to Eq. (3) then Eqs. (30), (31) and (32) will approximate

(no.of Ruysr)) x (no. of Rup) )
(no.of Rup)? (no. of Rus ))

the solution within T,,* m
solution of the CME, which is Eq. (3).

and 1,,(leak), respectively, of the true

Computational experimental results
The JSP method is initialized and parameterized using the initial conditions of the
models. Due to a large number of mathematical operations and equations, simultan-
eous parameter predictions with a limited number of experimental values is often com-
plicated for dynamic systems. Therefore, the consistency with the available
experimental data was ensured at each step of the JSP. This method has led to the suc-
cessful development of several functions that integrate large number of processes sup-
porting extensive expansion of the state-space.

To demonstrate the JSP LAS algorithm, we first consider the catalytic reaction system
[37] defined by the reactions

s‘S BSc, B+PSBH+E (33)

depicted as a network in Fig. 4 as:

In this biochemical system (dimension = 5), reactant P will transform into product E
via complex B when reactant S acts as a catalyst for the reaction and produces C. We
rewrite this catalytic reaction system as a network of three reactions:

Ri:S “3B, (34)
ky

Ro:B“3C, (35)

R3:B + P“3B +E: (36)

with the initial copy counts Sp= 50, Po= 80, By = Co = Ey The reaction rate parameters are
k,= 1, kp»= 1000, k3= 100. These species counts are used as a state-space to define the
model and these copy counts are tracked as ({S], [B], [C], [P], [E])€N=(xo,%1, %2, 3, Xa).

In reaction Rj, the copy count of S is reduced by 1, which increases the copy count of
B by 1. In reaction Ry the copy count of B is reduced by 1, which increases the copy
count of C by 1. In contrast, reaction R3 decreases the B and P counts by 1 and in-
creases the B and E counts by 1. As in R3, B acts as a catalyst to convert P to E and B
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 14 of 42

is retained in the same reaction. We can now define the transitions associated with R,,

Ro, Rz in the stoichiometric vector V,, matrix as:

V4 -1 10 00
Vu=|v}=| 0 -11 0 O}. (37)
V3 0 00-11

For LAS method compatibility, the associated Markov chain of this model is con-
verted into a Markov chain tree with the states in terms of the nodes with additional
information such as the number of Ry reactions required to reach the state. In the
growing Markov chain tree, the transition between the nodes:

Vv Xo 1 gerne A.
Ny, AMOR XR) (38)

is defined in the typical form of the dictionary Dict. We express the propensity func-
tions of the three reactions in terms of the states ([S], [B],[C],[P], [E])eN. Node
N, = (Xo, dy) carries the initial state X) of the system at an initial depth of level 1. Fur-
ther, ny = (Xx, dq), 2, ...) is expanded and the states updated by following the LAS order.
The corresponding propensities Aq;; are updated in the A;; matrix in every iteration, based
on the LAS updating trend (for example, see SI 2). The system began with Sp= 50, Po= 80.
Gradually all the reactants are transformed to products, E and B. The system ends in
ny=(X1, 2, ...... 14666, dj).

Figure 5 shows the LAS method’s response when solved with 1,,=1le-6 for t¢=
0.5 sec. Due to the nature of the model reaction rates, small steps fs, =0.01 sec
are taken to capture the moments based on non-negative, non-zero states for the
domain. LAS successfully creates the domain of an optimum order, with 14666
states at t, by introducing the new states to the domain with time, as shown in
Fig (a) in Fig. 5. This pattern demonstrates that the frequency (the number of
states at any time ¢) of expansion increases in depth when the number of active re-
actions increase in the system. With the addition of probable states, the domain

contains enough probability mass to approximate the solution up to fy The states

 

 

Fig. 4 Catalytic reaction network with five N = 5 species S, B, C, P, and E in a network defining reaction, as
given in Eq. (33)

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 15 of 42

 

 

 

  
 
    
  
   
  
 

 

-k Set of states * i

 

 

 

a
o
o

states population in domain
Set of states

 

 

 

 

oF i be ERG

 

0 O1 0.2 0.3 0.4 05 0 0.05 01 015 02 0.25 03 0.35 0.4 0.45
Time (sec) Time (sec)
(a) b

Fig. 5 Expansion and updation of the states and set of states explored for the catalytic reaction system
using the LAS method. (a) demonstrates that state-space expansion increases the number of new states in
the domain. The size and colour of gj shows the increase in the size of the domain with the states’
population. In (b), LAS unfolds the state-space pattern to update the states in the domain and expands
14666 probable states in 0.5 secs

 

 
 

 

  

are updated in sets as seen in Fig (b) of Fig. 5, for the catalytic system after every

iteration.

The state-space pattern in Fig (b) of Fig. 5 can be used as a blueprint of the catalytic
systems’ state-space to compare with other model’s blueprints for their characteristics
and occurrence of reactions. Such a pattern can be used to predict the behavior of large
network state-space expansions when the set of occurrences of the initial reactions are
similar in different systems. The solution of Eq. (5), up to t for the domain created by
LAS, is shown in Table 1. The system’s conditional probabilities based on its species
are shown in Fig. 6.

In three test runs, the JSP LAS run time for the catalytic system was 4677 secs when
solving Eq. (5) with 14666 states. The probability of the species in Figure SI 17 (see SI
9) shows the nature of the reactions affecting each species’ count in the system. The in-
volvement of species B in all the reactions results in its highest probability at t Species
B also acts as a catalyst for R3, converting species P to E; therefore, both have equal
probabilities at the time of solution.

Figure 6 shows the total probability bunked at ¢ while progressing with the expan-
sion. Bunking produces an error (w.r.t approximation), with time when the number of
states increases with the expansion. LAS produces minimal error of order 107”, as given
in Table 1.

To demonstrate the JSP LOLAS algorithm, we consider the coupled enzymatic reac-

tions defined by the reactions

S4+E3C 3S4+Hh, C, SPHE (39)
ka ks Ke (40)
P+E,—-> Cy —> P+ Eo, C4 > S+E

depicted as a network in Fig. 7 as:
This biochemical system (dimension = 6) describes two sets of enzymatic reactions
transforming species S into species P and transforming species P back into S. We re-

write C reactions system as a network of six reactions:
Kosarwal et al. BMC Bioinformatics (2020) 21:515

Table 1 LAS expansion response and solution at t; for the catalytic system

 

 

te= 0.5, Run-time Domain Expansion time Error at t¢
tstep = 0.01 (sec) (sec)
ISP LAS 4677 14666 0.5 1.865e — 05
ky ko
Ri: + Fy — Ch, Ro:Cy > § + Ey
k k
R3:C, + P + Ey, R4:P + E> + ) (41)

R5:C> Ks P+ Eo, Ro:C2 Ks S+ E>

with initial copy counts S= 50, E,= 20, E,= 10, C; = Cy = P= 0 and reaction rate parame-

ters of kj = ky= 4, kp = ks= 5, k3 = ke= 1. These species counts are used as a state-space

to define the model. These copy counts are tracked as:

((S], [Ei], [Ci], [P], [£2], [Ca])€N=(%0, a1, 2,3, %4,¥5).

As in the previous example, we can now define the transitions associated with Rj, Ro,

R3, Ry, Rs, Re in the stoichiometric vector Vj, matrix as:

V1 -1 -1 1 0 O 0
V9 1 1 -1 0 O 0
V v3} 0 1 -1 1 O 0
M™ ly) 0 0 oO -1 -1 1
Vs 0 oO O 1 1 -1
V6 1 0 0 0 1 -1

For the LOLAS method, the associated Markov chain of this model is converted to a

Markov chain tree with the states in terms of nodes with additional information, such

1.48E-05

1.16E-05

9.12E-06

5.72E-06

3.07E-06

0 0.1 0.2 0.3 0.4
Time (Seconds)

Fig. 6 Total probability of states bunked at t from the domain of the catalytic system produced by ISP LAS

iteration while expanding and solving the CME

 

0.5

 

Page 16 of 42
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 17 of 42

 

 

Fig. 7 Coupled enzymatic reactions network. The figure shows six N=6 species, S, Ey, Cy, P, Ez, Co, ina
network, defining reactions, as given in Eqs. (39) and (40)

 

as the number of Ry, reactions required to reach the state. In growing Markov chain
tree, the transition between the nodes:

Vu Xo x1 yeenegX
ny, MOH Kel) (43)

is defined in the typical form of the dictionary Dict. We express the propensity func-
tions of the six reactions in terms of the states ([S], [Ei], [C1], [P], [Eo], [Co])eN.
Node N, = (Xo, d;) carries the initial state Xo of the system at the initial depth (level 1).

Then ny=(Xx, 1, 2,
LOLAS order. The corresponding propensities Aa;; are updated in the A;; matrix in every

) is further expanded and the states updated by following the
iteration, based on the given LOLAS updation trend (for example, see SI 2). Initially, the
system started with S= 50, E\= 20, E>= 10. Gradually all reactant species were transformed
into products resulting in the system ending in ny= (Xj, 2, ......8296, dy).

Figure 8 shows the LOLAS method response when solved with 1,,, = le - 6 for t= 2.0
sec. Due to the nature of the model reaction rates, small steps fs, = 0.01 sec are taken
to capture the moments. These are based on non-negative, non-zero states for the do-
main. LOLAS successfully creates the domain of an optimum order with 8296 states at
ty by introducing the new states to the domain with time, as shown in Fig (a) of Fig. 8.
In Fig (b) of Fig. 8, demonstrates that the frequency (the number of states at any time
t) of expansion increases in depth when the number of active reactions increases in the
system. With the addition of probable states, the domain contains enough probability
mass to approximate the solution up to ¢

Fig (a) of Fig. 8 depicts state-space expansion which increases the number of addi-
tions of new states in the domain. In Fig (b) of Fig. 8, JSP LOLAS unfolds the state-
space pattern to update states in the domain and expands 8296 probable states in 2.0
sec. As a blueprint of the dual enzymatic reaction network, the state-space pattern in
Fig (b) of Fig. 8 can be compared with other model blueprints in terms of its character-
istics and reactions. Such a pattern is considered to predict the behavior of a large net-
work state-space expansion when the set of occurrences of the initial reactions are
similar in different systems. The solution of Eq. (5), up to t for the LOLAS-created do-
main is shown in Table 2. The system’s conditional probabilities based on species are
shown in Figure SI 18 (see SI 10)
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 18 of 42

 

  
   
  
     
     

 

 

 

—*— Set of states

 

 

 

6000

w
So
So
°o

nN
°
°
°

‘|| 41000

 

 

 

 

 

 

0 05 1 15 2 0 0.2 0.4 0.6 0.8 1 12 14 16 18 2
Time (sec) Time (sec)
(b)
Fig. 8 Expansion and updating of the states and set of states explored for the dual enzymatic reaction network using
the ISP LOLAS method. g@& shows the increase in the domain size with the states’ populations. x4 shows the point

in time where new set of states are explored and updated in the domain

 

In three test runs, JSP LOLAS run time for the dual enzymatic reaction network was
~1614 secs when solving Eq. (5) with 14666 states. The probability of the species in Fig-
ure SI 18 (see SI 10) shows the nature of the reactions which affect each species’ count

in the system. At ¢ the probabilities of E, and Cy remain high compared to E; and C,
at different molecular counts. This results in a low probability of P compared to S. We
know that this network transforms species S into species P and then transforms species
P back into S. Based on the current probabilities of the species at f; the future probabil-
ity of P will increase. S will remain the same or decrease. With this change, the prob-
abilities of E, and Cz decrease in comparison to E, and Cj.

Figure 9 shows the total probability bunked at ¢ while progressing with the expan-
sion. The bunking produces an error (w.r.t approximation) with time when the number
of states increases with the expansion and provided that, LOLAS produces a minimal
error of order, 107”, as given in Table 2.

We extend the application of our JSP method to simulate a large model of the G1/S net-
work [38] under the condition of DNA-damage. We want to determine the number of
states at different points in time and predict the conditional probabilities of the protein
species based on events leading to the formation of different complexes in the system.

The G1/S model (dimension = 28) with a DNA-damage signal transduction pathway
is considered to be very stiff in nature, so while some molecular counts of certain pro-
teins increase very rapidly others do so slowly. This makes it tough to solve, even for a
short time period. The model is solved for f= 1.5 sec with Bjimiz= 1, T, = 1e -— 6, tstepy =
0.1. The systematic exploration of nodes carrying probable states are undertaken in a
similar way as discussed in Table SI 4 of SI 3 and depicted (see Figure SI 7 of SI 3) in
six stages (denoted as S), representing Ry, reactions with propensity, a,, with the arcs
as transitions.

The nodes are expanded up to f; to enable identification of the reaction channels re-
sponsible for variations in the proteins. From the transitioning factor of the 2"*_tier, we

Table 2 LOLAS' expansion response and solution at t; for the dual enzymatic reaction network

 

t= 2.0, Run-time Domain Expansion time Error at t¢
tstep = 0.01 (sec) (sec)

ISP LOLAS 1614.22 8296 2.0 5.953e — 05

 

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 19 of 42

 

   
 
 
 
 
 
 
 
   
  
 
     

0.00005

5.17E-05

0.00005 4.90E-05

4.30E-05

0.00004 3.76E-05

3.15E-05
0.00003

Error

2.32E-05

0.00002
1.53E-05

0.00001 7.39E-06

 

0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
Time (sec)

Fig. 9 Total probability of states bunked at t from the domain produced by dual enzymatic reactions
system in the /SP LOLAS iteration while expanding and solving the CME

 

can see that every node has an average of at least ~97 possible child nodes carrying

states. Further, Dict is expanded for n-tiers of the child nodes to add more states to the

domain. Additionally, ny=(Xx,d;-12......

ISP LOLAS trend (see Table SI 5 of SI 3).
The ISP LOLAS method response for the number of states in the domain and time, ¢, is

) is expanded and updated, as per the

shown in Fig. 10. The initial response suggests that only a few reactions were active until
t= 0.4 sec. After that time, more reactions triggered that explosively take the exploration
above 0.5 million states in 0.5 sec. For such a large model, this combination of explosion
states was expected as proteins undergo several excursions due to the number of reactions
in fractions of time, t. The second explosion of states occurs after 1.0 secs when almost all
the reactions (involving the species, given in SI 4.1) become active in the network. The
size and colour of the 2D pyramid in Fig (a) of Fig. 10 shows the increase in the size of
the domain with the state explosions. The number of sets of states that create the bounds
at ¢ are shown in Fig (b) of Fig. 10. With the exploration of the set of 517584 states, the
Bound(3) upper = (Xo, 1, 2....6046773 is formed at 0.5 sec carrying 604677 states. Some states
were bunked at 0.5 secs resulting in approximation errors that reach 2.42e — 06 at 0.6 sec.
At t, the LOLAS ends up with a domain defined by Bound(4) wyyer = (Xo, 1, 2.....3409899} Car-
rying 3409899 states with 3.52e — 06 approximation errors.

Fig (a) of Fig. 10 demonstrates that the state-space expansion increases the number
of additions of new states in the domain. [SP LOLAS quickly expands the state-space
up to ~3.5 million states in 1.5 secs. In Fig (b) of Fig. 10, JSP LOLAS unfolds the state-
space pattern to update states in the domain and expands 3409899 states up to ft,

The corresponding propensities, Aa;,, ;, are updated in the A; ; matrix in every iter-
ation, based on the JSP LOLAS update trend. The system started with the initial state
of the protein species and gradually, as protein levels change in the system, it exploits
the copy counts that shift the system to a new state. The change in protein levels
causes the system to transform into new states: here we see the manifestation of the
Markov process of the system. The JSP LOLAS captures this process and defines several
Kosarwal et al. BMC Bioinformatics

(2020) 21:515

 

  
   
  
    
  
    
   

 

 

3M;

3.5M
2804823} -- -------------------------- k

 

3.5M>
3409899

 

 

-&- Set of states

 

3M 3M 2.5MP

N
i) uw
= =
nN
=

5M

Number of states
states population in domain
Set of states

tered eo ceeecteeeeeeeeeeee tees Te Ye cceceteeeeedipeeeente 4

 

 

 

 

 

 

0 02 04 06 08 1 12 14 16 0 01 02 03 04 05 06 07 0809 1 WM 12 13 14 15
Time (sec) Time (sec)
(a) (b)
Fig. 10 The expansion and updating of the states and set of states explored for the G1/S model based on
the ISP LOLAS method. gi shows the increase in the domain size with the states’ populations. ye shows

the point in time where new set of states are explored and updated in the domain

 

bounds of the domain at different time intervals, as indicated by the pyramid in Fig. 3.

To investigate the expansion of states more closely, the order of bounds at different
time intervals, and the number of states present in the bounds, are provided in Table 3.
The size of bound created in each duration reveals that for every step, the growth of
the domain is eight-to-ten times the previous size of the domain.

The set of nodes Nj, No, . .

state(N3499900) = (Xo, 1, 2, ...3409899) that forms the state-space of the model. It is import-

...N3409900 Carries unique states representing the set of

ant to note that some proteins are synthesized and promoted by the network itself, as
evidenced by some reactions of the pathway, which increase the frequency of the re-
peated states. However, JSP LOLAS validation does not consider them for the domain.
Equation (5)’s solution, up to ¢, for the domain, created by ISP LOLAS, is shown in
Table 4.

Over three test runs, the JSP LOLAS’ run time for the G1/S model was 1372 secs for
solving Eq. (5), with the optimal domain having 3409899 states. The [SP LOLAS re-
sponse given in Fig. 11, shows the system’s probabilities bunked at t during the expan-

sion (w.r.t approximation), when the number of states increases with the expansion,

Table 3 Lower and upper bounds of the domain for the G1/S model given by the ISP LOLAS trend
based on bound limit Dyn,

 

 

Z Bound2) jower Bound(2) upper States Duration

| Bound] )iower = Ko} Bound()upper = Xo, 1, 2....9808) 9808 0.0 — 0.1 sec
formed at t= 0.0 sec formed at t= 0.1 sec
Approximation = 1 Approximation = 0.999999867
Dimie = 1, COUNt(Byimi) = 9, 1,

2 Bound (2)iower = BOUNA upper Bound (2) upper = {Xo, 1, 2....873935 87393 0.1 — 0.2 sec
formed at t= 0.1 sec formed at t= 0.2 sec
Approximation = 0.999999847 Approximation = 0.9999991 73
Dimit = 1, COUNt(Djimit) = 0, 1

3 Bound(3)iower = Bound (2) upper Bound (3) upper = {Xo, 1, 2....6046773 604677 0.4 — 0.5 sec
formed at t= 0.4 sec formed at t= 0.5 sec
Approximation = 0.999999157 Approximation = 0.99999701
Dimit = 1, COUNt(Djimit) = 0, 1

4 Bound (4)iower = Bound (3)upper Bound(A) upper = {Xo, 1, 2.. 34098995 3409899 1.1 — 1.5 sec

formed at t= 1.1 sec
Approximation = 0.99999699

Dimie = 1, COUNt(Byinid) = 0, 1

formed at t= 1.5 sec
Approximation = 0.99999648

 

Page 20 of 42
Kosarwal et al. BMC Bioinformatics (2020) 21:515

Table 4 /SP LOLAS' expansion response and solution at t- for the G1/S model

 

tp= 1.5 sec, Run-time Domain Expansion time Error at t¢
tstep = 0.1 (sec) (sec)

 

ISP LOLAS 1372 3409899 1.5 3.52e — 06

and provided that JSP LOLAS produces minimal errors of the order of 10°, as given in
Table 4 and Fig (a) of Fig. 11. We set the checkpoint to examine the initial state’s prob-
ability over time. The response in Fig (a) of Fig. 11 indicates that the probability of the
system remaining in the initial (normal) state decreases with time in the presence of
DNA damage, which triggers the change in protein levels.

The conditional probabilities of the species’ systems are given in Fig. 14 and SI 8. In
the case of DNA-damage, large numbers of the most notable parameters increase, com-
pared to normal conditions (in cell cycle progression). The increase is predominantly
related to x14 (p21) having a high initial probability, see Fig(14) of Figure SI 19 (see SI
11). The feedback (negative) of x4 (p53) increases its probability, see Fig(24) in Figure
SI 19 (see SI 11), such as the association rate of x16 (p21/CycE/CDK2 - P), the rate of
synthesis of x14 (p21) by x24 (p53), the rate of degradation of x,4 (p21), and the rate of
synthesis of x24 (p53) by DNA-damage signal. The conditional probabilities of the two
key proteins, x19 (p27) and x, (CycE), are affected by the change in the cell’s response
to the level of the DNA-damage signal, see Fig (10) and Fig (3) in Figure SI 19 (see SI
11). The parameters related to x19 (p27), as well as x; (CycE), greatly affect the prob-
ability of x2, (E2f) with time, see Fig (21) in Figure SI 19 (see SI 11). The impact of x,
(CycE) involves additional parameters related to CycA, because the release of supple-
mentary x2; (E2f) depends on x29 (Rb - PP/E2f) hyperphosphorylation by the activation
x7 (CycE/CDK2 - P), which affects the probability of x2; (E2f).

When the release of x2; (E2f) is affected, the probability of x, (CycE) increases, see
Fig (3) in Figure SI 19 (see SI 11). This leads to the progression to the S-phase, followed
by the temporary suspension of cell cycle progression. The increase in probability of
X24 (p53) shows cell support to repair the DNA damage. The parameters and the prob-

abilities relating to x14 (p21) and x24 (p53) become important in the case of DNA

   

 

 

  

4.95E-06

    

«— Initial state [Xo]

  
   

    

°
N

°
a

Probability

  
 

   
  

2.42E-06

°
un

1.62E-06

 

   
  

°
b

 
    

 

 

 

   

 

     

1.0

 

1 1 1 l 1 1 0.3
0 0.2 0.4 0.6 0.8 1 1.2 1.4 : 0.6 0.8
Time (Seconds) Time, t(sec)

(a)
Fig. 11 The /SP LOLAS' response for total probability bunked at t from the domain and checkpoint for
examining the initial state probability over time. (a) shows how ISP maintains accuracy by keeping low
errors. (b) shows the decline in the probability of the system remaining in the initial state in the presence
of DNA damage

 

Page 21 of 42
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 22 of 42

damage. When combined, the conditional probability of these parameters indicates the
involvement of the DNA-damage signal in the transition of G1/S.

Discussion
In this section, we discuss JSP performance, focusing specifically on the speed and ac-
curacy of the expansion, domain size and accuracy of the solution in comparison with

other methods.

Comparison with other methods

An approximation of 10” is used to find the approximate number of realizations required
by the SSA for the 10~* global error. Realizations were computed until the difference was
less than 10~* between the known distribution and the empirical distribution.

Approximately 10° and 10° runs were required to obtain the right distribution for the
catalytic and dual enzymatic reaction networks, respectively. In the catalytic system, we
observe (see Table 5) that both versions of JSP are faster than the OFSP of r-step reach-
ability and the SSA of sliding windows. We attribute this greater efficiency to LOLAS
having fewer states and less computational time than the OFSP method. LOLAS has
better accuracy at t; Similary, the JSP was much faster than the SSA, and the total
number of realizations required from the SSA to have an error at ¢; still large than that
of LOLAS is 10°. In the dual enzymatic network, we observe (see Table 5) that both
versions of JSP are faster than the OFSP of r-step reachability and the SSA of sliding
windows; we attribute the improvement to both JSP variants having an efficient domain
with a small approximation error and less computational time than that of the OFSP
method and better accuracy at f Similarly, both JSP variants were much faster than
SSA, as the total number of realizations required to have an empirical distribution with
the error at fyis ~12 times more than the domain produced by JSP.

We also compared the error at f to determine the solution’s efficiency. As seen in
the results, the increase in the step error in OFSP affected the solution at f Figure 12
(see Fig (a) and (b)), compares the JSP (LAS and LOLAS) with OFSP on the basis of the
approximation error at tf during the expansion of the catalytic and dual enzymatic

Table 5 Comparison of the solution of the catalytic reaction system based on ISP, OFSP and SSA

 

 

tp=0.5, ISP OFSP SSA

tstep = 0.01 LAS LOLAS

Catalytic reaction system (t¢= 0.5, tstep = 0.01)
Run-time (sec) 4677 2706 8767 17428
Domain at t; 14666 13089 14665 10° Runs
Expansion time 0.5 0.5 0.5 -
Error at t; 1.865e — 05 1.532e—05 1.917e—05 =9.81 x 10°

Dual enzymatic reaction system (t= 2.0, tstep = 0.01)
Run-time (sec) 2386 1614 2804 6374
Domain at t; 8282 8296 8266 10° Runs
Expansion time 2.0 2.0 2.0 -

Error at t; 7.470e — 05 5.953e —05 1,060e — 04 =9.94 x 10°?

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 23 of 42

 

 
  
   
  
   
 
    
   
   
      
   

 

0.00002F
0.0000192
0.0000186

0.000105
0.0001}

0,0000153
0.000015 0.00008}

0.0000747

0.0000595
0.00001}

Error
Error

0.00004}

0.000005}
0.00002}

 

 

 

 

 

0

 

n L L 1 be
0 0.1 0.2 0.3 0.4 0.5 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
Time (sec) Time (sec)

(a) (())

Fig. 12 Comparing /SP (LAS and LOLAS) with OFSP, based on the solution of the catalytic and dual
enzymatic reaction networks

 

reaction networks, respectively. Addressing the step error in JSP and the selection of

the probable states results in a more efficient solution at tf; compared to OFSP.

The typical firing nature of reactions in the catalytic system makes them stiff. There-
fore, the selection of states becomes difficult to approximate. While some species in the
system increase abruptly, others do so very slowly because the kinetic parameters (k,=
1, k= 1000, k3= 100) have large differences: this triggers reactions at different rates. Re-
action Rj, is categorized as a slow reaction in the network: it affects the fast reaction,
R,. As the computation results of Table 5 show, the JSP found that only 13089 probable
states were required to solve the system up to & This not only saves computational
time (see Fig. 13) compared to OFSP and SSA, and improves the solution’s accuracy. In
OFSP, applying the compression at every step or after a few steps is still computation-
ally expensive for a model like the catalytic reaction system, as seen in Table 5 and Fig
(a) of Fig. 13.

The network shown in Fig. 7 consists of two interlinked enzymatic reaction systems.
These systems transform species S and P into each other via the other species, making
the system stiff in nature. The selection of states thus becomes difficult for approxima-
tion. This is due to some species (S and £,) in the system increasing abruptly, while
others take longer to increase. Some of the kinetic parameters (k, = ky= 4, kp = ks= 5)
have large differences from other kinetic parameters (k3 = kg= 1): this triggers reactions
at different rates. Categorized as the fastest reaction in the network, R affects species S,
C, E. It is followed by other reactions involving other species. As the computation

=
nN
uw
oO
o
oO
T

@ os
~~ ~~

Oo
x
Computational time (sec)

a
oO
o

2
a
E

E

3
Cc

S

~
©

£
a
a
E
°

Oo

Sy
roe

nN
Pa

 

 

 

 

oO
oO

LAS LOLAS OFSP SSA LAS LOLAS OFSP SSA
(a) ))
Fig. 13 Comparing computational time for /SP (LAS and LOLAS) with OFSP and SSA by computational time.
All methods were applied to the catalytic and dual enzymatic reaction networks that were previously
integrated in the experimental results section

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 24 of 42

results in Table 5 show, the JSP LAS indicated that only 8282 probable states are re-
quired to solve the system up to & Likewise, ISP LOLAS identified that only 8296 prob-
able states are required to solve the system up to t; This saves computational time (see
Fig. 13), compared to OFSP and SSA, and improves the solution’s accuracy. In OFSP,
applying the compression at a defined step or after a few steps is still computationally
expensive for models like the dual enzymatic reaction system, as seen in Table 5 and
Fig (b) of Fig. 13.

The total computation effort required at every step, when compressing the number of
states up to ¢, is approximately equal to the total computation effort required when the
compression is applied in the gaps in some steps on a set of states up to t; Moreover, the
state-space will remain the same at ¢, regardless of when the compression is applied.

A comparison of the computational times in Table 5 shows that both versions of ISP
are significantly faster than other methods. Figure 14 shows the CPU utilization (%) of
LOLAS and OFSP with respect to run-time (minutes). The dedicated throughput (see
SI 1.1) between EC2 and EBS was not used to solve the model. The average CPU exer-
tion is about 60%, which is a considerable workload for a given model. The expansion
and approximation began when CPU use was at ~1.6422% in the catalytic reaction sys-
tem and ~1.23% in the dual enzymatic reaction system, at t= 0 sec. It increases up to
60.0% and then drops down to zero at

Theoretical interpretation of methods

Although, SSA recognizes the support and wastes no time in searching for the right do-
main and creating independent realizations which can be run parallel on multi-core en-
vironment, solving the system via Eq. (5) is quicker than creating realizations via
stochastic simulation [39-41]. This is because the N-term approximation [42, 43] of
the probability distribution to create the required number of realizations is always less

than, or equal to, the minimal support approximation up to same error. These

 

o 6
oO
o 5
a
= ff
Oo 3g
o
N 2
Ss 4
x iO
O

0 20 40 60 80 100 120 140 160 180 Oo 5 10 15 20 25 30 35 40 45 50
Time (Minutes) Time (Minutes)
(a) (b)

oS 6 o 6

2 2

© 5 © 5

o o

c& - & +

£ 3 S$ 3

N 2 N 2

5 1 5 1 —*— LOLAS

a Oo a Oo

me 0 5 10 15 20 25 30 35 40 45 50 = 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28
Time (Minutes) Time (Minutes)

CPU Utilization (Percent)
On NW DUA

(c) (d)

Fig. 14 AWS® CPU utilization percentage, when the catalytic reaction system is solved up to te 0.5 sec, and
the dual enzymatic reaction system solved up to te 2.0 sec, using OFSP and LOLAS. The performance
analysis was carried out using CloudWatch® (Statistic: Average, Time Range: Hour, Period: 5 Minutes)

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 25 of 42

realizations were computed until the difference was less than the prescribed approxi-
mation between the known distribution and the empirical distribution. In terms of the
system dimension, which is usually defined by the number of species in the system, the
approximation of Eq. (5) in ISP becomes smaller problem to solve compared to ap-
proximation through SSA. This enables [SP to perform better.

In contrast, OFSP creates a hyper-rectangle and applies truncation to guarantee the
minimal order domain for the approximation. OFSP truncates the state space after every
few steps to ensure the minimum size of the domain and enable greater computational
speed. However, differences in reaction firing changes the probability of some states at a
later stage; therefore, truncating the state space in OFSP after every few steps or at every
step would remove probable states from the domain, which in turn would affect the
accuracy of the solution. As a result of this, OFSP s overall performance is compromised.
In contrast, JSP first explores the states based on guided exploration through the BLNP
function (see method section (a)) and then leaks the set of states Xx which have the low-
est probabilities in the bunker at t without removing them (see Eq. (20)). It recalls these
sets of states when the probabilities of these states increase at later time.

In ISP, the time and space complexity (refer to SI 7) of removing and accessing the
states in the bunking and recalling process is optimum, compared to the overall time and
space complexity of the truncation step in OFSP [20]. As seen in Table 5, the number of
states present in the domain for the catalytic reaction network in JSP LAS is approxi-
mately equal to number of states present in the domain produced by OFSP. Additionally,
the number of probable states in the domain for the dual enzymatic reaction system in
ISP LAS is quite more as compared to the domain produced by OFSP. However, better
complexity and the guided selection of probable states for the domain produces low ap-
proximation errors and means that JSP LAS performs better overall than OFS. Similarly,
ISP LOLAS outperforms OFSP in finding the optimum domain due to its bi-directional
exploratory nature (see methods section (c)). This feature helps JSP LOLAS to achieve a
more accurate solution (see Table 5 and Fig. 12) as well as a quicker computational time
(see Fig. 13). These benefits are also due to fact that JSP visualizes the state-space as a
Markov chain graph or a tree (see Markov chain as a Markov chain tree section) which ul-
timately decreases the complexity in the expansion phase.

Conclusions

This paper has introduced a novel approach, [SP,to model biochemical systems. This
new approach addresses both performance and accuracy problems in CME solutions.
Provided all probable states are not added into the domain, up to the desired ¢ variants
of ISP (LAS and LOLAS) provide systematic ways of expanding the state-space. We
have demonstrated the effectiveness of our methods with several experiments using real
biological models: the catalytic reaction system, the dual enzymatic reaction system,
and the G1/S model (large model). The results and the algorithm’s responses reveal im-
provements in how different sized biological networks can be modeled: even state-
spaces with 3409900 nodes (see Table 3) carrying states up to ~3.5 million can be ex-
plored within a reasonable time. The results also show that the domain laid out by JSP
had an optimal order and was successful in finding probable system states, all the while
maintaining high levels of accuracy and efficient computational timing.
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 26 of 42

We have compared the JSP results against two popular methods: OFSP (r-step reach-
ability) and SSA (t leaps adaptive). JSP outperformed the other methods, in computa-
tional expense, accuracy and projection size. The JSP was more effective in terms of
predicting the behavior of the state-space of the system and in performance manage-
ment, which is a vital step in modeling large biochemical systems. Unlike other
methods, the JSP keeps the lowest states probabilities in the bunker without removing
(as removed in OFSP) them, before calculation (as removed without calculation in
FSP GORDE). It computes the probabilities at £ without computing large numbers of
realizations (as done in SSA).

The diverging nature of the JSP response, with respect to OFSP in Fig. 19, also
shows that the solution improved with ¢ and at a higher tf, For example, in the
large model (case study 2), the computation time was 1372 sec and the solution
was 3.52e-06 at t which was lower than the small model results (the catalytic
reaction system). These results show JSP’s compatibility with the distinct size of
biochemical models.

These examples have demonstrated that [SP is a very promising technique for sys-
tem’s biology. For stiff models, such as the G1/S and Candida albicans models, the ISP
yielded plenty of information. Likewise, it provided opportunities for stochastic analysis
of large models. JSP can be used to compute the probabilities of the species up to the
required time. One could also use JSP to conduct robustness and sensitivity analysis on
the dynamics of biochemical systems and to keep track of what reactions are more ac-
tive in the system at a particular time. JSP is also able to determine the complexity of
the system by defining the bounds with number states and keep track of the nested
state-space patterns (called the JSP model blueprint) that were updated at the end of
each step. Outlining the patterns of expansion of states to predict the projection folds
can be used for updating the new states.

We anticipate that the current structure of the JSP variants can be employed for dif-
ferent classes and varieties of biological systems. Additionally, they can be used to com-
pute the configurations with many reactions, as long as the notable part of the state-

space density is present between Bound(Z) owe, and Bound(Z) When there was a

upper
high probability of the molecular population of the species undergoing several excur-
sions in a fraction of time, then the JSP uses a small t,:-, to capture these moments.
While such computations were still challenging in the expansion phase for typical
models they can be addressed more closely in combination with the second part of the
CME solution: that is, the approximation phase. There are several methods which can
be used to address these challenges.

Approximation methods, such as the Krylov sub-space, can be used to effectively
compute the matrix exponential times of a vector. While it was mathematically attract-
ive to aggregate the states or decompose the large sparse matrix into a small dense
matrix using the Krylov sub-space, this method may not be computationally efficient in
the absence of an efficient domain. Performance can also be enhanced by employing
the fast math functions, compatible with the multicore environment. We have clearly
outlined the core ideas behind the JSP variants. We have highlighted the differences
and similarities between them and other methods that cover the computational and
theoretical considerations that are essential before any of the approximation methods
becomes feasible for an efficient CME solution.
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 27 of 42

Methods

To understand and predict the dynamics of the state-space response in biochemical
systems, we have developed an analytical numerical method called JSP. This method in-
tegrates the reactions’ propensities describing the Markovian processes through set of
nodes governing set of states of the system. The two variants of JSP, named LAS and
LOLAS, consist of several modules that incorporate sets of inputs and functions within
several compartments. Figure 15 depicts all the JSP modules. The integrated form is
discussed later in Tables 7 and 8.

These modules and sub-modules constitute the JSP method. They track key changes
in the components that follow changes in the reaction propensities by population and
activation of the species. The modules also describe the dynamics of the biochemical
system. The method also permits the time form quantification of state-space based on
the size and model dimension.

The JSP states expansion strategy is based on the Artificial Intelligence (AI) standards
[44-47], state-space search and relative successor (S,,,) operator or function which per-
form operations on inputs. AJ refers to the study of intelligent agents [48] of a system
that perceives and takes action to successfully achieve goals. Most of the problems can
be formulated as searches. They can be solved by reducing to one of searching a graph
or a tree. To use this approach, we must specify the successor operator which defines
the sequence of actions leading from initial to goal state at different time intervals, that
lead to the solution.

In terms of AJ, we define the state-space as a set of states in the system we can
get to by applying S,. to explore new states of a biochemical network. S,, can be
applied explicitly, which maps the current state to the output states, or defined im-
plicitly, in that it acts on the current state and transforms it into a new state. In
the state-space graph for biochemical networks, we do not define the goal state (or
end state) explicitly. Instead, it is defined by S,, implicitly in intervals based on the
nature (fast, slow, reversible and irreversible) of the reactions in the system, the

Input Module
iterate

 

State Module Explore Module

Dictionary
Module

Sort Module Update Module

1
il

Pp
c
oD
c
@
~
Ww
or
®
CO
~
<
©
a.
©
fa)
w
©
a
eo.
<
©
.
©
@

 

Fig. 15 Comprehensive /SP method flow chart. A description of the modules (steps), sub-modules and the
list of components are discussed in SI 5

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 28 of 42

duration of expansion and the introduction of stochasticity into the system. This
should systematically expand the state-space from Xx at ¢ to Xx, at t+1 by
going through each node n, at depth d; of the Markov chain graph to evaluate the
Markov processes; the expansion aims to occupy most of the probability mass
during [Xx + Xx ,], and the Markov processes can be solved for probability distri-
bution at ¢+ 1.

Where X, is the finite set of states and G,,. = (Xj, V,) is the Markov chain graph on
X, associated with A = [a;,], given Xo as the initial state and Xx as the set of the ex-
plored state, where Xp € Xx then the implicit successor is defined as,

Sue Vy (Xx(€)). (44)

Equation (44) gives the new states of the system, where, V, is the set of stoi-

ue
chiometric vectors v, function defining the state transitions from any present
state X;¢Xx to new state X,¢Xx. The sample space in the graph contains the
unique state of the system stored in a transition matrix, which satisfies Eq. (7)
conditions. This transition matrix is a compressed row format (CSR) [49, 50]
based on an index of rows — columns delimited by commas generating the dic-
tionary Dict of the model which defines the transitions between nodes in the
state-space and the mapping of states. Through S,,., we can know nothing more
than the neighbors (child nodes) of the current node (states reachable through a
single reaction). We then consider these neighbors (child nodes) as our only goal
states; there can be many in numbers. In a situation such as this, search trails
are referred to as blind or uninformed searches. In the following section, we dis-
cuss the infrastructure of an uninformed search, the type of data structure we will

be dealing with.

Infrastructure for searching
A data structure is required to retain the search track in the graph for problem state-
space expansion. For each node, N;, of the tree, we create a structure consisting of five

elements:

1) N;.State: represents state X; in the state-space corresponding to Nj;
2) N;.Parent: represents the parent node of the child node N;;

) N;.Depth: represents the depth of state state X;;

)

of the transition from N; to N, in the state-

i

(
(
(3
(4) N;.Cost: represents the cost CN
space;

(5) N;.Action: represents the action applied via S,,, on the parent node to reach N;.

To explore new states in the system, we consider the initial state state(N,) = (Xo, d))
as input to the successor, S,,,. Once the expansion is initiated, the Dict will temporarily
(in run-time) store the information for the transition from one node to another in the
state-space that binds to the reaction propensities a,. This shift is denoted by an arrow
—, which shows multiple transitions from the parent nodes to the child nodes contain-

ing the end state. The set of nodes n; = {Nj,N,..... Nw is a data structure that

incorporates the Markov chain graph G,,,,. We explore all the nodes that store the set
Kosarwal et al. BMC Bioinformatics (2020) 21:515

of states Xx as well as some additional information about the state, such as the depth
and transition cost, from one state to another in the system. If a set of states(n)) = X;,

then Cun’ is the transition cost to reach state(N,,) =X, from state(N;) =X, and
depth(n,) = d; defines the depth of the set of nodes in G,,,,, then the standard relation be-
tween a set of nodes and a set of states is given by ny=(X,, d)) or By = (X y) d),€ N,N} )
and the standard relation between a single node and a single state is given by N;=(X;, d))
or NV. i (X i di), C Ni, Ni ) if the transition cost is considered.

For example, Fig (a) of Fig. 16 shows the Markov chain graph, G,,,, with ny = 10, d;=
4. Its equivalent tree EX is shown in Fig (b) of Fig. 16 with ny=15, d;=5. In the tree
nodes N; = Ni; =N}j> carries the same state, X; at d;=1, 2 and 3, respectively, where walk
N>— Ni; and N7— Nj> represent the backward reaction of the forward reaction repre-

sented by walk N,; — N> and N, — N;, respectively.
The set of nodes with states are represented as

In general, the transition cost, C N,,Np is defined as:

Cyn! = A1,2 + 2,3 + + Gy-iy- (47)

C N,N} is the summation of all the propensities a, of the Ry reactions that take the

system to its final state. For example, Cy LN to expand to state(Njo) = X19 of Fig (a) of
Fig. 16 is given by

Path 1: a,2 +23 +034 +46 + 6g + Ag 10

Path 2: a,2 +423 +435 +56 + deg + Ag 10
CNLNio = Path 3: a,2 +25 +As¢ +468 t+ Agi0

Path4: a,7 + a7g + Ag 40

Path 5: 0,if not reachable

If these are the possible paths for the expansion that expands Xx at every iteration
then € NiN io (min) will be defined by the only path that has the lowest P“ (X,,). This

can be generalized as follows:

C, (min) «

1
NpNij PO(X,) (48)

which means that in order to minimize the expansion cost for the optimal do-
main Xx at least one path should have states with high probabilities for Xx. It is

best to follow the path with € N,N} (min), which leaks the minimum probabilities

of the system.

Page 29 of 42
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 30 of 42

 

   
   
   
   
 
    
   

  

 

depth 1
depth 1
depth 2
depth 2
depth 3
depth 3
depth 4
depth 4 depth 5
(a) (b)
Fig. 16 A Markov chain graph and its equivalent tree. (a) depicts a Markov chain graph (G,,.) with ny nodes
carrying X, states. The arcs show transitions between the nodes. Together, they form a Markovian process.
(b) depicts an equivalent tree MK of Gre as DAG representing the state-space of the system

 

 

For large biochemical models, there exist infinite cases when the node is unreachable

from the _ initial or another node; such cases are’ ignored when

C N,N, (min) = {Path: 0} because some probabilities are always dropped in the ap-

proximation. Therefore, © N,Nt (min) as defined by the lowest P (X;,) is strictly limited

to,

POX) > Cy yr (min) > 0, (49)

Upon expanding the root node Ni, we expand the child nodes carrying new states,

V (XK
and then the child-child nodes are explored. The walk between nodes N; We) Ni+d

is defined by dictionary Dict. This represents the occurrence of Ry reactions through
M elementary channels. For Fig (a) of Fig. 16, the typical form of dictionary is given
below:

D = ({1-2, 7], [2-+1, 3, 5], [3 4, 5], [4.6], [5-6], [68], (50)
(7-1, 8, 9], [810], [9—Nil], [10-9]),

and is indexed with the propensities, [a,j], for all the Rj reactions. As the propen-
sities are changing by Aq;;, we consider the recent values of a,j in every iteration of JSP

that corresponds to the reactions involved. To make the C N..N! (min) feasible for any
vou

type of biochemical system (stiff, non-stiff) to capture probable states, it is important to
consider the expansion cost for small te, (time step). This may be because there are

some cases when € N..N! (min) to reach two or more different child nodes are equal
vere

or very close to each other. In addition, we intend to expand the state-space in the dir-
ection of carrying states with high probability mass. To achieve this, we treat or convert
our uninformed search to an informed search infrastructure at run-time to have intui-
tive knowledge beyond our reach. Figure 17 shows the limits of our visibility in the
state-space.

Consequently, it is important to track the reactions which have high propensity func-
tion values. As it is difficult to determine the direction of the expansion, in the follow-
ing section (a), we develop the post successor function on Bayes’ theorem [31, 32] to
prioritize the expansion direction based only on those reactions that can be triggered at
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 31 of 42

 

   

‘ 4
Vag
eg, "
yO
7
=.

(4 aN
) 7. \
» @ GY i
Fig. 17 Limits of our visibility in the state-space before expansion. Visualized using a Markov chain graph,
where @ is the initial node and @ are nodes that are directly reachable from the initial node when
exactly one Ry occurs. When a further Ry occur, the system jumps to other Gi: ) nodes

 

a particular time point. In sections (b) and (c), we outline the direction strategy with
the depth and bounds of the expansion.

Bayesian likelihood node projection function

Bayesian methods [32, 51] are based on the principle of linking prior probability with
posterior probability through Bayes’ theorem [31, 32]. Posterior probability is the im-
proved form of prior probability, via the likelihood of finding factual support for a valid
fundamental hypothesis. Therefore, we employ the standards of Bayes’ theorem to de-
velop a function targeted to ensure the quality of the expansion based on Ry, reactions
active in the network at any particular moment. For a concise definition for the pur-
pose of fundamentals, refer to SI 6.

To improve the quality of expansion through a projection function, one may find
it useful to remove the set of states which have low probabilities before calculating
Eq. (3). However, removing these states will compromise accuracy as the step error
will increase at every t. Moreover, removing these probabilities will greatly affect
the solution, as defined at ft (at which a solution is required), for large dimension
systems which have large state-spaces, as the step error will be much higher due
to dropping probabilities without solving Eq. (3). In large systems, any species may
change its behavior after a certain number of firing of reactions triggering inactive
reactions in the network that will affect the probabilities of the states. If a change
in behavior increases the probabilities of certain states, then removing them in an
earlier stage is not wise.

Through the Bayesian Likelihood Node Projection (BLNP) function, we seek to
predict the posterior probability based on the parent state’s probability and calcu-
late the likelihood of the occurrence of reactions that will take the system from
the present state to the future state. Through BLNP, we can capture knowledge
about the system that will help us to make better predictions about the future
state. We are also able to ensure the accuracy of the solution and an optimal

domain.
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 32 of 42

It is important to decide on the direction of the expansion when choosing the
future state of the system, as any reaction can occur and take the system to any
new state. To understand this situation more clearly on a node level, we assume
a Markov chain graph as shown in Fig. 18 of this system which has almost the
same number of species count. In Fig. 19, the expansion is at intermediate pos-
ition, as the initial state state(No) =Xo is already expanded and now the expan-
sion of state(N2)=X>2 can be undertaken. To calculate the likelihood of the
occurrence of reactions Rj, R2, Rs, we consider the propensities a;, ; as a param-
eter. Aa;; depends on the kinetic parameter of the reaction. To assign weight to
our belief, we deduce a function that will calculate the probability of reactions
occurring and prioritize the expansion in order from reactions resulting in states
with high probabilities to reaction giving states with low probabilities. It is im-
portant to note that none of the probabilities will be removed before the calcula-
tion of Eq. (5). With this function, the likelihood of occurrence of Ry can be

computed.
° ° ° ° ° t t °
We consider each node as a junction of the prior reactions {R,....... R,,} with propen-
oye ! / ° ° ° ° / !
Sities {d) yer Ay} having prior likelihood values {b, y......-. by ni and future reac-
tions {R}....... Ry} with propensities {a,, y....... an, n’} having likelihood values {b;, 1

beveees by, n’}, aS given in Fig. 18.

To calculate the likelihood of the reactions, it is necessary to have prior information
about the occurrence of reactions. If the expansion is to be done at the initial node sta-
te(No) = Xo (at level 1), then the prior likelihood value by wr is considered as the initial
probability or as ~1. Once the initial node has been explored, we can calculate the like-
lihood of the reactions inductively. To calculate the probabilities b, yy, ...., by, nv of

the occurrence of Rj, .... , Ry, we first calculate the weighted probabilities Py ;(@), ...

ooe 4P

wn’ (@) of a system leaving any state by:

 

 

 

R>, A? 4

Fig. 18 Current state (Nz) = Xz, and future states (N3 4 5) = (X3, 4, 5, d)=3) with corresponding reactions Rj, Ro,
Rs and assumed propensities dz, 3 = 38, d2, 4= 39, do, 5 = 40, respectively, at any time ¢, given bo 2=0.4871,
be, I= 0.5128

 

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 33 of 42

 

R,,a\ y,, b; R,,Q,Nn,,D
«NY TAIN y «ANT ING,

  

Fig. 19 Node N as a junction of forward and backward reactions Ry, where dq, Nir we Ay yr are propensities

of the prior reactions. b; y, ... .Dyy are the likelihood of the prior reactions

 

ay(X — vy) _ Propensity of Ry reaction leaving state X, at d,
ust ay (X — vy) Sum of propensities of all the reactions (51)
leaving state X; at d,

/

and multiply it with the prior probability by because b

NN of the system. This will calcu-

late the likelihood inductively, as Ry, is responsible for transforming the system to the
present state(N;) = X; at t, leading to a function,

! xX _— ! !
b(Nan.valb, yxy) = fu “ «Dy y(X - Vy) (52)
7 (x ~ Vy)
p=l1
where,
a,(X -—v
Prvrt...NWN! (@) = ul 2 (53)

ay (X - vy)

|
IMs

b(Nwi,.amlB yy. x) = Puan (@)*by) y (X - vy) . (54)

/

Once b(Nn1,.nm|P, yy’ ny)

is calculated for all the adjacent nodes, the values are

arranged in descending order. Every value is bound to one reaction and represents
the likelihood of the occurrence of the reaction that takes the system from the
present node to the child nodes. Based on likelihood values (highest to lowest), the
corresponding reactions are considered one by one and labelled as true events for
expansion. For example, if a system has Rj, Ro, R3 reactions that bound to BLNP
likelihood values in order from highest to lowest, respectively, then three events
take the system to new state. When R, is considered for expansion, Ry and R3 are
labeled false events and R, as the true event. When the second highest BLNP like-
lihood value is considered, which is for R5, then it is labeled the true event and
the others, Ri, R3 are labeled false events. Similarly, the last and lowest BLNP like-
lihood value is for R3, which is labeled as the true event and the others as false
events. All states are added in the domain in order from the 1* true event to the
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 34 of 42

/

3 true event. The Eq. (54) of probabilities b(N Ni,.NM|D, Nw ny?

BLNP function.

Figure 18 shows the Markov chain tree for selection present at level 2 (assuming that
the initial node is already expanded). Here we calculate the weighted probability of a
system leaving state(N>) = X2 by:

is what we call a

Py 3(w) = 0X = Ve) = 0.3247
» ay (X - Vy)
p=l1

similarly, Pz, 4(@) = 0.3333 and P2, 5(@) = 0.3418.
At level 2, the conditional probability of the occurrence of reaction Rj, given the
probability of occurrence of reaction R, at level 1, is given by:

/ a,(X -v /
b(Noslbo2) — 3 a “ *Do 9 (x - Vu)
7 (X - vy)

u=1

Similarly, the occurrence of reaction R, at level 2, given the probability of occurrence
of reaction Rg, at level 1, is given by:

' ay,(X -—v '
b(No3|be2) = KWH) ag (x - Mu) -
» Ay (x 7 Vu)
p=1

If at level 1, state(N,) = X, and at level 2, state(N2) = Xz are explored through R, then
we Say that this is a true event and temporarily consider other events false events with
respect to the other reactions. Such a condition holds true for the other two cases,
when, at level 1, state(N,) =X, is explored through R, followed by an exploration of
state(N>) = X> either by R» or Rs. Given by o(X — Vy) and bo(X — Vu); we calculate the
likelihood of all the Ry, events, as given in Table 6. The likelihood values of future reac-

tions cannot be equal, as they are based on the probabilities of occurrence of prior
reactions.

From Fig. 18 and Table 6, we can infer, based on the prior reactions for Ry, where
M = {1, 6} that:

Table 6 Events with the likelihood of future reactions. Here true events define the expansion of

 

 

 

nodes

Oy No, 2 Ne, 2 by, nValue) Rnext
b(N2,3|bp.) True False 0.1581 Ry

b(N2,3|b5.) False True 0.1665 Re, 1

b(N2,4|bp.) True False 0.1623 Ri >

b(N24|b5.) False True 0.1709 Re, 2

b(N2,5|bp.) True False 0.1664 Ris

b(N2,5|¢.2) False True 0.1752 Re, 5

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 35 of 42

Case 1 (R,): At level 2, if the prior reaction is R,; and holds a true event for No > No
then:

b(Noslbo2) > b(N2albo2) > b(N23|bo2)

as per by, yn” the likelihood of occurrence of reactions will be in the order Rs; > Rz > Rj.
Case 2 (Rg): At level 2, if the prior reaction is Rg and holds a true event for Ng — No
then

b(Noslb62) > b(Nolbs2) > b(No3|b62)

as per by, yn” the likelihood of occurrence of reactions will be in the order Rs; > Rz > Rj.

There will be ©! number of cases (equal to elementary chemical reaction channels) if
there are R,, prior reactions in the system that bring the system to the current node. The

likelihood value will change based on b,, yi - vy). The BLNP function cannot be used

standalone for expansion because it only assigns weightage to direction for expansion. In
the Intelligent state projection section, we have derived the condition for our expansion
strategies to work with the Markov chain graph state-space and defined the criteria for
the formation of bounds (domain formed at anytime ¢) with time. The BLNP function
(with expansion strategies), will choose the probable states in large biochemical systems
where it is important to capture the moments at time ¢ that define a system’s behavior.
BLNP will be useful for identifying the most active reactions in the system while guiding

the expansion towards the set of states with high probability mass.

Latitudinal search strategy

We delve deeper into the first subroutine of the JSP called the Intelligent State Projec-
tion Latitudinal Search (ISP LAS). Figure 20 manifests the infrastructure of the LAS
strategy, showing G,,,, the queue and the domain. LAS’ queue data structure is based
on the FIFO (First In, First Out) method. In this method, the oldest state added to the
queue is considered first. We define and exploit the direction of expansion step-by-step
based on intuitive knowledge (as discussed in section (a)), gained from the probability

of future reaction events. We follow the conditions (as discussed in the results section).

 
 

       
   
 

Gmc representing Front end ~
state-space a —
( ) Po Ng Dequeuing ~ ~~~
Ve the node

 
   
   

  

   

R,: a,([N species vector])
R,: a([N species vector])

 

Species vector
Propensity

 
  

i Enqueuing j
‘ the node i. Ni ‘ Reaction channel
‘ Ru: Initial & other nodes

ay ([N species vector])
Approximation

   
 
 

   

 

Rear end

 

Fig. 20 Infrastructure of the Latitudinal Search strategy, ShowiNg Gmc, the queue and the domain
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 36 of 42

Furthermore, we show step-by-step how the nodes are explored, and states updated in
the domain in J/,, iterations.

At level d; the states are expanded only after all the states at level d;—1 have
been expanded; that is, the search is undertaken level-by-level and depth d, increases
in every [,, iteration. In the case of networks with reversible reactions, the JSP condi-
tion will prevent LAS from returning to the state it came from and also prevent transi-
tions containing cycles resulting in DAG with no repetition of any state whatsoever;
however, changes in propensities a;; are validated. Verifying the explored states in Xx
in iterations ensures that the algorithm completes and that a deadlock in the state tran-
sition cycles cannot occur.

The time complexity of LAS depends on the average transitioning factor F and depth
d, and is given by (see SI 7 for detailed discussion),

14+ F14+ P2404 FU 4+ (FetT— F) = OCFNt4), (55)

where,

re Total no. of walk between different nodes (56)
7 Total no. of nodes explored

For the nodes at the deepest level d;, all walks are valid except for the very last
node which stores the end state of the system. Therefore, once the end state is found,
based on Eq. (20), LAS will zip X,, further leaking the highest probabilities to Xx
for the solution of Eq. (3) which includes the end state of the system. As no state
is ever repeated in the domain, space complexity will decrease when the set of

states Xx is bunked at t seconds in iterations if Eqs. (19) and (20) are satisfied. In
Eq. (13), Pp) (X,) is computed according to Eq. (5) (the exponential form of the
CME), where 1,, is the tolerance and J is the identity matrix. Due to this stepping

/

bunking of xX, from Xx, the time complexity O(F7*+) reduces to
O(min(F%*4, |X 7\)) where |X,| is the size of the state-space [13]. In contrast,
the expansion of mew nodes carrying similar states tend to _ increase
O(min(F%*4, F|X 7\)) ; however, repetitive states are ignored.

If the input 1,, is too small, the algorithm automatically uses the default value of
sqrt(eps). Here sqrt is the square root and eps is the default value of the epsilon on ma-
chine. The expansion of child nodes containing state(N;) = X; stops if the condition of
Eq. (32) is not satisfied. If the criterion of slow and fast reaction [12] is considered, then
the condition of Eq. (31) or (32) is used, depending on the number of Ryys,) and Ry).
Table 7 shows the steps of the LAS method with the embedded BLNP function, from
steps 4a to 5b.

LAS will be optimal if the transitions between all the states are uniform; that is,
all the Ry reactions have the same propensity values. However, in real biochemical
models, this condition is unusual. To see a step-by-step demonstration of the
ISP LAS algorithm on a toy model, refer to SI 2. We now turn our attention to
the second variant of the JSP. We apply the method to a toy model to see how it
differs from LAS.
Kosarwal et al. BMC Bioinformatics (2020) 21:515

Table 7 Steps of /SP latitudinal search (LAS) algorithm

Step 0: Inputs: Initial node No, a, Viv, tol Tm, ty, tstep
Initialize: Bound ower = Xx, by xy (X—v,) =P (Xo), A=]

 

Step 1: Start from parent node N; = (Xo, Aj) <— Current State of the system at tg,

Step 2: Flag the current node as explored, update A and add the state xX; in the domain so that; if 1 — /' exp
(t. A)). P(X5) > Tn(leak) holds true go to Step 3; else stop the algorithm

Step 3: Sort exp(t. A). P(X,) and shift the set of states in X, att’ having smallest probabilities, if
P(X,) = ty(leak) > PO (X,. ) and at ty update Xx<-Xx — X,

Step Extend the graph dictionary Dict by v,,(X(t)) by 1 level to check all the nodes n;= (Xj, d), €,, Nv :( min))

4a: adjacent to Nj: Boundupper — Ry(Boundigwer) reachable by exactly Ry reactions (from fast to slow)
having Gy (min). If n= (Xx, a, Ey, ( min)) be the set of adjacent nodes such that nx € n, then

go to next Step,
Step Compute the BLNP function for NE BouNdypper:

4b: b(Nw1,..wulO, NNN = Pryit....nw (W)*6,) yX~¥i)
Step If N= (Xx, Gy, Ey ( min))edomain, then update the values of the set of states Xx present in domain
5a: and take domain « <— dOMAINprevious U domain and go back to Step 1; else If nx = (Xx, A, C NN yy ( min) )€

domain, then add it to the queue in order, according to reachability and go to the next Step,

Step sort b(Nwi.,. nulOvr., nm) in descending order and update queue—(queue; b(Nwi,, nul,
5b:

Step 6: Pull out the nodes nx= (Xx, d, Ey, 7 ( min)) from the queue in order and add the set of states Xx in
the domain as domain <— domain - + X, and take dOMAINprevious ¥ domain, then go back to Step 1,

WN. nw)

Output: domain with probable states

 

Longitudinal-latitudinal search strategy

Here, we delve deeper into the second sub-routine of JSP called the Intelligent State Pro-
jection Longitudinal Latitudinal Search (ISP LOLAS). Figure 21 visually represents the in-
frastructure of the LOLAS strategy, showing the G,,,.. stack and the domain. The stack
data structure of LOLAS is based on the LJFO (Last In, First Out) method. In this method,
the newest state added to the stack is considered first. In particular, we define the bound
limit and exploit the direction of the expansion step-by-step based on intuitive knowledge
(as discussed in section (a)), gained from the probability of future reaction events and fol-
low the conditions (as discussed in the Results section). Furthermore, we show step-by-
step how nodes carrying states are explored in a bidirectional way and how these states
were updated in the domain in /,,. iterations.

The states at level d; are expanded only after the neighboring states at level d;— 1 have

been expanded for Rj,: that is, the search is undertaken /evel-by-level. Depth d, increases

   
 

 
   

Pushing
the Node

  
   

Popping
the Node

      

R,: a,({[N species vector])

R,: a([N species vector]) Species vector
: Propensity

 
  

Reaction channel

Gmc representing Initial & other nodes

Ry: ay({N species vector]) re

Maps

    

 

Fig. 21 Infrastructure of the Longitudinal Latitudinal Search strategy, showing the Ging the stack and the domain

Page 37 of 42
Kosarwal et al. BMC Bioinformatics (2020) 21:515

in the same /,, iteration up to a certain b;,,,,;, (bound limit). The expansion limit is set by
Biimit» Ustey (depth step). In contrast to LAS, it is not set by depth d),. The LOLAS search
updates the dictionary Dict of G,,- by the stoichiometric vector function, v,(X(£)) on state
at level d; to explore the child nodes carrying states on levels d;+ 1, d;+2 ... .. dy+J,
where /= {1,2.... 00} and then retracts to level d; at which new state exploration decisions
can be made. In the case of networks with reversible reactions, the JSP conditions will pre-
vent LOLAS from returning to the state 1t came from and prevent transitions containing
cycles resulting in DAG with no repetition of any state whatsoever; however, the change
in propensities a; ; is validated. Verifying the explored states in Xx in iterations ensures
that the algorithm completes and that deadlocks in the state transition cycles cannot occur.

In the absence of 6j;,,;;, the algorithm will not retract. It will explore longitudinally by
tracking only one Ry, reaction. In addition, the algorithm will not terminate with an opti-
mal order domain carrying a maximum probability mass. This would lead to an increase in
the approximation error. Instead, it will terminate when carrying only those set of states as
a result of tracking only a few Ry, creating an insufficient domain for approximation.
Therefore, by default, the value of 6,;,,,;,= 1 1s kept for large systems and can be increased
depending upon the model’s dimension and the availability of the testing environment’s
random access memory (RAM). LOLAS’ worst-case time complexity depends on the aver-
age transitioning factor ¥. Depth d; is given by (see SI 7 for a detailed discussion):

(F+1)¥F° + (F)F1 + (F-1)F2 tet SFURH2 4+ 2FU-1 4 Fo = OCF), (57)

Table 8 Steps of /SP longitudinal latitudinal search (LOLAS) algorithm

 

Step 0: Inputs: Initial node No, Astep, Biimniti Oy Vy, tOl Tm, te tstep
Initialize: Boundjower = Xx, by y(X — vy) = P\(Xo),A = |

Step 1: Initialize count(Djmiz) and start from parent node N; = (Xo, Gd) <— Current state of the system at tg,

Step 2: Flag the current node as explored, update A and add the state X; in the domain so that;
if 1 — I" exp (t. A). P"(Xo) = Tlleak) holds true go to Step 3; else stop the algorithm.

Step 3: Sort exp(t. A). P'(Xo) and shift the set of states in X, att’ having smallest probabilities,
if P (Xx) = tm(leak) > P (X,) and at ty update X~<-Xx — X;

Step For step, extend the graph dictionary Dict by v,(X{t)) for count(Bjimiz) to check all the nodes

Aa: = (Xj, A, Ey y ( min)) adjacent to Nz Boundupper — RiABoundiowe) reachable by exactly
R,, reactions ( Grom fast to slow) having €,, vi ( min). \f nx= (Xx, A, Ey y (-min)) be the set

of adjacent nodes such that nx € n, then’ go to the next Step,

Step Compute the BLNP function for NKE Bound upper:

4b: b(Nwn,..wmlO, NoMND = Prsi..nint(W)*B yy 9 (X - vj)

Step If N= (Xx A, Ey, 7 ( min) )edomain, then update the values of the set of states Xx

5a: Pe . in domat and take domain — doMAiNprevious U domain and go back to Step 1; else If
= (Xx, , Ey yy (:min))¢domain, then add it to the stack in order, according to reachability

snd go to next Step,

Step sort 6(Nwi.. nO, nm) in descending order and update stack(stack; b(Nwi, nul,
5b:

Step 6: Pop of the top nodes nx= (Xx, Gd, Ey 7 ( min)) from the stack and add the set of states Xx in the
domain as domain — domain + Xe and take domdinprevious U domain, and go to next Step,

iN. nw)

Step 7: If count(Djimi) = Biimir creates BoUNd ypper = {domain} up to Bjimir then label Boundjower — BOUNA upper
and go back to Step 1; else if count( Dimi) < Dimic creates {domain} up to count(Djmit) then go to next
Step,

Step 8: count(Djimit)<— Count( Dini) + 1 and go to Step 4a
Output: domain with probable states

 

Page 38 of 42
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 39 of 42

LOLAS only stores the transition path to the end state besides the neighbors of each
relevant node in the exploration. Once all descendants are updated with the relevant
propensities in the projection, it discards the node from the domain (explored), making
it ready for the approximation. LOLAS first considers the R, reaction and the corre-
sponding stoichiometric vector v, of the system, to explore all the neighboring states
up to bound limit bj,,;;. It then considers Ro, R3, ... ..,Raz for the same 6j;,,,;, and the cor-
responding v>, V3, ... .. , Vaz to explore the states. For count(Byjnjiz), LOLAS retracts to the
R, reaction and explores the new neighboring states longitudinally. It then reconsiders R3,
R3, ... .. , Ry to explore the other states in a similar fashion. Provided with this reaction
tracking pattern, the BLNP function alters this trend and guides this tracking by consider-
ing reactions in a different order based on their propensities and the number of probable
states of the system.

If the system is ending in a set of state Xx carried by nx at f then LOLAS will ex-
plore the states efficiently, as long as count(Bjimiz) < Byimiz, otherwise count(Byn iz) is reset
for further expansion. Choosing the appropriate dj,,,;, and 6,,.-, depends on the type of bio-
chemical reaction network and the computing configuration. Starting with a depth 1 — b,;,.
mit, LOLAS explores all the states until they return nu//. It then resets the count(Bjj,,;,) and
retracts to explore again. In most cases, fewer states are positioned at the lower level.
They increase at a higher level when the number of active Rj, reactions increases, so
retracting provides the ability to track all the reactions simultaneously. The nature of the
LOLAS expansion means that it is able to find more states at any time ¢ compared to LAS.
It is also able to find them at the deepest level of the graph. The states at depth d; are ex-
plored once, the states at depth d;— 1 are explored twice, states at depth d;— 2 are explored
three times and so on, until it has explored all the system’s states. If the input 1,,, is too
small, the algorithm automatically uses the default value of sqrt(eps). Here sqrt is the
square root and eps is the default value of the epsilon on machine. The expansion of the
child nodes containing state(N;)=X; stops if the condition of Eq. (32) is not satisfied. If
the slow and fast [12] reaction criterion is considered, then either Eq. (31) or (32) condi-
tions are used depending on the number of Rays) and Ry). Table 8 shows the LOLAS
method, with an embedded BLNP function from steps 4a to 5b.

Refer to SI 3 for the step-by-step demonstration of the JSP LOLAS algorithm, where

we assume the same toy model system.

Supplementary information
Supplementary information accompanies this paper at https://doi.org/10.1 186/s12859-020-03668-2.

Additional file 1.

Abbreviations

qij OF ay: Propensity of chemical reaction; Aaj; Change in propensity; ann ves yy" Propensities of the prior
reactions; ag: Probability of a jump process from state X;_ ; to X; per unit time; a,y: Probability of a jump process from
state X; to X;_; per unit time; A or Aj; Defines the transition between i, j and its weightage; Djimir: Exploration bound

limit in LOLAS; by X=): Prior Bayesian likelinood values {bi y: Loess by wt b(Nwr..wm lO, yay): Represents
Bayesian likelihood value given prior by i Bound )pwer OF Bound: Define the set of states {X; 2.5, 01,23, .... timitt at
Biimie already present in the domain for current iteration; BouNdupper or Boundy: Define the set of states {X72 5,61, 2,
3... imitt At Dymie added in the domain at the end of current iteration; ¢, cy, C2: Constants; Cy! Total transition/
walk cost from node N; to N.; Dict: Dictionary of the model having transition records; dy Exploration depth limit in LAS;
step: Exploration depth step in /SP; dim: Dimension of sub-matrix in Sliding Windows Method; domain: Defines the set

of states of domain in current iteration that forms Boundypper; AoMAINprevious: Defines the set of states of domain in
previous iteration that forms Bound)owe,; Dj Diagonal matrix whose diagonal entries are one; e: Markov chain tree
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 40 of 42

edge, representing walk from N; to N; e,: First unit basis vector in Krylov Subspace Method; @;ror: Represents error
value in calculation; exp(): Exponential function; eps: Epsilon; ‘Ey: Denote the sequence of events E;, ‘E>... ./
fly): Represents the positive real value function of y; Gmc: Represents graph associated with the Markov chain tree; Hgim

: Upper matrix (Hessenberg Matrix); I": Identity matrix |= diag (1,1, .....1)': I~: Denote the iterations in ISP; ky: Kinetic
parameter of the chemical reaction where M = {1, 2.... co}; / Used as subscript for length of depth, for example d;
ny: Set of nodes as {Nj,No,....... N ai nx. Set of nodes carrying set of X, at any iteration; ny: Set of nodes carrying set

of Xi at any iteration; Ng: Root node carrying initial state Xo; Nj or Ni Any node; N or {5y,....... Sat Number of

different species; num, num: Random number generated by uniform random number generator (URN); P°) (Xo)
: Initial probability at t= 0; P(X): Probability of set of states at time t Py (w): Weighted probability of transition

from N, to N.; Ry. M elementary chemical reaction channels {Ry, Ro....... Ry Ruy: Prior M elementary chemical reaction
channels {R, , R, Leseees Ri}; Rus: M elementary chemical reaction channels of fast reactions; Rsy: MI elementary

chemical reaction channels of fast reactions; Rrrace: Number of retractions in LOLAS; SN. Approximate number of states;
S: Number of stages in expansion {1, 2,....... }; Sk: Supporting information; Syc: Implicit successor or operator;

sqrt: Square root; tg: Time at which initial conditions of system are defined; t: Time at which Xi is dropped from the
domain; t: Any random time in seconds; tg: Time at which Xx is updated in the iteration; tg Final time at which
solution is required; ¥: Transitioning factor; Ux,,x,: Set of all arborescences; | U|: Define the cardinality of any set;

v: Krylov Sub-space method - A column vector of a length equal to the number of different species present in the
system; Vy Or vy: Stoichiometric vector represents the change in the molecular population of the chemical species by
the occurrence of one Ry reaction. It also defines the transition from state X; to X; in the Markov chain tree; v,(X(t)) or
vulX(t)): Stoichiometric vector function, where X is any random state; vy, Or vy: Matrix of all the Stoichiometric vectors
[Vij Voj oe... v,]; W%: Probability that is computed inductively by W°’ = P® in uniformization method; x1,....... Xe

: Number of counts of different species; X or X; or X,: Any random state; Xo: Initial state or initial condition, Xy Ordered
set of possible states {X),....... x wi of the system; Xx: Set of new states or domain at any iteration; Xx Set of states
5

dropped from domain at t at any iteration; y, yo: Positive integers; Yy: Poisson process given that 0<y <M; Z: Number
of bounds in /SP; T, or tol,,; Tolerance value; T,,(leak): PO (X,) leakage point; AE: Approximate solution of the CME;
w: Weight or cost of single transition from X; to X,. It is equivalent to qj; K Markov chain representing biochemical

process; BK: Markov chain tree with n,; At: Uniformization rate; vj. Number of nonzero elements in P; @: Sample space;

Q: Asymptotic lower bound; O: Asymptotic upper bound; O: Asymptotic tight bound; {1, 2....... kK}: Indexing of set of
states and set of nodes; u={1,2,....M} Channels of chemical reaction propensity

Acknowledgements
RK acknowledges the project funding and necessary resourcing received from Lincoln University, New Zealand for the
duration of three years.

Authors’ contributions

RK and DK developed the research questions and designed the research. RK developed and implemented the
algorithm; DK and SS directed the project, RK and DK wrote the manuscript and SS independently critiqued the
manuscript, which has been read, improved, and approved by all three authors.

Funding
RK acknowledges the writing scholarship received from AgLS Faculty, Lincoln University, New Zealand to write this
paper for a month (NZD2000). Only obligation is to submit a paper to a journal.

Availability of data and materials

All the result data files generated and analyzed during the current study are available in the “isp” repository of https://
github.com/rkosarwal/isp. Codes are not publicly available due to search engine privacy but are available from the
corresponding or first author on reasonable request.

Ethics approval and consent to participate
Not applicable.

Consent for publication
Not applicable.

Competing interests
Not applicable.

Received: 19 April 2020 Accepted: 17 July 2020
Published online: 11 November 2020

References
1. Roberts RM, Cleland TJ, Gray PC, Ambrosiano JJ. Hidden Markov model for competitive binding and chain elongation. J
Phys Chem B. 2004;108(20):6228-32.
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 41 of 42

 

20.

22.

23.

24.

25.

26.

2/7.

28.

29.

30.

31.
32.

33.

34,

35,
36.

37.

38.

Kholodenko BN. Negative feedback and ultrasensitivity can bring about oscillations in the mitogen-activated protein
kinase cascades. Eur J Biochem. 2000;267(6):1583-8. https://doi.org/10.1046/j.1432-1327.2000.01197.x.

Ozer M, Uzuntarla M, Perc M, Graham LJ. Spike latency and jitter of neuronal membrane patches with stochastic
Hodgkin-Huxley channels. J Theor Biol. 2009;261(1):83-92.

Murray JM, Fanning GC, Macpherson JL, Evans LA, Pond SM, Symonds GP. Mathematical modelling of the impact of
haematopoietic stem cell-delivered gene therapy for HIV. J Gene Med. 2009;11(12):1077-86. https://doi.org/10.1002/jgm.
1401.

Hogervorst E, Bandelow S, Combrinck M, Irani SR, Smith AD. The validity and reliability of 6 sets of clinical criteria to
classify Alzheimer’s disease and vascular dementia in cases confirmed post-mortem: added value of a decision tree
approach. Dement Geriatr Cogn Disord. 2003;16(3):1 70-80.

Schulze J, Sonnenborn U. Yeasts in the gut. Dtsch Aerzteblatt Online. 2009. https://doi.org/10.3238/arztebl.2009.0837.
Zhou Y, Hou Y, Shen J, Huang Y, Martin W, Cheng F. Network-based drug repurposing for novel coronavirus 2019-
nCoV/SARS-CoV-2. Cell Discov. 2020;6(1). https://doi.org/10.1038/s41421-020-0153-3.

Gillespie DT. A rigorous derivation of the chemical master equation. Phys A Stat Mech its Appl. 1992;188(1-3):404—25.
Gillespie DT. Exact stochastic simulation of coupled chemical reactions. J Phys Chem. 1977;81(1):2340-61. https://doi.
org/10.1063/1.2710253.

Weber R. Markov chains. 2011. http://www:sstatslab.cam.ac.uk/~rrw1/markov/M.pdf. Accessed 22 Nov 2016.

Goutsias J, Jenkinson G. Markovian dynamics on complex reaction networks. Phys Rep. 2013;21218(2):199-264. https://
doi.org/10.1016/j.physrep.2013.03.004.

Goutsias J. Quasiequilibrium approximation of fast reaction kinetics in stochastic biochemical systems. J Chem Phys.
2005;122(18):1-15.

Burrage K, Hegland M, Macnamara S, Sidje R. A Krylov-based finite state projection algorithm for solving the chemical
master equation arising in the discrete modelling of biological systems. Proc Markov Anniv Meet. 2006:1-18.

Jones MT. Estimating Markov transition matrices using proportions data: an application to credit risk. IMF Work Pap.
2005;05(219):1. https://doi.org/10.5089/978145 1862386.001.

Gillespie DT. Markov processes - an introduction for physical scientists. Cambridge: Elsevier, 1992. p. 592.

Mouroutsos SG, Sparis PD. Taylor series approach to system identification, analysis and optimal control. J Franklin Inst.
1985;319(3):359-71. https://doi.org/10.1016/0016-0032(85)90056-0 Cited 2018 Jul 12.

Eslahchi MR, Dehghan M. Application of Taylor series in obtaining the orthogonal operational matrix. Comput Math
with Appl. 2011,61(9):2596-604.

Wolf V, Goel R, Mateescu M, Henzinger T. Solving the chemical master equation using sliding windows. BMC Syst Biol.
2010;4(1):42. https://doi.org/10.1186/1752-0509-4-42.

Sidje RB, Vo HD. Solving the chemical master equation by a fast adaptive finite state projection based on the stochastic
simulation algorithm. Math Biosci. 2015;269:10-6.

Sunkara V, Hegland M. An optimal finite state projection method. Procedia Comput Sci. 2010;1(1):1579-86. https://doi.
org/10.1016/j.procs.2010.04.177.

Munsky B, Khammash M. The finite state projection algorithm for the solution of the chemical master equation. J Chem
Phys. 2006;124(4):1-13.

Mikeev L, Sandmann W, Wolf V. Numerical approximation of rare event probabilities in biochemically reacting systems.
Lect Notes Comput Sci (including Subser Lect Notes Artif Intell Lect Notes Bioinformatics). 2013;8130 LNBI:5-18.
MacNamara S, Bersani AM, Burrage K, Sidje RB. Stochastic chemical kinetics and the total quasi-steady-state assumption:
application to the stochastic simulation algorithm and chemical master equation. J Chem Phys. 2008;129(9):095105.
Dinh KN, Sidje RB. An application of the Krylov-FSP-SSA method to parameter fitting with maximum likelihood. Phys
Biol. 2017;14(6):065001. https://doi.org/10.1088/1478-3975/aa868a.

Harrison RL, Granja C, Leroy C. Introduction to Monte Carlo simulation; 2010. p. 17-21. https://doi.org/10.1063/1.
3295638.

Dinh KN, Sidje RB. Understanding the finite state projection and related methods for solving the chemical master
equation. Phys Biol. 2016;13(3):035003.

Munsky B, Khammash M. A multiple time interval finite state projection algorithm for the solution to the chemical
master equation. J Comput Phys. 2007;226(1):818-35.

Sunkara V. Analysis and numerics of the chemical master equation. 2013. http://www.math.kit.edu/ianm3/~sunkara/
media/thesis_sunkara.pdf. Accessed 25 May 2018.

Padgett JMA, llie S. An adaptive tau-leaping method for stochastic simulations of reaction-diffusion systems. AIP Adv.
2016;6(3):035217.

Cao Y, Gillespie DT, Petzold LR. Efficient step size selection for the tau-leaping simulation method. J Chem Phys. 2006;
124(4):1-11.

Schlecht V. How to predict preferences for new items. Invest Manag Financ Innov. 2014;5(4):7-24.

Fahidy TZ. Some applications of Bayes’ rule in probability theory to electrocatalytic reaction engineering. Int J
Electrochem. 2011;2011(1):1-5. https://doi.org/10.4061/201 1/404605.

Anantharam V, Tsoucas P. A proof of the Markov chain tree theorem. Stat Probab Lett. 1989;8(2):189-92. https://doi.org/
10.1016/0167-7152(89)90016-3 Cited 2018 May 15.

Aldous D. The Continuum random tree Il: an overview. In: Barlow MT, Bingham NH, editors. Stochastic analysis.
Cambridge: Cambridge University Press; 1992.

Diaconis P, Efron B. Markov chains indexed by trees. Ann Stat. 1985;13(3):845-74.

Gursoy BB, Kirkland S, Mason O, Sergeev S. On the markov chain tree theorem in the max algebra. Electron J Linear
Algebr. 2012;26(12):15-27.

Mastny EA, Haseltine EL, Rawlings JB. Two classes of quasi-steady-state model reductions for stochastic kinetics. J Chem
Phys. 2007;127(9). https://doi.org/10.1063/1.2764480.

Ling H, Kulasiri D, Samarasinghe S. Robustness of G1/S checkpoint pathways in cell cycle regulation based on
probability of DNA-damaged cells passing as healthy cells. BioSystems. 2010;101(3):213-21. https://doi.org/10.1016/j.
biosystems.2010.07.005.

 

 

 
Kosarwal et al. BMC Bioinformatics (2020) 21:515 Page 42 of 42

39. MacNamara S, Burrage K. Krylov and steady-state techniques for the solution of the chemical master equation for the
mitogen-activated protein kinase cascade. Numer Algorithms. 2009;51(3):281-307. https://doi.org/10.1007/s11075-008-
9239-y.

 

 

40. Jahnke T, Huisinga W. A dynamical low-rank approach to the chemical master equation. Bull Math Biol. 2008;70(8):2283-
302. https://doi.org/10.1007/s1 1538-008-9346-x.

41. Hegland M, Hellander A, Lotstedt P. Sparse grids and hybrid methods for the chemical master equation. BIT Numer
Math. 2008;48(2):265-83. https://doi.org/10.1007/s10543-008-01 74-z.

42. DeVore RA. Nonlinear approximation. Acta Numer. 1998;7:51-150. https://doi.org/10.1017/S0962492900002816.

43. DeVore RA, Howard R, Micchelli C. Optimal nonlinear approximation. Manuscripta Math. 1989;63(4):469-78. https://doi.
org/10.1007/BF01171759.

44. Chijindu EVC. Search in artificial intelligence problem solving. IEEE: African Journal of Computing & ICT. 2012;5(5):37-42.

45. Barr A, Feigenbaum E. The handbook of artificial intelligence vol |. Math Soc Sci. 1983;4:320-4.

46. Korf RE. Artificial intelligence search algorithms. In: Algorithms Theory Comput Handb; 1996.

47. Korf RE. Depth-first iterative-deepening. An optimal admissible tree search. Artif Intell. 1985;27(1):97-109.

48. Rudowsky I. Intelligent agents. Commun Assoc Inf Syst. 2004;14(August):275-90.

49. Lawlor OS. In-memory data compression for sparse matrices. In: Proc 3rd Work Irregul Appl Archit Algorithms, vol. 6;

 

2013. p. 1-6. https://doi.org/10.1145/2535753.2535758,

50. Koza Z, Matyka M, Szkoda S, Mirostaw t. Compressed multi-row storage format for sparse matrices on graphics
processing units; 2012. p. 1-26. https://doi.org/10.1137/120900216.

51. Manoukian EB. Modern concepts and theorems of mathematical statistics. New York: Springer New York; 1986. (Springer
Series in Statistics). https://doi.org/10.1007/978-1-461 2-4856-9,

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Ready to submit your research? Choose BMC and benefit from:

e fast, convenient online submission

e thorough peer review by experienced researchers in your field

e rapid publication on acceptance

e support for research data, including large and complex data types

e gold Open Access which fosters wider collaboration and increased citations

e maximum visibility for your research: over 100M website views per year

At BMC, research is always in progress.

Learn more biomedcentral.com/submissions > BMC

 

 

 
