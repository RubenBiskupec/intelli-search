Journal on Multimodal User Interfaces (2020) 14:313-319
https://doi.org/10.1007/s12193-020-00346-8

EDITORIAL ®)

Check for
updates

Multimodal interfaces and communication cues for remote
collaboration

Seungwon Kim! - Mark Billinghurst! - Kangsoo Kim?

Published online: 3 October 2020
© Springer Nature Switzerland AG 2020

Abstract

Remote collaboration has been studied for more than two decades and now there is the possibilities for new types of collabora-
tion with the recent advances in immersive technologies such as Virtual, Augmented, Mixed Reality (VR/AR/MR). However,
despite the increasing research interest in remote collaboration study with VR/AR/MR technologies, there is still a lack of
academic venues specifically focusing on VR/AR/MR remote collaboration research. This special issue provides high-quality
papers on the topic of remote collaboration research and increases visibility of this timely interesting and important research
area. We particularly focus on three research aspects in remote collaboration: (1) use of multimodal communication cues, (2)
awareness of the task space, and (3) human factors understanding. In this editorial, we first describe five essential factors for
remote collaboration: task, local user, remote user, communication, and tool/interface, and then summarize a brief history of
the research areas. We also cover the feature papers accepted in this issue, which introduce novel multimodal interfaces for
remote collaboration and the effects on task performance and perceptual factors. Finally, we discuss some potential future
research directions while concluding the editorial.

Keywords Remote Collaboration -Communication Cues - Multimodal Interfaces - Mixed Reality

1 Introduction

While our society and culture have been evolving, we have
continuously developed skills to work/collaborate remotely
with other people [46]. Here, collaboration is multi-person
activities to achieve a common goal [12], and could be cat-
egorized into two sectors according to the collaborators’
locations—e.g., whether they stay in the same place or not.

People usually have better collaboration when staying
in the same place rather than when they are in remote
places [22]. People intuitively and immediately shares

CJ Kangsoo Kim
Kangsoo.Kim @ucf.edu

Seungwon Kim
Seungwon.Kim @unisa.edu.au

Mark Billinghurst
Mark.Billinghurst@ unisa.edu.au

School of Information Technology and Mathematical
Sciences, University of South Australia, GPO Box 2471,
Adelaide, SA 5001, Australia

University of Central Florida, 3100 Technology Parkway,
Orlando, FL 32826-3281, USA

social/communication cues in co-located collaboration, but
this is not always available in remote collaboration, espe-
cially with conventional remote communication technolo-
gies, e.g., telephone and videoconferencing tools.

Thanks to the recent advances in immersive technolo-
gies with multimodal interfaces, such as Virtual, Augmented,
and Mixed Reality (VR/AR/MR) with support for gesture
and gaze input, we encounter a new era of technologically
enhanced remote collaboration with unique opportunities
and challenges [19,37]. In remote collaboration studies,
AR/VR/MR technologies have been used for visually show-
ing spatial information in the real world task scene, and for
providing an identical environment of the task scene to the
remote collaborator who is not in the task space [2].

There have been a few research events related to this
remote collaboration, for example, VR in VR: IEEE VR Work-
shop on Virtual Conferencing, 3DCVE: IEEE VR Workshop
on 3D Collaborative Virtual Environments, MVAR: ISMAR
Workshop on Multimodal Virtual & Augmented Reality, and
CoMiRE: ISMAR Workshop on Collaborative Mixed Reality
Environments. However, there was a lack of academic jour-
nals, which could provide comprehensive yet novel findings

Q) Springer
314

and knowledge, particularly focusing on AR/VR/MR remote
collaboration with multimodal input.

In this special issue, we compiled a collection of high
quality contributions that reflect the latest state of the art
of research in AR/VR/MR remote collaboration, and share
research on multimodal interfaces and communication cues
while providing insights for the future research. All submit-
ted papers were peer-reviewed, and five accepted papers were
selected based on their research quality and significance of
the contribution to the existing literature. As an introduction
to our special issue, this editorial describes the definition
of the remote collaboration with five constructing factors,
briefly summarizes the research aspects in remote collab-
orations, and introduces the five accepted feature papers.
Finally, we will discuss some potential future research direc-
tions while concluding the editorial.

2 Remote collaboration

As mentioned earlier, collaboration can happen in different
distance settings, either among co-located users in the same
place (face to face collaboration), or between distributed
users in remote places (remote collaboration). Remote col-
laboration has been studied in term of the collaborative per-
formance and the user’s perception for more than two decades
in different research fields, such as telecommunication and
social psychology. While there are dramatic advances in
mobile and immersive technologies, e.g., VR/AR/MR, per-
vasive and personal mobile/wearable computing, the poten-
tial of such technologies is growing in collaboration tasks
where multiple remote users are equipped with personal and
immersive devices. Not surprising, research on remote col-
laboration is receiving more and more attention from not
only the expert researchers and practitioners, but also public
consumers, with the rapid development and enhancement of
such technologies.

One of the common remote collaboration scenarios is
a remote-expert and local-novice collaboration in different
application contexts. The main purpose of this collaboration
is to increase the availability of a remote expert, so that the
local novice users could get help from the remote expert
anywhere and anytime with appropriate technological sup-
ports. For example, a local novice might experience a broken
machine that needs an expert’s help repairing, a local doctor
with little expertise might need to conduct an operation on a
patient with the essential help from a distant expert surgeon,
or acrime scene investigator could need help from a forensics
expert in a remote place.

Here, we describe five essential factors for remote col-
laboration before addressing relevant research aspects in the
following sections: (1) tasks, (2) local users, (3) remote users,
(4) communication, and (5) tools for collaboration.

Q) Springer

Journal on Multimodal User Interfaces (2020) 14:313-319

(1) Task: Since collaboration is a multi-person activity
for a shared goal, there should be a task that collab-
orators are trying to complete together. A task often
involves task objects that collaborators manipulate. Sev-
eral researchers reported that the result of the remote col-
laboration can be task dependent because the required
object manipulation could be different depending on the
task type [28]. For example, a task for selecting an object
most likely requires the object’s position information
shown with a pointing operation, but tasks like fixing a
car or a bike require both position and orientation infor-
mation for more complex object manipulation.

(2) Local user A local user is a person who 1s in the place
where the task and the task objects are normally located
[37]. The local user requests a help from a remote user
who might have expert knowledge to complete the task.
In a conventional setting like the remote-expert and
local-novice scenario, the local user would manipulate
task objects following the remote user’s guidance.

(3) Remote user As opposed to the local user, a remote user
is a person who provides information and guidance to
the local user for completing the shared task [37]. The
remote user normally does not stay at the place where
the task objects are, but usually communicates with the
local user to show or tell them what to do for the task.

(4) Communication Communication is the activity that the
users perform to share their thoughts and intent while
completing the task together. It involves using vari-
ous communication cues, such as verbal and nonverbal
behaviors, to establish common ground, improve the
quality of communication, and eventually complete the
task successfully. For example, the remote user may
explain the solution to the local user using communi-
cation cues [27]. Verbal cues are usually considered
as more fundamental and basic communication cues,
but they have various limitations because of the lack
of visual representations, which are often intuitive and
efficient. Therefore, many remote collaboration systems
employ visual communication cues, such as a pointer,
sketching, and hand gestures, in addition to verbal com-
munication.

(5) Awareness tool/interface Since the local and remote
users are in different spaces, there should be aawareness
tool (or an interface) to show the current state of the
task and the activity progress [24]. Sharing the users’
views is common feature of the collaboration interface,
often using conventional video conferencing systems.
The local and remote user’s collaborative activities can
be shown in the shared view(s), and communication cues
also visually displayed. Itis important to understand that
the affordances of the collaboration tools can determine
the quality of communication and so the general user
experience and collaboration performance.
Journal on Multimodal User Interfaces (2020) 14:313-319

Considering these five factors for remote collaboration,
researchers have mainly studied three research aspects: (1)
understanding of the effects of individual communication
cues and the combinations of multimodal cues, (2) sharing
the view of the task space for user awareness, and (3) human
factors that include perception and cognition research and
the investigation of user profiles. We will cover these three
research aspects in the following sections.

3 Communication cues and multimodalities

Communication cues are a critical component for effective
remote collaboration. Many researchers reported that it is
more challenging to achieve better user experience and per-
formance in remote collaboration compared to co-located
collaboration. The main reason behind this is because of the
lack of communication cues [15,25]. Therefore, researchers
have been trying to develop novel remote collaboration
systems that could offer richer and more efficient commu-
nication cues for the users to easily understand the task
situation, while understanding the effects of those cues
[9,32].

With typical video conferencing systems, researchers and
practitioners started to add additional visual cues onto the
live video stream of the task space, to provide richer com-
munication channels, such as a visual pointer, sketches, and
the users’ hand gestures [9]. Due to the technical challenge of
capturing dynamic gestures of the users, the visual communi-
cation cues used in early remote collaboration systems were
generally limited to a simple pointer and sketches. Later, Kirk
et al. [31] implemented an system that could simply capture
and share a live video of hand gestures. However, the cues
were still primitive pointers, sketches, and hand gestures in
2D, and the system had limited portability with a fixed camera
on a tripod and a monitor display. It also did not support any
sophisticated computer vision tracking, which caused diffi-
culty to visualize the cues robustly even when the camera had
moderate motions. While addressing this issue for pointer
and sketch cues, Kato and Billinghurst [18] used vision-based
tracking for stabilizing the sketches—the sketches were at
the position where they were drawn regardless of the view
changes. Kim et al. [30] also developed a system that sig-
nificantly increased portability by using a handheld tablet,
and Gauglitz et al. [10] and Kim et al. [28] further enhanced
both portability and sketch stabilization. Regarding hand ges-
ture cues, Alem et al. [1] increased the portability for a local
worker wearing lightweight glasses, and Huang et al. [14]
introduced a system that could capture and share the user’s
hand gestures and local task space in 3D.

Recently, there are three trends appearing in the study of
the communication cues. First, researchers have started more
focusing on the use of gaze cues as an effective communica-

315

tion cue [6,38], especially given the enhanced performance
and popularity of low cost eye trackers. By using the gaze
cues, the users in local and/or remote places can identify
where the collaboration partner is attending and understand
the task state and the partner’s intentions. Gupta et al. [11]
developed a system that visualizes the local user’s gaze as a
pointer on the shared view, while Higuchi et al. [13] showed
the remote users’ gaze pointers. Lee et al. [35] chose a bidi-
rectional gaze sharing approach that adds both remote and
local users’ gaze pointers on the shared view.

Second, it is becoming more and more popular to show
multiple visual cues simultaneously in remote collaboration.
While individual visual cues have their own benefits and
drawbacks, the combination of them could offer an opportu-
nity to have richer social interactions while complementing
each other. For example, Huang et al. [15] explored the use
of the combined communication cues using sketch and hand
gesture cues.

Third, in addition to the multiple visual cues, combining
different sensory modalities for communication is gaining
attention from multimodal interfaces and remote collab-
oration researchers. For example, DeVincenzi et al. [4]
combined a spatial audio cue, the remote user’s spatialized
voice, together with a live video stream, and found that the
local user could more easily identify where the remote user
was located by the direction of the audio source.

4 View sharing and situational awareness

Another important aspect that influences the collaboration
experience in remote settings is to share the user’s view with
the collaboration partner. For example, the local user’s view
can be shared with the remote expert so that he/she can under-
stand the task progress by watching the shared view [16,34].
This sharing view feature is particularly important because
the users could not only see the current state of the task, but
they could also put themselves in another person’s shoes. In
collaboration, social understanding and grounding are crucial
for developing better communication and rapport among the
collaborators [32]. The view sharing methods are advancing
both quantitatively and qualitatively, for example use of high
definition shared views, large panoramic views, and immer-
sive technology like first-person VR experience [44].

In the early stage of remote collaboration technologies,
researchers used a video conferencing system that could only
show a simple live video of the task space [9] and the live
video was streamed from a fixed camera on the local user’s
head or environment [29]. With a handheld tablet, it was
not convenient to share the local user’s perspective with the
remote users because the user’s hands should be occupied
by holding the tablet/camera while they also need to perform
the given task [8]. Recent compact wearable camera systems,

Q) Springer
316

such as GoPro cameras [41], or see-through head-mounted
displays (HMDs) with embedded cameras, like Microsoft
HoloLens, could free the user’s hands and make view sharing
simple and easy.

While the shared view or the layout can be independently
controlled by the remote/local user, some researchers stitched
the images from a live video and then constructed a large
image covering the overall task space [10,17]. With the large
image, the remote expert could have an independent view
regardless of the current viewpoint of local user’s live video
and freely navigate around the task space. For example, Lee et
al. [36] introduced a system sharing a 360-degree live video,
so both local and remote users could individually control the
viewpoint in the 360-degree simulated environment. Pium-
somboon et al. [42] also developed a system that can share a
3D reconstructed environment of the local user’s task space
and support both local and remote users’ navigation in the
environment.

5 Human factors

In remote collaboration contexts, the users should be able
to interact with the remote collaborators in different places
both effectively and efficiently for best task performance.
However, the overall user experience regarding their feeling
of ease of communication, understanding of the other users’
emotional states, the sense of social/co-presence and comfort
is also important to evaluate when considering the effective-
ness of the collaboration systems. Thus, while developing
novel interfaces and effective tools for remote collaborations,
human factors research to understand perception, cognition,
and behavior with the technology should be conducted and
supported.

Researchers have actively studied how visual cues and
tools of sharing a task space influence the collaborators’
sense of social/co-presence and cognitive load during the col-
laboration. For instance, Kim et al. [28] conducted a study
that investigated the effects of augmented visual cues on the
user-perceived co-presence with the remote user. Piumsom-
boon et al. [43] compared different sharing view modes, such
as shoulder-mounted or hand-held camera views, and found
that users generally preferred the shoulder-mounted view
and reported a higher level of co-presence when there was a
shared view. Regarding the effects of visual cues, Huang et
al. [15] found that local and remote users have lower cogni-
tive load during a remote collaboration when they had more
visual communication cues available. More recently, Dey et
al. [5] studied the effect of sharing the user’s physiological
signals with the remote users, such as heart beat as a signal for
representing the user’s emotional state, and found a positive
influence for emotional experience in a gaming context.

Q) Springer

Journal on Multimodal User Interfaces (2020) 14:313-319
6 Featured work

In this section, we introduce the five accepted papers in this
issue and briefly summarize their results and findings, in the
context of the three research aspects; communication cues
and multimodalities, view sharing and situational awareness,
and human factors.

The combination of visual communication cues in mixed
reality remote collaboration In this paper (Kim et al. [26]),
explores the impact of combining different visual cues on
the user-perceived quality of communication and collabora-
tion performance. The visual cues were a pointer, sketches,
and hand gesture cues, which are traditionally independent
in remote collaboration.

Two conducted user studies compared four combina-
tions of the three cues: (1) hand only, (2) hand+pointer, (3)
hand+sketch, and (4) hand+ pointer+ sketch, while varying
the sharing view mode, either a dependent or an independent
view. In the dependent view mode, in which the users shares
the same view, participants mostly used the hand gestures
due to ease of use and intuitiveness, while the sketches were
also useful for clarifying any misunderstandings between the
users. Interestingly, in the independent view mode, in which
the users had their own views, the hand gesture cues were
perceived difficult to identify where the hand referred to,
because of the different perspectives of the users. However,
the pointer cue turned out to be very useful as an alternative
for the hand gestures.

The effects of spatial auditory and visual cues on mixed real-
ity remote collaboration: Despite the importance of auditory
cues in remote collaboration, the current state of research
in MR remote collaboration largely focuses on investigating
the effects of visual cues. To fill the gap, this paper (Yang et
al. [47]), presents a multimodal system that can provide both
spatial auditory and visual cues to the users, and investigates
the effects of such multimodal cues on task performance and
perception in an indoor visual search task through. The user
study results show that the remote user’s spatialized voice
guidance and auditory beacons could improve the local user’s
performance to find objects in a highly cluttered environment
by providing auditory clues for spatial directions. The paper
further discusses the potential implications in the integration
of spatial auditory and visual cues for better user experience
and performance in remote collaboration contexts.

Sharing gaze rays for visual target identification tasks in
collaborative augmented reality: Sharing social cues among
collaborators is a powerful strategy to improve the quality of
communication. In this paper (Erickson et al. [7]), authors
explore the use of a shared gaze ray in a target identification
task while investigating the influence of different gaze errors
in the task performance, such as response time and error rate.
Journal on Multimodal User Interfaces (2020) 14:313-319

The results show that different error levels in the shared
gaze had strong effects on participants’ performance, while
the distance to the task object had less influence on the
performance and user experience. Interestingly, the study
also reveals that participants’ self-assessed performance was
lower than the actual performance in the target identification
task.

Exploring interaction techniques for 360 panoramas inside
a 3D reconstructed scene for mixed reality remote collab-
oration: In this paper (Teo et al. [45]), authors investigate
the effects of different ways to show 360-degree live video
on the user’s sense of social/co-presence and cognitive load
in collaborative object moving tasks. The conducted user
study compared two modes: (1) projecting the 360-degree
live video in a sphere, called the “photo-bubble” mode, and
(2) projecting it as a texture on a low-resolution 3D recon-
structed mesh, called the “projective texture ’” mode. The
paper presents the results suggesting that both modes could
provide a high level of social/co-presence and reduce cog-
nitive load, and discusses the advantages and limitations of
each method.

Effects of personality traits on user trust in human—machine
collaborations: Beyond collaborations between human users,
there is increasing research in human—machine (or human—
agent) collaborations, considering the convergence of
advanced artificial intelligence (AI), such as intelligent vir-
tual assistants, and immersive AR/VR technologies [39].
This paper Zhou et al. [48] explores the effect of a user’s
personality trait on the level of trust in a machine’s deci-
sions when collaborating with the machine. The user study
conducted in the paper varied the task design in terms of
the levels of uncertainty and cognitive load, and examined
how participants with different personality traits perceived
the trustworthiness of the collaborative machine’s decisions
while performing the task. The results reveals that the users’
personality traits affect their perceived trust in machine deci-
sion differently with respect to the uncertainty and cognitive
load conditions.

7 Conclusion and future work

To provide a comprehensive background and share recent
significant findings in remote collaboration research, we pre-
pared this Special Issue on Multimodal Interfaces and Com-
munication Cues for Remote Collaboration, while focusing
on but not necessarily limited to immersive technologies
like AR/VR/MR. This editorial described the concept of
remote collaboration and the research aspects of multimodal
communication cues, sharing the users’ views, and related
human factors for the reader’s understanding of the focused
research, and the summaries of the accepted papers written

317

by the domain-expert researchers and students were pro-
vided.

While technologies for remote collaboration are evolv-
ing even beyond the limited scope of “remote,” the research
aiming at better collaboration and communication tools is
anticipated to be growing in the future. Despite the good
amount of research on collaboration-support technologies,
there are still a lot of gaps that need to be filled by researchers
and practitioners. For example, regarding the multimodal
communication cues, the use of multimodal cues could be
extended to various sensory modalities, such as haptics and
olfactory [23]. The use of communication cues could also
go beyond the scope of human sensory channels, e.g., inter-
pretable brain signals through electroencephalogram (EEG),
which are already actively researched in brain-computer
interface [33].

The design of the communication cues can also be adap-
tively changed according to user needs. For instance, when
using hand gesture cues in a large task space where such
small gestures could be difficult to understand due to the far
distance from a target object, the hand gesture cue can be
transferred on the object in a different form, such as a mag-
nified gesture cue or a virtual pointer [3]. The pointer cue can
have different size, color, and level of transparency according
to distance to the task objects, and appropriate changes of the
pointer in color and size can moderate the balance between
attracting the user’s attention and reducing the disturbance
to see the target objects during the task.

Considering the recent popularity of social VR platforms,
embodied virtual avatars or agents could also be an effective
approach to support or replace human users in collabora-
tion tasks [20,21], and research to understand the effects of
different avatar shapes/appearances in AR/VR is already in
progress [40].

For the immersive and realistic view sharing, there is still
no practical implementation for generating and sharing fine-
grained 3D reconstruction of the dynamic task space in real
time. In the future, various convergence research should be
conducted to achieve this using advanced computer vision,
data compression, and network techniques, while the percep-
tion and cognition studies continue to use different measures
and tools as a broader scope of human—computer interaction
and human factors research.

References

1. Alem L, Tecchia F, Huang W (2011) Remote tele-assistance system
for maintenance operators in mines. In: 11th Underground coal
operators’ conference. University of Wollongong

2. Baecker RM (1994) Readings in groupware and computer-
supported cooperative work: assisting human—human collabora-
tion, Ist edn. Morgan Kaufmann, San Francisco

Q) Springer
318

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

. Choudhary Z, Kim K, Schubert R, Bruder G, Welch GF (2020)

Virtual big heads: analysis of human perception and comfort of
head scales in social virtual reality. In: IEEE conference on virtual
reality and 3D user interfaces, pp 425-433

DeVincenzi A, Yao L, Ishii H, Raskar R (2011) Kinected confer-
ence: augmenting video imaging with calibrated depth and audio.
In: Proceedings of the ACM 2011 conference on computer sup-
ported cooperative work, pp 621-624

. Dey A, Piumsomboon T, Lee Y, Billinghurst M (2017) Effects of

sharing physiological states of players in a collaborative virtual
reality gameplay. In: Proceedings of the 2017 CHI conference on
human factors in computing systems, pp 4045-4056

Erickson A, Norouzi N, Kim K, LaViola JJ, Bruder G, Welch GF
(2020) Effects of depth information on visual target identification
task performance in shared gaze environments. IEEE Trans Visu
Comput Graph 26(5):1934—-1944

Erickson A, Norouzi N, Kim K, Schubert R, Jules J, LaViola JJ,
Bruder G, Welch GF (2020) Sharing gaze rays for visual target iden-
tification tasks in collaborative augmented reality. J Multimodal
User Interfaces. https://doi.org/10.1007/s12193-020-00330-2

. Fakourfar O, Ta K, Tang R, Bateman S, Tang A (2016) Stabilized

annotations for mobile remote assistance. In: Proceedings of the
2016 CHI conference on human factors in computing systems, pp
1548-1560

Fussell SR, Setlock LD, Yang J, Ou J, Mauer E, Kramer ADI (2004)
Gestures over video streams to support remote collaboration on
physical tasks. Hum—Comput Interact 19(3):273-309

Gauglitz S, Nuernberger B, Turk M, Hollerer T (2014) World-
stabilized annotations and virtual scene navigation for remote
collaboration. In: Proceedings of the 27th annual ACM sympo-
sium on user interface software and technology, pp 449-459
Gupta K, Lee GA, Billinghurst M (2016) Do you see what 1 see?
The effect of gaze tracking on task space remote collaboration.
IEEE Trans Vis Comput Graph 22(11):2413-—2422

Gutwin C, Greenberg S (1998) Design for individuals, design for
groups: tradeoffs between power and workspace awareness. In:
Proceedings of the ACM conference on computer supported coop-
erative work, pp 207-216

Higuch K, Yonetani R, Sato Y (2016) Can eye help you? Effects
of visualizing eye fixations on remote collaboration scenarios for
physical tasks. In: Proceedings of the 2016 CHI conference on
human factors in computing systems, pp 5180-5190

Huang W, Alem L, Tecchia F, Duh HBL (2018) Augmented 3D
hands: a gesture-based mixed reality system for distributed collab-
oration. J Multimodal User Interfaces 12(2):77—89

Huang W, Kim S, Billinghurst M, Alem L (2019) Sharing hand
gesture and sketch cues in remote collaboration. J Vis Commun
Image Represent 58:428-438

Irlitti A, Piumsomboon T, Jackson D, Thomas BH (2019) Convey-
ing spatial awareness cues in xR collaborations. IEEE Trans Visu
Comput Graph 25(11):3178—3189

Kasahara S, Rekimoto J (2014) JackIn: integrating first-person
view with out-of-body vision generation for human—human aug-
mentation. In: Proceedings of the 5th augmented human interna-
tional conference, pp 46:1-46:8

Kato H, Billinghurst M (1999) Marker tracking and HMD cali-
bration for a video-based augmented reality conferencing system.
In: Proceedings 2nd IEEE and ACM international workshop on
augmented reality (IWAR’99), pp 85-94

Kim K, Billinghurst M, Bruder G, Duh HBL, Welch GF (2018)
Revisiting trends in augmented reality research: a review of the 2nd
decade of ISMAR (2008-2017). IEEE Trans Vis Comput Graph
24(11):2947-2962

Kim K, Boelling L, Haesler S, Bailenson JN, Bruder G, Welch GF
(2018) Does a digital assistant need a body? The influence of visual
embodiment and social behavior on the perception of intelligent

Q) Springer

21.

22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

Journal on Multimodal User Interfaces (2020) 14:313-319

virtual agents in AR. In: IEEE international symposium on mixed
and augmented reality, pp 105-114

Kim K, de Melo C, NorouziN, Bruder G, Welch G (2020) Reducing
task load with an embodied intelligent virtual assistant for improved
performance in collaborative decision making. In: Proceedings of
the IEEE conference on virtual reality and 3D user interfaces, pp
529-538

Kim K, Norouzi N, Losekamp T, Bruder G, Anderson M, Welch G
(2019) Effects of patient care assistant embodiment and computer
mediation on user experience. In: Proceedings of IEEE interna-
tional conference on artificial intelligence and virtual reality, pp
17-24

Kim K, Schubert R, Hochreiter J, Bruder G, Welch G (2019) Blow-
ing in the wind: increasing social presence with a virtual human via
environmental airflow interaction in mixed reality. Comput Graph
83:23-32

Kim S, Billinghurst M, Lee C, Lee G (2018) Using freeze frame and
visual notifications in an annotation drawing interface for remote
collaboration. KSII Trans Internet Inf Syst 12(12):6034—-6056
Kim S, Billinghurst M, Lee G (2018) The effect of collaboration
styles and view independence on video-mediated remote collabo-
ration. Comput Support Cooper Work 27(3):569-607

Kim S, Lee G, Billinghurst M, Huang W (2020) The combination of
visual communication cues in mixed reality remote collaboration. J
Multimodal User Interfaces. https://doi.org/10.1007/s12193-020-
00335-x

Kim S, Lee G, Huang W, Kim H, Woo W, Billinghurst M (2019)
Evaluating the combination of visual communication cues for
HMD-based mixed reality remote collaboration. In: Proceedings
of the 2019 CHI conference on human factors in computing sys-
tems, pp 1-13

Kim S, Lee G, Sakata N, Billinghurst M (2014) Improving co-
presence with augmented visual communication cues for sharing
experience through video conference. In: Proceedings of the IEEE
international symposium on mixed and augmented reality, pp 83-
92

Kim S, Lee GA, Ha S, Sakata N, Billinghurst M (2015) Automat-
ically freezing live video for annotation during remote collabora-
tion. In: Proceedings of the 33rd annual ACM conference extended
abstracts on human factors in computing systems, pp 1669-1674
Kim S, Lee GA, Sakata N (2013) Comparing pointing and drawing
for remote collaboration. In: Proceedings of the IEEE international
symposium on mixed and augmented reality, pp 1-6

Kirk D, Stanton Fraser D (2006) Comparing remote gesture
technologies for supporting collaborative physical tasks. In: Pro-
ceedings of the SIGCHI conference on human factors in computing
systems, pp 1191—1200

Kraut RE, Gergle D, Fussell SR (2002) The use of visual informa-
tion in shared visual spaces: informing the development of virtual
co-presence. In: Proceedings of the 2002 ACM conference on com-
puter supported cooperative work, pp 31-40

Lecuyer A, Lotte F, Reilly R, Leeb R, Hirose M, Slater M (2008)
Brain—computer interfaces, virtual reality, and videogames. Com-
puter 41(10):66—72

Lee G, Kang HY, Lee JM, Han JH (2020) A user study on view-
sharing techniques for one-to-many mixed reality collaborations.
In: Proceedings of the IEEE conference on virtual reality and 3D
user interfaces, pp 343-352

Lee G, Kim S, Lee Y, Dey A, Piumsomboon T, Norman M,
Billinghurst M (2017) Mutually shared gaze in augmented video
conference. In: Adjunct proceedings of the 2017 IEEE international
symposium on mixed and augmented reality-adjunct, pp 79-80
Lee G, Teo THL, Kim S, Billinghurst M (2018) A user study on mr
remote collaboration using live 360 video. In: Proceedings of the
IEEE international symposium for mixed and augmented reality,
pp 153-164
Journal on Multimodal User Interfaces (2020) 14:313-319

37.

38.

39.

40.

4].

42.

43.

Lukosch S, Billinghurst M, Alem L, Kiyokawa K (2015) Col-
laboration in augmented reality. Comput Support Cooper Work
24(6):5 15-525

Norouzi N, Erickson A, Kim K, Schubert R, Laviola Jr, JJ, Bruder
G, Welch GF (2019) Effects of shared gaze parameters on visual
target identification task performance in augmented reality. In:
Proceedings of ACM symposium on spatial user interaction, pp
12:1-12:11

Norouzi N, Kim K, Hochreiter J, Lee M, Daher S, Bruder G, Welch
G (2018) A systematic survey of 15 years of user studies published
in the intelligent virtual agents conference. In: Proceedings of the
ACM international conference on intelligent virtual agents, pp 17—
22

Norouzi N, Kim K, Lee M, Schubert R, Erickson A, Bailenson J,
Bruder G, Welch G (2019) walking your virtual dog: analysis of
awareness and proxemics with simulated support animals in aug-
mented reality. In: Proceedings of IEEE international symposium
on mixed and augmented reality, pp 253-264

Paro JA, Nazareli R, Gurjala A, Berger A, Lee GK (2015) Video-
based self-review. Ann Plast Surg 74:S71—S74

Piumsomboon T, Lee GA, Hart JD, Ens B, Lindeman RW, Thomas
BH, Billinghurst M (2018) Mini-Me: an adaptive avatar for mixed
reality remote collaboration. In: Proceedings of the 2018 CHI con-
ference on human factors in computing systems, pp 1-13
PiumsomboonT, Lee GA, Irlitti A, Ens B, Thomas BH, Billinghurst
M (2019) On the shoulder of the giant: a multi-scale mixed real-
ity collaboration with 360 video sharing and tangible interaction.
In: Proceedings of the 2019 CHI conference on human factors in
computing systems, pp 1-17

AA.

45.

46.

47.

48.

319

Teo T, Lawrence L, Lee GA, Billinghurst M, Adcock M (2019)
Mixed reality remote collaboration combining 360 video and 3D
reconstruction. In: Proceedings of the ACM CHI conference on
human factors in computing systems, vol 201. ACM Press, New
York, pp 1-14

Teo T, Norman M, Lee GA, Billinghurst M, Adcock M (2020)
Exploring interaction techniques for 360 panoramas inside a 3D
reconstructed scene for mixed reality remote collaboration. J
Multimodal User Interfaces. https://doi.org/10.1007/s12193-020-
00343-x

Tomasello M (2016) Precis of a natural history of human thinking.
J Soc Ontol 2(1):59-64

Yang J, Sasikumar P, Bai H, Barde A, Sérés G, Billinghurst M
(2020) The effects of spatial auditory and visual cues on mixed
reality remote collaboration. J Multimodal User Interfaces. https://
doi.org/10.1007/s12193-020-0033 1-1

Zhou J, Luo S, Chen F (2020) Effects of personality traits on user
trust in human-machine collaborations. J Multimodal User Inter-
faces. https://doi.org/10.1007/s12193-020-00329-9

Publisher’s Note Springer Nature remains neutral with regard to juris-
dictional claims in published maps and institutional affiliations.

Q) Springer
