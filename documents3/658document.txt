Huang et al. EURASIP Journal on Wireless Communications and i
Networking (2020) 2020:203 EURASIP Journal on Wireless

https://doi.org/10.1186/s13638-020-01818-x Communications and Networking

RESEARCH Open Access

Adaptive modulation and coding in ®
underwater acoustic communications: a ji
machine learning perspective

Lihuan Huang! ©, Qunfei Zhang, Weijie Tan?, Yue Wang’, Lifan Zhang?, Chengbing He! and Zhi Tian4

 

 

*Correspondence:

hyb504@mail.nwpu.edu.cn Abstract

‘Research and Development The increasing demand for exploring and managing the vast marine resources of the
Sitectincal Unverety in planet has underscored the importance of research on advanced underwater acoustic
Shenzhen, Shenzhen, 518057, China communication (UAC) technologies. However, owing to the severe characteristics of
choo! of Marine Science and the oceanic environment, underwater acoustic (UWA) propagation experiences nearly
Technology, Northwestern |

Polytechnical University, Xian, the harshest wireless channels in nature. This article resorts to the perspective of
710072, Shaanxi, China machine learning (ML) to cope with the major challenges of adaptive modulation and

Full list of author information is
available at the end of the article

coding (AMC) design in UACs. First, we present an ML AMC framework for UACs. Then,
we propose an attention-aided k-nearest neighbor (A-KNN) algorithm with simplicity
and robustness, based on which an ML AMC approach is designed with immunity to
channel modeling uncertainty. Leveraging its online learning ability, such A-KNN-based
AMC classifier offers salient capabilities of both sustainable self-enhancement and
broad applicability to various operation scenarios. Next, aiming at higher
implementation efficiency, we take strategies of complexity reduction and present a
dimensionality-reduced and data-clustered A-KNN (DRDC-A-KNN) AMC classifier.
Finally, we demonstrate that these proposed ML approaches have superior
performance over traditional model-based methods by simulations using actual data
collected from three lake experiments.

Keywords: Underwater acoustic communication (UAC), Harsh oceanic environment,
Adaptive modulation and coding (AMC), Machine learning (ML)

 

1 Introduction
Ocean, as the origin of life, covers two thirds of our planet, supports 90% of the world’s
freight traffic, and contains a vast amount of underutilized resources. However, human
understanding of the deep ocean is even less than space. Therefore, growing atten-
tion needs to be cast to researches and exploitations of the mysterious ocean. Recently,
thanks to the rapid development of related technologies, underwater acoustic communi-
cation (UAC) systems have found broad applications, such as environmental monitoring,
offshore exploration, disaster detection, and national security [1].

Traditional UAC systems are generally equipped with a fixed set of physical layer (PHY)
parameters, corresponding to a single modulation and coding scheme (MCS). However,

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
GQ) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
— credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 2 of 25

underwater acoustic (UWA) channels are varying temporally and spatially. As a result, it
is impossible for an UAC system to cope with a large variety of UWA channel dynamics
well by only using one fixed MCS [2-5]. To this end, the adaptive modulation and coding
(AMC) technique has emerged to be an appealing avenue for UAC efficiency improve-
ment through tracking channel dynamics and adaptively switching among a set of MCSs
to achieve the most efficient transmission.

In 1968, as the origin of the AMC technology, Hayes proposed an adaptive scheme
where the transmitter uses the channel state information (CSI) fed back from the receiver
to adjust parameters [6]. From then on, lots of research efforts on applying AMC to terres-
trial wireless communications have been made. In 1992, Webb presented a variable-rate
quadrature amplitude modulation (QAM) system, which offered an attractive solution
to the bandwidth restricted microcellular networks [7, 8]. In [9], a bit error rate (BER)
comparison was made among various modulation schemes that are used for AMC and
then came out with the optimal signal-to-noise ratio (SNR) range of each scheme. In [10],
adaptive systems were introduced by evaluating the performance of some simple QAM
schemes in both perfectly known and predicted channels. Moreover, in [11], a cross-layer
combination of AMC with the truncated automatic repeat request (ARQ) technology was
made for the communications of secondary users in cognitive radio networks, which can
adapt well to the radio conditions and make full use of the available resources.

Unfortunately, in contrast to terrestrial wireless communications, UACs have to face
several unique challenges caused by the undesirable UWA channel characteristics, such
as the much more complex spatio-temporal channel variability, more severe multipath
fading, and more limited bandwidth [12]. As a result, the development of AMC in UACs
is far behind its terrestrial-based counterpart. Some existing results are summarized as
follows. Stojanovic used the product of Doppler spread and multipath spread as a cri-
terion for switching between coherent and non-coherent communication modes [13].
For UWA orthogonal frequency-division multiplexing (OFDM) systems, Wan et al. uti-
lized the effective signal-to-noise ratio (ESNR) as a new performance metric for AMC
[14]. In [15], Shen et al. selected SNR as the switching metric and presented an adaptive
multimode orthogonal multicarrier (MOMC) technology.

So far, the underwater AMC researches have generally focused on the model-based
methods. Unfortunately, although extensive efforts have been put on UWA channel mod-
eling, there is not a general channel model yet that fits accurately in various practical
scenarios (detailed analysis will be given in Section 2.1), due to the high uncertainty and
complexity of UWA channels. As such, those model-based AMC methods can be either
insufficient or inaccurate in practical UAC scenarios. To address this problem, we resort
to the data-driven machine learning (ML) technology to empower underwater AMC with
intelligence, so as to offer immunity to channel modeling uncertainty and thus enabling
flexible system optimization and sustainable performance improvement. The ML meth-
ods can make predictions or decisions from data observations without the aid of a specific
model.

The recent revival of the ML technology has found its wide applications in broad fields,
including image/audio processing, economics, and computational biology [16]. More-
over, there are also some interesting results obtained by introducing ML into the field
of communications. In terrestrial radios, deep learning (DL) has been advocated for
demodulation in OFDM systems [17]. For 5G wireless systems, an efficient online CSI
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 3 of 25

prediction scheme which learns the historical data via deep neural networks (DNNs) has
been designed [18]. For non-cooperative communication systems, a DL-based method
was proposed to perform automatic modulation classification [19], while for UACs, an
adaptive and energy-efficient routing protocol for underwater delay/disruption tolerant
sensor networks has been proposed [20]. Moreover, NATO has developed a decision tree-
based approach that is capable of choosing the modulation scheme with the highest data
rate among several predefined single-carrier signals depending on CSI [21]. In [22], a rein-
forcement learning-based adaptive transmission strategy was presented for time-varying
UWA channels, which formulates the adaptive problem as a partially observable Markov
decision process. These early successes illuminate the feasibility and potential benefits of
applying ML in wireless communication systems.

In this paper, we focus on a novel ML-based AMC framework for UACs. Therein, the
AMC procedure is formulated as a classifier that has been trained by a pre-organized
and labeled database (i-e., training set). After performing model training to establish the
functional mapping, we treat such a classifier as a black box, with the input being the
real-time channel state and the output being the corresponding optimal MCS. Further, we
adopt an online learning mechanism to enable continuous classifier updating during the
AMC operation. In doing so, our strategy has salient capabilities of both sustainable self-
enhancement and broad applicability to diverse UAC scenarios. The main contribution of

this paper can be summarized as follows:

e This paper resorts to the perspective of ML and gives a complete ML AMC
framework for UACs, which consists of not only the specific classification algorithm
but also the procedure of data preprocessing and labeling. The latter is essential to
the success of ML but is often overlooked in generic ML literature.

e A new online learning attention-aided k-nearest neighbor (A-k NN) AMC classifier
based on supervised learning is proposed, which enables a novel implementation of
AMC with immunity to channel modeling uncertainty.

e Aiming at higher implementation efficiency, we further design an improved
approach called the dimensionality-reduced and data-clustered A-k NN
(DRDC-A-kNN) AMC classifier, which yields lower complexity by performing
feature dimensionality reduction and training set condensation.

e The above contributions have been verified by extensive simulations using actual

data collected from lake experiments.

The remainder of this paper is organized as follows. Section 2 analyzes the reason for
lacking a general UWA channel model and then defines the system model of ML-based
AMC. Section 3 describes our proposed A-KNN-based AMC method. Section 4 focuses
on the implementation efficiency improvement of A-KNN-based method and designs
the DRDC-A-KNN AMC classifier. Section 5 presents the simulation results. Finally,
Section 6 concludes this paper and discusses some future directions.

2 System model

In this section, we first explore the reason for the current lack of a general model for
UWA channels. Then, we define the system model of AMC in UACs. Next, we formulate
the AMC procedure as a classification problem from an ML perspective and discuss the
considered ML algorithm, followed by an introduction of the MCSs to be used.
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 4 of 25

2.1 Analysis of UWA channel model

Since almost all electromagnetic frequencies are severely absorbed and dispersed in water,
underwater information transmission is conducted dominantly by acoustic waves [23].
As summarized in Table 1, UWA channels suffer from much more complicated distor-
tions and interferences compared with its terrestrial wireless counterpart and thus posing
serious performance-degrading factors to UACs.

Recently, due to its interpretability and simplicity, the ray-tracing model is widely used
to formulate the propagation of UWA waves, which assumes that the sound energy prop-
agates along some eigenrays from the source (i.e., transmitter, denoted by TX) to the
destination (i.e., receiver, denoted by RX). Therein, following Snell’s law, acoustic rays
always bend toward the region with lower propagation velocity. Let T, S, and z denote
temperature, salinity, and depth, respectively, we can calculate the speed of UWA waves
(denoted by c) empirically as

c =1448.96 + 4.591T — 5.304 x 1072T? + 2.374 x 10°-*T? + 1.340(S — 35)...
+ 1.630 x 10~7z + 1.675 x 10°’z” — 1.025 x 10°7T(S — 35)... (1)
— 7,139 x 107 Tz?.

Such formula reveals that any change in these specific measurements crucial for UAC will
result in variation of c, which can induce refraction of acoustic ray paths [3, 24]. Conse-
quently, the complexity of UWA propagation comes from the irregularity of the sound
speed profile (SSP), which shows the speed of sound in water at different vertical lev-
els. However, as marine environment is a typical inhomogeneous medium with strong
dynamic characteristics of seasonal changes and day-night temperature variations, there
is still no widely accepted method that can effectively and accurately predict the compli-
cated SSP variations [25]. Such complexity further makes it quite challenging to construct
accurate and general UWA channel models in an affordable manner.

2.2 System model for AMC in UACs

Considering a node-to-node UAC link from the TX to the RX, we define the system model
of AMC as depicted in Fig. 1. Once receiving a data frame that has been encapsulated into

Table 1 Comparison of electromagnetic waves in the air and UWA waves

 

Characteristic

Electromagnetic waves in the air

UWA waves

 

Medium dependence
Propagation uniformity

Absorption loss under water
Speed in the air
Speed under water

Typical working frequency and
wavelength

Communication latency
Multipath delay

Doppler
Variation in time and space

Propagate regardless of medium, even
in vacuum

Generally along a straight line, at a stable
speed

3 dB/m@10kHz
3 x 108 m/s
2.25 x 108 m/s

GSM—frequency, 900 MHz; wavelength,
0.33 m

Small
Small multipath delay

Small scaling factor (< 107°)

Related to change of communication
scenarios and_ variation in short-wave
ionospheric reflection

Must rely on medium vibration

Along a curve, with speed greatly
affected by temperature

1.1 dB/km@10kHz
340 m/s
1490 m/s

Sonar—frequency, 5 kHz;
wavelength, 0.3 m

Large

Large delay (> 10 ms), across
dozens of symbols.

Large scaling factor (107?)

Related to rapid changes of waves
and periodic changes of seawater

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 5 of 25

 

Inform

     

  
 

Waveform
Shaping

Data

 

Fig. 1 The system model of AMC in UACs

 

 

 

acoustic waveforms at the jth (j € {1,2,...,/}) time instance, RX first performs channel
estimation to sense the channel condition h; ¢ H, where H represents the set of all the
observed hj. h; can be represented by a P-dimensional CSI feature set f; € R”, in the form
of

§ = ar fos fp), (2)

where fj, is the pth measured CSI feature. Then, according to the obtained fj, a proper
MCS m; € M,i € {1,2,...,/} that best matches it will be selected as the optimal solution

Mopt under a specific policy 2
Mopt = {mj € M|z}, (3)

and then fed back to the TX. Given the harsh UWA channel dynamics, it is necessary to
develop and maintain a finite set of allowable MCS realizations (i.e., M) for trading off
throughput and reliability in practice, where each m; defines a channel coding scheme
with rate R, plus a modulation scheme with rate Rp, and the corresponding actual physical
layer data rate [26, 27]

R;(bps) = Rp X Re. (4)

Next, once notified with a new Mop;; the TX will switch to this scheme immediately for
subsequent transmissions.

Note that the abovementioned policy z is a mapping from channel quality measure-
ments to the MCS to be picked [28]. According to different application scenarios, 7 can
aim to either maximize the throughput or minimize the bit error rate (BER). In this paper,
for the purpose of maximizing the link throughput R; while satisfying a certain BER con-
straint @ (ie., BER; < ), index of the desired solution for a given channel state will be
selected depending on

arg max Rj, if BER; < @
i= i (5)

Do not transmit, if BER; > g.
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 6 of 25

2.3. AMCas classification: an ML perspective
To improve the efficiency of AMC systems, designing an appropriate MCS switching
algorithm is of great importance. Existing AMC methods for terrestrial wireless commu-
nications can be categorized into two groups: one is based on instantaneous CSI (ICSI)
obtained from channel estimation, while the other is based on statistical link information
(SLI) inferred through long-term observations or historical knowledge. Unfortunately,
due to the complicated SSP variation in the UWA environment, the ICSI-based meth-
ods often fail to work effectively for UAC due to the lack of a general channel model
that accurately represents complicated UWA propagation effects. Meanwhile, the SLI-
based methods hinge on long-term channel statistics and thus suffer severely from
slow response speed to fast dynamics and sudden changes in UAC links. These draw-
backs of conventional methods motivate us to develop ML-aided AMC approaches for
performance improvement.

Turning to the perspective of ML, the AMC procedure can be formulated as a classi-
fication problem that aims to partition R” into nonoverlapping feasible regions for each
m;. As Fig. 2 depicts, AMC is equivalent to a classifier G(-)

G(-):m; = Gf), f € R’. (6)

As such, we further propose a novel framework of ML-based AMC for UAC systems. As
illustrated in Fig. 3, it is appealing to track and adapt to complex UWA scenarios, with
immunity to channel modeling uncertainty.

2.4 Classification algorithm for ML-based AMC

Generally, typical ML algorithms can be classified into four broad categories depend-
ing on the nature of the dataset for learning or the feedback mechanism available to
the learning system. They are supervised learning (SL), unsupervised learning (UL),
semi-supervised learning (SSL), and reinforcement learning (RL), where SL algorithms

 

 

 
   

 

 

 

 

Feature dimension
Do not transmit

 

Feature dimension

 

 

Fig. 2 Partition a two-dimensional feature space into feasible regions for each available MCS

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 7 of 25

 

 

 

 

 

 

 

     

 

 

TX RX

Input ) ¢ Output

pur Modulation & Coding |G) (®) Demodulation & Decoding ;>—> 'P
data data

Low _ CSIs obtained
[ihutceyntca ~ through channel
features ISImboOE-LLoyel
extraction allocation

The determined new MCS (ff
ML AMC classifier

        
    
    

Classification
model design

Training set

 

ML algorithm
selection

  

bee

Output (Predict)
Fig. 3 ML model for AMC in UACs

a

 

 

 

are more convenient at solving classification problems due to their ability to infer an
input-output mapping function from labeled training data.

In this paper, we adopt the KNN algorithm to investigate the potential of ML for AMC in
UACs and obtain the AMC classifier G(-). As a non-parametric method among the most
popular SL approaches, KNN is often used as a benchmark for more complex algorithms,
such as support vector machine (SVM) and deep neural network (DNN), thanks to its
simplicity and robustness that leads to achievable results even facing small training sets
[29].

Assume a training set T

T = {0 Xn)sn = 1,2,..,N}, (7)

where y,, denotes the labeled membership of each observation and x, = (fyi, fina-0SnP)
represents the associated feature values. Once given a query w, the kKNN algorithm
first searches in T to find its k-nearest neighbors depending on some specific distance
measurements d(-), where the Euclidean distance

2

P
d(@,Xn) = S (fup — wp)” (8)
p=1

is the one that has been widely utilized. Then, KNN proceeds to the voting stage and labels
w with the class y,, that the majority of the k neighbors belong to. Such a process can be
expressed as

K
Yo = argmax ) 5 (y = Yk) (9)
k=1
where 6 is the Dirac function that equals to 1 if y = yz or 0 otherwise.

However, since different distances reflect different degrees of similarity, the information
provided by each of the k-nearest neighbors to support the classification process is obvi-
ously of different importance. Thus, directly adopting the conventional KNN algorithm
where each neighbor has a equal weight in the voting stage will inevitably bring some per-
formance degradation to the classification, or even lead to incorrect results. To address
this issue, we resort to the attention mechanism and propose the A-kKNN algorithm for
the underwater AMC task. As a cognitive process of selectively concentrating on a few
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 8 of 25

features while ignoring others, the attention mechanism can help ML models assign dif-
ferent weights to each part of the input, extract more critical and important information,
and make more accurate judgments without incurring more costs to model computation
and storage [30, 31].

In the A-KNN algorithm, the specific job of the attention mechanism is to produce a set
of w; for the concerned neighbors, where w; denotes the weight of the kAth-nearest neigh-
bor of w. Then by assigning nearer neighbors with higher w ;, attention can dynamically
highlight the importance of different neighbors in the voting stage. Thus, we have

K
yA = arg max ) We X O(y = yx). (10)
k=1
Note that attention weights can be trained, or predefined based on some sort of correla-
tion metric, or even be Gaussian shaped with tunable parameters. In this work, we set wx
to the Squared Inversion (SI) kernel, i.e.,

we = w(d;) = (11)

“72°
dy
2.5 MCS model

In this work, we adopt the convolutional coded multicarrier multiple frequency shift
keying (CC-MC-MFSK) as the transmission scheme to evaluate the proposed ML-based
AMC system, where Fig. 4 depicts its structure.

2.5.1 MC-MFSK

With the advances in UAC technologies, considerable efforts have been made in the
design of modulation schemes. From FSK and phase-shift keying (PSK), through orthog-
onal frequency-division multiplexing (OFDM), to the latest orthogonal signal-division
multiplexing (OSDM) [32-34], these modulation schemes have been investigated exten-
sively and proven useful in the harsh oceanic environment.

In this paper, we adopt the scheme of MC-MFSK, which combines the techniques of
MEFSK and OFDM to transmit information in parallel over multiple orthogonal subchan-
nels [35, 36]. As such, this method not only inherits the robust performance of MFSK,
but also integrates the high spectral efficiency of OFDM. Moreover, by introducing the

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Transmitted
Data Convolutional MFSK .
——_———_——_> —> —>
— Mod S\P CP Insertion
Received
Data .
C lut | MFSK
<< _| onvoraional |g <+— P\S CP Removal
Encoder Demod
Fig. 4 Block diagram of CC-MC-MFSK

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 9 of 25

parameter of frequency diversity, an additional degree of freedom in the MCS table design
can be obtained to improve its scope of application.

2.5.2 Convolutional code

To reduce the transmission errors caused by the noise and interference in UWA chan-
nels, there have been extensive works on the design of error-correcting code (ECC) that
can improve the ability of error controlling at the price of adding redundancy to the
original message. Among existing ECC approaches, such as Reed-Solomon (RS) codes,
low-density parity-check (LDPC) codes, and turbo codes, the simple convolutional code
with Viterbi decoder is selected as the coding scheme in the following discussions, thanks
to its ability to obtain a good trade-off between error-correcting performance and imple-
mentation complexity. To be more specific, the adopted convolutional code is with coding
rate R, = 1/2, constraint length 7, and generator polynomial (171, 133).

3 A-kKNN AMC for UAC

In this section, we present a novel ML framework for AMC in UAC systems, where
an online learning A-KNN classifier serves as the switching method for predicting the
optimal MCS to maximize the link throughput.

3.1 System assumptions
Specifically, we consider the following assumptions in our A-KNN AMC method:

e Accurate channel knowledge. We assume that through channel estimation, the RX
obtains CSI accurately, thus enabling a high-quality training process.

e Perfect feedback. Generally, RX informs TX of the selected MCS by sending a message
through the feedback channel. In this paper, we assume an error-free feedback stage.

3.2 A-KNN classifier
Figure 5 illustrates the architecture of our A-KNN AMC method, where a two-stage pro-
cess is conducted. During the offline training stage, the mapping function between the

Construct A-kNN classifier
training set design
Attention
Offline Training mechanism

eee ee ee ee ee ie ee ee ee ee ee a) ee a a ee ee a a ee ee ee ee ee Ee ee

Optimal MCS

 

   
   
     
 

 

Updatae

Fig. 5 System architecture of the A-KNN-based AMC

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203

input CSI and the output MCS modes is established by training the A-KNN classifier iter-
atively until a certain stopping criterion is satisfied, a.k.a. an expected model prediction
accuracy. Therein, the training set is constructed based on the signal samples generated
from predefined M for various kinds of H. During the online deployment stage, the
trained classifier is applied to analyze the real-time input CSI vector w and generate the
optimal MCS to best match the practical UAC channel conditions. Further, an online
learning mechanism is incorporated to update the AMC classifier as new data arrive, so
as to constantly improve the applicability of the model. Then, we summarize the A-KNN
AMC method in Algorithm 1.

Algorithm 1 Online learning A-KNN based AMC
Input: 7, @, K(1 < K <N)
Output: 7;, Tew
1: Calculate distance:
for n=1 to N do

calculate d(w, x) via Eq. (8), i.e., d(@,Xy) = (a (fi, — wp)”)

2: Find the K nearest neighbors depending on d(a, x;,)

NI-

3: Calculate attention weights:
fork = 1to K do
calculate w, via Eq. (11), ie., we = w(d(a, xx))

4: Vote: y4 = arg max, >) We x O(y = Yk)

5: if w has the same number of nearest neighbors that belongs to different MCSs then
y4 equals to the MCS with higher R;

6: Update T: Trey = {T U (mj, w)}, where ; = yi

7; return (;, Tew)

In the framework of the proposed A-KNN AMC discussed above, there are some critical
steps that are needed to be further clarified. Next, we elaborate on two techniques of the

model training: feature set selection and training set construction.

3.3 Feature set selection

To apply A-KNN to AMC, we start with collecting a set of synthetic and real labeled data
from both simulations and field experiments. Without loss of generality, various UWA
channel models and test scenarios are used to generate the input channel data, each of
which is represented by a P-dimensional feature set.

To support a good training accuracy, we assign P with a large value to provide enough
information, or the capability of our A-KNN classifier will be restricted seriously. How-
ever, due to the so-called curse of dimensionality, each dimensionality added to R” leads
to a significant computational complexity increase in both feature extraction and model
training [37]. As such, there is an important trade-off between information sufficiency
and computational efficiency. To this end, the current practice is to preset the feature
space by experience or prior knowledge. In this work, we construct a six-dimensional fea-
ture set f ¢ R®° to represent different UWA channel conditions by extracting the following
CSI parameters: signal-to-noise ratio (SNR), time delay spread (t,ax), time delay of the
strongest path (Tyg), total power of the first three paths (e3), total power of all paths

Page 10 of 25
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 11 of 25

(€rotaj), and the normalized amplitude of the first path (|41|). Note that e3, ezo¢q;, and ||
are related to the normalized format of the raw channel impulse response at each obser-
vation instance, where the amplitude of each path has been scaled to [0, 1] by dividing the
absolute value of the strongest path amplitude. Besides, e3 and e;o¢g; can reflect the energy
distribution of the first three paths and of all paths, which is related to the complexity of

channel structure.

3.4 Training set construction

As the foundation of ML techniques, training data is an essential set of input information
that enables ML algorithms to learn the underlying principles and extract key features. For
the proposed A-KNN AMC classifier, the constructed training set has to involve the cor-
responding BER, denoted by BER,(mj, f;), of each m; in all kinds of f;. Once the required

information of each observation
04 = {mi, fj, BER; (-)|BER(-) < 9} (12)
is made available, we first store them in the corresponding subsets according to mj, i.e.,

Tom; = {Qi 0;)),] = 1,2, od} , (13)

and then merge all to form the training set:

I
To = U Ton . (14)
i=l

So far, an original training set has been successfully constructed, as illustrated in Fig. 6.
However, as an important step before the training starts, further preprocessings to To are
needed to turn raw data into a cleaner and more reasonable format for the AMC task.

3.4.1 “One-to-one” mapping

Each original To,,, includes the observation of m; in all the possible channels, thus making
the mapping relationship between M and H, provided by the whole training set, one-to-
many. Unfortunately, such mapping relation will significantly confuse the classifier and
make it impossible to determine the optimal MCS for each specific f; through training.
To solve this problem, we use Eq. (5) to modify the sets and only retain information of the
desired Mopr, So as to obtain a one-to-one mapping function for model training. Then, the
processed To, can be expressed as

Tin; = {is Oin;), Hi = 1,2, wy Ni} , (15)

 

 

Training set

 

 

Subset: m, Subset: m> Subset: m,

Index| f <IR” /|BER| |Index| fcIR’ |BER Index| fcIR® |BER

Fig. 6 Structure of the training set

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 12 of 25

with N; denoting the number of observations retained in the ith subset.

3.4.2 Feature scaling
Since the various features included in To are almost impossible to have a consistent mag-
nitude, the one with a wider range of value will dominate the distance calculated by
A-kNN, which means other features’ influences will be overpowered, and thus, significant
loss in training accuracy will be caused. To address this issue, we perform feature normal-
ization across all variables. Specifically, for each f,, its normalized counterpart fp can be
calculated via

fi _ Sp = Somin (16)
Somax — fpomin
with fomin and fypmax denoting the minimum and maximum values of f,, respectively. After
feature scaling, a new training set T with normalized feature quantities has been success-
fully obtained. Let N = 4 Nj represent the total number of observations that belong
to different T;,,, we have the data matrix of the whole channel observations as

t T T
X] fi fiz --- Sip

T T T
X2 tay fry +++ Sop
XNxP = . —!, . . (17)
t T T
XN Ini Ino .- + fyp

Note that all the training sets we discussed in this paper have been preprocessed by the
abovementioned two steps.

4 Anefficiency-enhancing A-KNN AMC approach

With the ability to implicitly learn the uncertain and complex UWA channel, the pro-
posed A-KNN AMC classifier is demonstrated to achieve higher efficiency and wider
applicability than the traditional model-based approaches. However, before deploying it
into practical scenarios, we should give enough attention to the inherent complexity of
kKNN algorithm and make efforts to improve its implementation efficiency.

4.1 Complexity of the A-KNN classifier

The implementation complexity of the A-KNN classifier can be generally divided into the
following two major aspects: (1) storage complexity of large amounts of training data and
(2) computational complexity in the searching of the nearest neighbors. Specifically, to
implement the proposed classifier in practice, a major difficulty is that we have to reserve
enough memory to store all the training data. Moreover, since the searching of the nearest
neighbors requires computing and sorting the distances from all stored observations, the
proposed classifier will be computationally intensive when facing huge amounts of data
or high dimensional feature space. All of these adverse characteristics pose significant
challenges for the proposed A-KNN AMC method to achieve a good performance in the
actual deployment.

4.2 DRDC-A-KNN classifier

To overcome the aforementioned challenges, we turn to design an improved approach
with lower complexity than previous solution, which is called the DRDC-A-kNN clas-
sifier. Figure 7 illustrates the architecture of this new approach by highlighting its
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 13 of 25

 

 

Construct
training set

Offline Training

Updatae DR ys
samples

 

 

 

Fig. 7 System architecture of the DRDC-A-ANN AMC

 

two significant improvements: (1) feature dimensionality reduction through principal
component analysis (PCA) and (2) training set condensation via k-means data clustering.

4.2.1. Dimensionality reduction

As a frequently used technique in data analysis, PCA provides a tool to seek linear com-
binations of the original variables which retain maximal variance and thus minimize
information loss over feature transformation. In the DRDC-A-kNN classifier, we adopt
PCA to reduce the complexity induced by high feature dimensionalities. Let X’ denote
the column-wise centralized form of the original N x P data matrix X

/ / / 7
x} Sir fiz +++ Sip
/ / / 7
x ty foo «+> &
2 21 J22 2P
Xx’ _ ; = . ; . , (18)
/ / / 7
Xn Ini In2 +++ Ip
which contains N observations with each represented by a P-dimensional feature set. We
perform singular value decomposition (SVD) to X’ and obtain

xX’=urv’!, (19)

where singular values in Uyyxp are sorted in descending order. Then, the columns of
Unxn =n xp are the principal components (PCs), while PC loadings are represented by
the corresponding columns of Vp, p, and sample variance of the gth PC can be calculated

as LS /(N — 1) [38]. Generally, holding more than an expected ratio y of total variance,
L.e.,
Q 2
4
etn Do ZW (20)
diq=l Gq

the first Q PCs are retained to compactly represent the original data for training. Along
this way, a great dimensionality reduction can be achieved by PCA through converting
f < R? into a lower dimensional subspace f € R&, ie.,
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 14 of 25

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Training set after DRDC ! Feature: f eR’ Label :
id | !
i | 2 3 .. NP Dimensionality |
Subset: m, Subset: m Subset: m, ! v _ | reduction ”
_ _ an DR: f >f €R? |
Si] Gp» f eR} |S5)| ef eR Sy| (vfeR —) ! ; 5 No Data :
J ; _ j I yy” v clustering !
Sy, Cy f E Re S»,, Cc, f € R2 Shy Cc, f E R2 ! 1 VIO |
Po Yo !

Fig. 8 Training set architecture for the DRDC-A-kKNN AMC

f=fx V[:,1: QJ]. (21)

4.2.2 Dataclustering

Another efficiency-improving measure is to cluster each class of training samples and
then use only some representative observations for the model training. Considering its
efficiency and robustness in cluster analysis, we adopt the k-means technique to perform
data condensation. Given a training set as depicted in Fig. 6, k-means is performed in each
subset T;,, to partition the N; observations into V(V < N;) clusters as

Tn; —= {Si1, Sj2) +++ Siv}; v = 1,2,..., V, (22)

with c;, denoting the corresponding centroid of each sj. Specifically, such procedure
can be accomplished through proceeding the following two steps iteratively until satisfy-
ing the stopping criteria that the assignments no longer change when the centroids are
updated:

e Step 1 (data assignment). Assign each observation to the cluster of the nearest cjy,
where the squared Euclidean distance is used, i.e.,

5s) = arg min ||oj,, — 113, (23)
cCivEC

Note that the initial c;, are some randomly selected points from Tj,,.

e Step 2 (centroid update). Once an assignment is finished, recalculate the means of
the new cluster to update its centroid as

t-+1 1
a a
Is’ llo
0;yEes!)

iv

Table 2 Complexity comparison of A-KNN and DRDC-A-KNN AMC classifiers. knew is the selected new
k after DRDC, which is equal to the least integer greater than /V/

 

 

 

. Algorithm
Comparison
A-kNN DRDC-A-KNN
Distance calculation O(PN) O(QV))
Processing Sorting O(NIogN) O(Vilog(VI))
Voting O(k?logk) O(k2.. logknew)

Memory (bits) b(N(P + 1)) b(VI(Q+1))

 
Huang et al. FURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 15 of 25

Table 3 MCS table

 

 

Index Name Modulation scheme Diversity Coding scheme Re R; (bps)
] MCS1 MC-2FSK 4 Convolutional code 1/2 227

2 MCS2 McC-4FSK 2 Uncoded 1 911

3 MCS3 McC-8FSK 1 Uncoded 1 1366

 

Finally, using the obtained centroids to represent each corresponding cluster, we
successfully obtain an efficient form of the training set

I Iv VI
Tprec = \\_) Torvc-m ¢ = 4_J}) one) t f = 4 Omen) f- (25)
i=1 i=1 lv=1 n=1

Figure 8 shows its architecture. Therein, the number of features in the training set is first
reduced from NP to NQ via DR, then further decreased to VIQ after DC. Assuming that
b-bit memory is required for the storage of each feature or label, we compare the com-
plexity of the DRDC-A-KNN AMC classifier and that of the previous A-kKNN approach, as
shown in Table 2. Remarkably, this novel design with enhanced computational efficiency
is demonstrated to be effective.

4.2.3, Online learning
Moreover, withthe ability of online learning, the DRDC-A-KNN AMC method is able
to continuously improve its understanding of the UWA environment. Therein, Tprpc is
updated through tuning the centroid c;, of each cluster as the new sample w arrives, i.e.,
ewe) — (cf x iy + @), (26)
where 7, is the number of observations included in each cluster before the new arrival,
and @ is the DR processed w. The algorithm of DRDC-A-kKNN AMC is summarized in
Algorithm 2.

 

"C

a
\

OPT to "ea tebe (201 19)

     

Danjiangkou Reservoir (2016.6)

_—Fuxian Lake (2013.7) _

Ganhe Reservoir (2011.10)

 

 

Depth(m)
a

10F

 

 

 

 

 

 

12 100 45
1470.4 1470.6 1470.8 1471 1471.2 1471.4 1471.6 1460 1465 1470 1475 1480 1485 1490 1495 1440 1450 1460 1470 1480 1490 1500
Sound velocity(m/s) Sound velocity(m/s) Sound velocity(m/s)

Fig. 9 Configuration of lake experiments

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 16 of 25

Algorithm 2 Online learning DRDC-A-kKNN based AMC
Input: Tprpc; ©, Knew < Knew < VI)

 

Output: 7;, Tew
1: Perform DR to m using the same loadings of X’:
w=w xX VpxQ
2: Calculate distance:
for n=1 to VI do
calculate d(@, cz) via Eq. (8), ie., d(@, ¢7) = (Sea (fig _ jq)”)
3: Find the Kyey nearest neighbors depending on d(a, cj)

Nile

4: Calculate attention weights:
for k = 1 to Kyew do
calculate w, via Eq. (11), ie., we = w (d(a, cx))
5: Vote: ys = argmax, >) we X O(y = x)
6: if @ has the same number of nearest neighbors that belongs to different MCSs then
yA equals to the MCS with higher R;
7: Update T through tuning cj:
Drew = om {Uy (Vis ont) )

8: return (7;, Tyew), where m; = 5

5 Simulation results

To evaluate the performance of the proposed two ML AMC approaches, several simula-
tions have been conducted in CC-MC-MFSK UAC systems with three predefined MCSs,
as depicted in Table 3. For data gathering, we collected a large set of real-world chan-
nel measurements from three previous field experiments conducted at Ganhe reservoir
(October 2011), Fuxian lake (July 2013), and Danjiangkou reservoir (June 2016) [39].
Figure 9 shows the configurations, and Table 4 provides the mean value of each selected
feature associated with these experiments. These data are then organized and labeled.
Specifically, for each channel condition, the corresponding MCS is labeled by testing
each MCS and selecting the best one according to Eq. (5). Eventually, a dataset of 1656
observations is made available, with labels covering all three MCS values.

Further, according to different simulation purposes, two categories of training sets are
constructed as depicted in Tables 5 and 6, respectively. The first category is used to train
and optimize the AMC classifier, aiming to validate the attention mechanism, select k
value, etc. To this end, each training set is a randomly extracted part from the whole 1656
observations. On the other hand, the second category is to evaluate the online learning
ability of this AMC approach when deployed in practice, where each training set includes
all the observations in a specific lake environment. Noticeably, throughout the simula-
tions, we adopt the technique of k-fold cross-validation with k = 10 [40], to calculate the
corresponding classification accuracy (n) for AMC.

Table 4 Lake environment

 

 

Index Name T max(ms) T hmax(Ms) e3 Ctotal |h1|
] DJK-h12 6.9 6.6 0.24 2.25 0.27
2 FXH-h2 29,2 28.6 0.4 2.43 0.26

3 GH-h5 12.5 12 0.21 2.60 0.24

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 17 of 25

Table 5 Category 1 of training sets
Index 1 2 3 4 5 6
Sample number 270 540 810 1080 1350 1656

 

 

 

5.1 Analysis of the A-KNN AMC

5.1.1. Impact of different k values

As a key hyperparameter in KNN, k is the number of instances that are taken into account
for the determination of affinity with different classes. However, a proper value of k that
leads to high prediction accuracy is challenging to be derived. Specifically, small k values
may increase undesired noise effects, while large values of k will make the system com-
putationally expensive or even produce errors when k exceeds a certain value. In Fig. 10,
given different k values ranging from 1 to 55, we investigate the performance of the A-
KNN classifier corresponding to all the training sets listed in Table 5. Therein, n is found
to improve rapidly as k increases at the beginning. However, this trend slows down and
almost saturates when k is greater than 15. Consequently, we set k to 15 in this work.

5.1.2 A-kKNN AMC versus traditional SNR-based AMC

To better understand the nature of A-KNN AMC performance, we learn the mapping
function from the input channel conditions to the output MCSs by training an A-KNN
classifier by training set 6. The learned results are evaluated on test data in terms of the
optimality of the predicted MCS (Fig. 11) and the achieved performances in terms of
average throughput (aTP) and BER (Fig. 12), with comparison to a traditional model-
based method that only adopts SNR as the MCS switching metric. Therein, aTP at the eth
SNR level is calculated via

1]

ante) = Nie)
€

 

S Nie) (1 — mean(BERie))) Ril, (27)
ieL

where N;<) denotes the total number of observations under such condition, while L, Nile)»
and BER;<) represent the set of indexes of the optimal MCSs, the number of correct
optimal solutions in m;, and the corresponding BER, respectively. Noticeably, since chan-
nels are represented by multidimensional features rather than a single SNR, each SNR
may correspond to multiple optimal MCS choices with different data rates, and hence,
aTP and BER do not vary monotonically in SNR. Instead, the BER curves stay rather
flat around the required BER threshold, while the aITP improves as SNR increases. As
confirmed by Figs. 11 and 12, the A-KNN AMC obtains near-ideal solutions in tracking
channel dynamics under different operation scenarios, thanks to its immunity to channel
modeling uncertainty and powerful multidimensional feature analysis capability. There-
fore, our intelligent ML system is demonstrated to offer better AMC performance than its
model-based counterpart, in terms of broad applicability to various operation scenarios.

Table 6 Category 2 of training sets
Index 7 8 9

Lake environment DJK-h12 FXH-h2 GH-h5
Sample number 468 1058 130

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 18 of 25

 

 

 

  

      

 

 

 

 

 

 

  

 

0.9 0.9
c “0.88
Dataldx: 1 (270) Dataldx: 2 (540)
0.85
5 15 25 35 45 55 5 15 25 35 45 55
k value k value
0.9
c 0.89 — 0.88
0.88 Dataldx: 3 (810) Dataldx: 4 (1080)
0.86 —
5 15 25 35 45 55 5 15 25 35 45 55
k value k value
0.9 0.9
= 089 <— 0.89
Dataldx: 5 (1350) 0.88 Dataldx: 6 (1656)
0.88
5 15 25 35 45 55 5 15 25 35 45 55
k value k value
Fig. 10 Comparison of different k values

 

 

5.1.3 The learning curve of A-kNN AMC classifier

Equipped with the online learning mechanism, the proposed AMC design has the capa-
bility of being adapted to various changing and unknown environments. To investigate
whether it works in practice, we use the second category of data for further simulations.
As illustrated in Fig. 13, an initial AMC classifier is built through offline training using
training set 9, which achieves a prediction accuracy of 90.4% in the UWA environment of
GH. Next, we deploy this classifier to DJK-h12 (i.e., training set 7) and FXH-h2 (i.e., train-
ing set 8). Thanks to the learning ability, our AMC system is found to achieve a steadily
improved prediction accuracy, and finally reaches an acceptable AMC performance, i.e.,
n =0.9. Such results suggest the proposed online learning AMC classifier could extend its
applicable scenarios intelligently.

 

 

 

Index of predicted MCS
No

   

 

SNR (dB) 20 Model-based
kNN-based

 

25 Ideal
Fig. 11 Predicted optimal MCS vs SNR

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 19 of 25

 

1500
& 1000
2
oO —**— Ideal
GS 500 — * — Model-based

 

— * —A-kNN

 

 

—*— Ideal
— * — Model-based
— * —A-kKNN

 

Fig. 12 alP and BER vs SNR

 

 

 

5.2 Analysis of the DRDC-A-kKNN AMC
5.2.1 Effectiveness of DRDC
First, the first category of training sets is adopted to evaluate the effectiveness of the
DRDC processing. During the DR procedure, to determine the number of selected PCs,
we adopt Eq. (20) and set w = 90%, which indicates that the retained PCs cumulatively
explain more than 90% of the total amount of information contained in the raw data.
Therefore, according to the explained variance (EV) and cumulative explained variance
(CEV) of PCs depicted in Fig. 14, the first three PCs are enough to satisfy Eq. (20). In
addition, we present the PC loadings of training set 1 in Table 7, where each PC is a linear
transformation of the original variables.

Once the dataset processed by DR is made available, we adopt the elbow method [41] to
explore the optimal value of V for the DC operation. Using training set 1 as an example,
Fig. 15 shows J, as a function of different V, where J, denotes the cost function as

 

 

 

====Initial accuracy
—— LC in DJK-h12
——LC in FXH-h2 |-

 

 

 

 

 

 

 

| 1 1 \ | 1 | | |
0.75
0 200 400 600 800 1000 1200 1400 1600 1800

Samples included in the updated training set

Fig. 13 The learning curve of A-kKNN AMC classifier (k = 15)

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

   

 

 

EV of each PC EV of each PC
60.62.51 COP 61.12
SN ~ ™
Sx 40F ON se 40 ™
= 926.45 : 927.99
mm 20 ~ 70 oA 6.23
sy 3.43 1.22
gh S! 0.01
1 2 3 4 5 6
ff fu
O O
1 2 3 4 5 6
PC
(a) Training set 1 (b) Training set 2
EV of each PC EV of each PC
6060.40 yl | 60‘. 60.55 | |
N, ‘.
Se 40 a se 40 =
S ‘28.50 S 28.04
i 20 ~. 20 rer
sé —esee 383 ee 1.32 0.01, fe Gn 3 po 1.27 0.01
0 {Rn mee $e
1 2 3 4 5 6 1 2 3 4 5 6
100 100 CEV
oa (Cid
~ —--- eQxec te SEE —- He we 095.13 a a
x /* 89.90 CEV=90 xe 88.59
> 80 4 > 80 7
i Yo Lu 4
O , O “
4 4
got 60.40 60.55
1 2 3 4 5 6 1 2 3 4 5 6
PC PC
(c) Training set 3 (d) Training set 4
EV of each PC EV of each PC
6046126 6061.21
N, ‘.
= 40 Se x
ec 27.91 &
> OL >
lu 20 SL iu
~s.3:92 3.67 1.23
poe 0.01
0 O----~-~-- Q.=.
1 2 3 4 5 6
CEV CEV
100. aa irre 1006. aaa irr
Wo gee 09 Wo ee OI
& /. 89.17 xe 88.83
> 80 4 > 80 a
Ww “4 i 7
oO va O Yo
“61.26 0 “61.21
1 2 3 4 5 6 1 2 3 4 5 6
PC PC
(e) Training set 5 (f) Training set 6
Fig. 14 Explained variance of modified PCs by PCA (category 1 of training sets). a Training set 1. b Training
set 2. ¢ Training set 3. d Training set 4. e Training set 5. f Training set 6

 

 

 

Table 7 Loadings of retained PCs after DR (Trainingldx 1)

 

 

 

PCs Loadings

SNR T max T hmax e3 Ctotal [hy]
] —0.0165 0.6872 0.6894 —0.1260 0.1733 —0.0796
2 0.9984 0.0186 0.0192 0.0438 —0.0252 —0.0040

3 —0.0360 0.0958 0.0831 0.8604 0.1296 0.4749

 

Page 20 of 25
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 21 of 25

 

 

30 T T T T T T T T

 

25

 

 

 

Cost function (Jc)
a 3

—
oO

 

 

 

 

1 2 3 4 5 6 7 8 9 10
Number of clusters (v)

Fig. 15 Determine the value of V (training set 1)

 

 

 

V
Je= D1)» loi — einll3, (28)
v=1 Ojy ESiy
which is the sum of squared errors (SSE) of samples in each cluster corresponding to
the centroid cj,. Remarkably, it can be seen that, with the increase of V, the curves first
drop sharply and then slowly approach zero. Aiming at finding a good trade-off between
J. and V, we set the optimal V for each m; to 2, which is the elbow of the curves and
represents that our returns will diminish as V continuously increases [41]. Further, by the
same analysis, V for the other training sets also holds the same optimal value.

Table 8 compares the training performance with and without DRDC, in terms of pre-
diction accuracy and system complexity. As expected, through performing the DRDC
processing, our classifier achieves a significant reduction in complexity by nearly 190.8%
at the price of an average accuracy loss of 2.5%. Moreover, given the online learning dur-
ing the actual deployment, DRDC will play an even more crucial role in system efficiency

improvement as the training set keeps expanding.

Table 8 Performance comparison between training with and without DRDC (knew = 3,V = 2)

 

 

Dataldx DRDC Accuracy (%) Accuracy Prediction Reduced
loss speed (obs/sec) complexity

Off 89.2 5077

] 4.5% 67.7%
On 84.7 8512
Off 89.3 3779

2 0.9% 120.6%
On 88.4 8337
Off 90.1 2912

3 3.6% 189.3%
On 86.5 8424
Off 89.0 2657

4 2.5% 215.9%
On 86.5 8393
Off 90.1 2358

5 1.6% 259.1%
On 88.5 8467
Off 89.6 2169

6 1.8% 292.3%
On 87.8 8509

Mean 2.5% 190.8%

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 22 of 25

 

 

MCS 1

 

 

 

 

  

-0.4

-0.5

(a) Training set after DR (b) Training set after DR+DC

Fig. 16 Variation of training set along with DRDC (training set 6). a Training set after DR. b Training set after
DR+DC

 

 

 

To make the DRDC process more intuitive, Fig. 16 shows the detailed variation of
training set 6 along with such processing, where the observations of different MCSs are
represented in different colors. The DR operation first converts the sophisticated six-
dimensional original samples to a visualized new set of only three dimensionalities. Then
further processed by DC, the previous 1656 six-dimensional observations are successfully
represented by only six data points with three dimensionalities, thus offering an excellent
efficiency enhancement to our ML-aided AMC system.

5.2.2 Learning curve of the DRDC-A-kNN AMC classifier

Following the same simulation procedure, Fig. 17 presents the learning curve of the
DRDC-A-kKNN AMC classifier and compares the performance with that of its A-KNN
counterpart. Remarkably, despite the introduction of DRDC, the prediction accuracy of
our ML-aided AMC classifier suffers only a slight loss, which is no more than 10%. More-
over, the DRDC-A-KNN AMC classifier is much more efficient and can still maintain
excellent learning ability, thus enabling a continuously increased applicability in the actual
deployment.

 

 

   

 

====Initial accuracy
—— LC in DJK-h12 4
—— LC in FXH-h2
LC of A-KNN in DJK-h12 | 4
LC of A-KNN in FXH-h2

   

 

 

 

 

 

 

|

|

| |

| 1 i \ | | |

0 200 400 600 800 1000 1200 1400 1600 1800

Samples included in the updated training set

 

 

0.6

Fig. 17 Learning curve of the DRDC-A-KNN AMC classifier (knew = 3, V = 2)

 

 

 
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 23 of 25

6 Conclusion and future work

This article turns to an ML perspective to cope with the major challenges of AMC
design in the harsh underwater environment. The proposed online learning A-KNN
classifier based on SL enables a novel implementation of AMC, which has excellent immu-
nity to channel modeling uncertainty. Moreover, to handle the inherent high-complexity
issues, we further present the DRDC-A-KNN classifier for feature dimensionality reduc-
tion and data condensation, which can offer a great complexity reduction compared to
the A-KNN approach, and facilitate an easier implementation of the AMC systems.

While the proposed two ML methods expand the applicability of AMC systems in UACs
compared with the traditional model-based approaches, there are still ample issues left
for future work. Currently, in order to reduce the demand for computing resources and
training time, the features used to train the AMC model are manually extracted from
CSI by experience. However, the uncertainty of experience may potentially impact on the
system performance, since there are no best-practice rules on which features are crucial
for MCS switching in underwater AMC. To alleviate this problem, it may be necessary to
investigate a DL-based AMC framework that can be categorized as a totally data-driven
solution, to enable sustainable model improvement through automatically detecting and
generating more complex and high-level features from raw data sources.
Abbreviations
UAC: Underwater acoustic communication; UWA: Underwater acoustic; ML: Machine learning; AMC: Adaptive modulation
and coding; A-KNN: Attention-aided k-nearest neighbor; DRDC-A-KNN: Dimensionality-reduced and data-clustered
A-kNN; PHY: Physical layer; MCS: Modulation and coding scheme; CSI: Channel state information; DL: Deep learning; DNN:

Deep neural network; TX: Transmitter; RX: Receiver, SSP: Sound speed profile; SLI: Statistical link information; SL:
Supervised learning; UL: Unsupervised learning; SSL: Semi-supervised learning; RL: Reinforcement learning.

Acknowledgements
The authors would like to thank all the referees for their constructive and insightful comments on this paper.

Authors’ contributions

Conceptualization, L.H. and Z.T.; methodology, L.H. and Z.T,; software, L.H,; validation, Q.Z.; formal analysis, L.H.;
investigation, L.H. and L.Z.; data curation, L.H.; writing—original draft preparation, L.H.; writing—review and editing, W.T.,
Y.W., and C.H,; visualization, L.H. All authors read and approved the final manuscript.

Funding

This work was supported in part by the Science, Technology and Innovation Commission of Shenzhen Municipality
(Grant No. JCYJ201803061 70932431), in part by the National Key R&D Program of China (Grant No. 2016YFC1400203), in
part by the National Natural Science Foundation of China (Grant Nos. 61771394, 61531015, and 61801394), and in part by
the Natural Science Basic Research Plan in Shaanxi Province of China (Grant No. 2018JM6042).

Availability of data and materials
The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable
request.

Competing interests
The authors declare that they have no competing interests.

Author details

"Research and Development Institute, Northwestern Polytechnical University in Shenzhen, Shenzhen, 518057, China.
School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, 710072, Shaanxi, China. *State
Key Laboratory of Public Big Data, Guizhou University, Guiyang, 550025, Guizhou , China. *Department of Electrical and
Computer Engineering, George Mason University, Fairfax, VA, 22030, USA.

Received: 5 May 2020 Accepted: 29 September 2020
Published online: 17 October 2020

References

1. IF. Akyildiz, D. Pompili, T. Melodia, Underwater acoustic sensor networks: research challenges. Ad Hoc Netw. 3(3),
257-279 (2005)

2. M.Badiey, Y. Mu, J. A. Simmen, S. E. Forsythe, Signal variability in shallow-water sound channels. IEEE J. Oceanic Eng.
25(4), 492-500 (2000)
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 24 of 25

16.
17,
18.

20.

21,

22.

23.

24,

25.

26.

2/.

28.

29.

30.
31.

32.

33.

34.

35.

36.

37.

38.

M. Stojanovic, J. Preisig, Underwater acoustic communication channels: propagation models and statistical
characterization. IEEE Commun. Mag. 47(1), 84-89 (2009)

A. Song, M. Badiey, H. Song, W. S. Hodgkiss, M. B. Porter, the KauaiEx Group Impact of ocean variability on coherent
underwater acoustic communications during the Kauai experiment (KauaiEx). J. Acoust. Soc. Am. 123(2), 856-865
(2008)

M. Chitre, S. Shahabudeen, L. Freitag, M. Stojanovic, in OCEANS 2008, Recent advances in underwater acoustic
communications & networking (IEEE, Quebec City, 2008), pp. 1-10

J. Hayes, Adaptive feedback communications. IEEE Trans. Commun. Technol. 16(1), 29-34 (1968)

W.T. Webb, QAM: the modulation scheme for future mobile radio communications?. Electron. & Commun. Eng. J.
4(4), 167-176 (1992)

W. Webb, R. Steele, Variable rate QAM for mobile radio. IEEE Trans. Commun. 43(7), 2223-2230 (1995)

M. Rajesh, B. Shrisha, N. Rao, H. Kumaraswamy, in 2016 IEEE Int'l Conf. Recent Trends in Electron., Inform. & Commun.
Technol. (RTEICT), An analysis of BER comparison of various digital modulation schemes used for adaptive
modulation (IEEE, Bangalore, 2016), pp. 241-245

A. Svensson, An introduction to adaptive QAM modulation schemes for known and predicted channels. P. IEEE.
95(12), 2322-2336 (2007)

Y. Yang, H. Ma, S. Aissa, Cross-layer combining of adaptive modulation and truncated ARO under cognitive radio
resource requirements. IEEE Trans. Veh. Technol. 61(9), 4020-4030 (2012)

K.-H. Park, M. A. Imran, P. Casari, H. Kulhandjian, H. Chen, A. Abdi, F. Dalgleish, IEEE Access special section editorial:
underwater wireless communications and networking. IEEE Access. 7, 52288-52294 (2019)

A. Benson, J. Proakis, M. Stojanovic, in OCEANS 2000 MTS/IEEE Conf. and Exhibition. Conf. Proc. (Cat. No. O0CH37158),
Towards robust adaptive acoustic communications, vol. 2 (IEEE, Providence, 2000), pp. 1243-1249

L. Wan, H. Zhou, X. Xu, Y. Huang, S. Zhou, Z. Shi, J.-H. Cui, Adaptive modulation and coding for underwater acoustic
OFDM. IEEE J. Oceanic Eng. 40(2), 327-336 (2015)

X. Shen, J. Huang, Q. Zhang, C. He, Achieving high speed UWA communication with adaptive MOMC technology. J.
Northwestern polytechnical university. 25(1), 147 (2007)

M. J. Er, Y. Zhou, Theory and Novel Applications of Machine Learning. (IntechOpen, 2009)

H. Ye, G. Y. Li, B-H. Juang, IEEE Wirel. Commun. Le. 7(1), 114-117 (2018)

C. Luo, J. Ji, Q. Wang, X. Chen, P. Li, Channel state information prediction for 5G wireless communications: a deep
learning approach. IEEE Trans. Netw. Sci. Eng. 7(1), 227-236 (2018)

Y. Wang, J. Yang, M. Liu, G. Gui, Lightamce: lightweight automatic modulation classification using deep learning and
compressive sensing. IEEE Trans. Veh. Technol. 69(3), 3491-3495 (2020)

T. Hu, Y. Fei, in 2010 IEEE Int’l Symp. Model, Anal. and Simul. of Comput. and Telecommun. Syst, An adaptive and
energy-efficient routing protocol based on machine learning for underwater delay tolerant networks (IEEE, Miami
Beach, 2010), pp. 381-384

K. Pelekanakis, L. Cazzanti, G. Zappa, J. Alves, in 2076 IEEE Third Underwater Commun. and Netw. Conf. (UComms),
Decision tree-based adaptive modulation for underwater acoustic communications (IEEE, Lerici, 2016), pp. 1-5

C. Wang, Z. Wang, W. Sun, D. R. Fuhrmann, Reinforcement learning-based adaptive transmission in time-varying
underwater acoustic channels. IEEE Access. 6, 2541-2558 (2018)

J. Heidemann, M. Stojanovic, M. Zorzi, Underwater sensor networks: applications, advances and challenges. Philos. T.
R. Soc. A. 370(1958), 158-175 (2012)

L.R. LeBlanc, F. H. Middleton, An underwater acoustic sound velocity data model. J. Acoust. Soc. Am. 67(6),
2055-2062 (1980)

A. Ahmed, M. Younis, in 2017 IEEE Int'l Conf. Commun. (ICQ), Distributed real-time sound speed profiling in
underwater environments, (Paris, 2017), pp. 1-7

A. J. Goldsmith, S.-G. Chua, Adaptive coded modulation for fading channels. IEEE Trans. Commun. 46(5), 595-602
(1998)

L. Huang, L. Zhang, Y. Wang, Q. Zhang, in 2019 IEEE International Conference on Signal, Information and Data
Processing (ICSIDP), Chongqing, China, A Two-dimensional Strategy of Adaptive Modulation and Coding for
Underwater Acoustic Communication Systems, (2019), pp. 1-5

A. Misra, V. Krishnamurthy, S. Schober, in [EEE 6th Workshop on Signal Process. Adv. in Wirel. Commun., 2005., Stochastic
learning algorithms for adaptive modulation (IEEE, New York, 2005), pp. 756-760

C. M. Biship, Pattern recognition and machine learning (information science and statistics). (Springer-Verlag New York,
Inc., New York, 2006)

G. W. Lindsay, Attention in psychology, neuroscience, and machine learning. Front. Comput. Neurosc. 14, 29 (2020)
M. Luong, H. Pham, C. D. Manning, in P. 2075 Conf. Empir. Methods in Nat. Lang. Process. (emnip), Effective approaches
to attention-based neural machine translation (ACL, Lisbon, 2015), pp. 1412-1421

J. Han, L. Zhang, Q. Zhang, G. Leus, Low-complexity equalization of orthogonal signal-division multiplexing in
doubly-selective channels. IEEE Trans. Signal Proces. 67(4), 915-929 (2018)

J. Han, Y. Wang, L. Zhang, G. Leus, Time-domain oversampled orthogonal signal-division multiplexing underwater
acoustic communications. J. Acoust. Soc. Am. 145(1), 292-300 (2019)

J. Han, S. P. Chepuri, Q. Zhang, G. Leus, Iterative per-vector equalization for orthogonal signal-division multiplexing
over time-varying underwater acoustic channels. IEEE J. Oceanic Eng. 44(1), 240-255 (2019)

R. Sinha, R. D. Yates, in Veh. Technol. Conf. Fall 2000. IEEE VTS Fall VIC2000. 52nd Veh. Technol. Conf. (Cat. No. 0OOCH37152),
An OFDM based multicarrier MFSK system, vol. 1 (IEEE, Boston, 2000), pp. 257-264

C. X. Gao, H. X. Yang, F. Yuan, E. Cheng, Underwater acoustic communication system based on MC-MFSK. Appl.
Mech. and Mater. 556, 4897-4900 (2014). Trans Tech Publ

K. Beyer, J. Goldstein, R. Ramakrishnan, U. Shaft, in /nt’l Conf Database Theory, When is “nearest neighbor”
meaningful? (Springer, Berlin, 1999), pp. 217-235

H. Zou, T. Hastie, R. Tibshirani, Sparse principal component analysis. J. Comput. Graph. Stat. 15(2), 265-286 (2006)
Huang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:203 Page 25 of 25

39. T. Yang, S. Huang, in P. the 17th ACM Int’ Conf. Underwater Netw. & Syst, Building a database of ocean channel impulse
responses for underwater acoustic communication performance evaluation: issues, requirements, methods and
results (ACM, Shanghai, 2016), pp. 1-8

40. T.T.Wong, P. Y. Yeh, Reliable accuracy estimates from k-fold cross validation. IEEE Trans. Knowl. Data En. 32(8),
1586-1594 (2020)

41. T.M. Kodinariya, P. R. Makwana, Review on determining number of cluster in k-means clustering. Int. J. Adv. Res.
Comput. Sci. Manage. Stud. 1(6), 90-95 (2013)

 

 

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
