Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 FEURASIP J ournal on Advances
https://doi.org/10.1186/s13634-020-00682-7 . Sj | Process . ng
in Signa |

RESEARCH Open Access

Achieve data privacy and clustering ®
accuracy simultaneously through quantized ~~
data recovery

Ren Wang!, Meng Wang! and Jinjun Xiong?

 

 

*Corr ndence:

vena Abstract

"Department of Electrical, This paper develops a data collection and processing framework that achieves

Foohncarin Ren eceloer Polytechnic individual users’ data privacy and the operator's information accuracy simultaneously.
Institute, Troy, NY, USA Data privacy is enhanced by adding noise and applying quantization to the data before

Full list of author information is transmission, and the privacy of an individual user is measured by information-theoretic

available at the end of the article analysis. This paper develops a data recovery and clustering method for the operator to
extract features from the privacy-preserving, partially corrupted, and partially observed
measurements of a large number of users. To prevent cyber intruders from accessing

the data of many users, it also develops a decentralized algorithm such that multiple
data owners can collaboratively recover and cluster the data without sharing the raw
measurements directly. The recovery accuracy is characterized analytically and showed
to be close to the fundamental limit of any recovery method. The proposed algorithm
is proved to converge to a critical point from any initial point. The method is evaluated
on recorded Irish smart meter data and UMass smart microgrid data.

Keywords: Subspace clustering, Quantization, Data recovery, Data privacy, Smart
meter

 

1 Introduction

Smart meters provide fine-grained measurements of power consumption of industrial
and residential customers and can enhance the distribution system visibility. Non-
intrusive load monitoring (NILM) approaches [1, 2] can identify individual appliances
from the high-time-resolution smart meter data of the aggregated power consumption.
Intruders can thus extract user behavior, and user privacy is an increasing concern.
One way to protect data privacy is by applying additive homomorphic encryption [3]. It
requires the network to have tree-like connections and can only decrypt the sum of the
load curves. The other way to enhance data privacy is data obfuscation whereby the actual
power consumption of each household is masked by adding noise to the smart meter
measurements either through signal processing approaches [4, 5] or by physically adding
rechargeable batteries to the households [6, 7]. Moreover, the aggregated consumption of
the load and the battery can be adjusted to a constant to obfuscate the information further

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
GQ) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
— credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 2 of 36

[8, 9]. Then, applying the NILM to these noisy and quantized measurements, an intruder
can no longer accurately identify the patterns of individual appliances and, in turn, the
user behavior. The increase in user privacy is achieved, however, at a cost of data distor-
tion and reduced data accuracy for the operating center [10-12]. Although the operating
center does not need high-time-resolution information of every individual appliances in
each household, it still requires accurate estimation of the aggregated power consump-
tion and the common load patterns among households for forecasting, demand response,
and planning. For example, the center clusters customers with similar load patterns and
then employs the load pattern of each cluster to enhance the load forecasting accuracy
[13] and determine the incentives for demand response [14, 15]. If noise and quantization
are added to the data to enhance the privacy, the information accuracy for the operator is
effectively reduced.

This paper shows that the data privacy can be protected for each individual user! and,
at the same time, the information accuracy at the operating center about user power con-
sumption and the major patterns among different users are maintained. To the best of
our knowledge, this is the first work that achieves data privacy and information accu-
racy simultaneously. In our proposed framework, each user’s actual power consumption
is masked by first adding noise to the measurements and then quantizing the output to
one of a few levels. The privacy of an individual user can be enhanced in this way, from
an information-theoretic perspective [16-19]. Once the data is quantized, the variation
information is blurred and hence NILM methods fail to identify individual appliances.
Although adding noise and quantization have been employed before to enhance privacy
(e.g., [6, 20]), this paper, for the first time, shows that such privacy enhancement does not
necessarily lead to a reduction in the information accuracy. The central technical contri-
bution of this paper is the development of a data recovery and clustering method, even
when the measurements are highly noisy and quantized, contain significant errors, and
are partially lost. Our method is proved to provide accurate data recovery and clustering
results, as long as the center has measurements from a sufficient number of users. In con-
trast, a cyber intruder with access to the measurements of a small number of users cannot
obtain accurate information even with the same approach. We develop a decentralized
algorithm that allows multiple data owners to cooperatively recover and cluster the data
without sharing their own raw measurements directly. Then, it is extremely difficult for
an intruder to access large amounts of data. Thus, the data privacy of an individual user
is enhanced while maintaining the information accuracy for the operating center.

Since the load profiles with similar load patterns can be represented by data points in
a low-dimensional subspace in the high-dimensional ambient space, all the load profiles
can be characterized by the Union of Subspaces (UoS) model [21], and the load clus-
tering problem can be formulated as a subspace clustering problem. Various subspace
clustering techniques have been developed, see e.g., [21-26]. None of these approaches,
however, considers the case that the measurements are highly quantized. To the best of
our knowledge, only one recent work considered subspace clustering and data recovery
from highly noisy and quantized data [27]. This paper follows the mathematical setup of
[27] but extends significantly in the following aspects. Ref. [27] does not consider data pri-
vacy, while this paper proposes a data collection framework to achieve data privacy and

 

! Throughout this paper, we refer to each household as one user.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 3 of 36

information accuracy simultaneously. We characterize the data privacy through mutual
information, and such analysis does not exist in [27]. Ref. [27] assumes that all the mea-
surements are available to the center, while this paper considers a more general setup that
partial measurements are lost during the transmission and do not arrive at the center.
This paper characterizes the data recovery error by our proposed method analytically as
a function of data loss percentage. Moreover, this paper characterizes the fundamental
limit of the recovery error by any possible recovery method and shows that our method
is nearly optimal in reducing the recovery error. All these fundamental analyses do not
exist in [27]. Furthermore, only a centralized algorithm Sparse-APA is discussed in [27].
This paper develops a Distributed Sparse Alternative Proximal Algorithm (DSAPA) for
multiple data owners to collaboratively solve the subspace clustering and data recovery
problem without sharing the measurements with others. Thus, the user data privacy can
be further protected. This paper is also related to the quantized matrix recovery prob-
lem [28-36], in which the data matrix is assumed to be low rank. The low-rank matrix
model is a special case of the UoS model by restricting to one subspace only. In fact, the
data matrix of the load profiles can be high rank or even full rank in our setup. Finally, we
remark that this paper considers smart meter measurements that measure the aggregated
energy consumption in a house, and does not consider applying NILM on the operator
side. Distributed smart metering can provide energy consumption of individual electrical
appliances in a house [20].

The rest of the paper is organized as follows. Section 2 introduces our proposed frame-
work, problem formulation, related works, and the data privacy enhancement analysis.
The theoretical analyses of our recovery and clustering method is presented in Section 3.
Section 4 introduces the details of the DSAPA with its convergence guarantee. Section 5
records the numerical experiments of our method on the real smart meter dataset.
Section 6 concludes the paper. All the proofs are deferred to Appendix 1, Appendix 2,
Appendix 3, Appendix 4, Appendix 5, Appendix 6, and Appendix 7.

2 Our proposed framework of privacy-preserving data collection and
information recovery
2.1 Our framework and problem formulation
Figure 1 visualizes our proposed framework of privacy-preserving smart meter data col-
lection and information recovery. To enhance the user data privacy, the actual power
consumption is mapped to a few fixed power levels at the output of the smart meter. One
can achieve this through signal processing in the smart meter or connecting a recharge-
able battery to each household. Thus, the actual consumption is masked in the noisy and
quantized smart meter measurements. As shown in Fig. 1, the measurements are col-
lected by W agents disjointly, and agents do not share measurements directly. The agents
recover the data and cluster the users with similar consumption patterns collaboratively
in a distributed fashion. When W = 1, it reduces to the case of one single center.

We defer the discussion of user privacy enhancement through the proposed frame-
work to Section 2.3. We first define the recovery and clustering problem from quantized
data mathematically as follows. L* € IR””*” denotes the actual power usages of 7 users,
with each column containing the power usage of one user in m time instants. We assume
that users with similar consumption patterns belong to the same group and there are p
groups in total. The corresponding columns of the same group belong to a d-dimensional
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 4 of 36

 

— eae ae ae cae caer caer carer car are ee a ere ee ere ere ere ee er er ere el ee

Customer 1
eu- _
J

rats Battery
control -

sea penton com tteet ane

  
  
 

 

gent
(Operating center
when W=1

Sjuade Buowe suoljejno|eo
a}e Ipawayu! Bueus AjUO

 

 

Fig. 1 The process of the quantization, measurement, collection, and transmission

 

subspace in R” with d < m. Let S; (i €[ p]) denote the ith subspace, and these p subspaces
are distinct”. Let r denote the rank of L*, then r < pd. Let L* denote the submatrix of L*
that contains points in S;, and let 1; denote the number of columns in L*, i.e., the number
of users in group i. We assume m < n; < &n/p for all i and some positive constant €. We
further assume m = n/kp for some positive constant « to simplify the representation of
main results.

There exists a coefficient matrix C™ € R’*” such that L* = L*C*, C7; = 0 for all
i €[n]. Moreover, Ci is zero if the ith and jth columns of L* do not belong to the
same subspace [21]. We summarize these two properties as self-expressive property and
subspace-preserving property in Definition 1. These properties have been exploited in the
literature of subspace clustering and are summarized as follows.

Definition 1 [27] A matrix L ¢ R””*” has the self-expressive property if L = LC for some
Ce R”*", and Ci; = 0 for alli €[n]. Moreover, C has the subspace-preserving property of
L if Ci; = 0 for columns i and j of L belonging to different subspaces.

 

2S;’s (i €[p]) are distinct provided for any i, j, there always exists some f that belongs to S; but not Sj.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22

Let matrix E* € R’”*” denote the additive errors in the measurements. We assume the
number of nonzeros s in E* is much smaller than mn. The partially corrupted measure-
ments can be represented by X* = L* + E*. We assume the energy consumption and the
errors are bounded, i.e., ||Z* |log < a1 and ||E*|loo < a2, for some positive constants a1, a2,
and the infinity norm || - ||.o measures the maximum absolute value.

The quantization process in each household is modeled as follows. The measured
energy consumption at each time step is mapped to one of K values in a probabilistic fash-
ion. Figure 2 shows the quantization process. It can be modeled as adding random noise
first and then quantizing to K levels. N € R””*” is independent from X*. Entries of N are
iid. generated from a fixed cumulative distribution function (c.d.f.) U(~). The quantiza-
tion boundaries wo < @1 <... < @j_| < @}... < wx and the quantized value Q;,/ €[K]
for the bin [| @j_1,q@;) are given. Then, the probability of mapping Xi to Yi; = Q), Vij is
represented by

g(Xj,) =P (Yi, = Qi1X};)

= ¥ (w — Xi) - ¥ (4 — X4),

and ey PI (x5) = 1. The noise N is introduced to hide the user information. One
choice of W(x) is the probit model with V(x) = Wporm(*/o), where Vnorm is the c.d.f. of
the standard Gaussian distribution (0,1), and o > 0 is the standard deviation. Note
that U (« — X¥) > w (or — X¥) + 6 for some positive 8. Then, 1 > g; > B > 0.
The quantized measurements Y are sent to the center. Data losses can happen dur-

(1)

ing the communication, visualized by the question marks in Fig. 2. Let set 2 denote the
indices of measurements that are not lost. In the general case that the measurements are
collected by W agents/nodes separately, we assume for simplicity that each node collects
the data from g = n/W users. Node 1 collects the data from the first g users; node 2 col-
lects the next g users and so on. Let ®; = {q@i—1) +1, qi—1) +2...., gi}, then Lo, denotes

the submatrix of L* with column indices in ®;. L* can also written as Li, Lo, besy Li, |
Similarly, E* = ES.) Foy Eby, | Node i collects Yo,.
The data recovery and pattern extraction problem for one center can be stated as

follows.

 

y L* E* N
You! Yoo! Yo; | You Lg! L'g)! Los NE NE 9)! gs! E oy!

- Q(

  

Multiple low dimensional | pace Sparse Independent noise
5 L, -columns belonging to subspace S) | columns belonging to subspace S>
i L’,: columns belonging to subspace S3 OL , columns belonging to subspace S;

Fig. 2 Quantization model (p = 4,W = 4)

 

 

 

Page 5 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 6 of 36

(P1) Given quantized measurement Yg, known boundaries wo < @, < ... < WK and
noise distribution V, can we recover the real power usages L* and cluster the users through
estimating C* simultaneously?

Moreover, if measurements Y;;’s are not shared among W nodes to protect the user
privacy,

(P2) Can we estimate L* and C* with W nodes in a decentralized fashion?

Some notations in this paper are summarized in Table 1.

2.2 Related work
When p = 1, ie., all the users share the same pattern, L* is approximately a low-rank
matrix. Then, (P1) reduces to the problem of low-rank matrix recovery from quantized
measurements [28-37], with motivating applications in image processing [38], collabora-
tive filtering [31], and sensor networks [39]. Note that since there is only one subspace in
this case, these works do not consider data clustering and only focus on data recovery.

When the quantization process does not exist, the problem (P1) reduces to the con-
ventional subspace clustering problem [21-—26, 40]. If the subspace preserving C™ is
estimated, one can apply the spectral clustering [41] method to obtain the clustering of
the data points. For example, Sparse Subspace Clustering (SSC) [21] is a common choice
for subspace clustering, and SSC estimates C* by solving a convex optimization problem.
Other clustering methods exist that cluster data points based on the Euclidean distance.
For instance, refs. Lin et al. [42] and Keogh et al. [43] leverage a linear combination of box
basis functions to approximate the original data, yet still retain the features of interest.

Reference [27] is the first paper that studies the subspace clustering from quantized
measurements when p > 1. Wang et al. [27] do not consider missing data and develop
a centralized data recovery method from full observations. This paper follows the same
problem formulation as [27] and extends to the general case of partial observations. We
provide both the recovery guarantee of our approach and the fundamental limit of the
recovery accuracy by any method. Moreover, a framework of privacy-preserving smart
meter data collection is proposed in this paper, and we further enhance the data privacy
by developing a decentralized data recovery method.

Our problem formulation and methods apply to other domains such as image and video
processing and phasor measurement unit (PMU) data analytics for power systems. In
image recovery and image clustering [27], images of the same person with varying illumi-

nation belong to the same low-dimensional subspace [44]. Columns of L* correspond to

Table 1 Notations

 

 

 

Si The ith subspace

L* Columns of matrix L* belonging to the ith subspace
L* The ith column of matrix L*

Le The ith row of matrix L*

Li Entry on the ith row and jth column of matrix L*

[ pl The set {1,..., p}

D; Index set containing {gVi — 1) + 1,q — 1) + 2...., gi}
Co, Columns of matrix C belonging to the set ®;

Cox Rows of matrix C belonging to the set ®;

(Co, ) oj Rows of matrix Cg; belonging to the set ®;

Coie The transpose of Co.

 
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 7 of 36

images of multiple people. The goal is to enhance the image quality and cluster the data
using low-resolution images. Similarly, in motion segmentation, each column of L* rep-
resents the trajectory of a reference point. The reference points in the same rigid object
belong to the same subspace. The motion segmentation becomes a subspace clustering
problem from the observed measurements. In PMU data analytics, the time series of
PMUs affected by the same event belong to the same subspace [32, 45]. The event location
problem can be solved by subspace clustering.

2.3 Data privacy enhancement in the proposed framework

Various methods have been developed to enhance the privacy of power consumption
data. For example, one can use pre-processing techniques like temporal averaging, adding
additional noise, and quantization [4, 5, 20] to alter the data. However, directly altering
data might affect the accuracy of some applications, e.g., billing and profiling [46]. Alter-
natively, rechargeable batteries and PV converter can be leveraged to mask the actual
power consumption [6, 7, 47]. The noise addition and quantization process in this paper
can be achieved by either signal processing or rechargeable batteries.

In general, privacy guarantee can be achieved through either computational hardness
[48—50] or information-theoretic analysis [16-19]. The existing analytical results of data
privacy only work for specific or simple models and do not easily generalize. For instance,
under the setup of communication between two nodes, ref. [17] analyzes the trade-off
between data sharing and privacy. Under the assumptions of i.i.d. input load sequence
and an i.i.d. energy harvesting process, the minimum information leakage rate is provided
with a certain energy management policy in [51]. Some other methods try to analyze
data privacy numerically. In [52], the information leakage rate is measured by the rela-
tive entropy of the probability measures of the original load data and the modified load
data and is calculated by Monte-Carlo method. Refs. [7] and [12] consider measuring
the information leakage through mutual information of the original load data and the
modified load data. Following the existing work on smart meter data privacy, see, e.g.,
[19, 52-54], this paper analyzes the data privacy from an information-theoretic perspec-
tive. The data privacy of an individual user is analyzed by comparing the original data
and the data after privacy enhancement through quantities like the Kullback-Leibler (KL)
divergence [52], mutual information, and normalized mutual information [18]. In our
framework, the actual energy consumption of user i, denoted by L*., is masked by additive
Gaussian noise and quantization, resulting in Y,;. Let Py» and Py,, denote the probabil-
ity distribution of L*, and Y,;, respectively. The privacy can be measured through the
normalized mutual information (NI) between L*, and Y,; [18], defined as follows:

Definition 2
NI (Li; Yxi)
dixex Duyey Pte, y,;) %y) log pag (2)
- Trev Pry, log py
where spaces X and ) are the feasible set of Li, and Y,i, respectively. PcL*,¥«i) # the joint
distribution of Li, and Y,;. Pix, and Py,, are the marginal distributions of Ly; and Y,i,

respectively.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 8 of 36

The numerator of (2) is the mutual information between L*, and Y,;, and the denomi-
nator is the entropy of L¥,. When L*, and Y,; are independent of each other, NI (L*., Y,i)

reaches its minimum value 0. When Y,; is exactly the same as L¥,, NI (L* Y,i) equals to

xi?
the maximum value 1. A smaller NI corresponds to a higher level of data privacy of L*,
and also indicates more significant difference between L¥, and Y,;. Note that rigorously
speaking, L*, belongs to the continuous space. However, since all measuring devices have
a finite resolution, L¥, can be viewed as a discrete random variable. When computing NI
in practice, one can divide the range of the values into small regions to compute sample
probability distribution.

The above information-theoretic measures show that when the data of individual users
are processed separately, a user’s data privacy is enhanced at the cost of reduced informa-
tion accuracy. We need to emphasize that the measures like NI or KL divergence focus
on an individual signal and do not characterize the information recovery when multiple
signals are processed together. In fact, when the data of multiple users are available, and
strong correlations exist among different users’ data, such correlation can be leveraged
to enhance the data accuracy. As stated in problems (P1) and (P2), the major technical
objective of this paper is to develop data recovery and clustering methods from quan-
tized measurements of multiple users, where the data correlations are characterized by
data points belonging to the same subspace. As we will show in Section 3 (Theorem 1 and
Proposition 1), the asymptotic information accuracy from quantized measurements can
be achieved when the number of users increases to the infinity. We need to emphasize that
this result does not contradict the data privacy enhancement by adding noise and apply-
ing quantization. This is because the asymptotic information accuracy is only achieved
when processing the correlated data of a large number of users, while a cyber intruder is
very unlikely to have access to the data of so many users. In our proposed decentralized
data collection and processing framework (Fig. 1), each agent collects the measurements
of a subset of users, and the measurements are not directly shared among the agents. A
cyber intruder needs to hack either all these agents or the smart meters of all users to be
able to access all the data. Since such attack is very unlikely to happen, the user’s data pri-
vacy is still protected. Privacy from the recovery perspective will be discussed in details
in Section 3.3.

3 Results: theoretical

Here, we consider solving (P1) at a single center and defer the discussion of solving (P2)
in a decentralized way through distributed nodes to Section 4. We propose to estimate
L*, C*, and E* by the solution (z, E,C ) to the following optimization problem,

 

 

 

 

 

 

 

 

min FUL,E) s.t.L,£,C) € Sy, (3)
L,EER™*",CeR"™”
where
K
FILE) =— >) > Uyj=on log (g (Li + Fis), (4)

(ijEQ I=1

rank(L) <7r,L =LC,||Cyillo < 4, Ci; = 0, Vi €[ 1] }.

Sp = {(L,E,C): ||Llloo < 01; ||Elloo < a2; ||Ello < s,
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 9 of 36

1,4] is the indicator function that takes value 1 if A is true and value 0 otherwise. || - ||) mea-
sures the number of nonzero entries in a vector or matrix. Data recovery and subspace
clustering are achieved simultaneously by solving (3)—(5).

Equations (3)—(5) are a constrained maximum log-likelihood estimation problem that
maximizes the likelihood of obtaining Yg when the underlying data matrix is 1, and the
error matrix is E. The formulation follows (8) of [27] by extending from full observations
to partial observations in Q. After obtaining C, spectral clustering [41] is applied to C to
obtain group labels.

Equations (3)—(5) are nonconvex due to the nonconvexity of the feasible set Sy in (5).
We first analyze the recovery and clustering performance, assuming that a solution exists.
We defer the algorithm to Section 4.

3.1 Data recovery guarantee
Two constants yy and Ly are needed for the recovery analysis,

 

 

i) we
4 = min inf ft (x) _ p(x) (6)
le[K] |elsoiton | p(x) gi)
Ly = max sup {|g(x)|/g(x)}, (7)

lELK] |x|<o +e
where @)(x) and @ (x) are the first- and second-order derivatives with respect to x. Note
that @j(x)* — G(x) gy ,(x) > 0 if gy is strictly log-concave. One can check that 4 is strictly
log-concave if VY is log-concave, which holds true for Gaussian and logistic distributions
[28]. Ly and y, are bounded by some fixed constants when a1, a2, and g are given.

Since the data recovery performance and the clustering performance are coupled
together, we first analyze the recovery performance, assuming that the clustering results
are not “arbitrarily bad” We follow the same assumption as [27], which essentially requires
that in the estimated clustering results, every cluster contains data points belong to at
most a constant number out of p original subspaces. Formally, we have

Assumption 1 [27]: Columns of L belong to p subspaces, each of which has a dimension
smaller or equal to d. Columns in L with indices corresponding to columns of L* in S;(i €
[ p]) belong to at most (g — 1) subspaces, where g is a constant larger than 1.

We follow the assumption in [28] about the location of the observed entries. We make
a minor change to handle multiple subspaces instead of one subspace in [28]. Assump-
tion 2 is a generalization of the uniform sampling and includes the uniform sampling as
a special case. We define a binary matrix G with G;; = 1 if and only if (i,7) € Q, Le., Yi;
is observed. Gj; = 0 otherwise. Let G; € R’”*” denote the submatrix of G with columns
corresponding to subspace i.

Assumption 2 Assume each column of G; has h nonzero entries. Let 0,(Gj) and 02(Gj)
denote the largest and the second largest singular values of Gj, respectively. Assume
01(G;) > hand o2(G;) < CVh for i €| p], where C is a positive constant.

Assumption 2 is similar to the sampling assumption in [28]. The difference is that we
make the assumption on columns belonging to each subspace instead of the whole matrix.

The above assumption is more general than the uniform sampling assumption [28].
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 10 of 36

Theorem 1 Suppose that g(x) is strictly log-concave in x, Vl €[K]. Then, under
Assumptions 1 and 2, with probability at least 1 — pCje~@5"/?, any global minimizer L to
(3)-(5) satisfies

 

 

 

li — L* : //mn < min (20 + 2a, | —, us) , (8)
where
dd dx>/4 1/4
uy ac KANE cy de (=)
fm f3/2m1/4 \imn )
LC vee ( S yr"
5 f \mn
for some positive constants Cy, Co, Ci (Las g,&), Ch(Lar Z,€, 02), and C3(Lq,g,8, 02). f =
[el = h is the data loss rate.

Theorem 1 characterizes the recovery error from partially observed, partially cor-
rupted, and quantized measurements. It can be interpreted from the following aspects.

(1) Correction of corrupted measurements. We first fix the data loss rate f and consider
the recovery performance with corrupted measurements. Suppose f is a constant, i.e., a
constant fraction of the measurements are available. Then, (8) indicates that as long as
the number of corrupted measurements s is at most © (md?p), we have®

/./mn < O [é . (10)
F M

Thus, the recovery method tolerates a constant number of corrupted per column without

Ji-v

 

 

degrading the recovery performance.
(2) Asymptotic recovery of the actual data. Since O (y ©) decreases to 0 when m

increases to infinity, and ||L*||- is in the order of ./mn, (10) indicates that the relative error
between L and L* diminishes asymptotically. Moreover, as long as p is o(n), the failure
probability 1 — pCje~©5"/P also decays to zero as n increases to infinity. The asymp-
totic recovery differentiates the operating center and cyber intruders. An operating center
with a sufficient number of measurements can recover L* accurately. In contrast, a cyber
intruder with access to a small number of users cannot recover the data even using the
same approach (3)—(5).

(3) Tolerance of the missing data. To the best of our knowledge, only refs. [28] and [31]
provided the theoretical analysis of low-rank matrix recovery from quantized observa-
tions with data losses. No corruptions are considered in [28, 31]. The relative recovery

error by [28] is O ( =) under the partial observation case when f is a fixed constant,

Mm

. . . . 1/4
where r is the rank of the matrix. The relative recovery error by [31] is O (Sr) under

the partial observation case. Our result in (10) indicates that when f is a constant, the
error is at most O (y ©) even with corrupted measurements. Note that the rank of L*

can be as large as pd when the subspaces are all orthogonal to each other. If one directly
applies the approach in [37] to our setup, the relative recovery error can be as large as

 

3We use the notations u(n) € O(v(n)), u(n) € o(v(n)), or u(n) = O(v(n)) if as n goes to infinity, u(7) < c- v(n),
u(n) > c-v(n) or cy - V(N) < u(n) < cz - vV(N) eventually holds for some positive constants c, cy; and c2, respectively.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 11 of 36

3 43
O (y re), which is ,/p® times our recovery error. Thus, our approach outperforms the

existing one by recovering and clustering data simultaneously even in the special case of

no corruptions.
When there is no missing data, the recovery error by [27] is O /4 , which is

slightly tighter than our error bound in (10). This is due to our techniques to handle the
missing data.

3.2 Fundamental limit of any recovery method

The following theorem establishes the minimum possible error by any method from
unquantized measurements. We consider the case that the number of corruptions is at
most a constant fraction of the measurements. To simplify the analysis, we assume

. 64m
s < min (Comm mn — "| (11)

where Co is a constant smaller than 1/2. Let

Spy = {X:X =L+E,(L,E,C) € S}. (12)

Theorem 2 Let N ¢ R”*” contain i.i.d. entries from N (0,07). Assume (11) holds.
Consider any algorithm that, for any X € Sry, takes My = Xj + Nij, (i,j) € Q as the input
and returns an estimate X of X. Then, there always exists some X € Sx such that with
probability at least 3,

 

 

x~3| ‘La|-%
F> min C3, C4o am * (13)
= , Ss
/mn fm— =
holds for some fixed constants C3 and C4, where C3 = 10 min(a1,Q@2) and C4 <

3, Sg is the number of errors in XQ.

 

Note that C3 is a constant. When f is a constant, (13) indicates that
|X — X\le//mn > O(/d/m). (14)
The recovery error from unquantized measurements is at least © (y z), Comparing it

with our error bound ,/ c in (10), one can see that our method is close to optimal. If the
corrupted entries are randomly distributed, sg is approximately ©(fs). Then, the second

term inside the minimization of (13) scales as © (sy a. .

3.3 Privacy from the recovery perspective

3.3.1 Recovery of a single user from its own data only

An intruder is often interested in the data of a certain user. If the adversary only has access
to one user’s data, then problems (3)—(5) are reduced to

min F(L, E)
L,EcER™ (15)
8.t.|[L loo < 1; ||Elloo < a2, ||Ello < s.

 

 

 

 
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 12 of 36

Note that since 1 = 1, there is no constraint on C. (15) maximizes the log-likelihood of
one user given the information about the quantized measurements. It can be viewed as
a special case of the low-rank matrix recovery from quantized measurements considered
in [37]. One can check that the average recovery error is upper bounded by O (a?) by
setting 1 = 1 in Theorem 5 of [37]. Similarly, the relative recovery by any method is at
least in the order of @(/d) by setting 1 = 1 in Theorem 4 of [37]. This error bound does
not depend on m, the number of measurements of this user. Therefore, if an intruder only
has one user’s data, even if m is very large, the average recovery error is nonzero and does
not diminish as m increases. Then, the privacy of the energy consumption behavior of
this user is protected.

3.3.2 Recovery of a single user by leveraging other users in the same group

One can exploit the measurements from other users to increase the estimation accuracy
of one target user. Suppose one can access 7 users’ data in m time steps, and these users
all share similar load patterns as the target user, then from either Theorem 1 of this paper

or Theorem 5 of [37], the average recovery error is at most O (y a) Compared

with the previous case of accessing the data of one single user only, the recovery error is
significantly reduced. We emphasize that the decrease of the recovery error results from
exploiting correlations among users.

The number of quantization levels K also affects privacy. Intuitively, a smaller value of
K corresponds to a higher level of privacy. However, the privacy level also depends on the
selection of bin boundaries, and decreasing K does not necessarily increase privacy. For
instance, if a pair of boundaries are chosen very close to each other so that no measure-
ments located within the interval, then K = 3 could reach the same privacy and recovery
error as K = 2. Therefore, K does not directly appear in Theorem 1 but rather affects the
privacy indirectly through y, and Ly. The bin boundaries usually tend to be closer in the
region where the measurements concentrate.

For smart meter data, the bin boundaries can be selected in the range of a typical house-
hold consumption level. If a certain house has some electrical appliances with an energy
consumption level significantly higher than normal households, this abnormal pattern of
high energy consumption can in fact be masked in the noisy and quantized measurements
due to the way how bin boundaries are selected. However, since this house has a different
load pattern from other households, one cannot exploit other users’ data to enhance the
recovery accuracy of this user. The recovered data of this user will have a nonzero error
as discussed in the first paragraph of Section 3.3.

3.4 Clustering guarantee
The clustering performance is evaluated through the subspace-preserving property of C.
A sufficient condition for C to be subspace-preserving is stated as follows.

Proposition 1 Suppose columns of L are i.i.d. drawn from certain unknown continuous
distribution supported on p distinct d-dimensional subspaces, then the global minimizer C
of (3) has the subspace-preserving property for L.

Ref, [27] also provides a sufficient condition for C to be subspace-preserving. The
subspaces are required to be independent with each other in [27]. Two independent
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 13 of 36

subspaces intersect only at zero. Here, we require subspaces to be distinct from each other.
Two subspaces are distinct if for each subspace, there exists one point that belongs to
this subspace but not the other. The data points are generated based on some continuous
distribution supported on these distinct subspaces.

4 Distributed sparse alternative proximal algorithm for data recovery and
clustering

We next propose a distributed algorithm to solve (3) by W nodes collaboratively such that

node i can estimate Lo, from its acquired measurements Y@,, while it does not know Yo j

or Lo, for all other j’s nodes. This further enhances user privacy.

We first follow [27] and move some constraints to the objective function to simplify the
algorithm design. Since the rank of L is at most r, we factorize L as L = UV", where V €
R’*" and U € R™*". We replace the equality constraints L = LC and L = UV" by adding
‘A } vi vTc|; and 12 } uv — L||; to the objective function. The parameters A; and
d2 affect the tightness of the original constraints. Note that V7 = V’C is a sufficient but
not necessary condition for L = LC. Then, (3) is changed into

(a, VLE, ¢)
= argmin H(U,V,L,E,C)s.t(L,E,C) € SF, (16)

UeR™*" ,VeR’*"
L,E,CER™*”

 

 

 

 

 

 

 

 

 

 

 

 

where
AL | pr T -\|2
H(U, Vs, E,C) =FLE) + = lv _y cl
Xr 2
+Fluvr—z
2 F

SF ={(L,E,C) : ||Llloo < 01, |IElloo S a2,

Ello < s, I|Caillo < d, Ci; = 0, Vi €[ 1] },

(18)

The solution of (16) is the same as that of (3) when A; and 42 approach the infinity.
We next decompose V into W parts, and let Vo,, € R4*",i €[ W] denote the rows of V
with row indices ®;. Then, the objective in (17) can be decomposed as follows:

Ww
HU, V,L,E,C) = )-H(U, V,Le;,E@;,Co;) (19)
i=1

where

H. (U, V, Lo; E£e@,;; Co;)
Xr 2
= FinsEo)+ Vb, Val °0

uve 1 |
+ 2 | Djx D; FE

K
F(Lo,Ee,)=- Yo >
(Kitig-g IE} (21)

Vke[m],je[q]

1i(¥9,)4;=a og @ (Loe + (Eo,),,))
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22

and V contains Vo,, to Voys, ie, V = . The constraint set SF in (18) is

Vo we
equivalent to the intersection of SF;’s (Vj €[q]), with*

 

 

SF; = | (Lo, Eo, Co;) : Le, lo <Q; E@,; | <

 

 

Eo, lo <

 

 

a, (Co,)sil|o < & (Co, diq-asy = OF

S
Ww’

Then, (16) can be equivalently written as

(16
(a, Veins Lo, £o;: Ca)
W
argmin ) H(U,V,Le,,Eo,Co,)
Co,eR"™1UER"™” ja] (23)

Vojx€ RIX?
Lo, Eo, € R49 Vie[W]

s.t. (Lo; Eo; Co,) € SFi, Vi €[ W].

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

where the estimated variables are U and W components of L, E,C, and V.

The constraints in (23) can be decomposed for W nodes, while the objective function
cannot, due to the coupling of U and V. Here, we develop a synchronized Distributed
Sparse Alternative Proximal Algorithm (DSAPA) to solve (23) with the convergence guar-
antee. The node i owns Yo, and estimates Vo,,, Lo,, Ee,, C,, and U. Since all nodes have
the estimates of U, and Lo, = UV 9,,, the key to protect user privacy of node i is not to
share the estimate of Vo,,, aS well as Y,, to any other nodes.

In the (¢ + 1)th iteration, node i sequentially updates Ce Vow Li, EQ) ut? in
Subroutines 1—5. Each subroutine essentially follows the projected gradient. The gradient
of H with respect to Vo;., Lo,, Eo,, U, and Co, are

Vos,H = AV (V5, — V7Co,)

Ww
= —-A\V Voix a Voix (Co) a, 7 » Vor (Co). ea
J=1j#i
= —h1 VMe;;

.
°

Vvot = 2 (Veil? - Li,) u

4
+ Aq (Vers — CB, v) 7 » (Coie. (Vey. 7 Co, v)

£ (25)
W
=h | M3, — Yo (Ca) 9, M5, | +42 (Vout? —1G,) Us
j=l
Vig,H = VE (Le;,Eo,) — d2 (UVS,. - Lei) (26)
VEo,H = VF (Le; Ee;); (27)

 

4\We assume for simplicity that the corruptions are distributed evenly such that the number of nonzero entries in Eg, is
at most 7. The algorithm can be easily extended to cases that the numbers of corruptions are different as long as a
reasonable accurate upper bound of the number of corruptions is available.

Page 14 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 15 of 36

W W
VuH = ho (uv? — L) V := Ap » u- >> :) ; (28)
i=1 i=1
where M = V! —V'C, 4; = V§,, Vo; 61 = Lo; Vo; and
[ VF(Lo;; Eo, )\kj —=

 

; (29)
v (oV»,), 7 (Xe,),.;) —W (o%»,),-1 — (Xe:),,)
Vk €[m],j €lq].
The step sizes in the (¢ + 1)th iteration are selected as
1 1
= IvevoTl. a. iew (30)
nO, ape],
(31)
Vs. = =
i er +A] MaXje(w] Fi
1
Le; = T4.” (32)
od; ip + ho
ts, = 0°B (33)
and
1 1
Tu ; (34)

= — T.. wT... al = . Ieew
fa (vy VE) Ja) De to,

laxa +(C$2) (Ca) "= (Conlon ((Con8st) |
These step sizes are no greater than the reciprocals of the smallest Lipschitz constants
of Vcg,H, Vvo,H Vie,Hs Veo,H, and VuH in the tth iteration, respectively. Details of
the calculations are shown in Appendix 6. This property is useful for the convergence
analysis of the DSAPA.

The constraints in (22) are met by projecting the updated estimates to SF;. For the

constraints on Cg,, in steps 10-15 of Subroutine 1, we first set diagonal entries of (Co Oe

t+1
*j
all other entries to zero for any j €[q]. The infinity norm on Lg, is met by setting all

 

where e7; =A9 | (ut)* ut

 

Fi=
F

 

 

 

to zero. Then, we keep the d entries with the largest absolute value of (Co,),* and set
entries larger than a to be q and setting all entries smaller than —a to be —q (step 4 in
Subroutine 3). A similar approach applies to Ep,. We also keep yy entries with the largest
absolute values and set other nonzero entries to zero (steps 3-6 in Subroutine 4).

Note that Lo, and E@, can be updated by node i independently and are not shared with
other nodes. Updating Co,, Ve,, and U needs communication from other nodes due to
the coupling in the objective function. Vg, cannot be shared with other nodes, since oth-
erwise other nodes can estimate Lo, by multiplying U and V@,. Thus, node i computes
the intermediate terms that depend on V@, and send to other nodes instead of sending
Vo,;, as illustrated in Fig. 3.

The algorithm is initialized as follows. Ls, in node i is defined as,

oe if (Yo) ej =L0<1<K

(Loki =) UBM if (Yogi =K (35)
“1! if (Yo,;)xj = 0
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 16 of 36

 

Node 2 Node 1
(Vo,«)'(Co,)' 2\*

NG, = Calculate Mg, using (Vo,2)"(Co,)'o,* (Vo,«)"(Co)' oo
@D2* “D;) O2*

Calculate Vcg, H using Vo,.Mo, Vo,Mo,

Update Col"

gradient descent

———< a! po thresholding on diagonal and columns
Vo,«Mo,

T, t+ =
(Voye) (Con) ox» Calculate My, using (Vo,0)'(Co)™ 0.x Vane) (Co) ox
(Vent) (Co) + | Calculate V v,+H using Mo, Mo, Mo,

Mg .
A Update Vo," by gradient descent

gradient descent
Update Lo
thresholding on entries

gradient descent
Update Ey,"

thresholding on entries

Calculate Vy using 1,' 1,' C! G'
Update U‘! by gradient descent

 

 

 

Fig. 3 Computation and communication in DSAPA (W = 2)

 

Then, node i performs the truncated singular value decomposition on LS, and let
u” mW” (Vy)? denote the rank-r approximation to Lb; Then, node i transmits yu” to
all other nodes. Each node initializes at

Ww
1 1/2
ur = — ou)” (2), (36)
i=1
—1
Voie = (L8,)' UP ((U)" UP), and (37)
E>, = Co, =0. (38)

The convergence of DSAPA is summarized as follows.
Theorem 3 From any initial point, DSAPA always converges to a critical point of (23).

The computational complexities of Subroutines 1-5 are O(nqgr), O(mgr), O(ma),
O(mq), and O(m@qr), respectively. The per-node per-iteration complexity of DSAPA is
O(nqr). In contrast, the complexity of the centralized algorithm in [27] is O(umr).
The communication cost of Subroutines 1, 2, and 5 are O(n”), O(nWr), and O(mWr),
respectively.

For data clustering, a central node collects Co, from all the nodes and applies spectral
clustering [41] to obtain the clustering results.

When A, and A» are large enough, (23) approximates (3), but the step sizes in (30)—
(32) and (34) are small and that reduces the convergence rate. One practical solution is
to dynamically increase 41 and A2 [55]. We suggest the following practical selection. Ini-
tialize with small A; and A2, and replace 42 with pA2 (o > 1) for the first To iterations.
Then, reset Az to the initial value and update them with pd; and pAz simultaneously in
each iteration. The algorithm terminates after T iterations.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22

Subroutine 1 Iterate Co, in the i-th distributed node

1: Compute (Ys, ) (Co) >. ,Vj €[ W].

2: Send (v4 © ) (Co) >. to the j-th node Vj €[W],j Fi.

: \t : \t ct Ww ; \t t

: Compute Mo; = (Vin) (V5,.) (Coi) oi 7 djaLjzi (V5,.) (Co) oj«"
: Send Mo, to the j-th node Vj e[ W],j Ai.
: Compute Vo.Mo;; Vj €[ W].
: Send Vi,,Ma, to the j-th node, Vj «[ W],j 4 i.
: Compute Vcg,H according to (24).

oN Dn oO Fe WD

: Compute Tt according to (30).
‘|
9: Compute Col" = Cy, — tet, Veo,H.
t+1
10: Set (Co,),7 44,, = 0 Wi €l4]
11: for every j = 1,2,...,q do

2: if>>,1 (ce Jest 40] > d, then

13: (Co, only keeps d entries with the largest absolute values. Other nonzero
entries are set to be zero.

144. endif

15: end for

16: Send (Co, Jove to the j-th node, Vj «| W],7 #i.

Subroutine 2 Iterate Vo,, in the i-th distributed node
1: Compute (Ys, ) (Co, oe Vi el W].
2: Send ( Vin) (Cos, " to the j- th node Vj €[ W],j 4 i.

; Compute Mo, = (V5,.) (Vine) (Co) Mi jer (Vign) (Cor)ith and Ff
4, Send Mo,, F; to the j-th node Vj €[ W],j #i.

5: Compute Vy, according to (25).

6: Compute Tye . by (31).

. t+] yt
7, Compute Va, = Vg, Tye Veal

Subroutine 3 Iterate Lo, in the i-th distributed node
1: Compute Tz, by (32).
2: Compute Vi_,H according to (26).
3: Compute Lg! = Ly, — to, Vio, H:
t+1 +1 t+1 t+1
4: If (Lo:),; > 4, set (Lo, ye = a}. If (Lo), < —Q1, set (Le;),; = —a}. Vk €

kj J
[m],7 €lq].

 

5 Results: numerical experiments

We evaluate the performance on the Irish smart meter dataset (ISMD) [56] and the UMass
smart* microgrid dataset (USMD) [57]. The ISMD consists of more than 5000 residen-
tial customers. The measurements are obtained every 30 min and have a unit of kilowatt

Page 17 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 18 of 36

Subroutine 4 Iterate Eo, in the i-th distributed node
1: Compute Vz, H according to (27).

: t+1 t
2: Compute Eg” = Ly. — TE», VEo,H-
t+1

t+1 t+1 t+1
3: If (Ex)i, > a, set (Eo:),; = a. If (Eo,) x, < —Q2, set (Ew;),; = —a7. Vk €
[m],j €lq].
4: if yj EK 1 (Ee,)¢i1 40] > s/W,Vk €[m],Vj €[q] then

5: EQ only keeps s/W entries with the largest absolute values. Other nonzero entries
are set to be zero.
6: end if

Subroutine 5 Iterate U in the i-th distributed node

t+)" ttl
1: Compute u) = (Voi) Vo,. and send to all other nodes.

2: Compute ¢/ = Ly Vou and send to all other nodes.
3: Compute ViH according to (28).
4. Compute Tz by (34).

5 Ut = Uf — pt Vy.

(kW). The UMSD contains 443 users in 24 h, and the power consumption is measured
every minute. Some users have long sequences of zero power consumption, and some
users have significantly high power consumption occasionally. We suspect these measure-
ments have data quality issues resulting from devices or communication and remove these
users from the datasets. We use 4780 customers in 30 days for ISMD and 438 customers
in 6h for USMD. Thus, the size of the data matrix L is 1440 x 4780 for ISND and 360 x 438
for USMD. The power consumption is at most 6 kW and 99 kW, respectively. Since the

° ° ° ° ok °
raw measurements are noisy, L is approximated by a rank-r matrix L* ,_. by keeping only
‘ ‘ ** 7 2 ** 2
the largest r singular values. The recovery error is measured by ||L7. — Lile/IlLoank ple

where L is the recovered matrix. We choose r to be about 10% of the total number of
the singular values. Then, r is set to 150 for ISMD and 40 for USMD. The following
experiments are tested on ISMD, if not otherwise specified.

As described in Section 2.3, normalized mutual information is used to measure the
data privacy. We now calculate the average normalized mutual information of 4780 users
NI = sai ee NI (Lyi, Ys;). AS a comparison, we also calculate the normalized mutual
information between the noisy data (before quantization) and the actual data. The quan-
tization level K is chosen as 2 or 5. The quantization boundaries and quantized values
are summarized in Table 2 (K = 2,5). We place more boundaries in the region where
data concentrate. Selecting the optimal quantized boundaries is beyond the scope of this
paper and will be left for the future work. We believe these parameters can be optimized
if a small portion of ground-truth data are available for training. The noise level o varies
from 0.1 to 0.4 with a step size of 0.02. To compute the probabilistic distribution of L,;,
we divide the range 0-6 kW into 100 or 300 equal intervals and compute the empirical
distributions. As shown in Fig. 4, the normalized mutual information between the power
after quantization and the actual power consumption is always smaller than that between
the noisy value before quantization and the actual power consumption. This indicates
the proposed quantization process enhances the data privacy. In addition, the norma-
Wang et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:22

Table 2 Quantization boundaries and quantized values

 

w(K = 2): Wo = —00, 0; = 1 kW, @5 = 00

Q(K=2):  Q, =O0.5KW,Q =3kW

w(K =5): Wp = —00, WF = 0.25 kW, w} = 0.5 kW, @} = 1 KW, @7; = 3 KW, wF = 00

Q(K = 5): Q; = 0.2 kW, Qo = 0.4 kW, Q3 = 0.85 kW, Q4 = 2.5 kW, Os = 4.5 kW

w(K =7): Wh = —&, oF = 0.5 kW, @5 = 1 KW, @} = 3 KW, @F = 5 KW, we = 10 KW, wE = 20 KW, @F = 00
Q(K = 7): Q; = 0.2 kW, Qo = 0.7 kW, Q3 = 2 kW, Q4 = 4 kW, Qs = 7 kW, Og = 15 kW, Q7 = 35 kW

 

lized mutual information NI decreases when either K decreases or o increases. That is
consistent with the intuition.

Since no ground-truth clustering result exists for this dataset, we define an index CJ
to evaluate the clustering performance. Let a; denote the maximum angle of all the data
points in group j to the estimated subspace of this group. Let b; denote the minimum
angle of any point in group j to the other subspaces. The clustering index C7 measures the
clustering accuracy and is defined as

1 N b; — a;
Cl=—) + ___ (39)

N & max{aj, bj}
CI is large if a;’s are small and b;’s are large, which means that points in the same group
are close to the subspace of that group and away from other groups. A larger CI corre-
sponds to a better clustering result. We apply Sparse Subspace Clustering (SSC) [21] to
this dataset with different cluster numbers and compare the resulting C/’s. We use the
Alternating Direction Method of Multipliers (ADMM) [58] to solve SSC. When the num-
ber of clusters is p = 4, we obtain the maximum CI = 0.085. Thus, we set the number of
clusters to be 4 in the following experiments.

We generate corruptions E* and noise N randomly. The nonzero entries of E* are
selected from [| —4,—0.5] and [0.5,4] uniformly. Every entry of N is drawn from the
N (0, 0.37). The quantization level K is set to 5. The locations of the missing data are
selected randomly. The simulations run in MATLAB on a computer with 3.4 GHz Intel
Core i7.

We evaluate DSAPA on the quantized measurements. We choose W = 5 agents. We
assume the upper bound of the magnitudes of the sparse error and the power consump-
tion are known. For simplicity, we use the largest value of the given error and set a2 = 4.

 

       

 

 

% -© No quantization (with noise) b. ->-No quantization (with noise)
035+ %, * K=5 0.35} “On. = K=5
% -¢ K=2 Og e ~¢ K=2
0.34 ‘% 0.3 Om Geng. _ |
PPP en nnn Gennd

0.25 -

o

iy

n
T

S
wy
S
Ny

S
ran
n

S
an
n

 

o-0-09- -©-
O1f ~ 9° OO -e--0-0- 0.1

-O~-0--6-

Normalized mutual information
Normalized mutual information

-@- . | l
O-0--6- © ®--0--0--@-0-0--0--6- 09.

0.05 1 1 1 1 1 1 1 0.05 1 1 1 1
0.1 0.14 0.18 0.22 0.26 0.3 0.34 = 0.38 0.42 0.1 0.14 0.18 0.22 0.26 0.3 0.34 0.38 0.42

Noise level Noise level

(a) (b)

Fig. 4 Normalized mutual information between the original power usage and the quantized measurements.
a 0.06 kW per interval. b 0.02 kW per interval

 

 

 

 

“O-O- 0 -0--0-6 6
1 1 1

 

 

 

 

 

Page 19 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 20 of 36

Similarly, we set a} = 6. We set d = 50. A, and A2 are initialized to be 0.5, and p = 1.05.
The maximum iteration number T is set to be 200. To is set to be 40.

Here, d is selected to be approximately r/(p — 1). We use p — 1 considering the overlap
between subspaces. We remark that varying d around the selected value does not affect
the result. 4; and A2 are self-adjusted in our algorithm as discussed in the last paragraph
of Section 4.

Figure 5 shows the energy consumption of a single user in 24 h. It compares the actual
data, the rank-150 approximation of the actual data, the quantized observations, the
recovered data by DSAPA, and the average quantized data of the users in the same group.
One can see that the rank-150 approximation of the actual data has a similar pattern to
the actual data. Clearly, the details of power consumption are hidden in the quantized
measurements. For instance, the two peak consumptions are no longer visible in quan-
tized measurements. Thus, an intruder does not know the user pattern if only accessing
the quantized measurements of that user only. On the other hand, DSAPA recovers the
power consumption trend accurately from the quantized data. The two peak loads are
accurately identified in the recovered data as shown in Fig. 5. The recovered data can be
used for grid planning.

After obtaining C using DSAPA, we implemented spectral clustering [41] to cluster the
data points. To visualize the recovered consumption pattern of users in each group, we
normalize the power consumptions and compute the average of users in the same group.
Figure 6 shows the average profile obtained by our method in 1 day (no missing data and
with 15% missing data). For comparison, the mean daily profile of the ground-truth data
clustered by SSC is also shown in Fig. 6. One can see that the data losses do not affect
the recovery performance of DSAPA. The recovered patterns are close to the actual pat-
terns obtained by SSC, considering that the measurements are highly noisy and quantized.
Now we pick some users in the same group and average the quantized value (K = 5) of
these users. We calculate the normalized mutual information between one user and the
averaged quantized value of the selected users. Figure 7 shows the normalized mutual

information when the number of selected users varies. The value does not decrease

 

 

 

 

 

 

 

 

 

 

4
— Actual data
354 ===" Rank-150 approximation of the actual data
| - — Recovered data
3/5 \ , -@« Quantized information
= { \ i -@ Average quantized data (same group)
™ 2.5/5 OB | GO000000009 «9 900 go |
: i: i: o£ 4G i :
60 14 yea n\ i 8 j\:
S$ 2F7 ay EB |! ie ' : i
= uy El f,/Asal\ i EL
o i i i : ‘ ~, *‘ ‘7 7. : : . 4 :
= 157, 74; e Vl a’! Ws i: fi ed
© yy na,: J RVG aN fy 7
Au ' 7 il! Wil, INV ‘3 Vii fe ai
Ip \e j a\h ‘ele / 2 y \P\p on 7 7
ay RA pepe AG
| ite in ld £ 2 _ f iS : =
0.5 3 * ao G OC FH Fs Q: 6 AQ = :
6 W Ot 6 6 6 V0oe
0 | | | | | | | | | | | |
0 2 4 6 8 10 12 14 16 #18 20 22 24

Time (hour)

Fig. 5 Comparison of actual data, rank-150 approximation, recovered data, quantized data, and average
quantized data in the same group

 

 

 
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 21 of 36

 

0.0.

 

0.05 T T T T T T T T T T

 

5

--DSAPA on quantized data (no missing data)
9.04|-** DSAPA on quantized data (15% missing rate)) -
- Applying SSC on actual data

 

 

S
So
o

   

Normalized average data
Normalized average data

 

 

$2
0.02 ¢
0.01 ¢
° 3 4 6 3 10 12 14 16 18 20 29 2A "0 2 4 6 8 10 12 14 16 18 20 29 24

Time (hour) Time (hour)

 

 

  

 

 

 

 

 

 

= 0.05 =z 0.05

= =

o 0.045 o 0.045

oo On

§ S

© 0.03} © 0.03}

Ss s

a oO

R 0.02 8 0.02 F

a 3

E 0.01} E 0.01}

° o

Z 0 1 1 1 1 1 1 1 1 1 1 1 1 4 0 1 1 1 1 1 1 1 1 1 1 1 1
0 2 4 6 8 10 12 14 16 18 20 22 24 0 2 4 6 8 10 12 14 16 18 20 22 24

Time (hour) Time (hour)

Fig.6 Average consumptions of four different groups in 1 day (actual and recovered by DSAPA)

 

 

 

much when the number of users increases. Compared with Fig. 4b, one can see that the
averaged quantized value of the same group does not provide much information to the
single user.

We compare DSAPA with Approximate Projected Gradient Method (APGM) [28] and
Quantized Robust Principal Component Analysis (QRPCA) [35] for data recovery in
Fig. 8a. We apply SSC on the recovered data by APGM (or QRPCA) to obtain the clus-
tering result, labeled by “APGM + SSC” (“QRPCA + SSC”) in Fig. 8b. If we simply use the
quantized value Q), Qo,--- ,Qs5 to estimate the actual power consumption, the relative
recovery error is 0.869, which is much larger than the results in Fig. 8a. When the missing
data rate changes from 0 to 0.4, our method always outperform the other methods both
in data recovery and data clustering. For comparison, CJ = 0.085 for SSC on the ground-
truth data, and CJ = 0.05 for a random clustering. Our method achieves C7 = 0.08 using

quantized measurements with 5% corruptions and no data losses.

 

 

 

 

 

0.3 T T T T T T T T
S
Ss
3 0.25 + :
E
&
5
<
3 --
S 02°F “@---@__._ 7
= O---@---@--~9.--9---0---0---9---90
3
oO
SN
“a
& 0.15 5 7
ol
o
Z
0.1 | | | | | | | |
100 200 300 400 500 600 700 800 900 1000
Number of users used in the same group
Fig. 7 Normalized mutual information between one user and the averaged quantized value of users in the
same group (0.02 kW per interval)

 

 

 
 

 

 

 

  

 

 

 

 

 

 

Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22
0.5 T T T T T T 0.085 T T T 7
+ QRPCA 4 -@ DSAPA+Spectral Clustering
0.45 | APGM aw 1 -+ APGM+SSC

5  |®DSAPAJ Lu. on © 0.08 =-=-9.--@ |} @-QRPCATSSC

5 0.4+ a , 2 OTT ORG gg

c 5

2 0.35 3 0.075

oO =

= 03 3

0 S 0.07

3 0.255 °

oO oO

a = 0.065

0.2F gee >
weer oO”
0.152% 1 1 1 1 1 1 1 0.06 1 1 1 1 1 1 1
0 005 O1 015 02 025 03 035 04 0 005 O1 O15 O02 025 03 035 0.4
Missing rate Missing rate
(a) (b)
Fig. 8 Relative recovery error and clustering accuracy when the missing rate changes (s/mn = 5%)

 

 

 

We vary the number of users by randomly selecting a subset of the 4780 users. Under
the 15% missing rate and no corruption, Fig. 9 shows the recovery error when the number
of users varies. The recovery error is 0.35 when the user number is to 500 and decreases
to 0.2 when there are 2500 users.

We test the case when no additional noise is added before quantization. We vary the
estimated noise level when implementing DSAPA since the measurements usually contain
observation noise. As shown in Fig. 10, DSAPA can recover the data with no additional
noise. However, adding no noise can lead to a low privacy level. The normalized mutual
information when K = 2 and K = 5 are 0.2862 and 0.9579, respectively (0.02 kW per
interval). These values are much higher than those shown in Fig. 4, indicating a lower
level of privacy when no noise is added.

In Fig. 11, we compare the relative recovery error and the clustering index CJ of DSAPA
and the centralized algorithm Sparse-APA in [27]. Since Sparse-APA does not consider
missing data, we study the case with full observations. The corruption rate is set as

 

 

 

 

 

 

0.35@ ! !
\
\
\
\

5 037 %

m \

o \

>

5 ‘

z \

5 0.25 + \

g ©.

© ae

a ~e. 7

= * “OC: ~

o

0.2 5 ~-©9-_.
* Or O~ ~~
0.15 | | | | | | |
500 1000 1500 2000 2500 3000 3500 4000 4500
Number of users

Fig. 9 Relative recovery error when the number of users increases

 

Page 22 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 23 of 36

 

 

0.23

0.225

0.22

0.215

0.21

0.205

Relative recovery error

0.2

 

 

 

0.195
0.15 0.2 0.25 0.3 0.35

Noise level

Fig. 10 Relative recovery error when noise level in the recovery process increases (without adding additional
noise)

 

 

s/mn = 5%. The recovery error of Sparse-APA is small than our method when the algo-
rithm initializes, because Sparse-APA can compute a better initialization in a centralized
fashion. However, the difference decreases as the iteration number increases. After 200
iterations, both algorithms perform similarly.

We next show the performance of DSAPA on USMD. Since the measurements vary
from 0 to 100 kW, we set K = 7, and a; = 50. The quantization boundaries and
quantized values are in Table 2 (K = 7). p and d are set to be 4 and 15, respec-
tively, using the same technique as discussed in the previous experiments. We gen-
erate the corruptions E* and the noise N randomly. The nonzero entries of E* are
selected from [—10,10] uniformly, and the corruption rate is 5%. Every entry of N is
drawn from the \(0, 0.37). Similar to Figs. 5 and 8a, we show the results on USMD
in Fig. 12.

 

 

 

    

 

 

 

 

 

0.35 1 1 0.08 T
-- DSAPA (W=5) -- DSAPA (W=5)
—Sparse-APA 9 —Sparse-APA

~ 03% “3 0.078 +
o ‘ & q~ 3s
5 ‘ on

\ S
> ‘ co
5 0.25¢ © 0.076
> n
° 3
3 O
fas . o
2 0.27 mS, -S 0.074
pb ‘ ~ ~ Gay
Ss ~~ >~. ° /
oO =~. oO !

3
0.15 a 0.072}
7 '
0.1 1 1 1 1 1 1 1 1 1 0.07 1 1 1 1 1 1 1 1 1
0 20 40 60 80 100 120 140 160 180 200 0 20 40 60 80 100 120 140 160 180 200

Iteration Iteration

(a) (b)
Fig. 11 Comparisons between DSAPA and Sparse-APA (centralized) [27]

 

 

 
Wang et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:22

 

  
   
     

 

  

8
-0-QRPCA

~ APGM

O7|-@DSAPAL ne 4

: | —— Actual data
: 1 |---Rank-50 approximation of the actual data
2 i |-> Recovered data

¢: |- Quantized information

   

 

Power usage (kW)
Relative recovery error

 

 

 

 

 

 

 

 

 

0 1 2 3 4 5 6 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
Time (hour) Missing rate

Fig. 12 a Comparison of actual data, rank-50 approximation, recovered data, and quantized data (USMD). b
Relative recovery error when the missing rate changes

 

 

 

6 Conclusion and discussions

This paper for the first time shows that the two seemingly contradicting objectives of data
privacy and information accuracy of smart meter data can be achieved simultaneously.
The central technical contribution is the development of a decentralized data recovery
and clustering method from highly quantized, partially lost, and partially corrupted mea-
surements. Distributed nodes do not share raw data with each other and cannot estimate
the actual data of other nodes. We propose a Distributed Sparse Alternative Proximal
Algorithm (DSAPA) with a convergence guarantee to solve the nonconvex problem. The
recovery error of our method is nearly optimal. The method is evaluated on actual smart
meter datasets. Future works include leveraging the time correlation within each user to
further improve the method and developing unsynchronized decentralized data recovery
algorithms.

Appendix 1
Supporting lemmas used in the Proof of Theorem 1
Lemma 1 Under Assumptions 1 and 2, the following inequalities hold

(i; - Li) | 4+ 2,/2gdab, (40)
iE

Via |(i 2), +2V Po as

dmncC . i i
vssamnne — . Q; includes the indices of the observed entries.
JV hp

 

       

2eda

 

 

 

A

L-L*|| <
F

 

 

 

 

 

where a = ae ,b=

Proof From aa 1 and 2,

ve aes
" GEionG
2./ 2eda,
TEN ASA (GN yP

s Vd (i: 7 Le

J va dmnC
Jip

 

      

 

 

 

(iH),

 

 

F

(42)

 

 

 

 

 

F

24/ 2¢day

Page 24 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 25 of 36

where (a) holds from Lemma 8 and Lemma 9 in [28], and the assumption n; < En/p. (b)
holds because of o1(G;) > h and o2(G;) < CVh.
Then

 

 

 

 

 

 

 

 

 

 

 

(c) Dp . 2
< \~{ 2¢da? (i a)
i=l i IF
+ 8gdoab||(Li — L*)e, \le + seta (43)

2 ate [(L-°), [+ 8eteedv|(0-1)

+ 8gda;"b*p

= (Va |(i 1), |, +22)

where (c) follows from (40) (or (42)). (d) holds from yt <

 

 

 

 

(i:- Li).
ill

JP | (Z — L*) - .- Then, we have the desired result. O

Lemma 2 Let 6 = vec(X), 0* = vec(X*), F(@*) = F(0*), and X, X* € Sjx. Follow the
same assumptions as those of Theorem 1. Then, with probability at least 1 — pCye~@5"/?,

(vor @*),6 _ 6*)

<aoataetavFi (8-2), as

+ 8.04Lygda,/Ena2,/s + 8.04Ly¢d,/E npoyb
+ 2a2S5Lq,

 

holds for the positive constants C, and C3. {.,.) denotes the inner product of two matrices,
i.e., the sum of entry-wise products.

Proof The proof is generalized from the proof of Lemma 2 in [27] which does not
consider missing data. Here we extend the analysis to handle missing data. According
to the definition, there exists a permutation matrix I’* such that L* can be written as
L* = [LE L5, 13 | I’*. By Assumption 2, L can be written as L = [indo rbp| l*,

where the dimension of 1; is smaller or equal to (g — 1)d.
Note that )*-, @ ((x7) ki) = land [L7'VxF (x*)| kj = —Ly hy oe
kj

Ly), j=1)- Combining them with (7), one can conclude that the elements of Ly VxF (X})

have zero mean, and the variances are bounded by one. Using the result of Lemma 1 in
[27], we have

|Log! VxF (X7)||, < 2.01Vén/p (45)
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 26 of 36

holds with probability at least 1 — Cye~©5"/?, X* is the same ith group as L* under the

permutation I’*. Then

(Vo F(6*),6 _ 6*)

 

— (VFO), X _ x*)

 

lA

(VxFor),L _ L*)

 

+ (VxFOr),£ _ E*|

 

oN

Pp
2)> (VFO), Li - Li)

i=l
+ (VxFO°),£ - E*|

(b) .
<)> | VxF (X7) |, |
i=l

 

 

 

L,—-L?

 

 

 

+ 2a2sLa
x

L;—-L}

P
< 201LyVEn]p )~ V2¢a |
i=1

 

 

 

+ 2a2sLa
F

(Hi),

 

 

 

 

F

© 2.01Ly./én/p Sy Bed eda
+ saaitos Tadd + 2a2sLy

© onto | (i-1"),
+ 8.04Lygd./Enpayb + 2azsLy

< 4.02L4(./gda)./Egdn | (x = x*) |
4 4.02Ly(./gda)./Egdn | (é - E*)
+ 8.04Lygd./Enpayb + 2a2sLy

© 4.02Lygda,/En | (x - x*) |

+ 8.04Lygda,/&na2/s
+ 8.04Ly¢d./Enpayb + 2asLy
holds with probability at least 1 — pCye~@5”/?,

(a) holds from the linearity of the inner product. The first term of (b) holds from
| (A, B) | < ||All2||Bl|.. The second term of (b) holds from the fact that both E, E* have at

F

ol:

F

 

 

 

most s nonzero entries and |VxF(X*);;| < 1. (c) holds from (45) and the fact | L,- Le <
/ 2gd | L,- L* e (d) holds from Lemma (40). (e) holds from a (i; — Li) <

 

 

 

 

 

 

 

F
JPL — L*)gl|le. (f) holds because ||(E — E*)a||r < 2a2./s, which results from the fact

that |E; j — £j,| is bounded by 2a2. The probability 1 — pC\e5"/P comes from the union
bound for P(maxjefy] || Vx F(X;) lo < 2.01Lq/&n/p). OO

Appendix 2
Proof of Theorem 1

Proof The proof follows and extends the proofs of Theorem 1 in [28] and Theorem 5 in
[37]. We extend from the low-rank matrices in [28, 37] to matrices with columns in p low-
dimensional subspaces. Moreover, ref. [28] does not consider corruptions, and ref. [37]

does not consider missing data. Here we consider both missing data and corruptions.
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 27 of 36

The first bound 2a, + 2an,/ 5 in (8) follows from the fact that L, L*,E, EX Sr. We
discuss the second bound in (8) as follows. We denote (4) to be F(X) when we treat X to
be the variable. Note that Sfx is a compact set, and the objective function is continuous
in X, F(X) then achieves a minimum in Spy. Suppose that Xe Sfx minimizes F(X).

Let 6 = vec(X) € R”” and Fg y(0) = F(X). By the second-order Taylor’s theorem, we
have

Foy (0) = Fa,y(8*) + (VoFa,y(6*), 0 — 0)

(46)

+4 - (6 ~ 6*, (V2, Fe y(6))(6 — 6*))
9 IL Vg9¥ Q,Y ,

where 6 = 6* + 7(6 — 0*) for some 7 €[ 0,1], with corresponding matrices X = X* +
n(X — X*).
From (46), Lemma 2, and Lemma A.3 in [38], we have
0 > F(X) — F(X*)

. Vo a (47)
> eX —Xalle + FX — XY alle — 0.

holds with probability at least 1 — pCje~©5”/P where ce = 4.02Lygda./én,
n = 8.04Lygda,/Enaz,./s + 8.04Lygd./Enpayb + 2asLy.

By solving (47), we then have

IX —X*olle < (G+) + 270) /Va- (48)

Thus,
IL — L* |e /./mn

© (/2¢da\\(E — L*) alle + 2 2¢dpeb)/ Jmn
< (/2gda)(\(X — X*alle + IE — E*)alle)/mn

 

 

+ 2,/2¢dpab/./mn
(b)
< (/2gda)((cp + \/c7 + 2¥an)/Yq + 202/s)//mn
+ 2,/2gdpa,b/./mn
©) Midinm? = Modinimi fi M3dni mist (49)
7 hp hip? hpi
Mad?s? Msd
+ + 1
hp2 h2
(d) dvd dx3/* 1/4
= ok a (—)
fe./m f3/2m'/4 \mn
VKd / s \1/2
+ C3 (—) ,
f  \mn

where M,—Ms5 are constants. (a) holds because of (41). (b) holds according to (48).
(c) holds because of the Cauchy-Schwarz inequality. (d) holds because f = h/m,

 

 

5 1 1 5
51 1 5

Mod 7 nm _ Myed* ang Med — M54 The order of both terms are smaller than
h4 p2 ftm2 hi f2m2

 

 

0 (4). A
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 28 of 36

Appendix 3
Supporting lemmas for Theorem 2

Lemma 3 There exists a set V C Spx with

| > exp nd (50)
16

such that the following properties hold for any y € (0, 1]:

1. For allX € X, Xj = tay or 0, Vi, j), where a = min(a1, a2).

2. For all X, XY EX,i Fj,

|X —XV|2 > ay? (> = s) . (51)

Proof Now we independently generate a set V of | exp (or) | random matrices
from the following distribution. According to columns’ indices, X is first been divided
into X;,X2,--- ,X ,, which correspond to indices {1,..., ane {5 ] + 1, wr 2LF Ih, {2[5 | +
1,...,3 LS! hee {(p—-1) LS! + 1,...,”}, respectively. For the first d rows of Xj, fix the loca-
tions of | om! entries in each row and set the values to zero. The remaining d| a. —al om!
entries take values tay with equal probabilities. For all i ¢ {d+ 1,...,m}, 7 €[ | p- iF

Xi,j = Xxj, wherek = i(modd) + 1. (52)

The same process is applied to X2, X3,--- , Xp». Then, one can see that X can be written as
X =L+E, where L can span subspaces with dimension smaller or equal to d, and E is a
sparse matrix. We further have

I[Llloo = ay Sa, ||Elloo = ay <a, and|lEllo <s. (53)

Each column of L can be represented by at most d other columns. Thus, V € Syy.
Note that the locations of the zero entries are the same for all matrices drawn from the
above distribution. Consider two different matrices X and X drawn as above, we have
12 rw)
|X — Xp = Yi — Xi)
ij

a (15! 215 |
M1 A A
= LSID | Oy X + DY Gy — Xi”
i=l \ j=l j=15IH1

(54)
fore S (Xi — Xj)?
J=0-DI5I+1
dn—d| = |
2.2,
= da*y* |= J d bi

where 6;’s are independent 0/1 Bernoulli random variables and the means are all 5 Fol-
lowing the same proof technique of Lemma 4 in [37], one can show that * satisfies the
property 2. L

Let Y = X +N, where the entries in matrix N are i.i.d. and generated from Gaussian
distribution N/(0,07). Suppose that X € 2 is chosen uniformly at random. Lemma 4
bounds the mutual information I[(XQq, Yq).
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 29 of 36

Lemma 4

I(Xq, Yo) < ae log (1 + (“*)") (55)

oO
Proof The proof is similar to the proof of Lemma 5 in [31], but [31] does not consider

corruptions. We modify the proof to handle corruptions. From Lemma 5 in [31], one can
obtain

(XQ, Ye) < H(Xe + Na) — H(Ng). (56)

where & denotes a matrix with all entries are iid. generated from {+1,—1}.X¥ = X-8
denotes the entry-wise product of X and &.
The vectorization of Xg + Ng is denoted by vec(X9q + Ng) € RI&!. We compute the

covariance matrix as
D := Elvec(Xg + Ng)vec(Xg +Ne)!]. (57)
Then, by Theorem 8.6.5 in [59], we have

H(Xq + No) < 5 log((2ze)!*"det())

1 (58)
= 5 log(2re) lary? + oI *20%2),
The equality holds since X has sg zero entries.
We have H(Ng) = 5 log((27e)!*!a7!2!) and thus
1 (a7y? 4 o2)|821-S2 g 282
(XQ; Ye) < 3 log (Ca ; (59)
which establishes the lemma. OO

Appendix 4
Proof of Theorem 2
Proof The proof follows Theorem 4 in [31] which does not consider the corruptions.

Our proof is more involved due to the corruptions. Choose € so that
1 — 2Co)a? dn — d|+| — 64
2 = min{ b7 2000" 2,200 ~ aint = 04

8 |S2] — se

where C4 is a constant to be determined later. The set ¥ is defined in Lemma 3. y is set

to be
< Ze 2 <1 (61)
ys a\V1—2Co ~

(60)

Qe | 2mn
a V mn — 2s

Suppose for the sake of a contradiction that there exists an efficient algorithm such that

lA

for any X € Syx, given the measurements Y, returns an X, and
|X — X||z/mn < (62)
holds with probability at least 1/4. Let
X* = arg min ||X’ — X|[2. (63)
X'EX
Following the proof of Theorem 4 in [31], one can find that if (62) holds, then X* = X. By
the assumption of (62),

P(X £ X*) < 3/4. (64)
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 30 of 36

Let X be a matrix chosen uniformly at random from ‘%. Considering running the
algorithm on X, then by Fano’s inequality, the probability that X 4 X™ is at least

AH(X|Yg)—-1
log || 65)
_ A(X) -1(%, Ya) -1 . _ (Xa, Ya) +1
— log |X| ~ log ||

We have obtained || from Lemma 3 and /(Xqg, Ye) from Lemma 4. Then, using the
inequality log(1 + z) < z, we obtain

A 16 |Q2] —se say 2
par é iy a1 5 (PE (2) +1). (66)

Combining (66) with (61) and (64), we obtain

a («is - —— (<)+1)> 4 (67)
dn — a \"! 8 79q Ne — 4’

which implies that
_ 2 —d\=|—
os (1 — 2Co)o~ dn — d|— | — 64

 

(68)

256 [QQ] — se
Setting C7 < ae leads to a contradiction, hence (62) must fail to hold with probability
at least 3/4. Using the definition f = el we obtain the desired result. O

Appendix 5
Proof of Proposition 1

Proof Given any i, from (5), we know that L,; = LC,;. Without loss of generality, we
assume Lei E Si where the p subspaces are denoted by Si (i €[p]). Then, from the con-

Px(1\xi)
straint Ci; = 0, Vi €[ nm], we have Ly; =[L1\si L-1] eD } where Lj\,; denotes all data
xi

points belonging to S, except L,;. L_1 denotes all data points belonging to {Sj}P_p. cow)
and COV are sparse coefficients corresponding to Li\si and L_1, respectively. Now we
only need to prove that cOY =0.

xi

If COV + 0, then L,; belongs to a subspace si which is different from S,, and spanned

Ca\xi)
by data points corresponding to nonzero entries of oy | Moreover, the dimension
5 Cl) " » 5
of S|; must be smaller or equal to d since || eD llo < d. Therefore, Ly; € S|} =
x1

Ss () S1, where () denotes the intersection of two subspaces. We first consider the case
when the dimension of sv is smaller than d. Since the data points of 1, are sampled from
a continuous distribution of p subspaces, the probability that the data point Lei lying in
a data-point-spanned hyperplane in S, that has dimension smaller than d is 0 (to see
this, consider the probability of a data point lying in a pre-fix line within a plane). Next
we show that the number of such hyperplanes is finite. Because the data points are fixed
beforehand, there is only a finite number of combinations of data points that can span
S and further intersect with S; to form S$”. Then, the probability of the union of a finite
of combinations is still zero. Therefore, the dimension of sv equals to d, which indicates
that the dimensions of si and S; are both d. This leads to 4 — Si — §). This results in
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22

a contradiction, since the data points corresponding to CoP # 0 do not belong to 8.
Thus, CoV = 0, and the claim holds. O

Appendix 6
DSAPA: proof of the Lipschitz differential property and calculation of Lipschitz constants
A function is Lipschitz differentiable if and only if all its partial gradients are Lipschitz

continuous. The definition is shown in Definition 3.

Definition 3 [60] For any fixed matrices Zj,Z2,..,Z,, matrix variable y, and a func-
tion y > Y(Y, 21,22) ++» Zn), the partial gradient VyY (y, 21, 22, + Zn) is said to be Lipschitz
continuous with Lipschitz constant Ly(Z1, Z2, .-, Zn), if the following holds

| Vy Vs 215225 09 Zn) — Vy Vs 21) Za) es ZndILE
< Lp (2; ZQ) very Zn) ly ~~ y' lle, Vy, 9.

We provide the Lipschitz differential property of H and compute the corresponding
Lipschitz constants of its partial gradients with respect to Co,, Vo, Lo, Ee,, Vi €[ W].
Let Ln Lip Lis Lia and Les denote the smallest Lipschitz constants of Vc,,H,
Viet Vie, Hy VEo,H: and V_/H in the (t + 1)th iteration. We have

IVco,H (Co,) — Veo, (Co, Ile

= [A V"(V‘)" (Co, — Co, Ile

< A V“(V)" IlellCa, — Co, Ile
Ww (69)
= Ar > dllellCo, — Co, lle
i=l

(a) /
= ———||Co, — Co _|lF;
TotW) |Co; — Co, lle
where (a) follows from (30). Equation (69) implies that

W
Li < lA1 So alle, and tc(V") < 1/ Lit. 70)
i=1

IV vet Vox) — Wot (Vo,. lle
= |A2(Veoe — Voog)(U') US +d Voir — Vor)"
(Igxq — (Coon — (Codey) + (CeCe) lle

(b) (71)
< Vou — Vowlle (Wan)? U6 lle + An

axq + (CH UCHDT — (Co) St — (Co) ee) Ie)
(c) 1

<< V. a Vi . d
—_ ty (Ut, Ct+1) | @; Ox lIE

where (b) follows from the triangle inequality, and (c) follows from (31). Equation (71)
implies that

ix

hy) S max Aalllqxg + (Co MCon)’ — (Co)ou-
ie[W] ’ ’

(Coen) lle + efp and ty(U’, C) < 1/155".

Page 31 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22

Vig, (Lo;) — Vig, Ho, lle =

| VE (La,, E%,) — VE (Lip, Eb,) + 42(Lo; — Lp, Ile
d . =

2 ||diag(V2F (Le,))vec(Le, — L’p,) Ilo

+ Ag||Lo; — Lo, lle

 

< (||diag(V*F (Le,))ll2 + Aa) IIL; — Lo, lle (73)
© (IV2ELalloo + AD ILo; — Lip lle
(f) 1
S (=pag + A2dIILo; — Lo, lle
o~B i

(g)

Sag lea — Lele
®;

where (d) comes from the differential mean value theorem. V7F(Lo,) € R”*4 has the
__, and diag(V*F(Le,)) € R™*4 is a diagonal
(Lo, kj _

matrix with the diagonal vector equaling to vec(V*F (Lo ,)). (e) follows from the fact that

0°F
(k,j)th entry equaling to > Lee |
the /) norm of a diagonal matrix is equal to its entry-wise infinity norm. Note that (1) is
lower bounded by £, and the probability density function of the normal distribution and
its derivative are upper bounded by Te and wet respectively. Then, one can easily
check that ||V?F(Lo,)|loo is bounded by sige: (f) is thus obtained by upper bounding
|| V7F(La,;) lloo- (g) follows from (32). Thus, tz (E%,) <

Ley
|| Veg, (Eo,) _ Veo, (Eo, Ile
= ||VF(LE", Eo;) — VFLG Elle
h . =
@ |Idiag(V2F (Eo,))vec(Eo, — E%, Ile
< ||V7F (Ee, lloollEo, — Eo, lle (74)

i) 1

< —

— a2 B2

(j) 1 /

= ——,,_|lEo; — Fe, lle
te (Ly. ')

/
Eo, — Eo, lr

where (h) follows from the differential mean value theorem. (i) is obtained by upper
bounding | V2F (Eo, ) Iloo by sig (j) follows from (33). (74) implies that te (Lp) =
o”B? < ——

B — uy

|VuH(U) — VuH(U')\le

= ||Ao(U — U')\(V4) VI

< JAVON) TV oI = Ue

(k)

< AV) PV |e — U' |e (75)

) ~

= ||A2 > ot ell — U' lle
i=l

(m)

=— ——__ || — U'|Ip,
ry (Vet) | Ile

Page 32 of 36
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 33 of 36

where (k) follows from the inequality || - |l2 < || - llz. () follows from (V*1)T V4! —
4 o Since ||A2 4 S le = Les (m) follows from (34). (75) implies that Les <

ao ee, andty (V4) < 1/L5E1.
Based on Definition 3, (69)—(75) guarantee the Lipschitz differentiability of H and
provide the Lipschitz constants and the step sizes of the DSAPA.

Appendix 7
Proof of Theorem 3
Proof The constraints in (22) can be transferred to the following indicator functions.

oO if there exists a
Kj (Co;) = (Co,)iq-qtij £9 Vi €l a] (76)
0 otherwise
oO if there exists a
C . , S.t. C . j d,
j €lq]
0 otherwise
if ||Le.
B(Lo,) = oo if || illo > Ol} (78)
0 otherwise
oo if ||Eo, lla > a2
Ni (Eo,) = 79
: ( %) | 0 otherwise (79)
00 if ||E@,llo > s/W
Eo.) = ; 80
Ja ( :) | 0 otherwise (80)

(76)—(80) correspond to the operations of projection in DSAPA.
Similar to the proof of Theorem 3 in [27], DSAPA globally converges to a critical point
of (16) from any initial point, provided that H is Lipschitz differentiable, and

WwW

H+ (Ki (Ca;) + K2(Co;)+
L. (81)

B(Le,;) +i (Ea;) + Jo (Ee;))

satisfies the Kurdyka-Lojasiewicz (KL) property.

The proof of the Lipschitz differentiable property of H is shown in Appendix 6. B(L@,),
I) (Eo;), Jo (Ew;), Ki(Co;), and Ky(C@,) are indicator functions of semi-algebraic sets.
Therefore, they are KL functions according to [60]. Since H is differentiable everywhere,
or equivalently, real analytic, H also has the KL property according to the examples in
session 2.2 of [61]. Thus, (81) satisfies the KL property. LO

Abbreviations

UoS: Union of Subspaces; DSAPA: Distributed Sparse Alternative Proximal Algorithm; c.d.f: Cumulative distribution
function; SSC: Sparse Subspace Clustering; PMU: Phasor measurement unit; KL: Kullback-Leibler; NI: Normalized mutual
information; APGM: Approximate projected gradient method; QRPCA: Quantized Robust Principal Component Analysis;
NILM: Non-intrusive load monitoring

Acknowledgements
This research is supported in part by ARO W911NF-17-1-0407 and the Rensselaer-IBM Al Research Collaboration (http://
airc.rpi.edu), part of the IBM Al Horizons Network (http://ibm.biz/AlHorizons).
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 34 of 36

Authors’ contributions

Ren and Meng conceived and designed the method and the experiments. Ren performed the experiments and drafted
the manuscript. Meng revised the manuscript. Jinjun provided many helpful suggestions. The authors read and
approved the final manuscript.

Availability of data and materials

The Irish smart meter datasets that support the findings of this study are available from the Irish Social Science Data
Archive (ISSDA) but restrictions apply to the availability of these data, which were used under license for the current
study, and so are not publicly available. Data are however available from the authors upon reasonable request and with
permission of the Irish Social Science Data Archive (ISSDA). The UMass smart* microgrid dataset analyzed during the
current study is available in http://traces.cs.umass.edu/index.php/Smart/Smart.

Consent for publication
Informed consent was obtained from all authors included in the study.

Competing interests
The authors declare that they have no competing interests.

Author details
'Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA. 7IBM
Thomas J. Watson Research Center, Yorktown Heights, NY, USA.

Received: 15 October 2019 Accepted: 17 April 2020
Published online: 07 May 2020

References

1. GW. Hart, Nonintrusive appliance load monitoring. Proc. IEEE. 80(1 2), 1870-1891 (1992)

2. E.J. Aladesanmi, K. A. Folly, Overview of non-intrusive load monitoring and identification techniques.
IFAC-PapersOnLine. 48(30), 415-420 (2015)

3. Z. Erkin, G. Tsudik, in International Conference on Applied Cryptography and Network Security, Private computation of
spatial and temporal power consumption with smart meters (Springer, Singapore, 2012), pp. 561-577

4. P. Barbosa, A. Brito, H. Almeida, S. Claufs, in Proceedings of the 29th Annual ACM Symposium on Applied Computing, SAC
‘14, Lightweight privacy for smart metering data by adding noise (ACM, Gyeongju, 2014), pp. 531-538

5. J. M. Bohli, C. Sorge, O. Ugus, in 2010 IEEE International Conference on Communications Workshops, A privacy model
for smart metering (IEEE, Cape Town, 2010), pp. 1-5

6. M. Backes, S. Meiser, in Data Privacy Management and Autonomous Spontaneous Security, Differentially private smart
metering with battery recharging (Springer, Berlin, 2014), pp. 194-212

7. D.Varodayan, A. Khisti, in 2071 [EEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Smart
meter privacy using a rechargeable battery: minimizing the rate of information leakage (IEEE, Prague, 2011),
pp. 1932-1935

8. D.Egarter, C. Prokop, W. Elmenreich, in 2074 IEEE International Conference on Smart Grid Communications
(SmartGridComm), Load hiding of household’s power demand (IEEE, Venice, 2014), pp. 854-859

9, S.McLaughlin, P. McDaniel, W. Aiello, in Proceedings of the 18th ACM Conference on Computer and Communications
Security, Protecting consumer privacy from electric load monitoring (ACM, Chicago, 2011), pp. 87-98

10. X. He, X. Zhang, C. C. J. Kuo, A distortion-based approach to privacy-preserving metering in smart grids. IEEE Access.
1, 67-78 (2013)

11. M. Savi, C. Rottondi, G. Verticale, Evaluation of the precision-privacy tradeoff of data perturbation for smart metering.
IEEE Trans. Smart Grid. 6(5), 2409-2416 (2015)

12. O.Tan, D. Gunduz, H. V. Poor, Increasing smart meter privacy through energy harvesting and storage devices. IEEE J.
Sel. Areas Commun. 31(7), 1331-1341 (2013)

13. F.L. Quilumba, W.-J. Lee, H. Huang, D. Y. Wang, R. L. Szabados, Using smart meter data to improve the accuracy of
intraday load forecasting considering customer behavior similarities. IEEE Trans. Smart Grid. 6(2), 911-918 (2015)

14. A. Albert, R. Ram, Smart meter driven segmentation: what your consumption says about you. IEEE Trans Power Syst.
28(4), 4019-4030 (2013)

15. N.Mahmoudi-Kohan, M. P. Moghaddam, M. K. Sheikh-El-Eslami, E. Shayesteh, A three-stage strategy for optimal
price offering by a retailer based on clustering techniques. Int. J. Electr. Power Energy Syst. 32(10), 1135-1142 (2010)

16. C. Dwork, in International Conference on Theory and Applications of Models of Computation, Differential privacy: A survey
of results, Differential privacy (Springer, Xi’an, 2008), pp. 1-19

17. L. Sankar, S. Kar, R. Tandon, H. V. Poor, in Proc. IEEE International Conference on Smart Grid Communications
(SmartGridComm), Competitive privacy in the smart grid: an information-theoretic approach (IEEE, Brussels, 2011),
pp. 220-225

18. C.Y. Ma, D.K. Yau, in Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security,
On information-theoretic measures for quantifying privacy protection of time-series data (ACM, Singapore, 2015),
pp. 427-438

19. S.Li, A. Khisti, A. Mahajan, Information-theoretic privacy for smart metering systems with a rechargeable battery. IEEE
Trans. Inf. Theory. 64(5), 3679-3695 (2018)

20. A. Reinhardt, F. Englert, D. Christin, Averting the privacy risks of smart metering by local data preprocessing.
Pervasive Mob. Comput. 16, 171-183 (2015)

21. E. Elhamifar, R. Vidal, Sparse subspace clustering: algorithm, theory, and applications. IEEE Trans. Pattern Anal. Mach.
Intell. 35(11), 2765-2781 (2013)
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 35 of 36

22.

23.

24.

25,

26.

2/.

28.

29,

30.

31.
32.

33.

34.

35.

36.

37.

38.

39.

 

 

 

50.

51.

52.

53.

54,

55,

B. Eriksson, L. Balzano, R. Nowak, in Proc. Int. Conf. Artif. Intell. Stat, High-rank matrix completion UMLR, La Palma,
2012), pp. 373-381

G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, Y. Ma, Robust recovery of subspace structures by low-rank representation. IEEE
Trans. Pattern Anal. Mach. Intell. 35(1), 171-184 (2013)

V. M. Patel, H. Van Nguyen, R. Vidal, Latent space sparse and low-rank subspace clustering. IEEE J. Sel. Topics Signal
Process. 9(4), 691-701 (2015)

M. Soltanolkotabi, E. J. Candés, A geometric analysis of subspace clustering with outliers. Ann. Stat. 40(4), 2195-2238
(2012)

M. Soltanolkotabi, E. Elhamifar, E. J. Candés, Robust subspace clustering. Ann. Stat. 42(2), 669-699 (2014)

R. Wang, M. Wang, J. Xiong, Data recovery and subspace clustering from quantized and corrupted measurements.
IEEE J. Sel. Topics Signal Process., Spec Issue Robust Subspace Learn. Tracking Theory Algoritm Appl. 12(6),
1547-1560 (2018)

S. A. Bhaskar, Probabilistic low-rank matrix completion from quantized measurements. J. Mach. Learn. Res. 17(60),
1-34 (2016)

Y. Cao, Y. Xie, in Proc. IEEE Int. Workshop Comput. Adv. Multi-Sensor Adapt. Process, Categorical matrix completion (IEEE,
Cancun, 2015)

T. Cai, W.-x. Zhou, A max-norm constrained minimization approach to 1-bit matrix completion. J. Mach. Learn. Res.
14(1), 3619-3647 (2013)

M.A. Davenport, Y. Plan, E. van den Berg, M. Wootters, 1-bit matrix completion. Inf. Infer. 3(3), 189-223 (2014)

P. Gao, M. Wang, J. H. Chow, M. Berger, L. M. Seversky, Missing data recovery for high-dimensional signals with
nonlinear low-dimensional structures. IEEE Trans. Signal Process. 65(20), 5421-5436 (2017)

O. Klopp, J. Lafond, E. Moulines, J. Salmon, Adaptive multinomial matrix completion. Electron. J. Stat. 9(2), 2950-2975
(2015)

J. Lafond, O. Klopp, E. Moulines, J. Salmon, in Adv. Neural Inf. Process. Syst, Probabilistic low-rank matrix completion on
finite alohabets (Curran Associates, Montreal, 2014), pp. 1727-1735

A. S. Lan, C. Studer, R. G. Baraniuk, in Proc. IEEE Int. Conf. Acoust Speech Signal Process, Matrix recovery from quantized
and corrupted measurements (IEEE, Florence, 2014), pp. 4973-4977

A. S. Lan, A. E. Waters, C. Studer, R. G. Baraniuk, Sparse factor analysis for learning and content analytics. J. Mach.
Learn. Res. 15(1), 1959-2008 (2014)

P. Gao, R. Wang, M. Wang, J. H. Chow, Low-rank matrix recovery from noisy, quantized and erroneous
measurements. IEEE Trans. Signal Process. 66(11), 2918-2932 (2018)

S. A. Bhaskar, in Proc. Asilomar Conf. Signals Syst. Comput, Probabilistic low-rank matrix recovery from quantized
measurements: application to image denoising, (2015), pp. 541-545

S. A. Bhaskar, Localization from connectivity: a 1-bit maximum likelihood approach. IEEE/ACM Trans. Netw. 24(5),
2939-2953 (2016)

Y. Yang, J. Feng, N. Jojic, J. Yang, T. S. Huang, in European Conference on Computer Vision, |0-sparse subspace
clustering (Springer, Amsterdam, 2016), pp. 731-747

A. Y.Ng, M. |. Jordan, Y. Weiss, in Adv. Neural Inf Process. Syst, On spectral clustering: analysis and an algorithm
(Morgan Kaufmann Publishers, Vancouver, 2002), pp. 849-856

J. Lin, E. Keogh, L. Wei, S. Lonardi, Experiencing sax: a novel symbolic representation of time series. Data Min. Knowl.
Discov. 15(2), 107-144 (2007)

E. Keogh, K. Chakrabarti, M. Pazzani, S. Mehrotra, in Proceedings of the 2001 ACM SIGMOD International Conference on
Management of Data, Locally adaptive dimensionality reduction for indexing large time series databases (ACM,
Santa Barbara, 2001), pp. 151-162

R. Basri, D. W. Jacobs, Lambertian reflectance and linear subspaces. IEEE Trans. Pattern Anal. Mach. Intell. 25(2),
218-233 (2003)

P. Gao, M. Wang, S. G. Ghiocel, J. H. Chow, B. Fardanesh, G. Stefopoulos, Missing data recovery by exploiting
low-dimensionality in power system synchrophasor measurements. IEEE Trans. Power Syst. 31(2), 1006-1013 (2016)
M. B. Hossain, |. Natgunanathan, Y. Xiang, L.-Xx. Yang, G. Huang, Enhanced smart meter privacy protection using
rechargeable batteries. IEEE Internet Things J. 6(4), 7079-7092 (2019)

A. Reinhardt, D. Egarter, G. Konstantinou, D. Christin, in 2075 /EEE International Conference on Smart Grid
Communications (SmartGridComm), Worried about privacy? Let your PV converter cover your electricity
consumption fingerprints (IEEE, Miami, 2015), pp. 25-30

L. Sweeney, k-anonymity: a model for protecting privacy. Int. J. Uncertain. Fuzziness Knowl-Based Syst. 10(05),
557-570 (2002)

R. L. Lagendijk, Z. Erkin, M. Barni, Encrypted signal processing for privacy protection: conveying the utility of
homomorphic encryption and multiparty computation. IEEE Signal Proc. Mag. 30(1), 82-105 (2012)

T. Baumeister, in 2077 IEEE International Conference on Smart Grid Communications (SmartGridComm), Adapting PKI!
for the smart grid (IEEE, Brussels, 2011), pp. 249-254

G. Giaconi, D. GUndUz, H. V. Poor, in 2075 IEEE International Conference on Communications (ICC), Smart meter privacy
with an energy harvesting device and instantaneous power constraints (IEEE, Miami, 2015), pp. 7216-7221

G. Kalogridis, C. Efthymiou, S. Z. Denic, T. A. Lewis, R. Cepeda, in 2070 First IEEE International Conference on Smart Grid
Communications, Privacy for smart meters: towards undetectable appliance load signatures (IEEE, Gaithersburg,
2010), pp. 232-237

J. Gomez-Vilardebo, D. GUnduz, Smart meter privacy for multiple users in the presence of an alternative energy
source. IEEE Trans. Inf. Forensic Secur. 10(1), 132-141 (2014)

Y. Hong, W. M. Liu, L. Wang, Privacy preserving smart meter streaming against information leakage of appliance
status. IEEE Trans. Inf. Forensic Secur. 12(9), 2227-2241 (2017)

J. A. Snyman, N. Stander, W. J. Roux, A dynamic penalty function method for the solution of structural optimization
problems. Appl. Math. Model. 18(8), 453-460 (1994)
Wang et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:22 Page 36 of 36

56.

57.

58.

59.
60.

61.

Commission for Energy Regulation Smart Metering Project. http://www.ucd.ie/issda/data/
commissionforenergyregulationcer. Accessed 5 July 2018

S. Barker, A. Mishra, D. Irwin, E. Cecchet, P. Shenoy, J. Albrecht, etal, Smart*: an open data set and tools for enabling
research in sustainable homes. SustKDD, August. 111(112), 108 (2012)

S. P. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, Distributed optimization and statistical learning via the alternating
direction method of multipliers. Found. Trends® Mach. Learn. 3(1), 1-122 (2011)

T. M. Cover, J. A. Thomas, Elements of Information Theory. (Wiley, Hoboken, 2012)

J. Bolte, S. Sabach, M. Teboulle, Proximal alternating linearized minimization for nonconvex and nonsmooth
problems. Math. Program. 146(1-2), 459-494 (2014)

Y. Xu, W. Yin, A block coordinate descent method for regularized multiconvex optimization with applications to
nonnegative tensor factorization and completion. SIAM J. Imag. Sci. 6(3), 1758-1789 (2013)

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
