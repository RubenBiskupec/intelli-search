Zhang et al. BMC Bioinformatics (2020) 21:480

https://doi.org/10.1186/s12859-020-03826-6 B M C B | Ol nfo rm ati CS

METHODOLOGY ARTICLE Oy ela w Aka -s1 4

. . ®
Variable selection from a feature chek

representing protein sequences: a case
of classification on bacterial type IV secreted
effectors

Jian Zhang', Lixin Lv', Donglei Lu', Denan Kong2, Mohammed Abdoh Ali Al-Alashaari* and Xudong Zhao? ©

 

*Correspondence:

zhaoxudong@nefu.edu.cn Abstract

eet ON Background: Classification of certain proteins with specific functions is momentous
Northeast Forestry University, for biological research. Encoding approaches of protein sequences for feature extrac-
No. 26 Hexing Road, tion play an important role in protein classification. Many computational methods
non enn: rion (namely classifiers) are used for classification on protein sequences according to vari-
is available at the end of the ous encoding approaches. Commonly, protein sequences keep certain labels corre-

article sponding to different categories of biological functions (e.g., bacterial type IV secreted
effectors or not), which makes protein prediction a fantasy. As to protein prediction, a
kernel set of protein sequences keeping certain labels certified by biological experi-
ments should be existent in advance. However, it has been hardly ever seen in prevail-
ing researches. Therefore, unsupervised learning rather than supervised learning (e.g.
classification) should be considered. As to protein classification, various classifiers may
help to evaluate the effectiveness of different encoding approaches. Besides, variable
selection from an encoded feature representing protein sequences is an important
issue that also needs to be considered.

Results: Focusing on the latter problem, we propose a new method for variable selec-
tion from an encoded feature representing protein sequences. Taking a benchmark
dataset containing 1947 protein sequences as a case, experiments are made to identify
bacterial type IV secreted effectors (T4SE) from protein sequences, which are com-
posed of 399 T4SE and 1548 non-I4SE. Comparable and quantified results are obtained
only using certain components of the encoded feature, i.e., position-specific scoring
matix, and that indicates the effectiveness of our method.

Conclusions: Certain variables other than an encoded feature they belong to do work
for discrimination between different types of proteins. In addition, ensemble classifiers
with an automatic assignment of different base classifiers do achieve a better classifica-
tion result.

Keywords: Feature selection, Variable importance, Accumulated scoring,
Classification, Bacterial type IV secreted effectors

 

© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this

article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not
included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/
licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies
to the data made available in this article, unless otherwise stated in a credit line to the data.

 

 
Zhang et al. BMC Bioinformatics — (2020) 21:480 Page 2 of 15

Background

Feature extraction from protein sequences plays an important role in protein classifica-
tion [1—4] of many areas, such as identification of plant pentatricopeptide repeat coding
protein [5], prediction of bacterial type IV secreted effectors [6—9], identification of heat
shock protein [10], prediction of mitochondrial proteins [11], etc. In general, prevailing
encoding approaches of protein sequences for feature extraction include pseudo-amino
acid composition (PseAAC) [10-22], position-specific scoring matrix (PSSM) [7, 23-
32], position-specific iterated blast (PSI-BLAST) [33-37] etc.

However, several problems do still exist and are listed as follows. First of all, it needs
to be decided which encoding approach is more effective. In fact, this problem can be
solved according to the results of classification on each encoding approach using vari-
ous computational methods (i.e., known as classifiers in the field of machine learning or
pattern recognition). In other words, the encoding approach corresponding to the most
accurate classification result should be considered. Prevailing classifiers including ran-
dom forest or decision tree classifier (RF or DTC) [1, 38], gradient boosting machine
(GBM) [39, 40], k-nearest-neighbor (kKNN) [41, 42], linear discriminant analysis (LDA)
[43, 44], logistic regression (LR) [45], multi-layer perceptron (MLP) [46, 47], naive bayes-
ian (NB) [5, 48], support vector machine (SVM) [49, 50] are credible.

Secondly, it needs to be discussed whether protein classification is predictive or not,
which is a little confused. Actually, classification labels have commonly been assigned
to protein sequences in advance. If these labels are definitive, i.e., having been certified
by biological experiments in advance, there won't be any need to predict the category
of a protein sequence again. Conversely, unsupervised learning (e.g. clustering) rather
than supervised learning (e.g. classification) should be considered, since these labels are
undetermined. And that corresponds to protein prediction. However, prevailing meth-
ods are always confusing protein classification an protein prediction.

Thirdly, the extracted feature using an encoding approach is considered to be entirely
effective. In fact, there may be only parts of the extracted feature that are effective. How-
ever, this phenomenon has been subjectively neglected. As a result, it is an important
issue how to select certain components or variables from a feature that really helps to
recognize proteins with specific functions. In other words, variable selection from a fea-
ture representing protein sequences is a new problem probably not yet avoidable, which
may be more helpful to classification of different protein sequences.

In this paper, we propose a new method for variable selection from an encoded fea-
ture. The selection of a feature from an encoding approach is excluded from our method.
Besides, no prediction work is executed. Focusing on components or variables of an
encoded feature, we implement our method at seven steps as shown in Fig. 1. First of
all, samples are divided in balance, which constitute a training and testing group. Sec-
ondly, a base classifier is automatically assigned in every resampling of the training
group. Thirdly, the score of each variable in an encoded feature is accumulated through
r rounds of resampling, training and scoring. Fourthly, a scatter plot and correspond-
ing order of variables with their accumulated scores in a descending order are obtained.
Fifthly, r rounds of training are made on resampling samples to achieve ensemble classi-
fiers in each dimension (i.e., from one to full dimension of the encoded feature) accord-
ing to variables incrementally added in the descending order. Sixthly, variable selection
Zhang et al. BMC Bioinformatics (2020) 21:480

 

 

 

 

   
  

 

   

 

 

 

 

(— >)
<
g samples) - ommanaaing,| 2
3S 50% 99% [anmnenenenriernrinie | Gp
g : “| @
= \ i one} | g
& as 30% —_—_ Rp aed f .&
Oo ( training group ) | ( testing group )_»>(_ line chart } - G
© Fi Zn gE RR ees ©)
2 ——— o =e) 2
& ‘resampling, 2 evaluation metric o
5 e < h \ 3 ©
© a’ id © _ _
5 training | scoring training | . (DTC } (GBM) =
2 r r a od ee)
4 y c rs =
© : - i 8
© _—— automatic —_ LO / g
2 ~—~_ assignment. | le
3 ( scatter plot ) (LDA) 2
oC a —- Oo
So | | | Yo
g (SVM) LR) 2
x variables in order _) ae —_— @
© : \ / \
S a ee ( NB } /MLP
Fig. 1 A framework of variable selection for identifying different proteins
Ne S

 

is accomplished using a line chart derived from the classification results on the testing
group. Seventhly, evaluation metrics are made to estimate the effectiveness of selected
variables. Experiments are made on the benchmark dataset [51] to identify bacterial type
IV secreted effectors from protein sequences, which indicates the effectiveness of our

method. More details can be seen in the following parts of this paper.

Results

In this section, we take a benchmark dataset [51] as a case to evaluate the performance
of our proposed method. The dataset is composed of 1947 protein sequences across
multiple bacterial species, categorized into two groups, i.e., 399 type IV secreted effec-
tors (T4SE) as the positive samples and 1548 non-T4SE as the negative samples. The
1947 protein sequences are randomly divided into two subsets for training and testing,
respectively. The training set consists of 973 sequences, among which 199 T4SE and 774
non-T4SE sequences are randomly selected from positive and negative samples, respec-
tively. The left 200 T4SE and 774 non-T4SE samples constitute the testing set. Besides,
we choose PSSM, which is composed of 400 variables, as the encoded feature. Following
the procedure shown in Fig. 1, the experimental results of score accumulation, ensemble
classification, variable selection and the corresponding classification results are listed as
follows.

Results of score accumulation

We randomly extract 70% of samples from the training set and choose a classifier with
the lowest classification error rate as the base classifier at each round. Meanwhile, scores
representing the importance of variables are calculated. After 1000 rounds of resam-

pling, training and scoring (i.e., 7 = 1000), we obtain the accumulated scores of each

Page 3 of 15
Zhang et al. BMC Bioinformatics (2020) 21:480 Page 4 of 15

variable from PSSM. The corresponding scatter plot is shown in Fig. 2a. Its horizon-
tal and vertical coordinates correspond to 400 variables and their accumulated scores,
respectively. In addition, the frequency of each selected base classifier is illustrated in
Fig. 2b.

It can be seen in Fig. 2a that the accumulated scores are all relatively low. Since the
accumulated scores of the 400 variables have no apparent distinction, all these variables
are considered to be enumerated at the following step instead of selecting variables with
high accumulated scores, as having been stated in [52].

In Fig. 2b, it can be seen that MLP is automatically assigned as the base classifier
for 74.7% of 1000 round resampling. On the contrary, DTC and SVM have never been

selected for score accumulation.

Results of ensemble classification on testing group

The ensemble classifiers have been built using 1000 rounds of resampling and training
on the training set in each dimension, with 400 variables incrementally added in the
descending order according to their accumulated scores. Then, the 400 ensemble classi-
fiers, each of which keeps 1000 base classifiers, are applied to the testing set. As a result,
a line chart (see Fig. 3) is obtained with its horizontal corresponding to the dimensions
with variables incrementally added in the descending order according to their accu-

mulated scores. The vertical coordinates are referred to the Acc and AUC values in

 

0.020

0.015

0.010

0.005

0.000

—0.005

 

—0.010

0 50 100 150 200 250 300 350 400

aA scatter plot of the accumulated scores corresponding to
the variables from feature PSSM.

   

LOGIST
NB

GBM

MLP

b A pan chart representing the frequency of each selected
base classifier.

Fig. 2 Results of score accumulation
XX S

 

 
Zhang et al. BMC Bioinformatics — (2020) 21:480 Page 5 of 15

 

1.00

0.95

(25,0.928721)

0.90

0.85

0.80

 

0.75
0 50 100 150 200 250 300 350 400

Fig. 3 A line chart of ACCs and AUCs corresponding to the incrementally added variables from feature PSSM

with their accumulated scores in a descending order
NX S

 

 

the incremental dimensions. It can be seen in Fig. 3 that it is the first 25 variables with
descending accumulated scores that form a 25-D feature from PSSM for effective identi-
fication of T45SE.

Moreover, some quantitative results are shown to indicate the effectiveness of the
obtained 25 variables for classification. Table 1 lists the max, mean and min values of
Accs and AUCs by incrementally adding the variables after the 25th one of the 400 vari-
ables in a descending order according to their accumulated scores. It can be indicated
that features enlarged with higher dimensions can achieve only similar Accs and AUCs
as the 25 variables do.

Classification results of the selected variables

In order to show the effectiveness of the selected 25 variables, the confusion matrix, Pre-
cision, Recall and F1 — measure are calculated in order to make a quantitative compari-
son. In addition, ROCs together with AUCs are listed as qualitative results.

Results of ROC and AUC between the first 25 selected variables and all 400 compo-
nents of PSSM using ensemble classification are shown in Fig. 4. The similar ROC curves
and AUC values indicate that the selected 25 variables keep a comparable classification
capability with PSSM.

Besides, results of ROC and AUC using the ensemble classifier consisting of 1000 sin-
gle base classifiers with the selected 25 variables are illustrated in Fig. 5. By making a
careful comparison between Figs. 4b and 5, it can be seen that ensemble classification
with automatic assignment of base classifier keeps a better ROC curve and AUC value
(i.e., 0.9287).

Moreover, quantitative results among ensemble classification with automatic assign-
ment of a base classifier and the ensemble classifier with a single base classifier are listed
in Table 2. It can be seen that the ensemble classifier with automatic assignment of a base
classifier on the 25 selected variables keeps a high TP (i.e., 157) compared with most of
the other classification strategies. Besides, it has better values of Precision, Recall and
F1 — measure (i.e., 0.904, 0.903 and 0.903) compared with the other ensemble classifier
with a single base classifier on the 25 selected variables. As to the results of the ensemble
Page 6 of 15

(2020) 21:480

Zhang et al. BMC Bioinformatics

 

 

£060 Lc6O SE60 C180 9060 6C€60 S760 LC60 abels puz

660 £060 SE60 S60 Lc6O 9060 £060 CL8'0 abe}s 1S | jas Bujules] 8160 CEO 0 0v6 0 INV
Le60 ve60 Sv60 0v6 0 8€6 0 vv6 0 SE60 8€6 0 abels puz jas Bulsel

9060 £060 906 0 6880 v060 LL6O 0160 606 0 abels puz

S060 L060 C060 v630 S880 6280 SS8°0 CELO abels S| yas Hujulres| €68°0 9060 0160 DOV
wad gd5DX WAS ARE dd aq NN aN [9] ul syjnsey onyjea ull] onyjea ues onyjea xe oinseow

 

[9] 82uasaJa1 UI UMOYUS $}{NSa1 UOSIIEdWOD dy} puke S2a102s

pa}ejnuind>e 1194} 0} Hulps0d.e Aapio Hulpuarsap e UI SajqeleA OOP JO BUO YIGZ BY} Jaye sayqeuer ay} Huippe Ajjeyuawasul Aq s}jNsa1 aAIVeWIUeNH | a1qGeL
Zhang et al. BMC Bioinformatics — (2020) 21:480 Page 7 of 15

Table 2 Quantitative results among ensemble classification with automatic assignment
of base classifiers and the ensemble classifier with single base classifiers

 

 

Classifier Dimension Confusion matrix Positive Precision Recall F1-measure
class
Automatic 400 Classifiedas | Non-T4SE T4SE Non-T4SE 0.943 0.938 0.940
assignment Labelnon- 726 48 TASE 0.765 0.780 0.772
T4SE
Label T4SE 44 156 Weighted 0.906 0.906 0.906
average
Automatic 25 Classified as Non-I4SE T4SE Non-T4SE 0.944 0.933 0.938
assignment Labelnon- 722 52 T4SE 0.751 0.785 0.768
T4SE
Label T4SE 43 157. Weighted 0.904 0.903 0.903
average
DTC 25 Classifiedas | Non-T4SE T4SE Non-T4SE 0.925 0.950 0.937
Label non- 735 39 T4SE 0.782 0.700 0.739
T4SE
Label T4SE 60 140 Weighted 0.896 0.899 0.896
average
GBM 25 Classified as Non-T4SE T4SE Non-T4SE 0.926 0.953 0.939
Label non- 738 36 ~=—s T4SE 0.797 0.705 0.748
T4SE
Label T4SE 59 141. Weighted 0.900 0.902 0.900
average
kNN 25 Classified as Non-T4SE T4SE Non-T4SE 0.931 0.919 0.925
Label non- 711 63 T4SE 0.700 0.735 0.717
T4SE
Label T4SE 53 147. Weighted 0.884 0.881 0882
average
LDA 25 Classified as Non-T4SE T4SE Non-T4SE 0.927 0.925 0.926
Label non- 716 58 T4SE 0.713 0.720 0.716
T4SE
Label T4SE 56 144 Weighted 0.883 0.883 0.883
average
LR 25 Classifiedas Non-T4SE T4SE Non-T4SE 0.883 0.957. 0.919
Label non- 741 33 T4SE 0.756 0.510 0.609
T4SE
Label T4SE 98 102 Weighted 0.857 0.865 0.855
average
MLP 25 Classifiedas Non-T4SE T4SE Non-T4SE 0.930 0.946 0.938
Label non- 732 42 T4SE 0.775 0.725 0.749
T4SE
Label T4SE 55 145 Weighted 0.898 0.901 0.899
average
NB 25 Classified as Non-T4SE T4SE Non-T4SE 0.942 0.875 0.907
Label non- 677 97 ~—«TASE 0.620 0.790 0.695
T4SE
Label T4SE 42 158 Weighted 0.876 0.858 0.863
average
SVM 25 Classifedas Non-I4SE T4SE Non-T4SE 0.925 0.924 0.924
Label non- 715 59 T4SE 0.706 0.710 0.708
T4SE
Label T4SE 58 142 Weighted 0.880 0.880 0.880

average

 
Zhang et al. BMC Bioinformatics (2020) 21:480 Page 8 of 15

 

 

 

(— >)
Roc Curve Show
Vv
©
[oa
Vv
2
3
°o
a
vu
2
ke
ROC Curve (AUC=0.9298)
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
a Results of all 400 components.
Roc Curve Show
Vv
6
a
Vv
2
=
°o
a
Vv
2
ke
ROC Curve (AUC=0.9287)
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
b Results of the selected 25 variables.
Fig. 4 Comparison results of ROC and AUC between all 400 components and the selected 25 variables using
ensemble classification

 

 

classifier with automatic assignment of a base classifier on PSSM (i.e., 400 components),

the selected 25 variables can also achieve comparable results.

Discussions
Experimental results have indicated the effectiveness of variable selection from the
encoded feature PSSM. In this section, we will further discuss the special composition of

our variable selection method and the classification results.

Purpose of using base classifier selection

The automatic assignment of a base classifier is creative in this paper. Giving consid-
eration to the sample distribution of resampling, we designed the strategy of automatic
assignment. Due to the limited sample size, resampling is only an approximation to the
population. In our previous work, it has been pointed out that different base classifi-
ers should be considered according to various sample distributions [52]. However, the
base classifier was interactively appointed in [52]. The pan chart in Fig. 2b can also show

this phenomenon, which indicates that base classifiers selected at a higher percentage
Zhang et al. BMC Bioinformatics

(2020) 21:480

 

 

NX

Roc Curve Show

True Positive Rate

ROC Curve (AUC=0.9140)

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

a Results using DTC.

Roc Curve Show

True Positive Rate

~~~ ROC Curve (AUC=0.9211)

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

C Results using kNN.

Roc Curve Show

True Positive Rate

ROC Curve (AUC=0.8921)

0.0 0.2 0.4 0.6
False Positive Rate

0.8 1.0

e€ Results using LR.

Roc Curve Show

True Positive Rate

~~ ROC Curve (AUC=0.8800)

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

g Results using NB.

 

 

 

 

True Positive Rate

True Positive Rate

True Positive Rate

True Positive Rate

Roc Curve Show

ROC Curve (AUC=0.9264)

 

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

b Results using GBM.

Roc Curve Show

~~ ROC Curve (AUC=0.9113)

 

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

d Results using LDA.

Roc Curve Show

1.0

0.8

<4
a

S
b

0.2

0.0 ROC Curve (AUC=0.9244)

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

f Results using MLP.

Roc Curve Show

~~ ROC Curve (AUC=0.8796)

 

0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate

h Results using SVM.

Fig. 5 Results of ROC and AUC using different base classifiers with the selected 25 variables

 

 

Page 9 of 15
Zhang et al. BMC Bioinformatics (2020) 21:480 Page 10 of 15

may fit the population better. Besides, quantitative results listed in Table 2 indicate the
power of using automatic assignment compared with the interactive appointment of a

base classifier.

Purpose of using a line chart for variable selection

In fact, a line chart shown in Fig. 3 is a semi-automatic way for variable selection. Here,
it goes against the interactive way that uses a manual selection within a table or on a 2-D
scatter plot [52]. Also, it abandons the automatic way of automatic clustering [53] on
accumulated scores. Due to the limited distribution of the accumulated scores with rela-
tively low values, the variables have no apparent distinction. It probably means variables
are highly correlated. In that case, variable orders instead of values are to be considered.

Comparisons between classification results

We compared the classification results of our method with PredT4SE-Stack [6]. As
shown in Table 1, the max, mean and min values of Accs and AUCs on the testing set
exhibit a better result than the classification results on its training set using PreT4SE-
Stack. The best classification Acc value using PredT4SE-Stack on its training set is 0.911.
Most of the other meta-classifier got a lower Acc value and AUC value than the mean
Acc and AUC obtained using our method. However, classification results of Pred4SE-
Stack on its testing set are better than those using our method. In fact, the classification
results on its testing set are even better than those on its training set. By careful obser-
vation, it is found that parameters of base classifiers are manually set every time in [6].
That’s why PredT4SE-Stack gets better classification results on its testing set. Anyway,
seeking better classification results by setting parameter values doesn’t make any sense
for variable selection.

Conclusion

In order to solve the problem of variable selection from an encoded feature representing
protein sequences, we propose a variable selection method based on ensemble classi-
fication with an automatic assignment of base classifiers. Variable ordering is obtained
according to score accumulation on training samples. Then, ensemble classifiers are
established from one to all dimensions of the encoded feature according to variables
incrementally added in the descending order. Using the ensemble classifiers on testing
samples, a line chart is drawn for variable selection. Ultimately, evaluation metrics are
made to estimate the effectiveness of selected variables. Taking a dataset containing pro-
tein sequences categorized into T4SE and non-T4SE group as a case, the performance of
the proposed method is evaluated.

Methods
In this section, we will expound our method in detail. As illustrated in Fig. 1, our method
has seven steps, each of which is framed in a dashed box. Each step which keeps its name

labeled within the dashed box, corresponds to a following subsection.
Zhang et al. BMC Bioinformatics — (2020) 21:480 Page 11 of 15

Sample division

We make a balanced sample division at the first step. That is, samples derived from a cat-
egory are divided into two equivalent parts. As a result, half of the samples from differ-
ent categories form a training group; while, the left ones constitute a testing group. It is

noteworthy that this sample division should be performed in a completely random way.

Base classifier selection

A base classifier is automatically assigned at the second step. We make a set of base
classifiers including decision tree classifier (DTC), gradient boosting machine (GBM),
k-nearest-neighbor (kNN), linear discriminant analysis (LDA), logistic regression (LR),
multi-layer perceptron (MLP), naive bayesian (NB) and support vector machine (SVM).
Each one is equally assigned in an automatic way corresponding to every round of res-
ampling and training module. In each round j, 70% of training samples are randomly
selected in a full dimension of an encoded feature for training each base classifier. The
remaining 30% of training samples are regarded as the out-of-bag samples for calcula-
tion of the classification error rate Err;, as is expressed in Eq. (1). The base classifier with

the lowest classification error rate is automatically assigned in round j.

Score accumulation

Score accumulation is made at the third step. Once a base classifier is automatically
assigned according to the classification error rate calculated on the out-of-bag samples
in round j, permutations are to be made. As to each variable i of the encoded feature,
only one-time permutation of the expression values from the out-of-bag samples is per-
formed. The corresponding classification error rate is denoted as Err? (i). Accordingly, a
score representing the importance of variable i is expressed as score;(i) = Err? (i) — Err;.
After r rounds of resampling, training and scoring, the accumulated score of variable i is

expressed as irl score;(i)/r.

Variable ordering

Variables are reordered at the fourth step. A 2-D scatter plot is to be made with its
horizontal and vertical coordinates corresponding to the variable indices and the accu-
mulated scores, respectively. Besides, variables are to be sorted in a descending order
according to the accumulated scores. If the accumulated scores of the variables have no
distinction (i.e., the accumulated scores are all relatively low), all the variables rather
than the significant variables selected using previously proposed clustering method [53]

are to be enumerated at the following step.

Ensemble classification

Ensemble classifiers are established at the fifth step. Again, r rounds of resampling and
training are performed to achieve ensemble classifiers in each dimension according
to variables incrementally added in their descending order. As to 1-D space, the vari-
able with the highest accumulated score is considered. At each round of resampling,
the base classifier with the lowest classification error rate is trained. Altogether, r base
Zhang et al. BMC Bioinformatics — (2020) 21:480 Page 12 of 15

classifiers are selected as the ensemble classifier in 1-D space. This procedure is repeated
with a variable keeping a lower accumulated score added, until the full dimension of the
encoded feature or the full dimension of significant variables has been considered.

Variable selection

Variable selection is accomplished at the sixth step. In each dimension, the established
ensemble classifier is applied to the testing samples. The accuracy (Acc) expressed in
Eq. (2) and the area under curve (AUC) of the receiver operating characteristic (ROC)
are calculated. Accordingly, a line chart is obtained with its horizontal and vertical
coordinates corresponding to the variable indices in their descending order and the
corresponding Accs and AUCs in different dimensions. A dimension threshold can be
made when Accs and AUCs are keeping almost the same with dimension incrementally
increasing. Thus, the variables that really help to recognize proteins with specific func-
tions are selected from the encoded feature.

Measure
Evaluation metrics are made to estimate the effectiveness of selected variables at the sev-

enth step. The classification error rate is expressed as follows,

FN + FP

Err = To
IP+FN + IN + FP

(1)
where TP, TN, FP and FN represent the number of true positive, true negative, false
positive and false negative, respectively. On the contrary, Acc is shown as follows,
A IN + TP (2)
cc = ——____—_.,
TP + FN + TN + FP
Besides, we choose four widely used quantitative measurements. The confusion matrix

illustrates TP, TN, FP and FN together. Besides, Precision and Recall are computed as

follows,
Precisi TP 3
ecision TP + EP (3)
Recall = ” 4
CO" TP EN (4)

In addition, Fl — measure is a harmonic average of Precision and Recall, which is
expressed as
2 * Precision * Recall

F1 — measure = . 5
Precision + Recall (5)

 

Moreover, the ROC and AUC are also provided here as qualitative measurements.

Abbreviations

Acc:: Accuracy;; AUC: Area under curve;; DTC:: Decision tree classifier;; GBM:: Gradient boosting machine; kNN:: k-nearest-
neighbor;; LDA:: Linear discriminant analysis;; LR:: Logistic regression;; MLP:: Multi-layer perceptron;; NB:: Naive bayesian;;
PseAAC:: Pseudo-amino acid composition; PSI-BLAST:: Position-specific iterated blast;; PSSM:: Position-specific scoring
Zhang et al. BMC Bioinformatics (2020) 21:480 Page 13 of 15

matrix;; RF:: Random forest; ROC:: Receiver operating characteristic;, SVM:: Support vector machine;, T4SE:: Type IV
secreted effectors.

Acknowledgements
This work is derived from Scientific Research Project Supported by Enterprise Suzhou Dachen Medical Technology Co.,
Ltd,

Authors’ contributions

X.D.Z conceived the general research and supervised it. J.Z performed the research and were the principal develop-
ers. D.L.L and L.X.L analyzed the data. D.N.K, M.A.A.A and X.D.Z wrote and revised the manuscript. All authors read and
approved the final manuscript.

Funding

This work has been supported by the financial support of This work has been supported by the financial support of
Natural Science Foundation of Heilongjiang Province (No. LH2020F002). The funding body of Fundamental Research
Funds for Natural Science Foundation of Heilongjiang Province played an important role in the design of the study, col-
lection, analysis and interpretation of data and in writing the manuscript.

Availability of data and materials
The public dataset analysed during the current study is available in reference [51], and can be downloaded from the
website https://github.com/LoopGan/Effective-prediction-of-bacterial-type-lV-secreted-effectors.

Ethics approval and consent to participate
Not applicable.

Consent for publication
Not applicable.

Competing interests
The authors declare that they have no known competing financial interests or personal relationships that could have
appeared to influence the work reported in this paper.

Author details

' College of Artificial Intelligence, Wuxi Vocational College of Science and Technology, No. 8 Xinxi Road, Wuxi 214028,
China. * College of Information and Computer Engineering, Northeast Forestry University, No. 26 Hexing Road, Har-
bin 150040, China.

Received: 21 July 2020 Accepted: 19 October 2020
Published online: 27 October 2020

References

1. Lv ZB, Jin SS, Ding H, Zou Q. A random forest sub-Golgi protein classifier optimized via dipeptide and amino
acid composition features. Fronti Bioeng Biotechnol. 2019;7:215.

2. Zhu XJ, Feng CO, Lai HY, Chen W, Lin H. Predicting protein structural classes for low-similarity sequences by
evaluating different features. Knowl-Based Syst. 2019;163:787-93.

3. Ru XQ,LiLH, Zou Q. Incorporating distance-based top-n-gram and random forest to identify electron transport
proteins. J Proteome Res. 2019;18:2931-9.

4. Li YJ, Niu MT, Zou Q. ELM-MHC: an improved MHC identification method with extreme learning machine algo-
rithm. J Proteome Res. 2019;18:1392-401.

5. QukK,WeiL, Yu J,Wang C. Identifying plant pentatricopeptide repeat coding gene/protein using mixed feature
extraction methods. Front Plant Sci. 2019;9:1-10.

6. Xiong Y,Wang OK, Yang JC, Zhu XL, Wei DQ. PredT4SE-Stack: prediction of bacterial type IV secreted effectors
from protein sequences using a stacked ensemble method. Front Microbiol. 2018;9:2571.

7. ZOULY, Nan CH, Hu FQ. Accurate prediction of bacterial type IV secreted effectors using amino acid composition
and PSSM profiles. Bioinformatics. 2013;29(24):3135-42.

8. Ashari ZE, Dasgupta N, Brayton KA, Broschat SL. An optimal set of features for predicting type IV secretion
system effector proteins for a subset of species based on a multi-level feature selection approach. PLoS ONE.
2018;13:e0197041.

9. YuLZ, Guo YZ, LiYZ, Li GB, Li ML, Luo JS, Xiong WJ, Qin WL. SecretP: identifying bacterial secreted proteins by
fusing new features into Chou’s pseudo-amino acid composition. J Theor Biol. 2010;267:1-6.

10. Feng PM, Chen W, Lin H, Chou KC. iHSP-PseRAAAC: identifying the heat shock protein families using pseudo
reduced amino acid alphabet composition. Anal Biochem. 2013;442(1):118-25.

11. Mirza MT, Khan A, Tahir M, Lee YS. MitProt-Pred: predicting mitochondrial proteins of Plasmodium falci-
parum parasite using diverse physiochemical properties and ensemble classification. Comput Biol Med.
2013;43(10):1502-11.

12. Ahmad J, Hayat M. MFSC: multi-voting based feature selection for classification of Golgi proteins by adopting
the general form of Chou’s PseAAC components. J Theor Biol. 2019;463:99-109.

13. Zhang SL, Duan X. Prediction of protein subcellular localization with oversampling approach and Chou's general
PseAAC. J Theor Biol. 2018;437:239-50.
Zhang et al. BMC Bioinformatics (2020) 21:480 Page 14 of 15

20.

21.

22.

23.

24,

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.
39.

 

 

Srivastava A, Kumar R, Kumar M. BlaPred: predicting and classifying beta-lactamase using a 3-tier prediction
system via Chou's general PseAAC. J Theor Biol. 2018;457:29-36.

. Sankari ES, Manimegalai D. Predicting membrane protein types by incorporating a novel feature set into Chou's

general PseAAC. J Theor Biol. 2018;455:319-28.

Sankari ES, Manimegalai D. Predicting membrane protein types using various decision tree classifiers based on
various modes of general PseAAC for imbalanced datasets. J Theor Biol. 2017;435:208-17.

Liang YY, Zhang SL. Predict protein structural class by incorporating two different modes of evolutionary infor-
mation into Chou's general pseudo amino acid composition. J Mol Graph Model. 2017;78:110-7.

Meher PK, Sahu TK, Banchariya A, Rao AR. DIRProt: a computational approach for discriminating insecticide
resistant proteins from non-resistant proteins. BMC Bioinform. 2017;18:190.

Tiwari AK. Prediction of G-protein coupled receptors and their subfamilies by incorporating various sequence
features into Chou's general PseAAC. Comput Methods Programs Biomed. 2016;134:197-213.

Han GS, Yu ZG, Anh V. A two-stage SVM method to predict membrane protein types by incorporating amino
acid classifications and physicochemical properties into a general form of Chou’s PseAAC. J Theor Biol.
2014;344:31-9.

Chou K. Using amphiphilic pseudo amino acid composition to predict enzyme subfamily classes. Bioinformatics.
2005;21:10-9.

Chou K. Prediction of protein cellular attrbutes using pseudo-amino acid composition. Proteins. 2001;43:246-55.
Wang JW, Yang BJ, Revote J, Leier A, Marquez-Lago TT, Webb G, Song JN, Chou KC, Lithgow T. POSSUM: a bioin-
formatics toolkit for generating numerical sequence feature descriptors based on PSSM profiles. Bioinformatics.
2017;33(17):2756-8.

Zhang LC, Zhao XQ, Kong L. Predict protein structural class for low-similarity sequences by evolution-

ary difference information into the general form of Chou’s pseudo amino acid composition. J Theor Biol.
2014;355:105-10.

Paliwal KK, Sharma A, Lyons J, Dehzangi A. A tri-gram based feature extraction technique using linear probabili-
ties of position specific scoring matrix for protein fold recognition. IEEE Trans Nanobiosci. 2014;13(1):44—-50.
Zahiri J, Yaghoubi O, Mohammad-Noori M, Ebrahimpour R, Masoudi-Nejad A. PPlevo: protein-protein interac-
tion prediction from PSSM based evolutionary information. Genomics. 2013;102(4):237-42.

Zhang SL, Ye F, Yuan XG. Using principal component analysis and support vector machine to predict protein
structural class for low-similarity sequences via PSSM. J Biomol Struct Dyn. 2012;29(6):634-42.

Jeong JC, Lin XT, Chen XW. On position-specific scoring matrix for protein function prediction. IEEE-ACM Trans
Comput Biol Bioinform. 201 1;8(2):308-15.

Jia CZ, Liu T, Chang AK, Zhai YY. Prediction of mitochondrial proteins of malaria parasite using bi-profile Bayes
feature extraction. Biochimie. 201 1;93(4):778-82.

Dong QW, Zhou SG, Guan JH. A new taxonomy-based protein fold recognition approach based on autocross-
covariance transformation. Bioinformatics. 2009;25(20):2655-62.

Cheng CW, Su ECY, Hwang JK, Sung TY, Hsu WL. Predicting RNA-binding sites of proteins using support vector
machines and evolutionary information. BMC Bioinform. 2008;9(S12):S6.

Chou KC, Shen HB. MemType-2L: a web server for predicting membrane proteins and their types by incorporat-
ing evolution information through Pse-PSSM. Biochem Biophys Res Commun. 2007;360(2):339-45.

An JY, You ZH, Chen X, Huang DS, Li ZW, Liu G, Wang Y. Identification of self-interacting proteins by exploring
evolutionary information embedded in PSI-BLAST-constructed position specific scoring matrix. Oncotarget.
2016;7(50):82440-9.

Qin YF, Zheng XQ, Wang J, Chen M, Zhou CJ. Prediction of protein structural class based on Linear Predictive
Coding of PSI-BLAST profiles. Open Life Sciences. 2015;10(1):529-36.

Ding SY, Li Y, Shi ZX, Yan SJ. A protein structural classes prediction method based on predicted secondary struc-
ture and PSI-BLAST profile. Biochimie. 2014;97:60-5.

Liu T, Zheng XQ, Wang J. Prediction of protein structural class for low-similarity sequences using support vector
machine and PSI-BLAST profile. Biochimie. 2010;92(10):1330-4.

Kaur H, Raghava GPS. Prediction of alpha-turns in proteins using PSI-BLAST profiles and secondary structure
information. Proteins-Struct Funct Bioinform. 2004;55(1):83-90.

Breiman L. Random forests. Mach Learn. 2001;45:5-32.

Tan CG, Wang T, Yang WY, Deng L. PredPSD: a gradient tree boosting approach for single-stranded and double-
stranded DNA binding protein prediction. Molecules. 2020;25(1):98.

Friedman JH. Greedy function approximation: a gradient boosting machine. Ann Stat. 2001;29(5):1 189-232.

He ZY, Liu H, Moch H, Simon H. Machine learning with autophagy-related proteins for discriminating renal cell
carcinoma subtypes. Sci Rep. 2020;10(1):720.

Cover T, Hart P. Nearest neighbor pattern classification. IEEE Trans Inf Theory. 1967;13(1):21-7.

lsopescu RD, Spulber R, Josceanu AM, Mihaiescu DE, Popa O. Romanian bee pollen classification and property
modelling. J Apicult Res. 2020.

Belhumeur PN, Hespanha JP, Kriegman DJ. Eigenfaces vs. fisherfaces: recognition using class specific linear
projection. IEEE Trans Pattern Anal Mach Intell. 1997;19(7):711-20.

. Wachters JE, Kop E, Slagter-Menkema L, Mastik M, van der Wal JE, van der Vegt B. de Bock GH, van der Laan

BFAM, Schuuring E. Distinct biomarker profiles and clinical characteristics in T1-T2 glottic and supraglottic
carcinomas. The Laryngoscope 2020.

Zhou Y, Li GQ, Li HQ. Automatic cataract classification using deep neural network with discrete state transition.
IEEE Trans Med Imaging. 2020;39(2):436-46.

Pal SK, Mitra S. Multilayer perceptron, fuzzy sets, and classification. IEEE Trans Neural Netw. 1992;3(5):683-97.
Domingos P, Pazzani M. On the optimality of the simple Bayesian classifier under zero-one loss. Mach Learn.
1997;29(2-3):103-30.
Zhang et al. BMC Bioinformatics (2020) 21:480 Page 15 of 15

49.

50.
51.

52.

53.

Meng CL, Jin SS, Wang L, Guo F, Zou Q. AOPs-SVM: a sequence-based classifier of antioxidant proteins using a
support vector machine. Front Bioeng Biotechnol 2019.

Cortes C, Vapnik VN. Support vector networks. Mach Learn. 1995;20(3):273-97.

Wang Y, Guo Y, Pu X, Li M. Effective prediction of bacterial type IV secreted effectors by combined features of
both C-termini and N-termini. J Comput Aided Mol Des. 2017;31:1029-38.

Zhao XD, Jiao Q, Li HY, Wu YM, Wang HX, Huang S, Wang GH. ECFS-DEA: an ensemble classifier-based feature selec-
tion for differential expression analysis on expression profiles. BMC Bioinform. 2020;21:43.

Liu T, Li HY, Zhao XD. Clustering by search in descending order and automatic find of density peaks. IEEE Access.
2019;7:133772-80.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Ready to submit your research? Choose BMC and benefit from:

e fast, convenient online submission

thorough peer review by experienced researchers in your field

rapid publication on acceptance

support for research data, including large and complex data types

gold Open Access which fosters wider collaboration and increased citations

® maximum visibility for your research: over 100M website views per year

At BMC, research is always in progress.

Learn more biomedcentral.com/submissions > BMC

 

 

 
