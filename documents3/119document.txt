Zhang et al. EURASIP Journal on Wireless Communications and Networking
(2020) 2020:20
https://doi.org/10.1186/s13638-019-1630-y

EURASIP Journal on Wireless
Communications and Networking

RESEARCH Oy else =e

Background perception for correlation filter
tracker

Yushan Zhang|, Jianan Li’, Fan Wu', Lingyue Wu! and Tingfa Xu'? @

Check for
updates

 

Abstract

Visual object tracking is one of the most fundamental tasks in the field of computer vision, and it has numerous
applications in many realms such as public surveillance, human-computer interaction, robotics, etc. Recently,
discriminative correlation filter (DCF)-based trackers have achieved promising results in short-term tracking problems.
Most of them focus on extracting reliable features from the foreground of input images to construct a robust and
informative description of the target. However, it is often ignored that the image background which contains the
surrounding context of the target is often similar across consecutive frames and thus can be beneficial to locating the

target. In this paper, we propose a background perception regulation term to additionally exploit useful background
information of the target. Specifically, invalid description of the target can be avoided when either background or
foreground information becomes unreliable by assigning similar importance to both of them. Moreover, a novel model
update strategy is further proposed. Instead of updating the model by frame, we introduce an output evaluation score,
which serves to supervise the tracking process and select high-confidence results for model update, thus paving a new
way to avoid model corruption. Extensive experiments on OTB-100 dataset well demonstrate the effectiveness of the

proposed method BPCF, which gets an AUC score of 0.689 and outperforms most of the state-of-the-art.

Keywords: Correlation filter, Background perception, Model update, Visual tracking

1 Introduction

Discriminative correlation filter (DCF) trackers [1-17] have
shown remarkable progress in recent years. The first CF-
based method is Minimum Output Sum of Squared Error
(MOSSE) [10], which receives a speed of more than 600
frames per second (FPS). After that, many improvements
have been made to escalate its performance. The circulant
structure of sequences is exploited to augment training
samples [1]. Kernelized correlation filter is proposed to get
a multi-channel extension of linear correlation filters [9].
To integrate multi-resolution feature maps, continuous
convolution operators for visual tracking are also proposed
[4] and utilized by many state-of-the-art trackers, such as
ECO [3] and CFWCR [12], among which CFWCR exploits
the great power of deep convolutional neural networks
(CNN) features without using any hand-crafted features
such as HOG [18] or color names [19], and achieves great
performance in both accuracy and robustness. Afterwards,

 

* Correspondence: ciom_xtf@bit.edu.cn

"School of Optics and Photonics, Image Engineering & Video Technology
Lab, Beijing Institute of Technology, Beijing 100081, China

Key Laboratory of Photoelectronic Imaging Technology and System,
Ministry of Education of China, Beijing 100081, China

D) Springer Open

 

there are also trackers focusing on foreground feature selec-
tion [14] and reliability learning [20].

However, most of these methods only focus on fore-
ground information, while they do not take good advan-
tage of the background information which is also
beneficial for tracking. Moreover, most trackers update
the model after each frame, or after every N frames by
using a sparse update scheme to avoid the tracker being
dominated by recent samples. Nevertheless, such
trackers still suffer from model corruption since they up-
date the model indiscriminately regardless of whether
the tracking result is accurate or not.

In this paper, aiming at the above issues, we propose a
novel tracker, background perception correlation filter
tracker (BPCF), based on an improved version of ECO [3],
Correlation Filters with Weighted Convolution Responses
(CFWCR) [12], which achieves remarkable results on
VOT challenges [21]. In order to better exploit and make
full use of the background information of the input, we
propose a background perception regulation term. Par-
ticularly, we first divide the search area of the input im-
ages into several small pieces. By introducing a regulation

© The Author(s). 2020 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0
International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to

the Creative Commons license, and indicate if changes were made.
Zhang et al. EURASIP Journal on Wireless Communications and Networking

term that minimizes the sum of the L2 distances between
the convolutional outputs of all possible pairs of the
pieces, we assign similar importance to all the different
small pieces regardless of whether the region belongs to
the background or the foreground part of the input sam-
ples. In addition, as for the problem of indiscriminate
model update, we introduce a novel model update strategy
by computing a confidence score for the tracking result
after every N frames, and only update the model when the
confidence score is higher than a preset threshold, ie., a
particular proportion of the average of all the previous
confidence scores. Figure 1 shows some qualitative results
of our proposed method BPCF compared to some state-
of-the-art on sample sequences of OTB-2015, from which
we can see that our method outperforms all the other
trackers. Moreover, quantitative results on OTB-2015
dataset show that our tracker BPCF has achieved state-of-
the-art performance.

To sum up, our work makes the following contributions:

We propose a new DCF-based tracking model which
integrates a background perception regulation term to
stress the equal contribution of the foreground and
background information and a novel model update strat-
egy to supervise the tracking results into a unified track-
ing framework.

A background perception regulation term is intro-
duced to the existing CFWCR tracker to exploit the
background information of the input samples and
emphasize equal contributions of the background and
foreground information to avoid the tracker being domi-
nated by unreliable parts.

CFVVCR

(2020) 2020:20 Page 2 of 12

A novel model update strategy is proposed to avoid model
corruption. Instead of updating the model by frame, we com-
pute a confidence score for the tracking result after every N
frames, and select high-confidence results for model update.

2 Related work

Correlation filter-based methods for visual object tracking
have shown dominant results in recent years. The first
CF-based tracker, MOSSE [10], which only uses grayscale
image and single-channel feature, could produce stable
tracking results when initialized by a single frame and
achieve the speed of 669 frames per second. After that,
CF-based trackers become increasingly popular and have
received impressive results on OTB-2015 object tracking
benchmarks [22]. Due to the short of training samples
when initializing the tracker, Henriques et al. [1] intro-
duced the circulant structure of input images to augment
training samples. Later, Henriques et al. [9] used kernel
regression, which has exactly the same complexity as its
linear counterpart, to combine different features and re-
ceived better results. Nevertheless, using the circulant
structure of image sequences could beget boundary effect.
To solve this problem, Danelljan et al. [8] introduced a
spatial regularization term. The proposed regularization
weights penalize the correlation filter coefficient by assign-
ing higher value at the edge of the filter and lower value at
the central part. Spatial-temporal regularization term is
also introduced by Danelljan et al. [10]. The new filter
could be solved efficiently via alternating direction method
of multipliers (ADMM) and provide a 5x speedup. Later,
Danelljan et al. [4] proposed continuous convolution

Fig. 1 Results on OTB2015. The comparison of BPCF and some state-of-the-art trackers on sample sequences

 
Zhang et al. EURASIP Journal on Wireless Communications and Networking

operators to enable the integration of multi-resolution
deep feature maps. Many subsequent trackers based on
this received good results. Danelljan et al. [3] revised C-
COT [4] by introducing a factorized convolution operator,
a compact generative model, which significantly reduced
the computational complexity. He et al. [12] exploited the
great power of deep CNN features without using any
hand-crafted features and got great performance both in
accuracy and robustness. Gundogdu et al. [14] put for-
ward the importance of feature selection. Sun et al. [20]
introduced a joint discrimination and reliability learning
method, which highlighted the importance of the fore-
ground and its different reliability.

Most of the previous methods only focus on the fore-
ground information and rely on hand-crafted features such
as HOG and CN. Differently, we only take deep features as
our input and propose a background perception regulation
term to ensure that the background and foreground of the
input samples have similar contributions during tracking.
By adding this regulation term, the tracker is unlikely to be
dominated by unreliable parts of the target object, which
could solve the problem of overfitting. Moreover, a novel
model update strategy is proposed. Instead of updating the
model regardless of whether the tracking result is precise
or not, we update only when the tracking result is reliable
and stop updating when the tracking result is incorrect or
the tracking target is undergoing severe occlusion

3 Methods

The approach is actually twofold: firstly, we found that the
background information in a given image sequence is always
similar in consecutive frames, which could help to recognize
a given target efficiently. Thus, we introduce a background
perception regulation term, which could help us additionally
exploit the background information and learn a more robust
correlation filter. Secondly, most existing trackers update the
tracker indiscriminately regardless of whether the tracking
result is precise or not. The problem of such update strategy
is that when the target is experiencing severe occlusion or
the tracking result is imprecise, it will cause the tracker to
corrupt. To solve this problem, a self-adaptive model update
strategy is proposed. We introduce an output evaluation
score, where the score is lower while the object is being oc-
cluded or the tracking result is incorrect. We then can select
those reliable samples to update the model.

3.1 Base framework

Our framework, like many other DCF trackers, is based on
C-COT, a theoretical framework for learning continuous
convolution operators. This kind of tracker adopted an im-
plicit interpolation model for the training samples. Assum-

xP

jr ve Hy
N, as the number of spatial samples in «4 4, where dé {0, 1,

ing each sample x; contains D feature channelsx‘ j x7

(2020) 2020:20 Page 3 of 12

2,...,D}. Unlike the traditional DCF trackers, where each
feature channel x4 eERN« consists of Nz discrete spatial vari-

ables, it transfers the feature maps into continuous spatial
domain by introducing an interpolation kernel bz to get an
interpolation operator/,:

t) = SS? Unb, (- 0), (1)

n=0 d

Jaf x" \(

where the interpolation kernel bz has a period of T (T'> 0).
Thus, the result N,{x% is a continuous feature map with a
period of T to be used for further computation. CFWCR
framework negates hand-crafted features such as HOG [18]
and CN [19], and adopts CNN features to extract feature
maps. Specifically, they use VGG-M [23] network pre-
trained on ILSVRC [24] dataset to extract multi-resolution
continuous feature maps, and employ the first and the fifth
convolutional layer as two deep feature channels. The filter
fis trained by minimizing the following function:

    

“lle

 

 

 

 

B= Say Ws Se Petal pew Yer “Jo, x} by,
J= a—

(2)

where a; represents the importance of each training sample,

and w is the spatial regulation term to avoid boundary effect.
The convolution responses of the two channels are

weighted summed to get a final confidence response:

Deonvi

Sr(x) = Wi dS «J a{x*} + Wo Yer «Jy {x’},

where the feature maps extracted from the first and fifth con-
volutional layers are first interpolated using Eq.1 and then
convoluted with filter f“ and /’ trained by Eq.2. The assigned
weights W, and W, denote the significance of each layer.

3.2 Background perception

We propose a_ background perception regulation
termR(h, X), by which we regulate the filter to assign
larger importance to the region where the extracted
feature map X has a smaller value. By this means, we
can assign similar importance to different regions of
the training samples and avoid the tracker being
dominated by unreliable parts. The regulation term
can be formulated as:

10 Z3 (Kecnatee Leen")

1 mn \d=1 d=1
(4)

where x, g¢ R**' is the kth cyclically shift of the input
vector x7€ R“*" for the dth channel. P¥ = diag(p’'(1),
.., py (K))eR*** is the mth binary mask (Fig. 2) which
Zhang et al. EURASIP Journal on Wireless Communications and Networking

crops the samples to the mth sub-region. hye R**" is
the target filter of dth channel. To simplify Eq.4, the for-
mula is rewritten as follows:

2

M D D M
R(A,X) = S>S_ xb Pha S~XTP hal) = S~\|XTP™h-X™P"h||,,
mn |\|d=1 d=1 2 mn

 

 

 

 

(5)

where P” = diag(P'", ..., Pm)ERPK*P® is a block diag-
onal matrix where P’’ is the dth diagonal block. X= |
1 @%X, gentx gle RS** denotes all the cyclical
shift of the input vector xg. X = IX? x2, ...,X7]"e
RPKxK® is a matrix that fits all the circulant matrices
of different channels together. he R?“*!' is the ultim-
ate filter.

By introducing this background perception term, the
filters are learned by minimizing the following objective:

 

 

 

 

 

 

 

 

D M || D D
E(h) = |ly- S$ Xihall +0 >_ S| X7PTha- YS) Xp Pha
d=1 mn \|d=1 d=1 2
D
+) |lohall’,
d=1

(6)

M
E(h) = ||y-XThl|” + S> ||X7P™h-X7P"A||, + || Wall?,

m,n

(7)
where y is the predefined Gaussian window objective
P” = diag(PY’,...,P5)€
is a block diagonal matrix where P/ is the dth

function. The binary mask
DK xDK

diagonal block. Input sample X = [X/],X3, Xe
RPK*® is a matrix that fits all the circulant matrices of
different channels together. And h¢ R°“*' is the ultim-

ate filter that we get.

(2020) 2020:20 Page 4 of 12

To get the optimal filter 4, we can solve the
minimization problem by using conjugate gradient des-
cent method. We first compute the derivative of Eq.7,
and then set it to zero to get the following equation:

Ah = Xy, (8)
where A is defined as:

M
A= XX" +245 — M(P")'XX"(P")-2y
m=1

+Ww'w,

(9)

M
dP"
m=1

M T
sm Xx?
m=1

 

 

 

To solve the normal equation by conjugate gradient

descent method, we employ the following iterative
procedure:
—_ rir
‘PT AP,’
hiss = hj + aPi, (10)

rig. = 1; + ajAPi,

T
reali
_ it1'irl
Pisd =Viga + (“a p,

ji /i

where we set /ip = 0, 9 = XY, Po = 7. And the number of
conjugate gradient descent iterations is set to 5.

3.3 Target localization

In the target localization step in Nth frame, we first ex-
tract the feature map x of the search region. x“denotes
the feature map of the dth channel. The convolutional
outputs of the two channels are computed by convolu-
tion operation and then weighted summed up to get a
final confidence response S;(x). The convolution process
is computed in Fourier domain to reduce the computing
burden.

 

Da

Fig. 2 Binary masks. When considering two-dimensional case and supposem = 9, the binary masks from p4,p3, ....93 are shown in the figure
where the gray pixels are assigned the value of 1 and the white pixels are assigned the value of 0

 
Zhang et al. EURASIP Journal on Wireless Communications and Networking

Sn (x) = S- ag (F(h“) OF («*)), (11)
d=1

where d=2 and the two feature channels are the first
and the fifth convolutional layer of VGG-16 net pre-
trained on ILSVRC dataset. And aq is the weight of each
layer which denotes the importance of each feature
channel.

3.4 Model update strategy

Most of the existing state-of-the-art DCF-based
trackers update the tracking model after each frame
[8, 10]. These methods suffer from model corruption
since they update the model indiscriminately no mat-
ter the tracking result is accurate or not. Moreover,
the model is easy to be dominated by recent frames,
in which case, if the tracking result of the recent
frames is imprecise, the tracking mission is prone to
be failed. Thus, when the target object is experiencing
severe background cluster, deformation, and occlu-
sion, the model will become highly unreliable. Other
trackers such as ECO [3] and CFWCR [12] adopt the
sparser updating scheme, where the filter is only up-
dated by starting the optimization process at every N
frames. Specifically, the traditional method of model
update usually sets N=1, and the sparser updating
method mostly sets N=5. By applying this scheme,
the tracker could avoid being dominated by recent
samples. Nevertheless, the problem of indiscriminate
model updating still exists.

Our proposed model update strategy also adopts the
sparse updating scheme, but unlike the traditional
methods where the input samples extracted from each
frame are used to update the model, we compute a con-
fidence score for the tracking result and discard those
with the confidence less than a threshold, i.e., a certain
proportion of the average of all the previous scores. The
confidence score for the tracking result is defined as:

Fmax ~ Frmin

Score = (12)

(Sony (Fxy~ Fave) ’) I/, ;

where Fyax and Fyi, denote the maximum and mini-
mum value of the confidence response S;,(x). F,,  de-
notes the xth row and the yth column of S;,(x). In this
equation, we can see that the confidence score is in dir-
ect proportion to the peak value of the response and in
inverse proportion to the standard deviation of the
response.

Figure 3 illustrates the significance of the proposed
method, from which we can see that when the target ob-
ject is experiencing severe occlusion, the confidence
score will reduce drastically due to the reduction of the

(2020) 2020:20 Page 5 of 12

peak value of the confidence response S;,(x) and the in-
crease of the standard deviation of the response. When
the output localization of the object is inaccurate, the
fluctuation of the confidence response is far more in-
tense and the confidence score reduces significantly.
Moreover, the confidence response of different se-
quences can vary a lot while all of their tracking results
are good. Thus, we cannot choose an invariable thresh-
old to decide whether the result is reliable or not. We
use the historical average values of the computed score
as a testing criterion. If the computed score of the
current frame is greater than its respective historical
average values with a certain ratio, which is set to 0.6 in
our proposed method, the output is considered reliable.
In sum, by applying our novel model update strategy, we
could discard those unreliable training samples during
the process of tracking and only preserve the reliable
ones for model update.

3.5 Tracking with background perception and self-
adaptive template update

The algorithm flow of our proposed method is described
in Table 1. For the target localization step, we compute
the confidence response S;(x) with Eq. (11) and then
optimize S;,(x) by the standard Newton’s method. The
optimization process will be completed in 5 iterations.
As for the update step, we first calculate the confidence
score S of the tracking result for the current frame and
stop model update while the result has low confidence.
The filter 1 is optimized with Eq. (8) by conjugate gradi-
ent descent method.

4 Results and discussion

In this section, we conduct comprehensive experiments
to evaluate our proposed method BPCF and compare it
with state-of-the-art DCF-based trackers. We first
present the implementation details in Section 4.1. Then
we compare our tracker with its baseline CFWCR [12]
tracker and some other state-of-the-art trackers on
OTB-2015 dataset [22] both quantitatively and qualita-
tively in Section 4.2 and Section 4.3.

4.1 Implementation details

Our proposed method is implemented in MATLAB
using Matconvnet tools, and the basic settings are the
same as CFWCR. The features are extracted from the
convl layer and the conv5 layer. The relative weight o =
W,/W, is set to 2. The search image is 4 times the size
of the target object. The maximum number of stored
training samples is set to 50, and the learning rate is
0.012 with a raining gap 5. The parameter of the pro-
posed regulation term is set to 1.5.
Zhang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:20 Page 6 of 12

 

Score=0.4027 | FEM) | = Score=0.2708

(a) Occlusion

Score=0.3478 4 Score=0.1967

(b) Imprecision

Fig. 3 Confidences scores. a shows the change of the confidence score when the target object is experiencing severe occlusion. b shows the
change of the confidence score when the localization is imprecise

 

 

Table 1 The algorithm flow

Inputs:

The target initial state (position, size) and template x ;

Initialize the binary mask p" according to Figure 2;

ideal correlation response y ;

outputs:

Optimized filter A ;

Estimated localization L' of the target in the next frame.

1) Calculate h, by the target initial state and the template x in the first frame with Equation
(8).

2) for every coming frame do

3) Update A, with h,_,.

4) Extract the feature map x and the Gaussian label function y from the candidate image
patch.

5) Calculate the response S), (x ) with Equation (11).

6) Acquire the position of the target object by finding the maximum value of the response
S, (x).

7) Calculate the confidence score S' of the tracking result with Equation (12).

8) Update the model X in the lightof S.

9) Optimize the filter 4 with Equation (8) by conjugate gradient descent method.

10) end for

11) return {h, L'*
Zhang et al. EURASIP Journal on Wireless Communications and Networking

(2020) 2020:20

Page 7 of 12

 

Success rate

Success rate

Success rate

Success rate

 

 

 

 

 

; Success plots of OPE
09 = = cFWoR 0540) |
~
08>
0.7 >
0.6 >
05>
04>
0.3 -
0.2 >
0.1 >
o!
0 0.1 0.2 03 0.4 0.5 06 07 08 09 1
Overlap threshold
Success plots of OPE - scale variation (42)
09 ~_ —cFWoR (0.665) |
08>
0.7 |
06>
0.5 >
04>
03>
0.2;
01>
% 04 02 03 04 05 06 07 08 09 1
Overlap threshold
Success plots of OPE - motion blur (16)
09 = Serene |
08)
07>
0.6 >
05>
04>
03>
0.2 |
01>
° 04 02 03 04 05 06 07 08 09 1
Overlap threshold
- Success plots of OPE - out of view (9)
= m= BPCF [0.582]
08 ~ _ === CFWCR [0.521]
0.7 >
0.6 >
0.5 +
04>
0.3 |
02>
0.1 |
° 0 04 02 03 04 05 06 07 08 09 1

Overlap threshold

Success rate

Success rate

Success rate

Success rate

Success plots of OPE - illumination variation (55)

mmm BPCF [0.691]

 

== BPCF [0.681]

0.9 | - m= CFWCR [0.654] 09 == CFWCR [0.634]
08 | 08
07) 07

L
06 | S 06

WO
0.5 + ® 05

O
04 + Ss 04}

”
03 + 03
02! 02
0.1) 0.1

0! 0!

 

0 0.1 0.2 03 04 05 0.6 07 08 09 1
Overlap threshold
Success plots of OPE - occlusion (65)

== BPCF [0.670]

 

 

Success plots of OPE - out-of-plane rotation (81)

   
      

 

0 0.1 02 03 04 05 06 07 0.8 0.9 1
Overlap threshold
Success plots of OPE - deformation (39)

m= BPCF [0.690]
=== CFWCR [0.647] | |

  

0.9 | _- == CFWCR [0.630] | | 08
08 | 08
0.7 | 0.7
2
06 | S 06
e
0.5 | ® 0.5 |
3
04 - Ss 04>
”
0.3 | 0.3
0.2 | 0.2
0.1 | 0.1
0 L 4 4 4 4 4 4 4 4 4 0 Ll 4 4 1 4 4 4 4 4 4
0 0.1 02 03 04 05 06 07 08 09 1 0 01 02 03 04 05 06 07 08 09 1
Overlap threshold Overlap threshold
Success plots of OPE - fast motion (39) Success plots of OPE - in-plane rotation (60)
r r ' r r r T r rt 7 1r t 7 t t t r t r T 4
lame umes BPCF [0.652] mums BPCF [0.647]
09 -=- == CFWCR [0.596] | | 0.9 == CFWCR [0.601] | |
08 | 08 ]
07> 0.7
2
06 | & o6
e
05> ® 05
3
04> Ss 04
”
0.3 | 03
0.2 | 0.2
0.1 | 0.1
0 cL 4 1 4 1 4 4 1 4 4 Oo u 4 4 4 4 4 4 4 1 4
0 0.1 02 03 04 05 06 07 08 09 1 01 02 03 04 05 06 07 08 09 1
Overlap threshold Overlap threshold
Success plots of OPE - background clutter (39) Success plots of OPE - low resolution (5)
1- T T T T T T T T T 7 T T T T T T T T T
= mmm BPCF [0.694] mmm BPCF [0.639]
09 =——=—_ == CFWCR [0.657] | | 0.9 == CFWCR [0.636] | |
08 | _ 08
07 | 07
L
06 & o6
e
05 - ® 05
3
04> 5 04 -
”
0.3 - 03
0.2 | 0.2
0.1 0.1
o- ot

 

01 02 03 04 05 O06 OF O08 0.9 1

Overlap threshold

 

01 02 03 04 05 06 07 08 09 1
Overlap threshold

Fig. 4 Baseline comparison. Success plot of OPE on OTB-2015 benchmark. The red line denotes BPCF with background perception regulation
term and the self-adaptive model update strategy jointly. The green line denotes CFWCR

Table 2 Baseline comparisons when using background
perception (BPCF-BP) and model update (BPCF-MU) separately
and jointly (BPCF). We report the AUC score on the OTB-2015
dataset

 

CFWCR
0.649

BPCF-BP
0.685

BPCF-MP
0.654

BPCF
0.689

Trackers
AUC

 

 

4.2 Quantitative evaluation

In this section, we mainly evaluated our algorithm on
OTB-2015 [22] benchmark. We tested our tracker on
all the 100 sequences with tracking difficulties includ-
ing illuminate variation, out-of-plane rotation, scale
variation, occlusion, deformation, motion blur, fast
motion, in-plane rotation, out of view, background
clutter, low resolution, etc. One commonly used
evaluation metric is the overlap score, which is
Zhang et al. EURASIP Journal on Wireless Communications and Networking

defined as R,1 R,/R,U Rg, where R, denotes the bounding
box of the tracking result and R, denotes the ground-truth
bounding box. Given a threshold between 0 and 1, we can
get an average success rate by comparing the overlap
score and the certain threshold. The main criteria of
OTB-2015 to determine whether the result is good or not
is the area under the curve (AUC) of each success plot,
which is the average of the success rates corresponding to
different overlap thresholds. In our experiment, we use
the area under the curve (AUC) to generate the success
plot of OPE.

(2020) 2020:20 Page 8 of 12

4.2.1 Baseline comparison
We first compare BPCF with the based work CFWCR.
Both are tested on OTB-2015 benchmark. We can see in
Fig. 4 that the performance of BPCF is escalated in all
the situations including illuminate variation, out-of-
plane rotation, scale variation, occlusion, deformation,
motion blur, fast motion, in-plane rotation, out of view,
background clutter, low resolution, etc.

As shown in Fig. 4, our tracker has a significantly bet-
ter performance compared with CFWCR. By implement-
ing the background perception regulation term and the

 

Success plots of OPE

m= BPCF [0.689]
== CCOT [0.671]

=== CFWCR [0.649]
mms ECO_HC [0.641]
== DeepSRDCF [0.634]

 

     

    
 
 
 
 
 
  
 
    
  

o = © © SRDCF [0.596]
os m= Staple [0.585]
o == SiameseFC [0.584]
oO === MEEM [0.531]
8 === DSST [0.517]
Q _
>
wn
03 -
02
0.1
0 W + 4 " 4 4 4 + 4 4
01 #02 03 04 O5 06 OF O8 o9 1
Overlap threshold
Success plots of OPE - scale variation (42)
1- T T T T T T T i T "|
=== BPCF [0.706]
0.9 == CFWCR [0.665]
os === CCOT [0.663]
ene mmm ECO_HC [0.626]
or "" eee, === DeepSRDCF [0.623]
© "ea, ‘ = =" SiameseFC [0.577]
® os sam SRDCF [0.559]
o . == Staple [0.547]
” 05) === DSST [0.507]
8 == MEEM [0.490]
Qo. ,
S 04
n
03 |
02
0.1
o|

 

0 0.1 02 03 04 05 06 07 08 09 1
Overlap threshold
Success plots of OPE - motion blur (16)

   
   
    
 
   
  
     
 

=== BPCF [0.680]
=== CFWCR [0.643]
=== CCOT [0.639]

m= DeepSRDCF [0.614]
=== ECO_HC 0.594]

  

   
  

07}
L&
© o¢ mum Staple [0.519]
& o
o == MEEM [0.499]
” o5+ === SiameseFC [0.489]
s mmm KCF [0.450]
3° |
S 04
”

03+

0.2

0.1

0 L

04 02 03 oA 05 06 07 o8 09 1
Overlap threshold
Success plots of OPE - out of view (9)

mms BPCF [0.582]

== CCOT [0.579]
=== ECO _HC [0.545]
mmm SiameseFC [0.523]
=== CFWCR [0.521]

  
   

© = == DeepSRDCF [0.494]
o mmm MEEM [0.465]
o == Staple [0.420]
o === SRDCF [0.418]
8 mms KCF [0.412]
Q ~
=}
” 03}
02
0.1

1 , 1 1 1 1 1 1 1
0.1 02 03 04 O5 O06 OF O8 O89 1

Overlap threshold

Success plots of OPE - illumination variation (55)

 

 

  
  

 

 

  

 

 

 
 
 
  
 
 
 
   
   
   

 

   

 

mums BPCF [0.691]
=== CCOT [0.670]
=== CFWCR [0.654]
mms £CO_HC [0.638]
=== DeepSRDCF [0.630]
© = © © SRDCF [0.608]
os m= Staple [0.601]
D == DSST [0.561]
n ==" SiameseFC [0.553]
8 === MEEM [0.535]
° | “?
Soa
”
0.3 +
02+
0.1
0 L 4 4 4 1 4 4 1 4 4
0 o1 O02 03 O04 OS O06 OF O8 O98 1
Overlap threshold
Success plots of OPE - occlusion (65)
1r + 1 + + + : : - -
=== CCOT [0.672]
=== BPCF [0.670]
=== ECO _HC [0.640]
=== CFWCR [0.630]
m= DeepSRDCF [0.624]
© = = = SRDCF [0.598]
o === Staple [0.585]
o A SiameseFC [0.566]
0 05: |" == MEEM (0.516)
8 [sss DSST [0.510]
S04
Y
03+
0.2 |
01
o!
0 o1 O02 03 O04 OS O06 OF O8 O89 1
Overlap threshold
Success plots of OPE - fast motion (39)
mms BPCF [0.652]
== CCOT [0.642]
=== CFWCR [0.596]
mms DeepSRDCF [0.582]
=== ECO_HC [0.580]
© {=== SRDCF [0.544]
S os! m= Staple [0.525]
— - 2
o === SiameseFC [0.525]
2 05} === MEEM [0.495]
8 mms DSST [0.449]
Soa
“”
03
02
0.1
Oo L 4 4 4. 4 4 1 4 4 1
0 o1 #02 03 04 OS O06 OF O8 O98 1
Overlap threshold
Success plots of OPE - background clutter (39)
1 cr T T T T T T T T T ”
mm BPCF [0.694]
0.9 | ee == CFWCR [0.657]
os ne = ~ === CCOT [0.648]
. — me mmm FCO_HC [0.636]
o7 | === DeepSRDCF [0.619] | |
o- ==" Staple [0.584]
o os | “¢ ,) mmm SRDCF [0.581] |
o {== SiameseFC [0.561]
O05 | === MEEM [0.546]
8 === DSST [0.515]
Ss 04 *
”
03
02
0.1
0 oO1 O02 03 O04 OF O06 OF O8 O98 1

Overlap threshold

Success plots of OPE - out-of-plane rotation (81)

=—

 

   

mmm BPCF [0.681]
== CCOT [0.668]

=== CFWCR [0.634]
m= DeepSRDCF [0.633]
== ECO_HC [0.631]

  
  

© = == SRDCF [0.596]
o mame SiameseFC [0.580]
o == Staple [0.577]
M 05+ @)= == MEEM [0.532]
8 |= DSST [0.511]
oO | ™
Loa
”

03

0.2

0.1

0 L

04 02 03 OA 05 06 07 08 09 1
Overlap threshold
Success plots of OPE - deformation (39)

ee === BPCF [0.690]

== CCOT [0.683]

=== ECO_HC [0.662]

9] me CFWCR [0.647]
== DeepSRDCF [0.641]

  
 

  

© 07) = == SRDCF [0.622]
+s os | m= Staple [0.591]
© 0, ,
5 === SiameseFC [0.557]
B os. === MEEM [0.547]
@ |= KCF [0.514]
So) %
”

0.3 +

0.2

0.1

0!

 

0 0.1 0.2 03 04 05 06 07 08 09 1
Overlap threshold
Success plots of OPE - in-plane rotation (60)
1p T T T T T T T T T 7
—— m= BPCF [0.647]

== CCOT [0.634]
=== CFWCR 0.601]

   
 

  
  

08 mms DeepSRDCF [0.598]
o7| SN == ECO_HC [0.593]
@ . {=== Staple [0.562]
3 os | mmm SRDCF [0.556]
® 0, .
a === =SiameseFC [0.555]
B os. === DSST [0.513]
8 sme MEEM [0.508]
Q ~
= 04
n
03>
0.2
0.1
oO L

04 02 03 04 05 06 07 08 09 1
Overlap threshold
Success plots of OPE - low resolution (5)

  
 
 
 
 
 
  
 
 
     
  

=== BPCF [0.639]

=== CFWCR [0.636]
=== CCOT [0.548]
mms FCO_HC [0.547]
=== SiameseFC [0.528]

  

© 2)" =" SRDCF [0.412]
_ 4
o === Staple [0.402] |
o |mmm — DeepSRDCF [0.382]
® 95! ass seee|" "8 DSST [0.382]
8 2 ae Sm MEEM [0.341]
oO L vv
S 04 °
”

03

0.2

0.1

0 L 4. 4 L 4 4. 1 1 4 =
01 O02 03 O04 O5 O06 OF O8 o9 1
Overlap threshold

Fig. 5 State-of-the-art comparison. Success plot of OPE for some of the state-of-the-art trackers and BPCF on OTB-2015 benchmark
Zhang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:20 Page 9 of 12

self-adaptive model update strategy jointly, the perform-
ance of our tracker improves by about 4%.

To get a better evaluation, we extend our experiment
on OTB-2015 benchmark by first separately implement
our proposed background perception regulation term
and model update strategy and then integrate them up
at the same time. Table 2 shows the analysis of our con-
tributions, from which we can see that our tracker BPCF
has a significantly better performance compared with
the based tracker. By implementing the background

perception regulation term, our tracker improves by
about 3.6%, and by adding the self-adaptive model up-
date strategy furthermore, it improves by about 4%.

4.2.2 State-of-the-art comparison

Here, we use both success plots and the precision plots
[22] over all 100 videos on OTB-2015 dataset to analyze
our approach BPCF. In the evaluation of success plots,
the area under the curve (AUC) of success plots is used
to rank the trackers. The precision plot reports the

 

Precision plots of OPE

=== BPCF [0.905]

== CCOT [0.899]

=== CFWCR [0.860]
mmm ECO_HC [0.854]
==" DeepSRDCF [0.851]
==" Staple [0.791]

m=" SRDCF [0.788]

=== =MEEM (0.787)

=== SiameseFC [0.774]
[m= KCF [0.698]

Precision

15 200 HSS
Location error threshold
' Precision plots of OPE - scale variation (42)

 

m= BPCF [0.920]

Sows wre eH  CFWCR [0.879]
=== CCOT [0.876]
m= DeepSRDCF [0.823]
== ECO_HC [0.823]
= =" SiameseFC [0.766]
m= MEEM [0.753]
== Staple [0.737]
=== SRDCF [0.723]
== DSST [0.662]

Precision

5 10 15 20 2 30 3 40 45
Location error threshold
Precision plots of OPE - motion blur (16)

m= BPCF [0.884]
wae ® == CCOT [0.847]
=== CFWCR [0.831]
= wos mmm DeepSRDCF [0.796]
val (== ~ECO_HC [0.767]
==" SRDCF [0.739]
mum MEEM [0.688]
== Staple [0.682]
==" SiameseFC [0.680]
|= KCF [0.595]

Precision

5 10 1 2 2 2% 3 40 45
Location error threshold
Precision plots of OPE - out of view (9)

m= BPCF [0.770]

=== CCOT [0.768]

=== ECO_HC [0.712]

mums SiameseFC [0.692]

=== CFWCR [0.687] Ls
ma" "© DeepSRDCF [0.674] i

meme MEEM [0.584]

== Staple [0.558]

=== SRDCF [0.521]

|= KCF [0.504]

Cul
iB. fa J

Precision

5 40 15 20 25 30 35 40 45 50
Location error threshold

Precision

Precision

Precision

Precision

 

Precision plots of OPE - illumination variation (55)

=== BPCF [0.903] =
== CCOT [0.895] e
=== CFWCR [0.855] lea
mmm £CO_HC [0.846]
=== DeepSRDCF [0.831]
= "© Staple [0.810]

meme SRDCF [0.805]

== MEEM [0.785]

=== DSST [0.738]

| sms KCF [0.731]

 

5 1 1 2% 2% 3% 3 4 45
Location error threshold
Precision plots of OPE - occlusion (65)

 

== CCOT [0.904]

=== BPCF [0.889]

=== ECO_HC [0.862] b
mmm DeepSRDCF [0.838]
=== CFWCR [0.830]

=== SRDCF [0.797]

mums Staple [0.794]

m=) ~=MEEM [0.765]

=== SiameseFC [0.754]
mmm KCF [0.709]

 

0 15 2 2 85S
Location error threshold
Precision plots of OPE - fast motion (39)

== CCOT [0.865] =
=== BPCF [0.849]

=== DeepSRDCF [0.786]
mms ECO_HC [0.784]
=== CFWCR [0.781] =
=== SRDCF [0.729]
mame Staple [0.713]

=== =MEEM (0.703)
==" SiameseFC [0.696]
mmm KCF [0.607]

 

0 1 2 2 3 35
Location error threshold
,Precision plots of OPE - background clutter (39)

=== BPCF [0.897]
"=== CFWCR [0.859

aeee ee CCOT (0846)
mmm ECO_HC [0.825] amt
m= DeepSRDCF [0.812] | |
== © Staple [0.763]
mem MEEM [0.759]
m= = SRDCF [0.743]
==" SiameseFC [0.727]
| mmm KCF [0.704]

 

15 2 2 3 3 40 45 50
Location error threshold

Precision

Precision

Precision

Precision

 

Precision plots of OPE - out-of-plane rotation (81)

=== CCOT [0.899] =
== BPCF [0.893] =
=== DeepSRDCF [0.849] |"|
mms ECO HC (0.847)
=== CFWCR [0.841]

==" SRDCF [0.796]

m= MEEM [0.793]

=== Staple [0.786]

=== SiameseFC [0.769]
|= KCF [0.701]

45
Location error threshold
Precision plots of OPE - deformation (39)

=== CCOT [0.894]
=== BPCF [0.886]
a)" "=" ECO_HC [0.856]
=, . Ps em CFWCR [0.831]
one === DeepSRDCF [0.829]

= == MEEM (0.797)

mms SRDCF [0.785]

== Staple [0.770]

==" SiameseFC [0.734]

| sem KCF [0.699]

 

1 L 1 1 1 1 1 a)
15 20 25 30 35 40 45 50

Location error threshold
' Precision plots of OPE - in-plane rotation (60)

4

=== CCOT [0.874] =

0.9] == BPCF [0.870] =

=== DeepSRDCF [0.826]

mms CFWCR [0.822] =

=== ECO_HC [0.813]

= =" Staple [0.778]

sm MEEM [0.772]

=== SRDCF [0.764]

05 - === SiameseFC [0.737]
|= DSST [0.700]

08 |
07>

0.6 |

04>
0.3 |

0.2)

10 15 20 25 30 35 0 45
Location error threshold
Precision plots of OPE - low resolution (5)

mmm BPCF [0.885]

=== CFWCR [0.884]

==8 CCOT [0.771]
goeeeeeeeee es § mMECO HC [0.733]

=== SiameseFC [0.727] | |

==" DeepSRDCF [0.557]

m==e== Staple [0.543]

=== =SRDCF [0.517]

= =* DSST [0.500]

| === MEEM [0.499]

25 30 35 40 45 50
Location error threshold

Fig. 6 State-of-the-art comparison. Precision plot of OPE for some of the state-of-the-art trackers and BPCF on OTB-2015 benchmark

 
Zhang et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:20 Page 10 of 12

 

I
=
Je
Ve

dis
Cg

nature} |

lg 7 7

 

Fig. 7 Qualitative comparison. a bird2; b carscale; ¢ matrix, skating1; d tiger1, tiger2; e football, footballl. The tracking result of BPCF, ECO, C-COT,
and CFWCR are marked in red, black, blue, and green
Zhang et al. EURASIP Journal on Wireless Communications and Networking

average distance precision score at 20 pixels for each
method. We conduct extensive experiment and compare
our tracker with 10 state-of-the-art methods: MEEM
[25], C-COT [4], CFWCR [12], ECO [3], DeepSRDCF
[26], SRDCF [8], Staple [27], SiameseFC [28], DSST [29],
and KCF [9].

The evaluation results of our proposed tracker BPCF
and the 10 competitive trackers are demonstrated in
Figs. 5 and 6, from which we can see that among all the
existing trackers including some DCF-based trackers
and some CNN based trackers, C-COT [4] and our
based method CFWCR [12] provide the best results on
both success plots and precision plots. The result of our
proposed method BPCF outperforms both of them and
provides the best result of 0.689 AUC score and 0.905
precision score. Besides, ECO [3] with hand-crafted fea-
tures gets an AUC score of 0.641 and a precision score
of 0.854, and SRDCF [26] with deep features gets an
AUC score of 0.634 and a precision score of 0.851.
Other trackers have an AUC score less than 0.6 and a
precision score less than 0.8.

4.3 Qualitative evaluation

To evaluate the performance of our tracker alone with
some other state-of-the-art trackers qualitatively, the
tracking results of BPCF along with ECO, C-COT,
CFWCR, etc. are presented in Fig. 7.

In Fig. 7a, the results of the sequence bird2 is shown,
in which the tracking target is undergoing out-of-plane
rotation and occlusion. We can intuitively see from the
result that, among all the trackers, BPCF tracker works
best and remains the most robust. All the other trackers
drift more or less during tracking.

Figure 7b shows the results of the sequence carscale,
where the tracking target is experiencing scale variation
and occlusion. We can see from the figure that BPCF
tracker stays stable while the target object experiences
severe occlusion. The other trackers experience overfit-
ting while the object’s scale varies.

In Fig 7c, the results of the sequences matrix and skat-
ingl are presented, where target objects are undergoing il-
luminate variation. We can see from the result of the
sequence matrix that due to the illuminate variation, other
trackers experience some degree of imprecision, while
BPCF remains the most robust. In the sequence skating],
CFWCR, and C-COT have lost the target halfway, while
BPCF and ECO remain robust to the end of the sequence.

In Fig. 7d, we present the tracking result of sequences
tigerl and tiger2, in which motion blur and occlusion
occur frequently. It is shown in the figure that all of the
trackers managed to track the target successfully, al-
though CFWCR has some occasional drift.

In Fig. 7e, the tracking results of sequences football
and footballl are given. Background clutter can be seen

(2020) 2020:20 Page 11 of 12

in these two video sequences. We can see from Fig. 6e
that when the target is occluded by other similar objects,
all the other trackers except BPCF have a certain degree
of drift. In the sequence football, one of the trackers
even fail to locate the correct object after the occlusion.

5 Conclusions

In this paper, we discard the traditional use of the hand-
craft features and adopt deep features only for visual
tracking. Moreover, we propose a background percep-
tion regulation term to alleviate the overmuch highlight-
ing of the foreground of the input training samples. By
making full use of the background information, our
tracker works more robustly. The self-adaptive model
update strategy is also implemented to avoid model cor-
ruption by selecting high confidence tracking results as
training samples. We evaluate our method on OTB-2015
benchmark and experimental results show that our
tracker achieves the state-of-the-art performance.

Abbreviations
CN: Color names; CNN: Convolutional neural network; DCF: Discriminative
correlation filter; HOG: Histogram of gradient; VGG: Visual geometry group

Acknowledgements
The authors would like to thank Tingfa Xu for the support.

Authors’ contributions

JL and TX conceived of the tracking method. YZ was responsible for the
programming. FW and LW verified the analytical methods. YZ wrote the
manuscript, and all authors revised the final manuscript. In addition, TX and
JL are the corresponding authors. All authors read and approved the final
manuscript.

Funding

This work was supported by the Major Science Instrument Program of the
National Natural Science Foundation of China under Grant 61527802.

This work was supported by the General Program of National Nature Science
Foundation of China under Grants 61371132 and 61471043.

 

Availability of data and materials
All data are fully available without restriction.

Competing interests
The authors declare that they have no competing interests.

Received: 14 September 2019 Accepted: 18 December 2019
Published online: 16 January 2020

References

1. J Henriques, R Caseiro, P Martins, J Batista, in European Conference on
Computer Vision. Exploiting the circulant structure of tracking-by-detection
with kernels (2012), pp. 702-715.

2. M Danelljan, F. S. Khan, M Felsberg, J. V. D. Weijer, in IEEE Conference on
Computer Vision and Pattern Recognition. Adaptive color attributes for real-
time visual tracking (2014), pp. 1090-1097.

3. M Danelljan, G Bhat, F. S. Khan, M Felsberg, in IEEE Conference on
Computer Vision and Pattern Recognition. ECO: Efficient Convolution
Operators for Tracking (2017), pp. 21-26.

4. M Danelljan, A Robinson, F. S. Khan, M Felsberg, in European Conference on
Computer Vision. Beyond correlation filters: learning continuous convolution
operators for visual tracking (2016), pp. 472-488.

5. R. Yao, S. Xia, F. Shen, Y. Zhou, Q. Niu, Exploiting spatial structure from parts
for adaptive kernelized correlation filter tracker. IEEE Signal Process Lett 23,
658-662 (2016)

 
Zhang et al. EURASIP Journal on Wireless Communications and Networking

6. M. Danelljan, G. Hager, F.S. Khan, M. Felsberg, Discriminative scale space
tracking. IEEE Trans Pattern Anal Machine Intell 39, 1561-1575 (2016)

7. _M Wang, Y Liu, Z Huang, Large margin object tracking with circulant feature
maps. IEEE Conference on Computer Vision and Pattern Recognition (2017),

pp. 4800-4808.

8. M Danelljan, G Hager, F. S. Khan, M Felsberg, in IEEE International
Conference on Computer Vision. Learning spatially regularized correlation
filters for visual tracking (2015), pp. 4310-4318.

9. — J.F. Henriques, C Rui, P Martins, J Batista, High-speed tracking with
kernelized correlation filters. IEEE Transactions on Pattern Analysis &
Machine Intelligence 37, 583-596 (2014).

10. D.S. Bolme, J.R. Beveridge, B.A. Draper, Y.M. Lui, in IEEE Conference on
Computer Vision and Pattern Recognition. Visual object tracking using
adaptive correlation filters (2010), pp. 2544-2550.

11. FLi,C Tian, W Zuo, L Zhang, M. H. Yang, in IEEE Conference on Computer
Vision and Pattern Recognition. Learning spatial-temporal regularized
correlation filters for visual tracking (2018), pp. 4904-4913.

 

12. ZHe, Y Fan, J Zhuang, Y Dong, H. L. Bai, In IEEE International Conference on

Computer Vision Workshop. Correlation filters with weighted convolution
responses (2017), pp. 1992-2000.

13. C Sun, D Wang, H Lu, M.H. Yang, in IEEE Conference on Computer Vision
and Pattern Recognition. Learning spatial-aware regressions for visual
tracking (2018), pp. 8962-8970

14. E. Gundogdu, A.A. Alatan, Good features to correlate for visual tracking. IEEE

Transactions on Image Process. 27(2526-2540) (2018)

15. T. Xu, Z.H. Feng, XJ. Wu, J. Kittler, Learning adaptive discriminative
correlation filters via temporal consistency preserving spatial feature
selection for robust visual object tracking. IEEE Trans Image Process 28,
5596-5609 (2019)

16. B Huang, T Xu, B Liu, B Yuan, Context constraint and pattern memory for

1016/j.neucom.2019.10.021

17. B. Huang, T. Xu, S. Jiang, Y. Bai, Y. Chen, SVTN: Siamese Visual Tracking
Networks with Spatially Constrained Correlation Filter and Saliency Prior
Context Model. IEEE Access. 7, 144339-144353 (2019)

18. N Dalal, B Triggs, in IEEE Conference on Computer Vision and Pattern Recognition.
Histograms of oriented gradients for human detection (2005), pp. 886-893.

19. M Danelljan, G Hager, F.S. Khan, M Felsberg, in Scandinavian Conference on

 

mage Analysis. Coloring channel representations for visual tracking (2015),
pp. 117-129.

20. © Sun, D Wang, H Lu, M. H. Yang, in IEEE Conference on Computer Vision
and Pattern Recognition. Correlation tracking via joint discrimination and
reliability learning (2018), pp. 489-497.

21. MKristan, A Leonardis, J Matas, M Felsberg, Z He, et al, in IEEE International
Conference on Computer Vision Workshop. The visual object tracking
vot2017 challenge results (2017), pp. 1949-1972.

22. Y.Wu, J. Lim, M.H. Yang, Object tracking benchmark. IEEE Trans Pattern Anal

Machine Intell 37, 1834-1848 (2015)

23. K Simonyan, A Zisserman, Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014).

24. ©. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A.
Karpathy, A. Khosla, M. Bernstein, et al, Imagenet large scale visual
recognition challenge. Int J Comput Vision 115, 211-252 (2015)

25. J Zhang, S Ma, S Sclaroff, in European Conference on Computer Vision.
MEEM: robust tracking via multiple experts using entropy minimization
(2014), pp. 188-203.

26. M Danelljan, G Hager, F. S. Khan, M Felsberg, in IEEE International
Conference on Computer Vision (ICCV) Workshops. Convolutional features
for correlation filter based visual tracking (2015), pp. 59-66.

27. | Bertinetto, J Valmadre, S Golodetz, O Miksik, P. H. S. Torr, in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR). Staple:
complementary learners for real-time tracking (2016), pp. 1401-1409.

28. L Bertinetto, J Valmadre, J. F. Henriques, A Vedaldi, P. H. Torr, in European
Conference on Computer Vision (ECCV) Workshops. Fully-Convolutional
Siamese Networks for Object Tracking (2016), pp. 850-865.

29. M Danelljan, G Hager, F. S. Khan, M Felsberg, Accurate scale estimation for
robust visual tracking. In BMVC (2014)

Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.

(2020) 2020:20

ong-term correlation tracking. Neurocomputing. In Press, https://doi.org/10.

Page 12 of 12

 

 

 

 

S
Submit your manuscript to a SpringerOpen®
journal and benefit from:
> Convenient online submission
> Rigorous peer review
> Open access: articles freely available online
> High visibility within the field
> Retaining the copyright to your article
Submit your next manuscript at > springeropen.com
J

 

 
