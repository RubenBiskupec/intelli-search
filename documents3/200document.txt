Journal of Parallel and Distributed Computing 146 (2020) 107-121

 

 

ELSEVIER

Contents lists available at ScienceDirect

PARALLEL AND
DISTRIBUTED
COMPUTING

J. Parallel Distrib. Comput.

 

 

journal homepage: www.elsevier.com/locate/jpdc
Expelliarmus: Semantic-centric virtual machine image management ®
in IaaS Clouds ‘tes

Nishant Saurabh®>*, Shajulin Benedict ‘, Jorge G. Barbosa“, Radu Prodan

ak

* Institute of Information Technology, University of Klagenfurt, Austria
> Institute of Computer Science, University of Innsbruck, Austria
“Indian Institute of Information Technology, Kottayam, India

4 TIACC, Faculdade de Engenharia da Universidade do Porto, Portugal

ARTICLE INFO

 

Article history:

Received 7 November 2019

Received in revised form 2 August 2020
Accepted 2 August 2020

Available online 14 August 2020

Keywords:

Virtual machine image management
Semantic similarity

Storage optimization

Virtual machine image publishing
Virtual machine image retrieval

ABSTRACT

Infrastructure-as-a-service (laaS) Clouds concurrently accommodate diverse sets of user requests,
requiring an efficient strategy for storing and retrieving virtual machine images (VMIs) at a large
scale. The VMI storage management requires dealing with multiple VMIs, typically in the magnitude
of gigabytes, which entails VMI sprawl issues hindering the elastic resource management and pro-
visioning. Unfortunately, existing techniques to facilitate VMI management overlook VMI semantics
(i.e at the level of base image and software packages), with either restricted possibility to identify and
extract reusable functionalities or with higher VMI publishing and retrieval overheads. In this paper, we
propose Expelliarmus, a novel VMI management system that helps to minimize VMI storage, publishing
and retrieval overheads. To achieve this goal, Expelliarmus incorporates three complementary features.
First, it models VMIs as semantic graphs to facilitate their similarity computation. Second, it provides a
semantically-aware VMI decomposition and base image selection to extract and store non-redundant
base image and software packages. Third, it assembles VMIs based on the required software packages
upon user request. We evaluate Expelliarmus through a representative set of synthetic Cloud VMIs on
a real test-bed. Experimental results show that our semantic-centric approach is able to optimize the
repository size by 2.3 —22 times compared to state-of-the-art systems (e.g. IBM’s Mirage and Hemera)

with significant VMI publishing and slight retrieval performance improvement.
© 2020 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license

1. Introduction

The evolving Cloud architecture [5,11,38,42] requires efficient
and scalable on-demand provisioning [8,23,41] and management
of time-critical computing services [46,47] over a federated [35]
and heterogeneous infrastructure. Moreover, with the recent ad-
vent of the Edge and IoT technologies [11,38] and their essential
integration with the Cloud, the requirement for efficient manage-
ment of the computing services is even higher. In this context,
virtualization [2,34] emerged as a key technology for enabling and
provisioning of such computing services. One common virtualiza-
tion technique that facilitates the deployment of computing ser-
vices is the virtual machine (VM) [10,12,39,40], instantiated using
a user-created template called VM image (VMI) [19]. Such VMIs
comprise an operating system (OS) and user-specific customized

* Corresponding author at: Institute of Information Technology, University of
Klagenfurt, Austria.
** Corresponding author.
E-mail addresses: nishant@itec.aau.at (N. Saurabh),
shajulin@iiitkottayam.ac.in (S. Benedict), jbarbosa@fe.up.pt (J.G. Barbosa),
radu@itec.aau.at (R. Prodan).

https://doi.org/10.1016/j.jpdc.2020.08.001

(http://creativecommons.org/licenses/by/4.0/).

software package(s). However, the ever increasing number with
size of each VMI in the magnitude of gigabytes induce important
management issues such as VMI sprawl [33], hindering the elastic
resource management and provisioning processes. For example,
Amazon Elastic Compute Cloud (EC2) alone consists of more than
30,000 public VMIs [3], where typical operations like cloning,
versioning, sharing, storing and transforming VMIs in dedicated
repositories introduce a high amount of storage redundancy and
maintenance costs.

To solve VMI management challenges such as sprawl, prior re-
search primarily focused on leveraging VMI deduplication
[20,25,28] and caching [16,29,32,43] by identifying similar byte
segments [15,18,30]. Such techniques optimize the VMI storage
and reduce redundant content by up to 80%, but limit the benefits
of virtualization, such as stronger isolation between software
packages, and a provenance record of changes and reusable func-
tionality at the VMI at semantic level [33]. Nevertheless, recent
works such as IBM’s Mirage [33] and Hemera [21] improved upon
the previous studies and explored VMI management at a more
fine grained file system level. Mirage employs a file system-based
VMI repository that maps file names to content descriptors by a
manifest, and use a data store to hold the content. In contrast,

0743-7315/© 2020 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
108 N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

the Hemera follows a hybrid file system and database-oriented
approach that transforms the VMI operations into database oper-
ations based on SQL queries. Both systems improve upon the VMI
sprawl, but with significant VMI publishing and retrieval over-
heads due to costly data deduplication and read-write operations
over thousand of files with varying sizes in a VMI repository.

The essential need is to explore a VMI management solution
that exploits semantic similarity [6] to decompose VMIs at a
functional level with low publishing and similarity computation
overheads. This results in the creation of functional blocks com-
posed of base images and various reusable software packages
among different VMIs. We propose in this paper such a technique
based on three important steps: (1) defining and computing the
semantic similarity between the VMIs based on their functional-
ity, (2) splitting the VMIs into base image and software packages,
thus avoiding storing of semantically redundant data, and (3) se-
mantic assembling of base images with various software packages
into fully functional VMIs with improved retrieval performance.

To address these challenges, we design a novel tool called
Expelliarmus that represents a VMI and its components as a struc-
tured graph [1], presenting the functional requirements between
the base image and different software packages. For each VMI
semantic graph, it extracts two induced subgraphs, called the
base image subgraph and software package subgraph. Their pur-
pose is to merge several semantically similar VMIs into one VMI
master graph, clustering their software packages with a seman-
tically similar base image. This approach reduces the similarity
computation overhead by comparing every VMI to the master
graph instead of multiple individual VMI graphs. To facilitate the
semantic aware decomposition, we employ techniques to only
extract the unique software packages not yet available in the
repository. Moreover, we devise a base image selection algo-
rithm and define a novel semantic compatibility metric to select
from a pool of semantically similar base images the one that is
functionally compatible to existing software packages replacing
the redundant ones. Our approach also provide means for VMI
assembly, either with identical uploaded software packages or
compatible with a base image already existing in the repository.
To study the benefits of semantically-aware VMI management,
we performed a series of experiments using a representative set
of synthetic Linux-based VMIs. The results demonstrate that
Expelliarmus optimizes the storage cost with significant VMI
publish and slight retrieval performance improvements compared
to the related [21,33] state-of-the-art solutions.

We summarize the contributions of this paper as follows:

(1) A novel semantic model that represents VMIs as structured
graphs and clusters them based on functionality with low
similarity computation overheads;

(2) A semantics-aware decomposition method of large VMIs
without costly content deduplication;

(3) An optimized VMI assembly method with semantically
similar base image and selective package retrieval.

The paper is organized as follows. Section 2 introduces some
relevant background concepts and summarizes the related work.
Section 3 presents the new model together with the VMI semantic
representation, and formulates the VMI semantic similarity and
compatibility metrics. Section 4 describes the architecture of the
Expelliarmus, including the VMI publishing, base image selec-
tion and VMI retrieval components and underlying algorithms.
Section 5 provides relevant implementation details and Section 6
presents the experimental results. Finally, Section 7 concludes the

paper.

User , User, Upload
QD) rescosscscecees O~ _customized I,
[ © , ? ( % \ .
1 Y ‘\
I 7 \
Upload and store VMI J, | Ra Request I,
I

        

Shared
storage

|
\ Ce Oop ute resources
“\

‘\ LA La ao
> I
1
I
I
1

     
       

   
 

Vv
Capture I 7 running state and store

Fig. 1. VMI management scenarios in IaaS Cloud.

2. Background

This section revisits common scenarios of VMI management
including VMI upload, customization and storage in IaaS Clouds.
We also discuss existing deduplication techniques for VMI man-
agement and review the related works.

2.1. VMI management in IaaS Clouds

Recent years saw an increased popularity of IaaS Clouds re-
sulting into an explosion in the number of VMIs. Typically, IaaS
Cloud providers allow users to create, upload, customize and
manipulate VMIs through a Web interface [21], and instantiate
them upon request across the data center computing resources.
The (large-sized) VMIs are typically stored in a centralized shared
repository, as the limited memory and storage availability of
computing resources make it infeasible to store them locally.
The increase in number of VMs with diverse software stack in-
stantiated across the data centers require maintaining tens of
thousand of VMIs [3]. Such scenarios pose efficient storage op-
timization challenges for Cloud providers due to increasing scale
and complexity of VMI management systems.

Fig. 1 depicts the VMI management scenario in IaaS Clouds.
Initially, a user uploads and stores a VMI J; in a shared repository
and instantiates it as a VM. Once the VM gathers a substantial
amount of updates, its running state is captured and stored in
the global storage as a different version. Other users may also
request for J;, customize it and store it in addition to existing
versions. Similarly, typical operations like cloning, customizing
and transforming VMIs in dedicated shared repositories introduce
a high amount of storage redundancy due to the similar stack of
software packages and base images included in different VMIs.
The maintenance costs arising due to this problem can be avoided
by restricting users in creating and cloning a limited number
of VMIs, however, such a strategy limits the benefits of the
virtualization technology [33] and requires other optimizations.

2.2. Deduplication

Deduplication is getting increasingly popular in storage man-
agement systems [49]. Basically, data deduplication is a com-
pression technique that eliminates redundant data chunks and
improves storage utilization. For example, [25] provides a com-
prehensive taxonomy to classify data deduplication techniques by
N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 109

segregating them along three functionalities: placement, timing,
and algorithm. First, the placement-based deduplication [45] is
performed either at the client (i.e. data origin), storage server or
dedicated middleware appliance. The client placement performs
deduplication at data origin and forwards the non-redundant
data chunks to the storage server. On the other hand, the mid-
dleware appliance placement receives the data from the client,
performs deduplication, and transfers it to the storage server.
Second, the timing functionality classifies deduplication as in-
band and out-of-band operations [37]. The in-band operations
synchronously eliminate redundant data chunks for every IO
write request, such that duplicate data is never stored. In contrast,
the out-of-band operations, also referred as an asynchronous
functionality, store the data and perform deduplication at reg-
ular intervals. Finally, algorithm-based deduplication techniques
separated by granularity are classified into three groups: delta
encoding, file-level and block-level. The delta encoding algo-
rithm applies the diff function [13] and stores the difference
between two files. The file-level algorithm [17] performs SHA1
and MD5 [7] hash functions to obtain and match the hash sig-
natures of the file content. On the contrary, the block-level [22]
granularity splits files into fixed [9,24,26] or variable-size [27]
chunks and uses Rabin Fingerprint [4,31] schemes to compute
and match the hashes of different chunks. Interestingly, each
of these deduplication methods possess a dependency over the
others. The placement-based deduplication driven by resource
utilization restricts the choices for timing and algorithm-based
deduplication classifications because of their time-critical and
large-scale content analysis requirements [25].

2.3. Deduplication for VMI management

To overcome VMI management challenges such as sprawl [33],
it is imperative to understand and identify the pattern and level
of similarity in different VMIs [15]. Essentially, VMI similarity
is of two kinds: content and semantic-based [6]. The content
similarity alludes to similar data chunks or files between VMIs,
while semantic similarity is characterized by similar functionality
(i.e at the OS and software packages level). Previous researches
focused on identifying VMI content similarity using deduplication
due to its wide adaptation in archiving systems [49], applied at
the block and file-level. The block-level technique treats VMI as a
byte-array and employs fixed or variable-size chunking schemes
to leverage similarity and eliminate redundant data chunks. Prior
works [15,16] demonstrate the effectiveness of fixed-size over
variable-size chunking to achieve a better compression ratio for
optimized VMI storage. In contrast, file-level deduplication treat
a VMI as a file system and remove files with redundant content.

2.4. Related work

One key factor affecting the performance of IJaaS Cloud man-
agement systems is the rapidly increasing number and size of
each stored VMI exceeding multiple GB. This introduces crit-
ical challenges to VMI management such as VMI sprawl, and
hence impacts elastic resource management and provisioning.
Moreover, these VMIs are usually excessively similar with a high
degree of redundancy, addressed in the community through dedu-
plication.

Jin et al. [16] explored the effectiveness of the block-level
deduplication, both with fixed and variable-size chunking using
Rabin fingerprinting [31] schemes. They showed that VMIs with
the same guest OS and different software packages share con-
siderable amount of data. This study also emphasized that VMI
deduplication at block level with fixed-size chunking is more effi-
cient than variable-size chunking, detecting up to 70% of identical
content between VMIs.

Jayram et al. [15] built upon similar work comparing different
VMI deduplication techniques and provided metrics for estimat-
ing VMI similarity. Their study showed that appropriate chunk
size selection is essential for the block-level deduplication factor
used in similarity computation.

Zhao et al. [48] proposed a scalable VMI file system called
Liquid that enables large-scale VM deployment through a fixed-
size block-level deduplication with low storage consumption. The
system also improves the IO performance with a peer-to-peer
VMI sharing and distribution.

Chun-Ho et al. [28] took a step forward and proposed a VMI
backup system based on a block-level reverse deduplication that
removes duplicates from old VMIs, while keeping the new VMI
layout as sequential as possible.

Xu et al. [43,44] proposed a VMI backup system named Crab
that also uses block-level deduplication, but implements an ad-
ditional k-means clustering method to group VMIs and speedup
the index lookup overhead.

Reimer et al. [33] and Ammons et al. [1] proposed a new
VMI format called Mirage as structured data, and performed file
system indexing together with file-level deduplication to improve
the inventory control and VM deployment.

Liu et al. [21] also approached the VMI as structured data in a
rigorous database structure called Hemera that, in contrast to the
Mirage, transforms the VMI operations into database operations
based on simple SQL queries.

All these works proposed optimization to VMI management
and VMI deduplication at content-level with restricted possibility
to identify and extract reusable functionalities. Although Mirage
and Hemera use additional semantic data to access VMIs at a
fine-grained file system level with significant VMI publishing and
retrieval overheads.

We improve over these approaches on both aspects. Instead
of splitting the VMI into chunks or files, we split a VMI at se-
mantic level into a base image and one or more software pack-
ages stored only once in the repository. In contrast, the related
approaches store additional non-redundant content of all base
images. Moreover, we assemble the VMIs not necessarily with
the same base image, but also with a semantically similar one, if
possible. Finally, the semantic-aware optimization of VMI storage
reduces the VMI size with significant publishing and retrieval
performance improvements.

3. VMI semantic model

This section presents a formal model and a set of basic defini-
tions essential to this work.

3.1. Virtual machine image (VMI)

A virtual machine image (VMI) I = (BI, PS, DS, Data) consists of
a base image BI with a standalone OS, a set of software packages
PS and DS installed on top, ranging from database to application
servers, and a set user data Data.

A primary package set PS is a suite of software packages eligible
to be hosted on an OS within a VMI. We assume that every VMI
consists of one or more primary packages, required by the user
upon instantiation.

A dependency package set DS contains libraries or other pack-
ages internal or external to the OS of the base image, used to build
or install the primary packages within the VMI.

The Data component corresponds to the user data (e.g. files,
directories) not recognized by the guest OS package management
(e.g. home directory in a Linux file system).
110 N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

Primary packages

Base Image

bash , Mariadb |‘ Tomeats |
a7 Ve ae ~~, x es Yow
‘Debian ! openjdk —— libc6 <—debconf <—ucf

wem_e ee ne

SN gawk

(a) VMI semantic graph example.

dpkg «pert base coreutils

Primary packages Base mage
ae ~ 7 ¥ oe
a pooee eee . ‘Debian |:
, Mariadb '' Tomcats : Sorts tsee
Fo ty ' bash oOpenjdk gawk
libc6 <—debconf <—ucf v
) libc6

» ert | NN
dpkg < -perl-base coreutils dpk perl-base
(b) VMI primary package sub- (c) Base image subgraph for the
graph for the VMI semantic VMI semantic graph from Fig-
graph from Figure 2a. ure 2a.

Fig. 2. VMI semantic graph, primary package subgraph, and base image
subgraph example.

3.2. VMI semantic graph

The VMI semantic graph is a high-level intermediate represen-
tation expressing the rich VMI structure in terms of functional
requirements and relationships between base image, primary
packages, and dependency packages. The semantic graph of a VMI
I as a directed cyclic graph G; = (V;, E;), where V; = BI U PS U
DS is the set of vertices including the base image, primary and
dependency packages, and E; € V; x V, is the set of edges, where
a direct edge e = (uv, v’) € E,; denotes a dependency of the base
image, primary package, or dependency package v on v’.

Fig. 2(a) shows a VMI semantic graph with a Debian base im-
age, two primary packages (MariaDB and Tomcat8), and several
dependency packages (i.e. bash, openjdk, gawk, libc6, dpkg,
debconf, perl-base, ucf and coreutils). The libc, perl-
base and dpkg packages have a cyclic dependency, meaning that
they need to be provided and installed together.

3.3. VMI attributes

Each base image BI of a VMI / has a quadruple of attributes
attrs(BI) = (type, distro, ver, arch), expressing the guest OS type
(e.g. Linux), its distribution (e.g. Debian), its version
(e.g. 16.04), and its architecture (e.g. x86_64).

Each primary or dependency package P € PS U DS holds a
quadruple of attribute information attrs(P) = (pkg, ver, arch, size)
about its name pkg (e.g. MariaDB), version (e.g. 2.5), architecture
(e.g. multiarch), and size (e.g. 1056).

Further, we prohibit a VMI semantic graph G; = (V,, E;)
to contain multiple packages with the same pkg attribute, Le.
V (P1, P2) € Vi, pkg(P1) A pkg (P2).

Table 1 summarizes the attributes associated to a VMI and its
semantic graph.

3.4. Semantic graph union

We define the union of two semantic graphs G; = (Vj, Ej)
and Gy = (V2, E2) as a new semantic graph G = G,; U G, where
G = (V; UV>, E; UE2). Two vertices v; € V; and v2 € V2 are
equivalent and belong to the intersection of the vertices sets of

Table 1
Attributes of a VMI semantic graph G,; = (BI, PS, DS, Data).

Semantic attribute Example

type : BI +> String
distro : BI +> String
ver : BI +> String

arch : BI +> String

pkg : PS UDS +> String

type (BI) = “Linux”
distro (BI) = “debian”
ver (BI) = “16.04”
arch(I) = “x86_64”
pkg(P) = “Tomcat8”

ver : PS U DS +> String ver(P) = “8.1.1”
arch : PS U DS +> String arch(P) = “multiarch”
size : PS U DS b> String size(P) = “4096”

the two semantic graphs (v; = v2 € Vi M V2) if they have the
same attributes: attrs (v;) = attrs (v2).

3.5. VMI subgraphs

A primary package subgraph G;[PS] is an induced subgraph of
the VMI semantic graph containing all primary packages PS C V,
and dependency packages reachable from them as vertices:

GPS] = |_) GIPI.

VPePS

where G;[P] is the induced connected subgraph containing all
packages reachable from a package P:

G;[P] = (Vp, Ep), PE Vp CIAEp CEA
V (Pi, P2) © E => (Py, P2) € Ep.

The number of connected components in G,[PS] is less than or
equal to the cardinality of the set PS, such that all packages are
reachable from the primary ones.

Fig. 2(b) shows the primary package subgraph for the VMI se-
mantic graph in Fig. 2(a) with Mariadb and Tomcat8 as primary
packages.

A base image subgraph G;[BI] is the induced connected sub-
graph containing the base image and all dependency packages
reachable from it:

G; [BI] = (Vg, Eg), BI € Vg A Vg C V \ PS A Ep
EEAV (Py, P2) € E = > (Pi, P2) € Ep.

The base image subgraph does not include primary packages.
Fig. 2(c) shows the base image subgraph for the VMI semantic
graph in Fig. 2(a) with Debian as base image.

3.6. VMI semantic similarity

Let us consider two VMIs I; and Jz with two semantic graphs
G, (V;, E;) and G2 (V2, E>). In this section, we define the similarity
between two VMIs as a number in the [0, 1] interval representing
how similar the attributes of each vertex in G; and G> are to each
other.

Initially, we define the base image similarity between two base
images BI, and Bly as a binary digit with the value of 1 if the two
images have the same attribute values and with the value of 0
otherwise:

1, attrs (BI,) = attrs (Bl) ,

simp (Bly, Bl) =
at (Bly, Blz) {) attrs (BI,) € attrs (Blz) .

Similarly, we define the primary or dependency package simi-
larity between two packages P; and P2 as 1 if they have the same
N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 111

name, version and architecture, as binary values:

1, pkg (P1) = pkg (P2),

0, pkg (Pi) # pkg (P2);

1, ver (P;) = ver (P2),

O, ver (P;) € ver (P2);

1, arch (P,) = arch (P2) V
arch (P;) = "all"v
arch (P2) = "all";

0, otherwise;

simp (P;, P2) = I]

Vattr €{pkg,ver,arch}

SIMpkg (P;, P2) =

SIM yer (Pi, P2) =

siMarch (Pi, P2) =

SiMattr (Pi, P2) .

An architecture attribute of “all” means that the package is
portable and available on base images with any architecture.

Further, we compute the size similarity between two packages
P, and P, as the ratio between the maximum size of the two
packages divided by maximum size of all packages from both
VMIs, where the size denotes the amount of disk space consumed
by a software package within a VMI, including any software
package updates:

max {size (P}) , size (P2)}
MaXypev,uv, {size(P)}

This allows us to compute a weighted composition of content and
semantic similarity [6] of packages within two VMIs.

Finally, we model the VMI semantic similarity between two
semantic graphs G; and G» based on the Jaccard index [14], also
known as intersection over union. We formulate it as a product
of the similarity between base images and the matched software
packages with normalized package size in the numerator, and the
union of all packages in both VMIs in the denominator:

SIMsize (Pi, P2) =

Simg (G1, G2) = simp, (Bl, Blz) -
> V(P, Po) eV; x V9 SIMsize (P1, P2) - Simp (P1, P2)

> V(P, .Pa)eV, xVp SIMsize (P1, P2)
3.7. Semantic compatibility

We define the semantic compatibility between a base image
subgraph G,(BI) = (Vz, Eg;) and a primary package subgraph
G;(PS) = (Vps, Eps) as the product of the similarities of their
packages with a homonym pkg attribute:

comp (GBI, GPs) = |]

Vv(Py P) €Vpy x Vps
Apkg (P; ) =pkg(P2)

simp (P1, P2).

If the semantic compatibility is 1, the primary packages can be
installed and used together with the base image. Otherwise they
are incompatible.

3.8. VMI master graph

A VMI master graph Gy[T, D, V, A] represents all VMIs with
the same type, distribution, version and architecture base image
attributes (T, D, V, A) stored in the repository. The master graph
contains one single base image subgraph semantically compatible
to all primary package subgraphs of the VMIs represented in
the master graph. The purpose of the VMI master graph is to
reduce the similarity computation overhead between multiple
VMI semantic graphs with one single master graph similarity
comparison. We therefore model the master graph as the union

of one base image and one or more primary package subgraphs
originating from VMIs with the same base image attributes:

GulT,D,.V.AJ= (J
VIeRepoA
attrs(BI)=(T,D,V,A)A
comp(G,[BI],G;[PS])=1

(G;[BI] U G/[PS]).

4. Semantic-centric VMI management

This section describes the architecture of Expelliarmus, includ-
ing its VMI publishing and retrieval algorithms.

4.1. Architecture overview

Fig. 3 describes the architecture of Expelliarmus through a use-
case in which multiple users initially upload a VMI for storage in
a proprietary Cloud image repository. Afterwards, they download
and instantiate the VMI multiple times at various Cloud locations.
Every time the users update a VMI and store it in the image
repository, they introduce a considerable amount of redundancy
and other management costs in terms of VMI publishing and
retrieval, hindering the elastic provisioning and deployment pro-
cess. At the same time, different VMIs with varying software
packages uploaded by different users could also be composed of
semantically similar base images and software packages. Expel-
liarmus semantically decomposes VMIs in reusable fragments so
that similar software packages and base images within different
VMIs are stored only once with reduced redundancy, publishing
and retrieval overheads.

The publishing, storage and retrieval of a VMI in Expelliarmus
takes place using the following steps (see Fig. 3):

(1) The user uploads a VMI and a list of primary packages for
storage in the VMI repository;

(2) The semantic analyzer creates a VMI semantic graph and
computes its semantic similarity with other VMIs;

(3) To publish the VMI, the decomposer splits a VMI into
a base image and multiple software packages exploiting
the semantic similarity with other VMIs for storing only
non-redundant packages and base images;

(4) The user requests the retrieval of a VMI;

(5) The VMI assembler assembles the VMI according to the
user request and delivers it.

4.2. VMI semantic analyzer

The semantic analyzer takes the VMI and the primary package
list as input and constructs the semantic graph, following the
model defined in Section 3.2. This automates the process of
optimizing and accessing semantic similarity of monolithic VMIs
without detailed content analysis, instead caching a subset of VMI
semantic data in the form of a graph.

The semantic analyzer creates a graph G, for every uploaded
VMI I along with a subgraph representation of the corresponding
primary packages and base image. Afterwards, it compares the
newly uploaded VMI with the appropriate master graph Gy with
the same type, distribution, version and architecture attributes,
previously stored in the repository according to the semantic sim-
ilarity defined in Section 3.6. Every VMI master graph is specific
to a characteristic base image with one or more semantically
compatible primary package subgraphs. If no such master graph
exists in the repository, the semantic analyzer forwards the VMI
to the decomposer.
112

N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

Step 1: Upload VMI with a list of primary packages

 
     
  
      
    
  
   
 
       

    
   
   

VMI DATABASE

F Package
ID_soft

(....)

i

VMI Upload

©

User

  

VMI Repository to store VMIs,
and semantic graphs

  

 

Multiple
Running VMs

Capture running
VMI State and

requests storage

Step 2: Semantic Analysis
1. Creates anew VMI

 
 

 
 

Semantic

  

Semantic Graph.
2. Create primary package
Graph. Step 5: VMI Retrieval

Analyzer

  
 
  

  
  

VMI Decomposer

Step 3: Initiate VMI Publishing

1. Check the existing software packages
and compatible base image in

       
     
      
 
 
  

3. Compute Similarity. 1. Fetch the stored base image and
software packages for requested VMI.

     
 
 
 
 

   

- VMI . repository.
Step 4 R t . Reset VMI and create local repository. 2. Extract non-redundant ones, store
@) eques 3. Import and install all software packages and index them

 

( ®

User VMI
retrieved

  
  

3. Create master graph if it doesn’t exist,
else add software packages to existing
compatible Mastergraph.

  
 

VMI Assembler

Initiate assembling for requested VMI

Fetch the semantic data to Assemble VMI

Fig. 3. Expelliarmus architecture and VMI management.

4,3. VMI decomposer

The VMI decomposer splits a VMI into a base image and
different software packages, exploiting semantic similarity such
that only non-redundant software packages and base images
are stored. To achieve this, the decomposer employs two algo-
rithms: VMI publishing (Algorithm 1) and base image selection
(Algorithm 2).

4.3.1. VMI publishing algorithm

Algorithm 1 outlines the step-wise VMI publishing process.
The algorithm takes as input a VMI J, its semantic graph G;, a VMI
repository, and list of primary packages PS. Initially, the algorithm
extracts the VMI’s primary package subgraph G;[PS] in line 1.
Afterwards, it iterates each primary package in the subgraph,
checks if it exists in the repository with same semantic attributes
(lines 2-5) and, if it does not exist, stores it (line 4). After checking
all primary subgraph packages, the algorithm stores the user
data in line 6. Next, line 8 removes the primary packages from
the VMI, including the user data and the dependency packages
not used by any software package still within the VMI (lines
10-11). At this point, the VMI contains only the base image BI
(line 12) with all its required software packages already stored
in the repository. To prevent redundant storage of the same
base image, a base image selection algorithm (see Algorithm 2
in Section 4.3.2) called at line 14 returns a similar base image,
together with a list of base images stored in the repository that
are no longer required. If Algorithm 2 returns the current base
image, we update the repository along with the corresponding
new master graph in lines 15-17. However, if Algorithm 2 returns
another already stored base image, line 19 retrieves its master
graph from the repository and updates the retrieved master graph
with the primary package subgraph G,[PS] in line 21. Algorithm
2 also returns a list of base images that can be replaced with the

selected base image. Line 22-28 iterates over this list and line 23
retrieves the master graph for each replaceable base image from
the repository. Each retrieved master graph corresponding to a
base image in the list is a union of a base image subgraph and
several primary package subgraphs. Lines 24-26 iterate over the
primary packages in each master graph and update the master
graph of the selected base image Gy with the extracted primary
package subgraph (line 25). Line 27 removes the obsolete base
images and line 29 updates the master graph in the repository.

4.3.2. Base image selection algorithm

As a part of the VMI publishing, Algorithm 2 returns an appro-
priate base image and a list of previously stored base images that
are no longer needed. The selected base image is semantically
compatible with the primary packages corresponding to the base
images in the list. The algorithm takes as input a base image BI,
the primary package subgraph G,[PS] of an image IJ, and a VMI
repository repo. Initially, line 1 initializes a triplet list with the
base image of a VMI J, the base image subgraph, and the primary
package subgraph. Afterwards, line 3 retrieves the list of all base
images stored in the repository. Lines 4-12 iterate over the list
of stored base images, and gets the corresponding base image
subgraph and master graph in lines 5 and 6. Next, the algorithm
checks the semantic similarity between the base image BI and
the stored base images in line 7. If the semantic similarity be-
tween the base images exist, line 9 extracts each primary package
subgraph from the master graph, while line 10 adds the stored
base image, its base image subgraph, and the primary package
subgraphs into a triplet list. Afterwards, lines 13-26 iterate over
all base images in this triplet list. For each current base image, the
algorithm adds first all base images BJ; that are not identical but
similar to BI;, and BI; is semantically compatible (see Section 3.7)
to their primary packages into the replace list (see lines 15-18,
meaning that the current base image BJ; can replace all the base
N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 113

Algorithm 1: VMI publishing algorithm.

Input : J = (BI, PS, DS, Data): VMI; G;: semantic graph of VMI I; PS: primary
package set; repo: VMI repository
G, [PS] = (Vp, Ep) < extractSubGraph (G; )
forall P € Vp do
if — exists(P, repo) then
| store (P, repo)

end
store (Data, repo)
forall P < PS do

| remove (P, /)

9 end
10 removeUnusedDependencies (J)
11 remove (Data, I)
12 BI <I
13 G,[BI] — createSubGraph (BI)
14 (base, list) < selectBaseImage (BI, G;[BI], G;[PS], repo)
15 if base = BI then
16 Gy <-createMasterGraph (G,[BI])
17 store (BI, repo);
18 else
19 | Gy < getMasterGraph (base, repo)
20 end
21 Gy < Gy UG|[PS]
22 forall b « list do

aonau sh WN =

23 Gub <- getMasterGraph (D, repo)

24 forall P € Gyp do

25 | Gy < GmU extractSubGraph(Gyp, P)
26 end

27 remove (b, repo)

28 end

29 update (Gy, repo)

images in its replace list). If the replace list is not empty (line 20),
line 23 computes the total size of all packages of the base image
BI; in the replace list. Finally, line 25 adds the base image, the
replace list, and the total size of its packages into a new quadruple
list. The fourth boolean component (i.e. BI; = BI) of the quadruple
indicates whether this base image is new or already existed in the
repository. Once this procedure completes for all base images in
the triplet list, line 27 sorts the generated quadruples list based
on three criteria: the replace list size (i.e. the more replaced base
images, the better), the total size of its packages (the smaller, the
better), and existence of a similar base image in the repository
(i.e. no unnecessary storage). Lines 29-32 iterate over the sorted
quadruples list, and extract the base image and its replace list in
line 29. Finally, it checks the first quadruple that either specifies
the base image BI or exists in the replace list in line 30 and
returns it in line 31. If no quadruple exists with the base image
BI, line 33 returns it with an empty replace list.

4.4, VMI assembler

Expelliarmus enables VMI assembly either with identical or
with differing functionality, provided that the requested packages
exist in the repository. For this, the VMI assembler employs a VMI
retrieval algorithm (Algorithm 3) that processes the requests for
assembly and deployment.

4.4.1. VMI retrieval algorithm

Algorithm 3 represents the stepwise VMI retrieval process
with two input parameters: a (nonexistent) VMI J identified by its
base image BI and primary package set PS, and a VMI repository
repo. Initially, line 1 obtains the base image and primary package
subgraphs from the repository. If they exist and are compatible
(line 2), line 3 retrieves the base image BIJ from the repository
and resets it to an initial state in line 4. Line 5 imports the
user data into the VMI J. Lines 6-10 iterate over each vertex in
the primary package subgraph and check their existence in the
base image subgraph in line 7. If the vertex does not exist, line
8 adds the vertex into the primary package set PS. Finally, the

Algorithm 2: Base image selection algorithm.

Input : BI: remaining base image after decomposition; G;[BI]: base image
subgraph; G;[PS]: primary package subgraph; repo: VMI repository
list3 <— [(BI, G,;[BI], G;[PS])]
list4 — %
baseList <- getBaseImageList (repo)
forall b € baseList do
G,[b] <— getSubGraph (b, repo)
Gump <- getMasterGraph (D, repo);
if simpy (BI, b) = 1 then
forall P € Gyp do
G;[P] < extractSubGraph (Gyp, P)
list3 < list3 U {(b, G;[b], G;[P])}
end

SCaAnN nu fF WN =

—=—
an)

end
forall i € list3 do
(BIj, G;[Bli], G;[PSj]) <1
forall j 4 i € list3 do
(Bl, G;[Blj], G;[PSj]) <i
if Bl; # BI; \ comp(G,[Blj], G;[PS;]) = 1 then
| replaceList < replaceList U (i;\

pk meek kk
on nau kf WN

end
if replaceList A then
size <- 0
forall P € G;[BI;] do
| size < size + size(P);
end
list4 <— list4 U {(Blj, replaceList, size, BI = BIj)}

NNMNN NN =
a PWN S&B © OO

 

end
list4 <— sort (list4)
forall i € list4 do
(BI;, replaceList,_,_) <i
if Bl; = BI v BI € replaceList then
| return (BI;, replaceList)

WwWNNN NY
a — 2)

end
return (BI, @)

WwW vw
WN

Algorithm 3: VMI retrieval algorithm.

Input : J] = (BI, PS,_, Data): VMI; repo: VMI repository
1 (G,[BI], G;[PS]) < getSubGraph (/, repo)
2 if G,;[BI] A NULL A G;[PS] 4 NULL A comp (G;[BI], G;[PS]) = 1 then
3 BI — getBaseImage (id, G;[BI], repo)
4 reset VMI (BI)
5 import (Data, I)
6 forall P € G;[PS] do
7 if P ¢ G, [BI] then
8 | PS <— PSUP
9 end
10 end
11 forall P < PS do
12 | install(P,1);
13 end
14 end
15 return |

VMI’s guest OS package manager installs the primary packages in
line 12 by importing the required software packages (including
primary and dependency packages), if necessary (see Sections 5.1
and 5.4 for software package installation details). Line 15 returns
the assembled VMI.

5. Implementation

We implemented Expelliarmus in Python and publicly re-
leased in GitHub! the complete source code of our implemen-
tation, including the reproducibility artifact of the experimental
validation. In the following, we briefly discuss the implementa-
tion details of Expelliarmus with respect to the VMI manipulation,
semantic graph creation, VMI publishing and retrieval, currently
limited to Linux VMIs only.

1 https://github.com/ExpelliarmusSuperComp/Expelliarmus
114 N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

5.1. VMI access

Expelliarmus uses the libguestfs library to access, manip-
ulate and modify VMIs for performing VMI publishing and re-
trieval including software packages installation (see Sections 5.3
and 5.4). Apart from performing modifications to a VMI file sys-
tem, libguestfs provides access to the guest OS through its
virtual appliance without instantiating the entire VMI. To achieve
this, libguestfs configures and launches a guestfs handle
that provides an interface to access the VMIs. The guestfs
handle creates a child process running a tiny supermin virtual
appliance (similar to a VM), usually around 100kB in size, in-
stantiated on-the-fly in a fraction of a second. The appliance
integrated with qemu? (i.e. a generic machine emulator and vir-
tualizer) holds the Linux kernel and runs as a guestfsd daemon
to facilitate the communication with Libguestfs functionalities.
This attaches the specified VMI to the gemu process. Subse-
quently, the system accesses the VMI and its guest OS without
instantiating it through the guestfsd daemon.

5.2. VMI graph representation

We represent VMIs according to semantic principles described
in Section 3.2 and store them in graph data structure using the
Python-based networkx* module, implemented depending upon
the suitable package management of the guest OS (e.g. APT or
DNF). We execute the package management commands through
libguestfs on the VMI guest OS to fetch the required semantic
information (e.g. architecture, version) about the base image, and
installed software and dependency packages associated to graph
vertices and edges.

5.3. VMI publishing

VMI publishing comprises a decomposition process, accom-
plished by recreating the binary package (e.g. .deb distribu-
tion files) for the required software packages and utilizing the
libguestfs calls to export them to the VMI repository. Fur-
thermore, we remove the specific package binaries, configura-
tions and the dependency packages no longer required in the
VMI, followed by cleaning up the cached repository files. Finally,
we employ the base image selection algorithm to select the
appropriate base image for storage.

5.4. VMI retrieval

VMI retrieval comprises the assembly procedure, achieved
by first resetting the base image using the virt-sysprep tool
(part of libguestfs), followed by importing software packages
and specific user data into the VMI. Furthermore, we scan each
imported software package to create the meta-data readable by
the package manager. Afterwards, we add a custom repository
configuration file (i.e. pointer to software packages in the local
repository) that enables the VMI’s guest OS package management
to install packages from the local repository instead of the online
ones. For software package installation, we execute the package
management commands through libguestfs on the VMI guest
OS without instantiating the entire VMI (see Section 5.1). Finally,

2 http://libguestfs.org
3 https://www.qgemu.org/
4 https://networkx.github.io/

Table 2

Experimental setup.
Parameters Value
OS kernel version 5.0.0-31
Processors 64
L1 cache 32k
L2 cache 1024 k
L3 cache 22528 k
RAM 384GB DDR4
Storage capacity 9.6TB SSD

we remove the local temporary repository including the custom
configuration, and restore the default repository configuration
files (i.e. pointer to online package management repository).

6. Experimental results

We evaluated Expelliarmus on an Intel(R) Xeon(R) Gold
5218 server at 2.30GHz running Ubuntu 18.04 (x86_64) OS,
with an attached 9.6TB SSD disk acting as a VMI repository.
Table 2 displays further experimental testbed details. We used
the SQLite” database engine, suitable for managing VMI meta-
data due to its self-contained, serverless, and zero configuration
characteristics. In principle, our system is capable of running on
any Linux-based OS with support for libguestfs, qemu and
SQLite software tools.

6.1. Experimental VMI sets

In the lack of any public VMI management benchmark, we
evaluated our approach using a synthetic VMI set based on the
Ubuntu Linux distribution with software packages recognized
by the package management tools. We will address the man-
agement of VMIs composed of software packages recognized
by non-package management tools (e.g. pip, snap) or installed
through compiled source code in future work. We create each
VMI using virt-builder,”° an efficient tool for building a variety
of images for local and Cloud use. The minimal script to create
an experimental image is available in the GitHub code repository
(see footnote 1). For a fair comparison, the evaluation set includes
four VMIs used in two previous studies and in the same config-
uration [1,21], namely Mini, Base, Desktop, and IDE. The VMI set
also includes 38 images with a similar software stack as provided
at the Amazon Web Services portal’ for free and enterprise use,
to provide a representative set of Cloud images for evaluation:

(1) Mini images with minimal non-desktop installation of the
Ubuntu Linux distribution;

(2) Base images with the LAPP software stack;

(3) Desktop images with X Windows and desktop productivity
tools, including LAPP and FIP/NFS servers;

(4) IDE image with an integrated development environment
including Eclipse, JDK, and Python;

(5) Infrastructure images with application servers (e.g. Redis,
Sqlite, Celery, httpd, Nginx, Tornado, Yarn, Flower, Gluster-
FS, Samba, MariaDB, Solr, MySQL, XDM, Neo4j, Lamp, Lemp,
Jenkins, Tomcat, MongoDB, CouchDB, Django, RabbitMQ,
Cassandra, PostgreSQL, ELK), and project management tools
(i.e. ownCloud).

5 https://www.sqlite.org/index.html
6 http://libguestfs.org/virt- builder. 1.html
7 https://aws.amazon.com/marketplace
N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 115
Table 3
LARGE VMI experimental set characteristics.
VMI number VMI name Mounted Number of Similarity Publishing Retrieval
size [GB] files [Sime] time [s] time [s]
1 Mini 1.63342 65011 0 26.13 12.77
2 Redis 1.63653 65081 0.97 7.79 12.86
3 Sqlite 1.63848 65 030 0.91 11.55 12.60
4 Celery 1.64746 66 151 0.93 8.21 15.08
5 OpenVPN 1.64166 65 331 0.97 8.45 12.96
6 Apache-httpd 1.64752 66014 0.69 34.29 15.04
7 Nginx 1.65350 65 351 0.88 9.63 14.36
8 Tornado 1.67456 67 042 0.71 11.68 14.59
9 Yarn 1.66278 65 336 0.80 11.06 14.16
10 Postgre SQL 1.69201 67 355 0.86 16.08 16.60
11 Django 1.69122 75530 0.71 15.23 14.37
12 Flower 1.70843 69791 0.76 13.61 16.76
13 Gluster-FS 1.69744 67 640 0.75 19.95 23.80
14 CouchDB 1.71215 67 388 0.76 20.25 18.82
15 Vagrant 1.70739 69024 0.79 15.82 15.67
16 Samba 1.73431 68 346 0.72 23.78 16.79
17 Tomcat 1.80853 66 145 0.40 44.53 18.93
18 MariaDB 1.79785 66038 0.35 32.01 24.13
19 Apache Solr 1.88867 67916 0.83 16.36 22.47
20 MySQL 1.82855 66 117 0.42 37.20 20.94
21 Ethereum 1.83135 65 357 0.38 62.38 19.26
22 Own Cloud 1.94165 83042 0.61 53.81 25.72
23 XDM 1.82452 66 069 0.98 4.32 15.07
24 MongoDB 1.88494 65 310 0.18 98.06 19.51
25 Puppet 2.03308 78257 0.57 56.81 23.98
26 Docker-env 1.95758 67005 0.30 113.36 21.11
27 VirtualBox 1.95391 68 489 0.70 67.46 48.37
28 Cassandra 2.01068 69 317 0.50 108.78 34.01
29 Base 1.95037 86 402 0.90 27.03 34.76
30 Jenkins 2.13690 68 834 0.86 34.55 33.85
31 Elastic Search 2.05871 69 256 0.85 33.06 34.26
32 Desktop 2.11517 91280 0.92 32.53 43.84
33 Lamp 2.27398 100 060 0.98 6.98 26.25
34 Lemp 2.28131 99580 0.98 9.45 25.64
35 Neo4J 2.26873 67553 0.72 63.36 41.00
36 RabbitMQ 2.21447 87070 0.84 33.18 30.65
37 Kibana 2.32360 108 798 0.70 109.81 30.82
38 Logstash 2.34014 83757 0.72 88.51 31.17
39 IDE 2.38635 73820 0.71 72.87 44.46
40 DevOps toolbox 2.42687 86 042 0.91 32.42 63.16
41 Kubernetes 2.46661 68 879 0.15 254.69 29.70
42 ELK-Stack 2.83361 125 340 0.98 11.15 34.30

(6) DevOps images with infrastructure-as-code tools (e.g. Pup-
pet, Vagrant), continuous monitoring and analytic servers
(i.e. Elastic Search, Kibana, and Logstash), and automated
application management solutions (i.e. Docker, and Kuber-
netes).

Table 3 lists the characteristics of the VMIs including their
mounted disk use, number of files in their file system, semantic
similarity, and publishing and retrieval times. We created the
VMIs from scratch as an extension to the VMI list in [36]. We
assumed that the repository is initially empty and the VMIs were
randomly uploaded for the first execution. We average the evalu-
ation results across five trials, as their variance is relatively small
in all experiments. To maintain the uniformity of the evaluation
over the five trials, we performed the remaining four executions
in the same sequence listed in Table 3. Additionally, we used
500 VMIs to demonstrate the scalability of Expelliarmus, derived
from the 42 VMIs listed in Table 3 through cloning and sprawling
operations (i.e. software installation and update).

We considered three evaluation scenarios that sequentially
store a number of VMIs, as follows:

(1) SMALL considers four VMIs from previous studies [1,21] for
direct comparison: Mini, Base, Desktop, IDE;

(2) LARGE considers the 42 VMIs listed in Table 3 to show the
growth of repository at larger-scale.

(3) IDE considers 50 IDE images obtained by successive builds,
similar to previous studies [1,21].

6.2. VMI repository optimization

For each scenario, we compared the Expelliarmus storage op-

timization with the following VMI encoding schemes:
(1) Qcow2 format with no compression®;
(2) Qcow2 + Gzip compressed format”:
(3) Mirage MIF [1] format with a manifest file to manage VMI
descriptors and store VMIs in a global data store;
(4) Hemera |21] format with a hybrid file system and database-
oriented approach for managing VMI content.

Fig. 4(a) shows the cumulative repository growth for the
SMALL scenario. On a repository that stores only four images
with a cumulative size of 8.08 GB in Qcow2 format, Expelliarmus
performs better requiring only 2.05GB compared to 3.08 GB for
images compressed with Gzip, and to 3.17GB for Mirage and
Hemera systems.

Fig. 4(b) estimates the performance of these systems for the
LARGE scenario. For a cumulative repository size of 81.61GB
in the Qcow2 format, Expelliarmus requires 2.95 GB, while Mi-
rage and Hemera perform again similarly requiring 15.41 GB. In
this scenario, the storage cost for Qcow2 images compressed
with Gzip is worse requiring 32.43 GB. An important observation

8 https://people.gnome.org/~markmc/qcow-image-format.html
9 https://www.gzip.org/
116

—@— Qcow2
=== Qcow2 + Gzip
— ~— Mirage
Hemera
=——@— Expelliarmus

 

 

 

 

 

 

Cumulative repository size (GB)
Cumulative repository size (GB)

 

| | | |
14 8

—O— Qcow2 + Gzip
—~«— Mirage
—— Hemera
——@—— Expelliarmus

 

| | | | |
12 16 20 24 28 32 36 40

Mini Base Desktop IDE
VMI VMI
(a) SMALL (4 VMIs). (b) LARGE (42 VMIs).

N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

—O— Qcow2 + Gzip

—«— Mirage
—s—. Hemera
=== Expelliarmus

 

Cumulative repository size (GB)

 

a ee | |
5 10 15 20 25 30 35 40 45 50

Fig. 4. Repository size growth for three evaluation scenarios.

from Figs. 4(a) and 4(b) is the improved performance of Mirage,
Hemera and Expelliarmus with increasing number of VMIs in the
repository over Gzip. The performance of Mirage and Hemera is
due to the file-level deduplication that stores common files from
the same and different VMIs only once. In contrast, Expelliar-
mus not only relies on deduplicating similar software packages,
but also optimizes the VMI storage by removing software pack-
ages not dependent on the primary ones, an opportunity not
captured by Mirage and Hemera. Moreover, Expelliarmus’ base
image selection reduces the storage by selecting one from a pool
of semantically similar base images, while Mirage and Hemera
store additional non-redundant content of other base images.
Evidently, the base image is a major contributor to the higher
repository size.

The advantage of Expelliarmus is better captured by the IDE
scenario in Fig. 4(c). For a cumulative repository size of 120.61 GB,
Expelliarmus requires 2.62 GB, while Mirage and Hemera-based
storage require 5.96GB. The storage size for the Gzip scheme
is even higher, requiring 57GB. In this scenario, Expelliarmus
performs 22 times better than Gzip, and 2.3 times better than
Mirage and Hemera, which in turn perform 9.5 times better than
Gzip.

6.3. VMI publishing

We evaluate the performance of publishing a VMI, as the
decomposition comprising the time to create a guestfs handle
for VMI access, export semantically unique software packages,
remove the unused software packages, and select the compatible
base image. On the contrary, the time to retrieve a VMI reflects
the assembly comprising the time to create a guestfs handle,
copy the appropriate base image to the local repository, reset the
VMI, and import the software packages. We evaluate the VMI
publishing for two scenarios, SMALL and LARGE, introduced in
Section 6.1.

For the SMALL scenario, Fig. 5(a) shows that Expelliarmus
optimizes not only the storage cost, but also publishes VMI faster
than Mirage and Hemera. The VMI publishing time in Expelliar-
mus depends not only on the mounted VMI size, but also on
the software packages installation size, as the space required by
the software package to be installed on a disk, which is always
larger than in the .deb or .rpm formats. The different software
packages with varying installation sizes largely affect the time to
create a binary software package (e.g. .deb) resulting in a higher
export time to the repository. The total installation size of the
exported software packages for the Base VMI is the largest, and
hence requires more time to publish in Expelliarmus compared
to other images. In contrast, Mirage and Hemera require longer

Publish time (seconds)

Publish time (seconds)

Publish time (seconds)

600
500
400
300
200
100

300

250

200

150

100

5

oS

600

500

400

300

200

100

 

 

 

 

 

VMI
(c) IDE (50 IDE VMIs).

i Expellairmus
2 @ Mirage
= = Hemera

J il |

Mini Base Desktop IDE

VMI

(a) SMALL (4 VMIs).

a Libguestfs handler creation

7 Base image selection
= Export

4 8

 

16 20 24 28 32 36
VMI

(b) Expelliarmus LARGE (42 VMIs).

@ Expelliarmus
o Semantic

A Mirage
OlHemera

 

VMI

(c) LARGE (42 VMIs).

Fig. 5. VMI publishing analysis.
N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 117

time for the Desktop VMI, as the publishing time is proportional
to the mounted size and the file sizes within a VMI.

Another reason for faster VMI publishing compared to the file
system-based approaches is the lower deduplication overheads.
Mirage and Hemera require matching content over thousands
of files incurring time penalties in the range of seconds to few
minutes. In contrast, Expelliarmus relies on VMI semantic graphs
for similarity computation and semantic clustering of similar
VMIs into a master graph, which allows the comparison of new
VMIs with a single master graph instead of multiple VMIs. The
similarity computation in Expelliarmus incurs time penalties in
the order of less than 100ms for each VMI, which eradicates
a large share of VMI publishing overhead with low similarity
computation cost.

The primary factors contributing to the VMI publishing time
in Expelliarmus are better represented by the LARGE scenario
over a repository with 42 VMIs, successively added as listed
in Table 3. Fig. 5(b) shows the publishing time for Expelliar-
mus as a combination of three operations: guestfs handler
creation, compatible base image selection, and exporting seman-
tically unique software packages. The first two operations share
nearly equal time for publishing different VMIs, while the export
time invariably differs. As already explained, VMI publishing in
Expelliarmus depends upon the installation size of the exported
software packages, which is highest in case of Kubernetes VMI.

Fig. 5(c) compares the VMI publishing times for the LARGE sce-
nario, using an additional variant of Expelliarmus called semantic
decomposition that exports all the required software packages
without considering semantic similarity. Expelliarmus publishes
again VMIs faster than Mirage, Hemera, and its own variant
semantic decomposition. While the ELK VMI required the longest
publishing time in Mirage and Hemera due to its mounted size
and large number of files (more than 120 thousand), the Kuber-
netes VMI had the longest publishing time in Expelliarmus due
to the highest installation size of the exported software pack-
ages. The longest publishing time for semantic decomposition
has ELK VMI, while Expelliarmus performed better as it only
exports the software packages that do not exist in the repository.
As the similarity computation overhead in Expelliarmus is very
low, the VMI publishing time is mostly dependent on exporting
the software packages. However, the more VMIs are uploaded
to the repository, the less software packages are exported by
Expelliarmus with lower publishing time compared to semantic
decomposition.

6.4. VMI retrieval

Fig. 6(a) shows the VMI retrieval in Expelliarmus for the LARGE
scenario as a composition of four operations: copy the base image
from the repository, create the guestfs handler, reset the VMI,
and import the required software packages into a VMI. The first
three operations share nearly equal time for different VMIs, while
the import time invariably differs. Evidently, the VMI retrieval
in Expelliarmus depends on the installation size of the imported
packages, which is highest in the case of DevOps toolbox VMI.

Fig. 6(b) compares the VMI retrieval time, which is fastest
for Hemera and Expelliarmus than Mirage. Expelliarmus’ better
performance is due to selective package retrieval and importing
into a VMI that significantly reduces the total size read from
the repository. Mirage’s VMI retrieval is worse because it re-
trieves more data by reading many files instead of reading linearly
through one file, and is also inefficient in reading small files (be-
low 1MB) from file system-based repository. Hemera improves
this overhead by taking a hybrid approach that stores large files
in the repository and small-sized files in the database, which
optimizes VMI retrieval as the database handles small files much

   

70

60 i Base image copy
4 a Libguestfs handler creation
36 50}-| VMI reset
UO
a 40 Ill Import
E
= 30
$ 20
v
Mtl | ll
~

0

1 4 8 12 16 20 24 28 32 36 40
VMI
(a) Expelliarmus breakdown.
@ Expelliarmus

3 200 || 4 Mirage
°
UO
a 160
vO
£ 120
va)
ry
3 80
w 40

 

20 24 28
VMI

32 36 40

(b) Retrieval time comparison.

Fig. 6. VMI retrieval in LARGE scenario (42 VMIs).

faster than the file system. Although Hemera performs slightly
worse than Expelliarmus for most VMIs, the retrieval time of
four VMIs (i.e. Lamp, Lemp, Kibana and ELK Stack) is considerably
different in both cases. While Expelliarmus retrieval takes 26.25 s,
25.64s, 30.82s, and 34.30s for four VMIs, Hemera needs around
three times longer (i.e. 78.22 s, 76.09 s, 86.58 s, and 106.86s) due
to increasingly large number of files (more than 100 thousand)
and VMI mounted size.

6.5. Scalability

To study the scalability of Expelliarmus, we used additional
500 VMIs derived through cloning and sprawling operations from
the 42 VMIs of the LARGE scenario listed in Table 3 in two
experiments. The first experiment studied the VMI retrieval with
the growing repository size, and the second explored the perfor-
mance of parallel VMI retrieval. The scalability study ignored VMI
publishing for two reasons. Firstly, VMI retrieval is the critical
performance measure amongst the two operations for efficient
and scalable provisioning of time-critical services [21]. Secondly,
Expelliarmus does not support synchronized IO write operations
for multiple concurrent VMI uploads (part of future work).

Fig. 7(a) shows the retrieval time for four VMIs (i.e. DevOps
Toolbox, Virtual Box, IDE, Neo4J ) in Expelliarmus with the grow-
ing repository size in Qcow2 format. We selected these four
VMIs due to their highest retrieval times amongst the 42 VMIs
according to Fig. 6(b). In general, the increase in data repository
size produces higher retrieval times inducing higher latency for
disk IO operations [21]. Contrarily, VMI retrieval in Expelliarmus
slightly changes by a few seconds with the growing number of
VMIs and corresponding repository size. While the repository size
exponentially grows to 1.2TB in Qcow2 format for 500 VMIs,
Expelliarmus’ repository size grows up to 3.84GB and requires
118 N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

 

 

100
—e— Virtual Box

<3 90 |] —e— Neo4J
3 —— IDE
Oo 80
Y —&— DevOps Toolbox
oO
E 70
Y
= 60
>
ao
6 50
oO
a

40

100 200 400 600 800 1,000 1,200
Cumulative repository size in Qcow2 format (GB)
(a) Retrieval time with increasing repository size.

1,200
@ 1,000
me}
Cc
°
2 800
2
cy)
£ 600
Y
S 400
vu
5
we 200

0

1 2 5 10 20 30 40

Number of processor

(b) LARGE parallel VMI retrieval (42 VMIs).

Fig. 7. Expelliarmus scalability analysis.

storing a fraction of updates with every additional VMI (see
Section 6.2). Hence, leads to a moderate variance in VMI retrieval
time across the large repository.

We study the performance of parallel VMI retrieval for provi-
sioning a heterogeneous VM cluster with diverse software pack-
ages. We retrieved the 42 VMIs from the LARGE VMI set intro-
duced in Section 6.1, in a repository with more than 500 VMIs
of 1.2 TB in total. We parallelized the retrieval on an increasing
number of processes, such that on p processes, every process

assembled ~ 2 | VMIs. Fig. 7(b) shows that the parallel retrieval

time of the 42 VMIs decreases almost linearly until 10 processors.
Assembling 42 VMIs on more than 10 processes that retrieve
software packages in parallel generates an IO bottleneck on the
SSD disk (see Table 2) and brings no more benefit to the re-
trieval process. This demonstrates that the parallel VMI retrieval
depends on the storage disk performance, including storage disk
access patterns [1] that limit its scalability.

7. Conclusion

We introduced Expelliarmus, a new VMI management sys-
tem with a semantic-centric design for VMI storage with op-
timized VMI publish and retrieval. Different from the existing
VMI management systems that ignore VMI semantics, Expel-
liarmus incorporates three features. First, it represents VMIs as
structured semantic graphs, efficiently expressing the functional
requirements between the base image and the different soft-
ware packages. Such an approach allows clustering multiple VMIs
into a single master graph that expedites similarity computation.
Second, Expelliarmus enables a semantic-aware VMI decompo-
sition and base image selection that extracts and subsequently

stores non-redundant base image and software packages only.
Third, Expelliarmus performs VMI assembly on-the-fly, either
by fetching initially uploaded software packages or by selecting
compatible and semantically similar packages already existing
in the repository. We evaluated Expelliarmus over a represen-
tative set of synthetic Cloud VMIs on a real testbed. Results
show that semantic-aware management of VMIs is able to re-
duce the repository size by 2.3 — 22 times compared to three
related systems, with significant VMI publishing and slight re-
trieval performance improvement. Currently, Expelliarmus sup-
ports Linux VMIs, while managing Windows VMIs is part of our
future work. We also plan to extend Expelliarmus to support au-
tomated containerization of VMIs with multiple container service
functionalities.

CRediT authorship contribution statement

Nishant Saurabh: Conception and design of study, Acquisition
of data, Analysis and/or interpretation of data, Writing - original
draft, Writing - review & editing. Shajulin Benedict: Writing
- original draft, Writing - review & editing. Jorge G. Barbosa:
Conception and design of study, Analysis and/or interpretation of
data, Writing - review & editing. Radu Prodan: Conception and
design of study, Analysis and/or interpretation of data, Writing -
review & editing.

Declaration of competing interest

The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared
to influence the work reported in this paper.

Acknowledgment

This work received funding from:

(1) European Union’s Horizon 2020 research and innovation
programme, grant agreement 825134, “Smart Social Media
Ecosystem in a Blockchain Federated Environment (ARTI-
CONF)”;

(2) Austrian Agency for International Cooperation in Education
and Research (OeAD-GmbH) and Indian Department of Sci-
ence and Technology (DST), project number, IN 20/2018,
“Energy Aware Workflow Compiler for Future Heteroge-
neous Systems”. All authors approved the version of the
manuscript to be published.

Appendix. Artifact description

We present in this section the reproducibility artifact of our
implementation and experimental validation.

A.1. Release

We prototyped Expelliarmus in Python with an easy command
line interface released on GitHub. !°

A.2. Hardware dependencies
We implemented and tested Expelliarmus on an Intel (R)
Xeon(R) Gold 5218 server @ 2.30GHz with 384GB of DDR4

memory and 9.6TB of SSD storage. The software, however, is
portable and has no hardware dependencies.

10 https://github.com/ExpelliarmusSuperComp/Expelliarmus
N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 119

A.3. Software dependencies

We tested Expelliarmus on an Ubuntu 18.04 LTS system,
however, it runs on any Linux-based environment. The software
dependencies include libguestfs, qemu, and sqlite.

A.4. Data sets

We created each VMI using virt-builder, an efficient tool
to build a variety of images for local and Cloud use. The minimal
script to create a VMI is listed below:

#!/bin/bash

# path to libuguestfs

../../libguestfs-1.36.13/run virt-builder ubuntu-16.04 \

-o Image.qcow2 \

--size 4G \

--format qcow2 \

--root-password password:123 \

--edit '/etc/default/grub:s/

« “GRUB_CMDLINE_LINUX_DEFAULT=

« .*/GRUB_CMDLINE_LINUX_DEFAULT= "console=ttyQ@

«+ console=ttyS@,115200n8"/' \

--run-command update-grub \

--install "dpkg-repack, fakeroot,dpkg-dev” \

--update \

--write /etc/default/locale: $”LANG=\"en_US.UTF-8\"" \

--append /etc/default/locale: $”LANGUAGE=\"en_US\"" \

--firstboot-command ‘userdel -r builder && useradd -m -p
"" builder ; chage -d @ builder'

We further shrank the created VMIs to remove the allocated
space not used for storage. Initially, we executed following com-
mands on the guest OS:

$ dd if=/dev/zero of=/mytempfile
$ rm -f /mytempfile

Furthermore, we used the following commands to shrink the
VMI disk use without compression:

$ mv Image.qcow2 Image.qcow2_backup
$ qemu-img convert -O qcow2 Image.qcow2_backup
so Image.qcow2

A.5. Expelliarmus installation

We tested the following installation steps on Ubuntu 18.04
LTS, however, Expelliarmus can be installed on any Linux envi-
ronment with manual build for libguestfs.

A.5.1. Requirements
1. Python version greater than 2.7;
2. Python newtorkx module;
3. libguestfs-tools module version 1.36.x or higher;
4. python-guestfs module.

A.5.2. Installation steps

$ mkdir Expelliarmus
$ cd Expelliarmus
$ git clone https://github.com/
s ExpelliarmusSuperComp/Expelliarmus
cd ../../
apt install libguestfs-tools
apt install libguestfs-dev
apt install python-guestfs

FAFA Ff Ff FA

wget" download. libguestfs.org/1.36
s -Stable/libguestfs-1.36.13.tar.gz”
tar -xf libguestfs-1.36.13.tar.gz
cd libguestfs-1.36.13
apt-get build-dep libguestfs
apt install autoconf automake libtool-bin gettext
./configure
make
cd ../Expelliarmus/Expelliarmus/
python main. py
Please provide path to libguestfs:

FAFA FFA FFF HF fF

Expelliarmus ready to use:

A.5.3. Troubleshooting
1. Error: “libguestfs: error: tar_in”

$ Echo dash >/user/lib /x86_64-linux-gnu
s /guestfs/supermin.d /zz-dash-packages

2. Error: “suprmin exited with status 1”

$ chmod 0644 /boot/vmlinuzx

References

[1] G. Ammons, V. Bala, T. Mummert, D. Reimer, X. Zhang, Virtual machine
images as structured data: The mirage image library, in: Proceedings of the
3rd USENIX Conference on Hot Topics in Cloud Computing, HotCloud’11,
USENIX Association, Berkeley, CA, USA, 2011, p. 22, URL: http://dl.acm.org/
citation.cfm?id=2170444.2 170466.
P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer, I.
Pratt, A. Warfield, Xen and the art of virtualization, in: Proceedings of the
Nineteenth ACM Symposium on Operating Systems Principles, SOSP ’03,
ACM, New York, NY, USA, 2003, pp. 164-177, http://dx.doi.org/10.1145/
945445.945462, URL: http://doi.acm.org/10.1145/945445.945462.
B. Beach, Pro PowerShell for Amazon Web Services: DevOps for the AWS
Cloud, first ed., Apress, Berkely, CA, USA, 2014.
A.Z. Broder, Identifying and filtering near-duplicate documents, in: Pro-
ceedings of the 11th Annual Symposium on Combinatorial Pattern
Matching, COM ’00, Springer-Verlag, Berlin, Heidelberg, 2000, pp. 1-10,
URL: http://dl.acm.org/citation.cfm?id=6478 19.736184.
R. Buyya, C. Yeo, S. Venugopal, J. Broberg, I. Brandic, Cloud computing and
emerging IT platforms: Vision, hype, and reality for delivering computing
as the 5th utility, Future Gener. Comput. Syst. 25 (6) (2009) 599-616,
http://dx.doi.org/10.1016/j.future.2008.12.001.
D. Campello, C. Crespo, A. Verma, R. Rangaswami, P. Jayachandran, Coriolis:
Scalable VM clustering in clouds, in: Proceedings of the 10th Interna-
tional Conference on Autonomic Computing, ICAC 13, USENIX, San Jose,
CA, 2013, pp. 101-105, URL: https://www.usenix.org/conference/icac13/
technical-sessions/presentation/campello.
D. Cherry, Securing SQL Server: Protecting Your Database from Attackers,
third ed., Syngress Publishing, 2015.
T.C. Chieu, A. Mohindra, A.A. Karve, A. Segal, Dynamic scaling of web
applications in a virtualized cloud computing environment, in: 2009 IEEE
International Conference on E-Business Engineering, 2009, pp. 281-286,
http://dx.doi.org/10.1109/ICEBE.2009.45.
[9] C. Constantinescu, J. Glider, D. Chambliss, Mixing deduplication and com-
pression on active data sets, in: 2011 Data Compression Conference, 2011,
pp. 393-402, http://dx.doi.org/10.1109/DCC.201 1.46.

[2

—

[3

—

[4

—

[5

—

[6

—

[7

—

[8

—
120

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121

RJ. Creasy, The origin of the VM/370 time-sharing system, IBM J. Res. Dev.
25 (5) (1981) 483-490, http://dx.doi.org/10.1147/rd.255.0483.

A.V. Dastjerdi, R. Buyya, Fog computing: Helping the internet of things
realize its potential, Computer 49 (8) (2016) 112-116, http://dx.doi.org/
10.1109/MC.2016.245,

W. Felter, A. Ferreira, R. Rajamony, J. Rubio, An updated performance
comparison of virtual machines and Linux containers, in: 2015 IEEE In-
ternational Symposium on Performance Analysis of Systems and Software,
ISPASS, 2015, pp. 171-172, http://dx.doi.org/10.1109/ISPASS.2015.7095802.
JJ. Hunt, K.-P. Vo, W.F. Tichy, An empirical study of delta algorithms, in:
Proceedings of the SCM-6 Workshop on System Configuration Manage-
ment, ICSE 96, Springer-Verlag, Berlin, Heidelberg, 1996, pp. 49-66, URL:
http://dl.acm.org/citation.cfm?id=647 175.7 16407.

P. Jaccard, The distribution of the flora in the alpine zone, New Phytol. 11
(2) (1912) 37-50, URL: http://www.jstor.org/stable/2427226?seq=3.

K. Jayaram, C. Peng, Z. Zhang, M. Kim, H. Chen, H. Lei, An empirical analysis
of similarity in virtual machine images, in: Proceedings of the Middleware
2011 Industry Track Workshop, Middleware ’11, ACM, New York, NY,
USA, 2011, pp. 6:1-6:6, http://dx.doi.org/10.1145/2090181.2090187, URL:
http://doi.acm.org/10.1145/209018 1.2090187.

K. Jin, E. Miller, The effectiveness of deduplication on virtual machine
disk images, in: Proceedings of SYSTOR 2009: The Israeli Experimental
Systems Conference, SYSTOR ’09, ACM, New York, NY, USA, 2009, pp. 7:1-
7:12, http://dx.doi.org/10.1145/1534530.1534540, URL: http://doi.acm.org/
10.1145/1534530.1534540.

R. Kaur, I. Chana, J. Bhattacharya, Data deduplication techniques for
efficient cloud storage management: A systematic review, J. Supercomput.
74 (5) (2018) 2035-2085, http://dx.doi.org/10.1007/s11227-017-2210-8.
A. Kochut, A. Karve, B. Nicolae, Towards efficient on-demand VM pro-
visioning: Study of VM runtime I/O access patterns to shared image
content, in: 2015 IFIP/IEEE International Symposium on Integrated Net-
work Management, IM, 2015, pp. 321-329, http://dx.doi.org/10.1109/INM.
2015.7 140307.

A. Liguori, E. Hensbergen, Experiences with content addressable storage
and virtual disks, in: Proceedings of the First Conference on I/O Virtual-
ization, WIOV’08, USENIX Association, Berkeley, CA, USA, 2008, p. 5, URL:
http://dl.acm.org/citation.cfm?id=1855865.1855870.

X. Lin, M. Hibler, E. Eide, R. Ricci, Using deduplicating storage for efficient
disk image deployment, in: TRIDENTCOM, 2015, http://dx.doi.org/10.4108/
icst.tridentcom.2015.259963.

H. Liu, B. He, X. Liao, H. Jin, Towards declarative and data-centric
virtual machine image management in IaaS clouds, IEEE Trans. Cloud
Comput. (2019) 1, http://dx.doi.org/10.1109/TCC.2017.2728066, URL: doi.
ieeecomputersociety.org/10.1109/TCC.2017.2728066.

M. Liu, C. Yang, Q. Jiang, X. Chen, J. Ma, J. Ren, Updatable block-level
deduplication with dynamic ownership management on encrypted data,
in: 2018 IEEE International Conference on Communications, ICC, 2018, pp.
1-7, http://dx.doi.org/10.1109/ICC.2018.8422446.

F. Longo, R. Ghosh, V.K. Naik, K.S. Trivedi, A scalable availability model
for infrastructure-as-a-Service cloud, in: 2011 IEEE/IFIP 41st International
Conference on Dependable Systems Networks, DSN, 2011, pp. 335-346,
http://dx.doi.org/10.1109/DSN.2011.5958247,.

G. Lu, Y. Jin, D.H.C. Du, Frequency based chunking for data de-duplication,
in: 2010 JEEE International Symposium on Modeling, Analysis and Simu-
lation of Computer and Telecommunication Systems, 2010, pp. 287-296,
http://dx.doi.org/10.1109/MASCOTS.2010.37.

N. Mandagere, P. Zhou, M. Smith, S. Uttamchandani, Demystifying data
deduplication, in: Proceedings of the ACM/IFIP/USENIX Middleware ’08
Conference Companion, Companion ’08, ACM, New York, NY, USA, 2008,
pp. 12-17, http://dx.doi.org/10.1145/1462735.1462739, URL: http://doi.
acm.org/10.1145/1462735.1462739.

D. Meister, J. Kaiser, A. Brinkmann, T. Cortes, M. Kuhn, J. Kunkel, A study
on data deduplication in HPC storage systems, in: Proceedings of the
International Conference on High Performance Computing, Networking,
Storage and Analysis, SC 12, IEEE Computer Society Press, Los Alamitos, CA,
USA, 2012, pp. 7:1-7:11, URL: http://dl.acm.org/citation.cfm?id=2388996.
2389006.

YJ. Nam, D. Park, D.H.C. Du, Assuring demanded read performance of data
deduplication storage with backup datasets, in: 2012 IEEE 20th Interna-
tional Symposium on Modeling, Analysis and Simulation of Computer and
Telecommunication Systems, 2012, pp. 201-208, http://dx.doi.org/10.1109/
MASCOTS.2012.32.

C.H. Ng, P. Lee, RevDedup: A reverse deduplication storage system
optimized for reads to latest backups, in: Proceedings of the 4th Asia-
Pacific Workshop on Systems, APSys ’13, ACM, New York, NY, USA, 2013,
pp. 15:1-15:7, http://dx.doi.org/10.1145/2500727.2500731, URL: http://doi.
acm.org/10.1145/2500727.2500731.

C.H. Ng, M. Ma, T.Y. Wong, P. Lee, J. Lui, Live deduplication storage
of virtual machine images in an open-source cloud, in: Proceedings
of the 12th ACM/IFIP/USENIX International Conference on Middleware,
Middleware’11, Springer-Verlag, Berlin, Heidelberg, 2011, pp. 81-100, http:
//dx.doi.org/10.1007/978-3-642-25821-3_5.

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

B. Nicolae, A. Kochut, A. Karve, Discovering and leveraging content similar-
ity to optimize collective on-demand data access to iaas cloud storage, in:
2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing, 2015, pp. 211-220, http://dx.doi.org/10.1109/CCGrid.2015.156.
M. Rabin, Fingerprinting by Random Polynomials, Center for Research in
Computing Technology: Center for Research in Computing Technology.
Center for Research in Computing Techn., Aiken Computation Laboratory,
Univ., 1981, URL: https://books.google.at/books?id=Emu_tgAACAA].

K. Razavi, T. Kielmann, Scalable virtual machine deployment using VM
image caches, in: Proceedings of the International Conference on High Per-
formance Computing, Networking, Storage and Analysis, SC ’13, ACM, New
York, NY, USA, 2013, pp. 65:1-65:12, http://dx.doi.org/10.1145/25032 10.
2503274, URL: http://doi.acm.org/10.1145/2503210.2503274.

D. Reimer, A. Thomas, G. Ammons, T. Mummert, B. Alpern, V. Bala, Opening
black boxes: Using semantic information to combat virtual machine image
sprawl, in: Proceedings of the Fourth ACM SIGPLAN/SIGOPS International
Conference on Virtual Execution Environments, VEE ’08, ACM, New York,
NY, USA, 2008, pp. 111-120, http://dx.doi.org/10.1145/1346256.1346272,
URL: http://doi.acm.org/10.1145/1346256.1346272.

J. Sahoo, S. Mohapatra, R. Lath, Virtualization: A survey on concepts,
taxonomy and associated security issues, in: 2010 Second International
Conference on Computer and Network Technology, 2010, pp. 222-226,
http://dx.doi.org/10.1109/ICCNT.2010.49.

N. Saurabh, D. Kimovski, S. Ostermann, R. Prodan, VM image repository
and distribution models for federated clouds: State of the art, possible
directions and open issues, in: Euro-Par Workshops, in: Lecture Notes in
Computer Science, vol. 10104, Springer, 2016, pp. 260-271.

N. Saurabh, J. Remmers, D. Kimovski, R. Prodan, J.G. Barbosa, Semantics-
aware virtual machine image management in IaaS clouds, in: 2019 IEEE
International Parallel and Distributed Processing Symposium, IPDPS, 2019,
pp. 418-427, http://dx.doi.org/10.1109/IPDPS.2019.00052.

G. Schulz, Cloud and Virtual Data Storage Networking,
Publications, Boston, MA, USA, 2011.

W. Shi, S. Dustdar, The promise of edge computing, Computer 49 (5) (2016)
78-81, http://dx.doi.org/10.1109/MC.2016.145.

J.E. Smith, R. Nair, The architecture of virtual machines, Computer 38 (5)
(2005) 32-38.

C. Sun, L. He, Q. Wang, R. Willenborg, Simplifying service deployment
with virtual appliances, in: 2008 IEEE International Conference on Services
Computing, Vol. 2, 2008, pp. 265-272, http://dx.doi.org/10.1109/SCC.2008.
53.

L.M. Vaquero, L. Rodero-Merino, R. Buyya, Dynamically scaling appli-
cations in the cloud, SIGCOMM Comput. Commun. Rev. 41 (1) (2011)
45-52, http://dx.doi.org/10.1145/1925861.1925869, URL: http://doi.acm.
org/10.1145/1925861.1925869.

B. Varghese, R. Buyya, Next generation cloud computing: New trends and
research directions, 2017, arXiv:1707.07452, CoRR abs/1707.07452. URL:
http://arxiv.org/abs/1707.07452.

J. Xu, W. Zhang, S. Ye, J. Wei, T. Huang, A lightweight virtual machine
image deduplication backup approach in cloud environment, in: 2014 IEEE
38th Annual Computer Software and Applications Conference, 2014, pp.
503-508, http://dx.doi.org/10.1109/COMPSAC.20 14.73.

J. Xu, W. Zhang, Z. Zhang, T. Wang, T. Huang, Clustering-based acceleration
for virtual machine image deduplication in the cloud environment, J. Syst.
Softw. 121 (C) (2016) 144-156, http://dx.doi.org/10.1016/j.jss.2016.02.021.
M. Xu, Y. Zhu, P.P.C. Lee, Y. Xu, Even data placement for load balance
in reliable distributed deduplication storage systems, in: 2015 IEEE 23rd
International Symposium on Quality of Service, IWQoS, 2015, pp. 349-358,
http://dx.doi.org/10.1109/IWQoS.2015.7404754.

Z. Zhao, P. Martin, J. Wang, A. Taal, A. Jones, I. Taylor, V. Stankovski,
I1.G. Vega, G. Suciu, A. Ulisses, C. de Laat, Developing and operating time
critical applications in clouds: The state of the art and the SWITCH ap-
proach, Procedia Comput. Sci. 68 (2015) 17-28, http://dx.doi.org/10.1016/
j.procs.2015.09.220, URL: http://www.sciencedirect.com/science/article/pii/
S1877050915030653. 1st International Conference on Cloud Forward:
From Distributed to Complete Computing.

Z. Zhao, A. Taal, A. Jones, I. Taylor, V. Stankovski, I.G. Vega, F.J. Hidalgo, G.
Suciu, A. Ulisses, P. Ferreira, C. d. Laat, A software workbench for inter-
active, time critical and highly self-adaptive cloud applications (SWITCH),
in: 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and
Grid Computing, 2015, pp. 1181-1184, http://dx.doi.org/10.1109/CCGrid.
2015.73.

X. Zhao, Y. Zhang, Y. Wu, K. Chen, J. Jiang, K. Li, Liquid: A scalable
deduplication file system for virtual machine images, IEEE Trans. Parallel
Distrib. Syst. 25 (5) (2014) 1257-1266, http://dx.doi.org/10.1109/TPDS.
2013.173.

B. Zhu, K. Li, H. Patterson, Avoiding the disk bottleneck in the data domain
deduplication file system, in: Proceedings of the 6th USENIX Conference
on File and Storage Technologies, FAST’08, USENIX Association, Berkeley,
CA, USA, 2008, pp. 18:1-18:14, URL: http://dl.acm.org/citation.cfm?id=
13648 13.1364831.

Auerbach
 

 

N. Saurabh et al. / Journal of Parallel and Distributed Computing 146 (2020) 107-121 121

Nishant Saurabh is a research assistant at Univer-
sity of Klagenfurt, Austria and pursuing his Ph.D. at
University of Innsbruck, Austria since 2015. Prior to
start of Ph.D., he obtained his MSc. degree special-
ized in High Performance Distributed Computing from
Vrije Universiteit, Netherlands; and received his B.E
degree specialized in computer science and engineering
from PRMITR affiliated to SGBAU University, India.
His research areas include resource management and
placement optimization in the field of parallel and
distributed systems.

Shajulin Benedict graduated from Manonmaniam Sun-
deranar University, India, in 2001; received his M.E
Degree in Digital Communication and Computer Net-
working from A.K.C.E, Anna University, India in 2004;
and got his Ph.D degree in the area of Grid scheduling
under Anna University, India. He served as Professor
at SXCCE Research Centre of Anna University, India.
Currently, he works at the Indian Institute of Informa-
tion Technology Kottayam, Kerala, India. His research
interests include compilers, HPC, Cloud, Grid scheduling
and performance analysis of exascale applications.

 

Jorge G. Barbosa received his BSc degree in Electrical
and Computer Engineering from Faculty of Engineering
of the University of Porto (FEUP), Portugal; MSc in
Digital Systems from University of Manchester Institute
of Science and Technology, England, in 1993, and PhD
in Electrical and Computer Engineering from FEUP,
Portugal, in 2001. Since 2001 he is an Assistant Pro-
fessor at FEUP. His research interests include parallel
and distributed computing, heterogeneous computing,
scheduling in heterogeneous environments and cloud
computing.

Radu Prodan is professor in distributed systems at
the Institute of Software Technology, University of
Klagenfurt. He received his PhD in 2004 from the
Vienna University of Technology and was Associate
Professor until 2018 at the University of Innsbruck,
Austria. His research interests include performance, op-
timization, and resource management tools for parallel
and distributed applications. He authored over 100
publications and received two IEEE best paper awards.
