Al & SOCIETY (2020) 35:957-967
https://doi.org/10.1007/s00146-020-00950-y

OPEN FORUM

-)

Check for
updates

Recommender systems and their ethical challenges

Silvia Milano’ - Mariarosaria Taddeo'” - Luciano Floridi'

Received: 13 October 2019 / Accepted: 24 January 2020 / Published online: 27 February 2020

© The Author(s) 2020

Abstract

This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature
review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical
impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a
variety of other stakeholders—as opposed to just the receivers of a recommendation—1in assessing the ethical impacts of a

recommender system.

Keywords Algorithms - Artificial intelligence - Digital ethics - Ethical trade-offs - Ethics of recommendation - Machine

learning - Recommender systems

1 Introduction

We interact with recommender (or recommendation) sys-
tems (RS) on a regular basis, when we use digital services
and apps, from Amazon to Netflix and news aggregators.
They are algorithms that make suggestions about what a
user may like, such as a specific movie. Slightly more for-
mally, they are functions that take information about a user’s
preferences (e.g. about movies) as an input, and output a
prediction about the rating that a user would give of the
items under evaluation (e.g., new movies available), and
predict how they would rank a set of items individually or
as a bundle. We shall say more about the nature of recom-
mender systems in the following pages, but even this general
description suffices to clarify that, to work effectively and
efficiently, recommender systems collect, curate, and act
upon vast amounts of personal data. Inevitably, they end up
shaping individual experience of digital environments and
social interactions (Burr et al. 2018; de Vries 2010; Karimi
et al. 2018).

RS are ubiquitous and there is already much technical
research about how to develop ever more efficient systems

><] Silvia Milano
silvia.milano @ oi1.0x.ac.uk

Oxford Internet Institute, University of Oxford, 1 St Giles,
Oxford OX1 3JS, UK

* The Alan Turing Institute, 96 Euston Road,
London NW1 2DB, UK

(Adomavicius and Tuzhilin 2005; Jannach and Adomavicius
2016; Ricci et al. 2015). In the past 20 years, RS have been
developed focusing mostly on business applications. Even
if researchers often adopt a user-centred approach focusing
on preference prediction, it is evident that the applications
of RS have been driven by online commerce and services,
where the emphasis has tended to be on commercial objec-
tives. But RS have a wider impact on users and on society
more broadly. After all, they shape user preferences and
guide choices, both individually and socially. This impact
is significant and deserves ethical scrutiny, not least because
RS can also be deployed in contexts that are morally loaded,
such as health care, lifestyle, insurance, and the labour mar-
ket. Clearly, whatever the ethical issues may be, they need
to be understood and addressed by evaluating the design,
deployment and use of the recommender systems, and the
trade-offs between the different interests at stake. A failure
to do so may lead to opportunity costs as well as problems
that could otherwise be mitigated or avoided altogether, and,
in turn, to public distrust and backlash against the use of RS
in general (Koene et al. 2015).

Research into the ethical issues posed by RS is still in its
infancy. The debate is also fragmented across different scien-
tific communities, as it tends to focus on specific aspects and
applications of these systems in a variety of contexts. The
current fragmentation of the debate may be due to two main
factors: the relative newness of the technology, which took
off with the spread of internet-based services and the intro-
duction of collaborative filtering techniques in the 1990s

Q) Springer
958

(Adomavicius and Tuzhilin 2005; Pennock et al. 2000); and
the proprietary and privacy issues involved in the design and
deployment of this class of algorithms. The details of RS
currently in operation are treated as highly guarded indus-
trial secrets. This makes it difficult for independent research-
ers to access information about their internal operations, and
hence provide any evidence-based assessment. In the same
vein, due to privacy concerns, providers of recommendation
systems may be reluctant to share information that could
compromise their users’ personal data (Paraschakis 2018).
Against this background, this article addresses both prob-
lems (infancy and fragmentation), by providing a survey
of the current state of the literature, and by proposing an
overarching framework to situate the contributions to the
debate. The overall goal is to reconstruct the whole debate,
understand its main issues, and hence offer a starting point
for better ways of designing RS and regulating their use.

2 Aworking definition of recommender
systems

The task of a recommendation system—1i.e. what we shall
call the recommendation problem—is often summarised as
that of finding good items (Jannach and Adomavicius 2016).
This description is common and popular among practition-
ers, especially in the context of e-commerce applications.
However, it is too broad and not very helpful for research
purposes. To make it operational, one needs to specify,
among other things, three parameters:

(a) what the space of options 1s;

(b) what counts as a good recommendation; and, impor-
tantly

(c) how the RS’s performance can be evaluated.

Specifying these parameter choices is highly depend-
ent on the domain of application and the level of abstrac-
tion [LoAs, see (Floridi 2016)]! from which the problem
is considered (Jannach et al. 2012). Typically, the literature
implements three LoAs: catalogue-based, decision support,
and multi-stakeholder environment. Let us consider each of
these in turn.

' A level of abstraction can be imagined as an interface that enables
one to observe some aspects of a s system analysed, while making
other aspects opaque or indeed invisible. For example, one may ana-
lyse a house at the LoA of a buyer, of an architect, of a city plan-
ner, of a plumber, and so on. LoAs are common in computer science,
where systems are described at different LoAs (computational, hard-
ware, user-centred, etc.). LoAs can be combined in more complex
sets, and can be, but are not necessarily always, hierarchical.

Q) Springer

Al & SOCIETY (2020) 35:957-967

In e-commerce applications, the space of options (that is,
the observables selected by the LoA) may be the items in the
catalogue, while a good recommendation may be specified
as one which ultimately results in a purchase. To evaluate the
system performance, one may compare the RS’s predictions
to the actual user behaviour after a recommendation is made.
In the domain of news recommendations, a good recom-
mendation may be defined as a news item that is relevant to
the user (Floridi 2008), and one may use click-through rates
as a proxy to evaluate the accuracy of the system’s recom-
mendations. Similar RS are designed to develop a model of
individual users and to use it to predict the users’ feedback
on the system’s recommendation, which is essentially a pre-
diction problem.

Taking a different LoA, RS may also be considered to
provide decision support to their users. For example, an
online booking RS may be designed to facilitate the user’s
choice of hotel options. In this case, defining what counts
as a good recommendation is more complex, because it
involves appreciation of the user’s goals and decision-mak-
ing abilities. Evaluating the system’s performance as a deci-
sion support requires more elaborate metrics. For example,
Jameson et al. (2015) consider six strategies for generat-
ing recommendations, which track different choice patterns
based on either of the following features: (1) the attributes
of the options; (2) the expected consequences of choosing an
option; (3) prior experience with similar options; (4) social
pressure or social information about the options; (5) follow-
ing a specific policy; (6) trial-and-error-based choice.

More recently, Abdollahpouri et al. (2017) have proposed
a different kind of LoA (our terminology), defining RS in
terms of multi-stakeholder environments (what we would
call the LoA’s observables), where multiple parties (includ-
ing users, providers, and system administrators) can derive
different utilities from recommendations. Epistemologically,
this approach is helpful because it enables one to conceptu-
alise explicitly the impact that RS have at different levels,
both on the individual users interacting with them, and on
society more broadly, making it possible to articulate what
ethical trade-offs could be made between these different,
possibly competing interests. Figure | presents a diagram
of the stakeholders ina RS.

In view of the previous LoAs, and for the purposes of this
article, we take recommender systems to be a class of algo-
rithms that address the recommendation problem using a
content-based or collaborative filtering approach, or a com-
bination thereof. The technical landscape on RS is of course
more vast. However, our choice is primarily motivated by
our finding that the existing literature on the ethical aspects
of RS focuses on these types of systems. Furthermore, the
choice offers three advantages. It is compatible with the most
common LoAs we have listed above. By focusing on the
algorithmic nature of recommender systems, it also singles
Al & SOCIETY (2020) 35:957-967

Fig.1 Stakeholders in a RS

e Make items available

e Have an interest in
their items being

recommended and
picked by users

Providers

out one of the fastest growing areas of research and applica-
tions for machine learning. And it enables us to narrow down
the scope of the study, as we shall not consider systems
that approach the recommendation problem using different
techniques, such as, for instance, knowledge-based systems.
With these advantages in mind, in the next section, we pro-
pose a general taxonomy to identify the ethical challenges
of RS. In Sect. 4, we review the current literature, structured
around six areas of concern. We conclude in Sect. 5 by map-
ping the discussion onto our ethical taxonomy and indicating
the direction of our further work in the area.

3 Howto map the ethical challenges posed
by recommender systems

To identify what is ethically at stake in the design and
deployment of RS, let us start with a formal taxonomy. This
is how we propose to design it.

The question about which moral principles may be cor-
rect is deeply contentious and debated in philosophy. Fortu-
nately, in this article, we do not have to take a side because
all we need is a distinction about which there is a general
consensus; there are at least two classes of variables that
are morally relevant, actions and consequences. Of course,
other things could also be morally relevant, in particular
intentions. However, for our purposes, the aforementioned
distinction is all we need, so we shall assume that a recom-
mender system’s behaviour and impact will suffice to pro-
vide a clear understanding of what is ethically at stake.

The value of some consequences is often measured in
terms of the utility they contain. So, it is reasonable to

 

959

e Receive and act upon

recommendations
¢ Make

. e Have an interest in
recommendations

receiving “good”
recommendations

Users

¢ Have an interest in
keeping the system
working

>A

f. >

| ¢ Systemic effects
and possible
externalities

assume that any aspect of a RS that could impact nega-
tively the utility of any of its stakeholders, or risk imposing
such negative impacts, constitutes a feature that is ethically
relevant.

While the concept of utility can be made operational
using quantifiable metrics, rights are usually taken to pro-
vide qualitative constraints on actions. Thinking in terms
of actions and consequences, we can identify two ways in
which a recommender system can have ethical impacts. First,
its operations can

 

 

(a) impact (negatively) the utility of any of its stakeholders;
and/or
(b) violate their rights.

Second, these two kinds of ethical impact may be imme-
diate—for example, a recommendation may be inaccurate,
leading to a decrease in utility for the user—or they may
expose the relevant parties to future risks. The ethics of risk
imposition is the subject of a growing philosophical litera-
ture, which highlights how most activities involve imposition
of risks (Hansson 2010; Hayenhjelm and Wolff 2012). In
the case of RS, for example, the risks may involve expos-
ing users to undue privacy violations by external actors, or
the exposure to potentially irrelevant or damaging content.
Exposure to risks of these sorts can constitute a wrong, even
if no adverse consequences actually materialise.”

* The idea that exposing someone to risks can constitute a wrong to
them, even if the adverse consequences fail to materialise, is familiar
from other contexts, e.g. medical ethics: for example, negligence in
treating a patient constitutes a wrong, even if the patient ultimately
recovers and does not suffer as a result of the negligence.

D) Springer
960

Table 1 Taxonomy of ethical issues of recommender systems

Immediate harm Exposure to risk

Utility Inaccurate recommendations

Rights

A/B testing (see Sect. 4.1)

Leaking of sensitive infor-
mation

Unfair treatment

Given the previous analysis, we may now categorise the
ethical issues caused by recommender systems along two
dimensions (see Table 1):

1. whether a (given feature of a) RS negatively impacts the
utility of some of its stakeholders or, instead, constitutes
a rights violation, which is not necessarily measured in
terms of utility; and

2. whether the negative impact constitutes an immediate
harm or it exposes the relevant party to future risk of
harm or rights violation.

Table 1 summarises our proposed taxonomy, including
some examples of different types of ethical impacts of rec-
ommender systems, to be discussed in Sect. 5.

With the help of this taxonomy we are now ready to
review the contributions provided by the current literature.
We shall offer a general discussion of our findings in the
conclusion.

4 The ethical challenges of recommender
systems

The literature addressing the ethical challenges posed by RS
is Sparse, with the discussion of specific issues often linked
to a specific instance of a RS, and is fragmented across dis-
ciplinary divides. Through a multidisciplinary, comparative
meta-analysis, we identified six main areas of ethical con-
cerns (see “Appendix” for our methodology). They often
overlap but, for the sake of clarity, we shall analyse them
separately in the rest of this section.

4.1 Inappropriate content

Only a handful of studies to date address explicitly the eth-
ics of RS as a specific issue in itself. Earlier work on the
question of ethical recommendations focuses more on the
content of the recommendations, and proposes ways to fil-
ter the items recommended by the system on the basis of
cultural and ethical preferences. Four studies are particu-
larly relevant. Souali et al. (2011) consider the issue of RSs
that are not culturally appropriate, and propose an “ethical
database’, constructed on the basis of what are taken to be
a region’s generally accepted cultural norms, which act as

Q) Springer

Al & SOCIETY (2020) 35:957-967

a filter for the recommendations. Tang and Winoto (2016)
take a more dynamic approach to the issue, proposing a two-
layer RS, comprising a user-adjustable “ethical filter” that
screens the items that can be recommended based on the
user’s specified ethical preferences. Rodriguez and Watkins
(2009) adopt a more abstract approach to the problem of eth-
ical recommendations, proposing a vision for a eudaimonic
RS, whose purpose is to “produce societies in which the
individuals experience satisfaction through a deep engage-
ment in the world”. This, the authors predict, could be made
achievable through the use of interlinked big data structures.

Finally, Paraschakis (2016, 2017, 2018) provides one of
the most detailed accounts. Focusing on e-commerce appli-
cations, Paraschakis suggests that there are five ethically
problematic areas:

the practices of user profiling,

data publishing,

algorithm design,

user interface design, and

online experimentation or A/B testing, 1.e. the practice
of exposing selected groups of users to modifications of
the algorithm, with the aim of gathering feedback on the
effectiveness of each version from the user responses.

The risks he identifies relate to breaches of a user’s pri-
vacy (e.g. via data leaks, or by data gathering in the absence
of explicit consent), anonymity breaches, behaviour manipu-
lation and bias in the recommendations given to the user,
content censorship, exposure to side effects, and unequal
treatment in A/B testing with a lack of user awareness, lead-
ing to a lack of trust. The solutions put forward in Parascha-
kis (2017) revolve around a user-centred design approach
(more on this in the next paragraph), introducing adjustable
tools for users to control explicitly the way in which RS use
their personal data, to filter out marketing biases or content
censorship, and to opt out of online experiments.

With the exception of Souali et al. (2011), who adopt
a recommendation filter based on geographically located
cultural norms, the solutions described in this section rely
on a user-centred approach. Recalling our taxonomy, they
try to minimise the negative impact on the user’s utility—
in particular, unwanted exposure to testing, and inaccurate
recommendations—and on the user’s rights, in particular,
recommendations that do not agree with the user’s values,
or expose them to privacy violations. However, user-cen-
tred solutions have significant shortcomings: they may not
transfer to other domains, they may be insufficient to protect
the user’s privacy, and they may result in inefficiency, for
example, impairing the system’s effectiveness in generat-
ing new recommendations, if enough users choose to opt
out of profile tracking or online testing. Moreover, users’
choice of parameters can reveal sensitive information about
Al & SOCIETY (2020) 35:957-967

the users themselves. For example, adding a filter to exclude
some kind of content gives away the information that the
user may find this content distressing, irrelevant, or in other
ways unacceptable. But above all, the main problem is that,
although user-centred solutions may foster the transparency
of recommender systems, they also shift the responsibility
and accountability for the protection of rights and utility
to the users. These points highlight how user-centred solu-
tions in general are challenged by their demanding nature, as
they may constitute a mere shift in responsibility when the
users are only nominally empowered but actually unable to
manage all the procedures needed to protect their interests.
This may, therefore, be an unfair shift since it places undue
burdens on the users, and is in any case problematic because
the effectiveness of these solutions varies with the level of
awareness and expertise of the users themselves, which
may lead to users experiencing different levels of protection
depending on their ability to control the technology.°

Implementing an “ethical filter” for a recommender sys-
tem, as suggested by Rodriguez and Watkins (2009), would
also be controversial in some applications, for example, if
it were used by a government to limit citizens’ ability to
access some politically sensitive contents. As for the eudai-
monic approach, this goes in the direction of designing a
recommender system that is an optimal decision support,
yet it seems practically unfeasible, and at least much more
research would be needed. Figuring out what is a “good
human life” is something that millennia of reflection have
not yet solved.

4.2 Privacy

User privacy is one of the primary challenges for recom-
mendation systems (Friedman et al. 2015; Koene et al. 2015;
Paraschakis 2018). This may be seen as inevitable, given
that a majority of the most commercially successful recom-
mender systems are based on hybrid or collaborative filter-
ing techniques, and work by constructing models of their
users to generate personalised recommendations. Privacy
risks occur in at least four stages. First, they can arise when
data are collected or shared without the user’s explicit con-
sent. Second, once data sets are stored, there is the further
risk that they may be leaked to external agents, or become
subject to de-anonymization attempts (Narayanan 2008). At
both stages, privacy breaches expose users to risks, which
may result in loss of utility (for example, if individual users
are targeted by malicious agents as a result), or in rights
violations (for example, if users’ private information is

> For a critical analysis of empowerment, see Jessica Morley and
Luciano Floridi (forthcoming), “Against Empowerment: How to
Reframe the Role of mHealth Tools in the Healthcare Ecosystem”.

961

utilised in ways that threaten their individual autonomy, see
Sect. 4.3). Third, and independently of how securely data
are collected and stored, privacy concerns also arise at the
stage of inferences that the system can (enable one to) draw
from the data. Users may not be aware of the nature of these
inferences, and they may object to this use of their personal
data if they were better informed. Privacy risks do not only
concern data collection because, for example, an external
agent observing the recommendation that the system gen-
erates for a given user may be able to infer some sensitive
information about the user (Friedman et al. 2015). Extend-
ing the notion of informed consent to the indirect inferences
from user recommendations appears difficult.* Finally, there
is also another subtle, but important, systemic issue regard-
ing privacy, which arises at the stage of collaborative filter-
ing: the system can construct a model of the user based on
the data it has gathered on other users’ interactions. In other
words, as long as enough users interact and share their data
with the system, the system may be able to construct a fairly
accurate profile even for those users about whom it has fewer
data. This indicates that it may not be feasible for individual
users to be shielded completely from the kinds of inferences
that the system may be able to draw about them. It could be
a positive feature in some domains, like medical research,
but it may also turn out to be problematic in other domains,
like recruitment or finance.

Current solutions to the privacy challenges intrinsic to
recommender systems (especially those based on collabo-
rative filtering techniques) fall into three broad categories,
covering architectures, algorithmic, and policy approaches
(Friedman et al. 2015). Privacy-enhancing architectures aim
to mitigate privacy risks by storing user data in separate and
decentralised databases, to minimise the risk of leaks. Algo-
rithmic solutions focus on using encryption to minimise the
risk that user data could be exploited by external agents for
unwarranted purposes. Policy approaches, including GDPR
legislation, introduce explicit guidelines and sanctions to
regulate data collection, use, and storage.

The user-centred recommendation framework proposed
by Paraschakis (2017), which we already encountered in the
previous section, also introduces explicit privacy controls,
letting the users decide whether their data can be shared,
and with whom. However, as we have already remarked,
user-centred approaches have limits, as they may constitute
a mere shift in responsibility, placing an undue burden on the
users. A possible issue that may arise specifically with user-
enabled privacy controls is that the user’s privacy prefer-
ences would, in themselves, constitute informative metadata,

* The recent ProPublica/Facebook exchange about auditing targeted
ads may configure as a privacy breach of this kind (Merrill & Tobin,
2019).

Q) Springer
962

which the system (or external observers) could use to make
sensitive inferences about the user, for example, to infer that
a user who has strong privacy settings may have certain psy-
chological traits, or that they may have “something to hide”.
When considering systemic inferences, due to the nature of
collaborative filtering methods, even if user-centred adjust-
ments could be implemented across the board in effective
ways, they would arguably still not solve the problem.

Crucially, due to the nature of recommender systems—
which, as we have seen, rely on user models to generate per-
sonalised recommendations—any approach to the issue of
user privacy will need to take into account not only the likely
trade-off between privacy and accuracy, but also fairness and
explainability of algorithms (Friedman et al. 2015; Koene
et al. 2015). For this reason, ethical analyses of recom-
mender systems are better developed by embracing a macro-
ethical approach. This is an approach that is able to consider
specifically ethical problems related to data, algorithms, and
practices, but also how the problems relate, depend on, and
impact each other (Floridi and Taddeo 2016).

4.3 Autonomy and personal identity

Recommender systems can encroach on individual users’
autonomy, by providing recommendations that nudge users
in a particular direction, by attempting to “addict” them to
some types of contents, or by limiting the range of options
to which they are exposed (Burr et al. 2018; de Vries 2010;
Koene et al. 2015; Taddeo and Floridi 2018). These inter-
ventions can range from being benign (enabling individual
agency and supporting better decision-making by filtering
out irrelevant options), to being questionable (persuasion,
nudging), and possibly malign [being manipulative and coer-
cive (Burr et al. 2018)].

Algorithmic classification used to construct user models
on the basis of aggregate user data can reproduce social cat-
egories. This may introduce bias in the recommendations.
We shall discuss this risk in detail in Sect. 4.4. Here, the
focus is on a distinctive set of issues arising when the algo-
rithmic categorization of users does not follow recognisable
social categories. de Vries (2010) powerfully articulates the
idea that our experience of personal identity is mediated
by the categories to which we are assigned. Algorithmic
profiling, performed by recommender systems, can disrupt
this individual experience of personal identity, for at least
two main reasons. First, the recommender system’s model
of each user is continuously reconfigured on the basis of
the feedback provided by other users’ interactions with the
system. In this sense, the system should not be conceptual-
ised as tracking a pre-established user identity and tailoring
its recommendations to it, but rather as contributing to the
construction of the user identity dynamically (Floridi 2011).
Second, the labelling that the system uses to categorise users

Q) Springer

Al & SOCIETY (2020) 35:957-967

may not correspond to recognisable attributes or social cate-
gories with which the user would self-identify (for example,
because machine-generated categories may not correspond
to any known social representation), so even if users could
access the content of the model, they would not be able to
interpret it and connect it with their lived experiences in
a meaningful way. For example, the category ‘dog owner’
may be recognisable as significant to a user, while ‘bought
a novelty sweater’ would be less socially significant; yet the
RS may still regard it as statistically significant when making
inferences about the preferences of the user. These features
of recommender systems create an environment where per-
sonalization comes at the cost of removing the user from
the social categories that help mediate their experiences of
identity.

In this context, an interesting take on the issue of personal
autonomy in relation to recommender systems comes from
the “captology” of recommender systems. Seaver (201 8a)
develops this concept from an anthropological perspective:

[a]s recommender[s] spread across online cultural
infrastructures and become practically inescapable,
thinking with traps offers an alternative to common
ethical framings that oppose tropes of freedom and
coercion (Seaver, 201 8a).

Recommender systems appear to function as “sticky
traps” (our terminology) insofar as they are trying to “glue”
their users to some specific solutions. This is reflected in
what Seaver calls “captivation metrics” (i.e. that measure
user retention), which are commonly used by popular rec-
ommender systems. A prominent example is YouTube’s
recommendation algorithm, which received much atten-
tion recently for its tendency to promote biased content and
“fake news’, in a bid to keep users engaged with its platform
(Chaslot 2018). Regarding recommender systems as traps
requires engaging with the minds of the users; traps can
only be effective if their creators understand and work with
the target’s world view and motivations, so the autonomous
agency of the target is not negated, but effectively exploited.
Given this captological approach, and given the effectiveness
and ubiquity of the traps of recommender systems, the ques-
tion to ask is not how users can escape from them, but rather
how users can make the traps work for them.

4.4 Opacity

In theory, explaining how personalised recommendations
are generated for individual users could help to mitigate the
risk of encroaching on their autonomy, giving them access
to the reasons why the system “thinks” that some options are
relevant to them. It would also help increase the transpar-
ency of the algorithmic decisions concerning how to class
and model users, thus helping to guard against bias.
Al & SOCIETY (2020) 35:957-967

Designing and evaluating explanations for recommender
systems can take different forms, depending on the specific
applications. As reported by Tintarev and Masthoff (2011),
several studies have pursued a user-centred approach to eval-
uation metrics, including metrics to evaluate explanations
of recommendations. What counts as a good explanation
depends on several criteria: the purpose of the recommenda-
tion for the user; whether the explanation accurately matches
the mechanism by which the recommendation is generated;
whether it improves the system’s transparency and scruta-
bility; and whether it helps the user to make decisions more
efficiently (e.g. more quickly), and more effectively, e.g. in
terms of increased satisfaction.

These criteria are satisfied by factual explanations.” How-
ever, factual explanations are notoriously difficult to achieve.
As noted by Herlocker et al. (2000), recommendations gen-
erated by collaborative filtering techniques can, on a simple
level, be conceptualised as analogous to “word of mouth”
recommendations among users. However, offline word of
mouth recommendations can work on the basis of trust and
shared personal experience, whereas in the case of recom-
mender systems users do not have access to the identity of
the other users, nor do they have access to the models that
the system uses to generate the recommendations. As we
mentioned, this is an issue in so far as it diminishes the
user’s autonomy. It may be difficult to provide good fac-
tual explanations in practice also for computational reasons
(the required computation to generate a good explanation
may be too complex), and because they may have distorting
effects on the accuracy of the recommendations Tintarev
and Masthoff (2011). For example, explaining to a user that
a certain item is recommended because it is the most popular
with other users may increase the item’s desirability, thus
generating a self-reinforcing pattern where the item will be
recommended more often because it is popular. This, in turn,
reinforces its popularity, ending in a winner-takes-all sce-
nario that, depending on the intended domain of application,
can have negative effects on the variety of options, plural-
ity of choices, and the emergence of competition (Germano
et al. 2019). Arguably, this may be one of the reasons why
Amazon does not privilege automatically products with less

° Factual explanations are usually contrasted to counterfactual ones
that describe what would have had to be the case, in order for a cer-
tain state or outcome (different from the actual one) to occur. For
example, suppose that while browsing an e-commerce website, Alice
is recommended a brand of dog food. A counterfactual explanation of
why Alice received this recommendation would specify what would
have had to be the case, for Alice not to be recommended this specific
product (for example, had she not browsed dog collars, she would
not have been recommended dog food). A factual explanation, on the
other hand, would specify why this specific item was recommended,
for example why this specific brand of dog food was deemed good for
Alice.

963

than perfect scoring but that have been rated by a large num-
ber of reviewers.

4.5 Fairness

Fairness in algorithmic decision-making is a wide-ranging
issue, made more complicated by the existence of multiple
notions of fairness, which are not all mutually compatible
(Friedler et al. 2016). In the context of recommender sys-
tems, several articles identified in this review address the
issue of recommendations that may reproduce social biases.
They may be synthesised around two approaches.

On the one hand, Yao and Huang (2017) consider several
possible sources for unfairness in collaborative filtering, and
introduce four new metrics to address them by measuring the
distance between recommendations made by the system to
different groups of users. Focusing on collaborative filtering
techniques, they note that these methods assume that the
missing ratings (1.e., the ones that the system needs to infer
from the statistical data to predict a user’s preferences) are
randomly distributed. However, this assumption of random-
ness introduces a potential source of bias in the system’s
predictions, because it is well documented that users’ under-
lying preferences often differ from the sampled ratings, since
the latter are affected by social factors, which may be biased
(Marlin et al. 2007). Following Yao and Huang (2017), Far-
nadi et al. (2018) also identify the two primary sources of
bias in recommender systems with two problematic patterns
of data collection, namely observation bias, which results
from feedback loops generated by the system’s recommenda-
tions to specific groups of users, and population imbalance,
where the data available to the system reflect existing social
patterns expressing bias towards some groups. They propose
a probabilistic programming approach to mitigate the sys-
tem’s bias against protected social groups.

On the other hand, Burke (2017) suggests to consider
fairness in recommendation systems as a multi-sided con-
cept. Based on this approach, he focuses on three notions of
fair recommendations, taking the perspective of either the
user/consumer (C-fairness), whose interest is to receive the
most relevant recommendations; or the provider (P-fairness),
whose interest is for their own products or services to be rec-
ommended to potentially interested users; or finally a combi-
nation of the two (CP-fairness). This taxonomy enables the
developer of a recommendation system to identify how the
competing interests of different parties are affected by the
system’s recommendations, and hence design system archi-
tectures that can mediate effectively between these interests.

In both approaches, the issue of fairness is tied up with
choosing the right LoA for a specific application of a recom-
mender system. Given that the concept of fairness is strongly
tied to the social context within which the system gathers
its data and makes recommendations, extending the same

Q) Springer
964

approach to any application of recommender systems may
not be viable.

4.6 Social effects

A much-discussed effect of some recommender systems is
their transformative impact on society. In particular, news
recommender systems and social media filters, by nature of
their design, run the risk of insulating users from exposure
to different viewpoints, creating self-reinforcing biases and
“filter bubbles” that are damaging to the normal functioning
of public debate, group deliberation, and democratic insti-
tutions more generally (Bozdag 2013; Bozdag and van den
Hoven, 2015; Harambam et al. 2018; Helberger et al. 2016;
Koene et al. 2015; Reviglio 2017; Zook et al. 2017). This
feature of recommender systems can have negative effects
on social utility. A relatively recent but worrying example is
the spread of propaganda against vaccines, which has been
linked to a decrease in herd immunity (Burki 2019).

A closely related issue is protecting these systems
from manipulation by (sometimes even small but) espe-
cially active groups of users, whose interactions with the
system can generate intense positive feedback, driving up
the system’s rate of recommendations for specific items
(Chakraborty et al. 2019). News recommendation systems,
streaming platforms, and social networks can become an
arena for targeted political propaganda, as demonstrated by
the recent Cambridge Analytical scandal in 2018, and the
documented external interference in US political elections
in recent years (Howard et al. 2019).

The literature on the topic proposes a range of approaches
to increase the diversity of recommendations. A point noted
by several authors is that news recommendation systems, in
particular, must reach a trade-off between the expected rele-
vance to the user and diversity when generating personalised
recommendations based on pre-specified user preferences or
behavioural data (Helberger et al. 2016; Reviglio 2017). In
this respect, Bozdag and van den Hoven (2015) argue that
the design of algorithmic tools to combat informational seg-
regation should be more sensitive to the democratic norms
that are implicitly built into these tools.

In general, the approaches to the issue of polarization and
social manipulability appear to be split between bottom-up
and top-down strategies, prioritising either the preferences
of users (and their autonomy in deciding how to configure
the personalised recommendations) or the social preference
for a balanced public arena. Once again, some authors take a
decidedly user-centred perspective. For example, Harambam
et al. (2018) propose the use of different “recommendation
personae’, or “pre-configured and anthropomorphised types
of recommendation algorithms” expressing different user
preferences with respect to novelty, diversity, relevance,
and other attributes of a recommendation algorithm. In the

Q) Springer

Al & SOCIETY (2020) 35:957-967

Table 2 Summary of identified ethical issues of recommender sys-

tems
Immediate harm Exposure to risk
Utility Inappropriate content (4.1) Opacity (4.4)
Questionable content (4.1)
Rights Unfair recommendations (4.5) — Privacy (4.2)

Encroachment on individual Social effects (4.6)

autonomy and identity (4.3)

same vein, Reviglio (2017) stresses the importance of pro-
moting serendipity even at the cost of sacrificing aspects
of the user experience, such as diminished relevance of the
recommendations.

5 Discussion

Based on the review of the literature presented in the previ-
ous section, we can now revisit the taxonomy that we pro-
posed in Sect. 3, and place the concerns that we have identi-
fied within the conceptual space that it provides. Table 2
summarises our results.

Starting with privacy, the main challenge that is linked
with privacy violations is the possibility of unfair or oth-
erwise malicious uses of personal data to target individual
users. Thus, from our review, it emerges that privacy con-
cerns may be best conceptualised as exposure to risk. More-
over, the types of risk to which privacy violations expose
users fall mainly under the category of rights violations,
such as unfair targeting and use of manipulative techniques.

Issues of personal autonomy and identity also fall under
the category of rights violations, and constitute cases of
immediate violations. Unfair recommendations can be asso-
ciated with a negative impact on utility but, as also noted
by Yao and Huang (2017), fairness and utility are mutu-
ally independent, and unfairness may be best classified as a
type of immediate right violation. Table 3 summarises the
findings of this paper in terms of the current issues and the
possible solutions that we have identified in the literature.

A notable insight that emerges from the review is that
most of the ethical impacts of recommender systems iden-
tified in the literature are analysed from the perspective of
the receivers of the recommendations. This is evident not
only in the reliance on accuracy metrics measuring the dis-
tance between user preferences and recommendations, but
also when considering that privacy, unfairness, opacity,
and the appropriateness of content are judged from the per-
spective of the individual receiving the recommendations.
However, individual users are not the only stakeholders of
recommender systems (Burke 2017). The utility, rights,
and risks carried by providers of recommender systems,
Al & SOCIETY (2020) 35:957-967

Table 3 Ethical issues of RS and possible solutions

Area of concern Current issues

Content

Privacy
Data leaks
Unauthorised inferences

Autonomy and personal identity Behavioural traps

Recommendation of inappropriate content

Unauthorised data collection and storage

965

Possible solutions

User-specified filters

Demographic or geographical filters
Architectural: store data in separate databases
Algorithmic: anonymization, encryption
Policy: legislation (e.g. GDPR)

Increase the transparency of user categorisation

Encroachment on sense of personal identity

Opacity Black-box algorithms
Uninformative explanations
Feedback effects

Fairness Observation bias

Population imbalance
Social effects
Feedback effects

Table 4 Number of reviewed
papers addressing each of the
six concerns by discipline

Inappropri-
ate content

Computer Science 6
Philosophy
Ethnography

Social sciences

and by society at large, should also be addressed explicitly
in the design and operation of recommender systems. And
there are also more complex, nested cases in which rec-
ommendations concern third parties (e.g., what to buy for
a friend’s birthday). Currently, this is (partially) evident
only in the case of discussion on social polarization and its
effects on democratic institutions (reviewed in Sect. 4.6).
Failure to address explicitly these additional perspectives
of the ethical impact of recommender systems may lead to
masking seriously problematic practices. A case in point
may be that of introducing a “bias” in favour of recom-
mending unpopular items to maximise catalogue cover-
age in e-commerce applications (Jameson et al. 2015).
This practice meets a specific need of the provider of a
recommendation system, helping to minimise the number
of unsold items, which in this specific instance may be
considered a legitimate interest to be traded off against the
utility that a user may receive from a more accurate recom-
mendation. However, modelling the provider’s interests
as a bias added to the system is unhelpful if the aim is to
identify what would be the right level of trade-off between
the provider’s and users’ interests.

Any recommendation is a nudging, and any nudging
embeds values. The opacity about which and whose values

Lack of exposure to contrasting viewpoints

Introduce factual explanations

Adopt a multi-sided recommendation framework

Recommender personae

Serendipitous recommendations

Privacy Autonomy and Opacity Fairness Social effects
personal identity

4 3 5 2
4 4

are at stake in recommender systems hinders the possi-
bility of designing better systems that can also promote
socially preferable outcomes and improve the balance
between individual and non-individual utilities.

The distribution of the topics by discipline also reveals
some interesting insights (summarised in Table 4). Among
the reviewed articles, the ones addressing privacy, fairness
and opacity come predominantly from computer science.
This is in line with the general trends in the field of algo-
rithmic approaches to decision-making, and the presence
of established metrics and technical approaches to address
these challenges.

In contrast, the challenges posed by socially trans-
formative effects, manipulability, and personal auton-
omy are more difficult to address using purely technical
approaches, largely because their definitions are qualita-
tive, more contentious, and require viewing recommender
systems in the light of the social context in which they
operate. Thus, the articles identified in this review that
relate to these issues are much more likely to come from
philosophy, anthropology, and science and technology
studies. The methodologies that they adopt are more var-
ied, ranging from ethnographic study (Seaver 2018b), to
hermeneutics (de Vries 2010), decision theory (Burr et al.
2018), and economics (Abdollahpouri et al. 2017).

Q) Springer
966

6 Conclusion

This article offers a map and an analysis of the main ethi-
cal challenges posed by recommender systems, as identi-
fied in the current literature. It also highlights a gap in the
relevant literature, insofar as it stresses the need to con-
sider the interests of providers of recommender systems,
and of society at large (including third-party, nested cases
of recommendations), and not only of the receivers of the
recommendation, when assessing the ethical impact of rec-
ommender systems. The next steps are, therefore, filling
the gap, and articulating a comprehensive framework for
addressing the ethical challenges posed by recommender
systems, based on the taxonomy and the findings of this
review.

Acknowledgements This work was supported by Privacy and Trust
Stream—Social lead of the PETRAS Internet of Things research hub.
PETRAS is funded by the Engineering and Physical Sciences Research
Council (EPSRC), Grant agreement no. EP/N023013/1; and Google
UK Limited.

Open Access This article is licensed under a Creative Commons Attri-
bution 4.0 International License, which permits use, sharing, adapta-
tion, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source,
provide a link to the Creative Commons licence, and indicate if changes
were made. The images or other third party material in this article are
included in the article’s Creative Commons licence, unless indicated
otherwise in a credit line to the material. If material is not included in
the article’s Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will
need to obtain permission directly from the copyright holder. To view a
copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.

Appendix: Methodology

We performed a keyword search on five widely used ref-
erence repositories (Google Scholar, IEEE Xplore, SCO-
PUS, PhilPapers and ArXiv), using a sting of the general
form:

((moral « OR ethic *) AND (recommend «
AND (system * OR algorithm *))).

The keyword search produced a total of 533 results, includ-
ing 417 results on Google Scholar, 54 results on Scopus, 48
results on IEEE Xplore, seven results on PhilPapers, and
seven results on ArXiv. After eliminating duplicate entries,
and screening out the irrelevant entries based on the title and
abstract, 50 relevant entries were left. These were reviewed
in more detail. Finally, additional entries were added fol-
lowing the citations in the reviewed articles. The result was

Q) Springer

Al & SOCIETY (2020) 35:957-967

a corpus of 37 relevant works, discussed in this review and
listed in References.

References

Abdollahpouri H, Burke R, Mobasher B (2017) Recommender systems
as multistakeholder. Environments. https://doi.org/10.1145/30796
28.3079657

Adomavicius G, Tuzhilin A (2005) Toward the next generation of rec-
ommender systems: a survey of the state-of-the-art and possible
extensions. IEEE Trans Knowl Data Eng 17(6):734—749. https://
doi.org/10.1109/TKDE.2005.99

Bozdag E (2013) Bias in algorithmic filtering and personalization.
Ethics Inf Technol 15:209—227. https://doi.org/10.1007/s 1067
6-013-9321-6

Bozdag E, van den Hoven J (2015) Breaking the filter bubble: democ-
racy and design. Ethics Inf Technol 17(4):249-265. https://doi.
org/10.1007/s 10676-015-9380-y

Burke R (2017) Multisided fairness for recommendation. arXiv
:1707.00093

Burki T (2019) Vaccine misinformation and social media. Lancet
Digital Health 1(6):e258—e259. https://doi.org/10.1016/S2589
-7500(19)30136-0

Burr C, Cristianini N, Ladyman J (2018) An analysis of the interaction
between intelligent software agents and human users. Mind Mach
28(4):735-774. https://doi.org/10.1007/s 11023-018-9479-0

Chakraborty A, Patro GK, Ganguly N, Gummadi KP, Loiseau P (2019)
Equality of voice: towards fair representation in crowdsourced
top-K recommendations. FATREC. https://doi.org/10.1145/32875
60.3287570

Chaslot G (2018) How algorithms can learn to discredit the media—
Guillaume Chaslot—Medium, Medium. https://medium.com/@
guillaumechaslot/how-algorithms-can-learn-to-discredit-the-
media-d1360157c4fa

de Vries K (2010) Identity, profiling algorithms and a world of ambi-
ent intelligence. Ethics Inf Technol 12(1):71-—85. https://doi.
org/10.1007/s 10676-009-9215-9

Farnadi G, Kouki P, Thompson SK, Srinivasan S, Getoor L (2018)
A fairness-aware hybrid recommender system. In: 2nd FATREC
workshop: responsible recommendation. arXiv: 1809.09030

Floridi L (2008) Understanding epistemic relevance. Erkenntnis
69(1):69-92. https://www.jstor.org/stable/40267374

Floridi L (2011) The construction of personal identities online. Mind
Mach 21(4):477-479. https://doi.org/10.1007/s1 1023-01 1-9254-y

Floridi L (2016) The method of levels of abstraction. In: Floridi L
(eds), The Routledge handbook of philosophy of information (pp
67-72). Routledge

Floridi L, Taddeo M (2016) What is data ethics? Philos Trans R Soc A
Math Phys Eng Sci 374(2083):20160360. https://doi.org/10.1098/
rsta.2016.0360

Friedler SA, Scheidegger C, Venkatasubramanian S (2016) On the (im)
possibility of fairness. arXiv:1609.07236

Friedman A, Knijnenburg B, Vanhecke K, Martens L, Berkovsky S,
Berkovsky CSIROS (2015) Privacy Aspects of Recommender
Systems. In: Ricci F, Rokach L, Shapira B (eds) Recommender
systems handbook, 2nd edn. Springer Science + Business Media,
New York, pp 649-688

Germano F, Gémez V, Mens GL (2019) The few-get-richer: a surprising
consequence of popularity-based rankings. arX1iv:1902.02580[Cs].
http://arxiv.org/abs/1902.02580
Al & SOCIETY (2020) 35:957-967

Hansson SO (2010) The harmful influence of decision theory on eth-
ics. Ethical Theory Moral Practice 13(5):585-—593. https://doi.
org/10.1007/s 10677-010-9232-0

Harambam J, Helberger N, van Hoboken J (2018) Democratizing
algorithmic news recommenders: how to materialize voice in a
technologically saturated media ecosystem. Philos Trans R Soc A
Math Phys Eng Sci 376(2133):20180088. https://doi.org/10.1098/
rsta.2018.0088

Hayenhjelm M, Wolff J (2012) The moral problem of risk impositions:
a survey of the literature. Eur J Philos 20:E26—-E51. https://doi.org
/10.1111/).1468-0378.2011.00482.x

Helberger N, Karppinen K, D’acunto L (2016) Exposure diversity
as a design principle for recommender systems. https://doi.
org/10.1080/1369118x.2016.1271900

Herlocker JL, Konstan JA, Riedl J (2000) Explaining collaborative fil-
tering recommendations. In: CSCW ’00: proceedings of the 2000
ACM conference on computer supported cooperative work, pp
241-250. https://doi.org/10.1145/358916.358995

Howard PN, Ganesh B, Liotsiou D, Kelly J, Francois C (2019) The
IRA, social media and political polarization in the United States,
2012-2018

Jameson A, Mrtijn CW, Felfernig A, de Gemmis M, Lops P, Semeraro
G, Chen L (2015) Human decision making and recommender sys-
tems. In: Francesco R, Rokach L, Shapira B (eds) Recommender
systems handbook. Springer, Berlin

Jannach D, Adomavicius G (2016) Recommendations with a purpose.
RecSys’ 16. https://doi.org/10.1145/2959100.2959 186

Jannach D, Zanker M, Ge M, Gréning M (2012) Recommender systems
in computer science and information systems—a landscape of
research. Int Conf Electron Commerce Web Technol. https://doi.
org/10.1007/978-3-642-32273-0_7

Karimi M, Jannach D, Jugovac M (2018) News recommender sys-
tems—survey and roads ahead. Inf Process Manag 54(6):1203-
1227. https://doi.org/10.1016/j.ipm.2018.04.008

Koene A, Perez E, Carter CJ, Statache R, Adolphs S, O’ Malley C,
McAuley D (2015) Ethics of personalized information filtering.
Int Conf Internet Sci.https://doi.org/10.1007/978-3-3 19-18609
-2_10

Marlin BM, Zemel RS, Roweis S, Slaney M (2007) Collaborative fil-
tering and the missing at random assumption. arXiv:1206.5267

Merrill JB, Tobin A (2019). Facebook moves to block ad transparency
tools—including ours. ProPublica. https://www.propublica.org/
article/facebook-blocks-ad-transparency-tools

Narayanan A (2008) IEEE Xplore—robust de-anonymization of large
sparse datasets. In: SP’08 Proceedings of the 2008 IEEE sympo-
sium on security and privacy. https://doi.org/10.1109/sp.2008.33

Paraschakis D (2016) Recommender systems from an industrial and
ethical perspective. In: Proceedings of the 10th ACM conference
on recommender systems—RecSys’ 16, pp 463-466. https://doi.
org/10.1145/2959100.2959101

967

Paraschakis D (2017) Towards an ethical recommendation framework.
In: 2017 11th International Conference on Research Challenges in
Information Science (RCIS), pp 211-220. https://doi.org/10.1109/
rcis.2017.7956539

Paraschakis D (2018) Algorithmic and ethical aspects of recommender
systems in e-commerce. Malmé. http://muep.mau.se/bitstream/
handle/2043/24268/2043_ 24268 %20Paraschakis.pdf?seque
nce=3 &isAllowed=y

Pennock DM, Horvitz E, Giles CL (2000) Social choice theory and
recommender systems: analysis of the axiomatic foundations of
collaborative filtering. AAAI-00. https://www.aaai.org/Library/
AAAI/2000/aaai00-112.php

Reviglio U (2017) Serendipity by Design? How to Turn from Diversity
Exposure to Diversity Experience to Face Filter Bubbles in Social
Media. In Conf Internet Sci. https://doi.org/10.1007/978-3-319-
70284-1_22

Ricci F, Rokach L, Shapira B (eds) (2015) Recommender systems
handbook (2nd ed). https://www.springer.com/gb/book/978 14
89976369

Rodriguez MA, Watkins JH (2009) Faith in the algorithm, Part 2: com-
putational eudaemonics. In: Velasquez JD, Rios SA, Howlett RJ,
Jain LC (eds) Knowledge-based and intelligent information and
engineering systems. KES 2009. Lecture notes in computer sci-
ence, vol 5712. Springer, Berlin, Heidelberg

Seaver N (2018a) Captivating algorithms: recommender systems as
traps. J Mater Cult. https://doi.org/10.1177/13591835 18820366

Seaver N (2018b) Captivating algorithms: recommender systems as
traps. J Mater Cult. https://doi.org/10.1177/13591835 18820366

Souali K, El Afia A, Faizi R (2011) An automatic ethical-based recom-
mender system for e-commerce. Int Conf Multimedia Comput
Syst 2011:1-4. https://doi.org/10.1109/ICMCS.2011.5945631

Taddeo M, Floridi L (2018) How AI can be a force for good. Science
361(6404):75 1-752. https://doi.org/10.1126/science.aat599 |

Tang TY, Winoto P (2016) I should not recommend it to you even
if you will like it: the ethics of recommender systems. New
Rev Hypermedia Multimedia 22(1—2):111-138. https://doi.
org/10.1080/13614568.2015.1052099

Tintarev N, Masthoff J (2011) Designing and evaluating explanations
for recommender systems. In: Recommender systems handbook
(pp 479-510). https://doi.org/10.1007/978-0-387-85820-3_15

Yao S, Huang B (2017) Beyond parity: fairness objectives for col-
laborative filtering. NIPS. https://doi.org/10.1177/0143831X03
024002003

Zook M, Barocas S, Boyd D, Crawford K, Keller E, Gangadharan
SP, Pasquale F et al (2017) Ten simple rules for responsible big
data research. PLOS Comput Biol 13(3):e1005399. https://doi.
org/10.1371/journal.pcbi. 1005399

Publisher's Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.

Q) Springer
