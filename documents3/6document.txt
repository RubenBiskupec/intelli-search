Environmental Modelling and Software 134 (2020) 104754

 

Contents lists available at ScienceDirect

Environmental
Modelling & Software

    

Environmental Modelling and Software

1H ”

ELSEVIER

ie

journal homepage: http://www.elsevier.com/locate/envsoft

 

Check for

Argument-based assessment of predictive uncertainty of data-driven updates
environmental models

Benedikt Kniisel®, Christoph Baumberger “, Marius Zumwald “> David N. Bresch’S,
Reto Knutti”
* Institute for Environmental Decisions, ETH Ziirich, Universitatsstrasse 16, 8092, Zurich, Switzerland

> Institute for Atmospheric and Climate Science, ETH Ziirich, Universitatsstrasse 16, 8092, Zurich, Switzerland
“ Federal Office for Meteorology and Climatology MeteoSwiss, Operation Center 1, 8058, Zurich-Airport, Switzerland

 

ARTICLE INFO ABSTRACT

 

 

Keywords:
Uncertainty
Data-driven models
Argument analysis
Predictions
Decision-making

Increasing volumes of data allow environmental scientists to use machine learning to construct data-driven
models of phenomena. These models can provide decision-relevant predictions, but confident decision-making
requires that the involved uncertainties are understood. We argue that existing frameworks for characterizing
uncertainties are not appropriate for data-driven models because of their focus on distinct locations of uncer-
tainty. We propose a framework for uncertainty assessment that uses argument analysis to assess the justification
of the assumption that the model is fit for the predictive purpose at hand. Its flexibility makes the framework
applicable to data-driven models. The framework is illustrated using a case study from environmental science.
We show that data-driven models can be subject to substantial second-order uncertainty, i.e., uncertainty in the
assessment of the predictive uncertainty, because they are often applied to ill-understood problems. We close by

discussing the implications of the predictive uncertainties of data-driven models for decision-making.

 

1. Introduction

Real-world policy decisions are taken in light of great uncertainties.
Several aspects of a decision situation can be uncertain, including the
problem framing, the available options from which to choose, the actual
or potential states of the world, and the values that decision-makers
attach to these states of the world (see Bradley and Drechsler, 2014;
Hirsch et al., 2015; Hansson and Hirsch Hadorn, 2016). In this paper, we
are concerned with uncertainty about the actual or potential states of the
world. Often, this uncertainty is related to uncertainty of scientific in-
formation on which a decision is to be based. This information relates to
what the world will be like, either in the form of a prediction of envi-
ronmental conditions, such as a severe weather forecast, or in the form
of a conditional prediction (so-called projection) of environmental
conditions in response to a policy measure, such as climatic conditions in
response to a certain socioeconomic pathway and associated greenhouse
gas emissions. In order to take decisions under uncertainty, the uncer-
tainty should be analyzed and, if possible, quantified. An important part
of the uncertainty analysis is the characterization of what kind of un-
certainty emerges because of which features of the research process.

Only such a thorough treatment of uncertainties can help to ensure that
societal decisions are based on “no more and no less than what one
actually knows” (Betz, 2016a, 138).

Recent years have seen large increases in data produced and stored.
This trend is also apparent in the sciences in general and in the envi-
ronmental sciences in particular (e.g., in climate sciences, see Overpeck
et al., 2011). The increase in the availability of data about environ-
mental systems enables the use of machine learning to analyze the data
and to construct data-driven models of phenomena (see Gibert et al.,
2018a, 2018b; ). Numerous applications of machine learning can be
found in the environmental science literature (see, e.g., Reichstein et al.,
2019). In this paper, we will discuss applications of machine learning for
the data-driven modeling of a phenomenon. In contrast to a
process-based model, in which the relationships between variables are
prescribed in the form of equations specified by an expert, a data-driven
model is constructed by algorithmically inferring the relationships or
parameters from a dataset using machine learning (for a more detailed
distinction of process-based and data-driven models, see Kniisel and
Baumberger, 2020, forthcoming).

If predictions from data-driven models are reliable, which seems to

* Corresponding author. Institute for Environmental Decisions, ETH Ziirich, Universitatstrasse 16, 8092, Zurich, Switzerland.

E-mail address: benedikt.knuesel@alumni.ethz.ch (B. Kntisel).

https://doi.org/10.1016/j.envsoft.2020.104754

Received 14 January 2020; Received in revised form 27 May 2020; Accepted 29 May 2020

Available online 8 June 2020
1364-8152/© 2020 The Authors.

(http://creativecommons.org/licenses/by-nce-nd/4.0/).

Published by Elsevier Ltd.

This is an open access article under the CC BY-NC-ND license
B. Kniisel et al.

be the case at least under certain conditions (see Pietsch, 2015; North-
cott, 2019), these predictions are potentially useful for decision-making
related to the modeled phenomena. One of the advantages of
data-driven models is that they can be constructed when the processes
producing a phenomenon are not fully understood. Hence, they might
provide decision-relevant information specifically about phenomena
which are quantitatively not understood well enough to construct
process-based models. However, no conceptual tools to appropriately
evaluate data-driven models in terms of their uncertainties are available
to date, which reduces their usefulness for decision-making. Here, we
introduce such a tool that can complement existing quantitative
methods to explore the uncertainty of predictions made with data-driven
models.

The remainder of this paper is organized as follows. In section 2, we
argue that existing frameworks for the characterization of the un-
certainties of model-based predictions are not appropriate for pre-
dictions from data-driven models. We hence introduce a new, more
general approach in section 3. It focuses on the justification of the as-
sumptions underlying a prediction that is possible based on the available
data and background knowledge. The assumptions that need to be
justified include the fitness-for-purpose of the used model. In sections 4
and 5, we demonstrate the application of this framework to a toy
example and to a case study from environmental science. In section 6,
we discuss the implications of this framework for the quantification of
uncertainties and for decision-making more generally. We conclude in
section 7. Before proceeding, we note here that in the present paper, we
do not distinguish between “pure” predictions of environmental condi-
tions, such as a forecast of severe weather conditions, and predictions of
the environmental response conditional on a human intervention, such
as climatic conditions in response to a certain path of greenhouse gas
emissions. The approach presented here is fairly general and can be
applied in both situations equally.

2. Existing frameworks

Existing frameworks for model-based decision-support and specif-
ically uncertainty analysis have been developed for process-based
models. An influential framework to analyze uncertainties of model-
based predictions is due to Walker et al. (2003). It distinguishes three
different dimensions of uncertainty that are arranged as an uncertainty
matrix, namely the location (where in the modeling complex does the
uncertainty manifest itself?), the nature (is the uncertainty due to the
inherent variability of the phenomenon or due to imperfect knowledge
of it?), and the level of uncertainty (how severe is the uncertainty,
ranging from complete certainty to complete ignorance?). Several var-
iations of this matrix have been developed (see e.g. Refsgaard et al.,
2007), and versions of it have been applied in different contexts (see
Kwakkel et al., 2010). The locations of uncertainty introduced by Walker
et al. (2003) are the context, the model structure and the technical
model, the driving forces and the system data, the parameters, and
finally the model outcomes, which obtain their uncertainty from the
preceding locations. Kwakkel, Walker, and Marchau (2010) have syn-
thesized adaptations and criticism of the uncertainty matrix introduced
by Walker et al. and have, amongst other things, suggested that some of
the dimensions, including the locations of uncertainty, be rearranged.

Similar locations of uncertainty have been discussed in specific dis-
ciplines. For climate models, for example, Knutti (2018) suggests that
the relevant locations of uncertainty are model structural uncertainty,
numerical approximations, parameterizations, natural variability due to
initial conditions, the emission scenario, boundary conditions, and
observational data uncertainty, all of which have an analog in the lo-
cations discussed above. Similar locations of uncertainty have been
discussed for climate models by other authors (Winsberg 2018b, chap.
7).

What the approaches presented above have in common is that they
suggest that different model-related aspects, the locations of

Environmental Modelling and Software 134 (2020) 104754

uncertainty, are investigated in order to characterize the uncertainty of
the model results. Namely, at each location, aspects should be identified
that are either intrinsically or epistemically uncertain, i.e., uncertain
due to system properties (“aleatory uncertainty”) or due to our imperfect
understanding of the system (“epistemic uncertainty”), and the level of
this uncertainty should be determined. Then, this uncertainty is propa-
gated in order to characterize the uncertainty of the processed model
outputs.

Thinking about uncertainty in terms of specific locations is not
informative for data-driven models for three reasons.’ The first reason is
that for many data-driven models, the model structure cannot readily be
tied to processes in the target system or be interpreted in terms of the
target system. Yet, model structure is one of the locations of uncertainty
common to the established frameworks. This is especially obvious for
models based on neural networks or bagging approaches like random
forest. While neural networks have a model structure consisting of a
number of neurons and layers, this structure cannot be interpreted in
terms of the target system in a straightforward way. For bagging ap-
proaches like random forest, it is unclear what the relevant model
structure would be because they make predictions based on the average
of multiple individual estimators. Due to these issues, considering the
model structure as a location of uncertainty is likely not helpful to assess
the predictive uncertainty of data-driven models.

For process-based models, structural model uncertainty arises
because different model specifications might seem equally plausible and
it is unclear how to best represent the target system for a specific pur-
pose. This representational uncertainty is related to predictive uncer-
tainty (see Parker, 2010). The best model setup can be more radically
underdetermined in the case of data-driven models since a very large
number of model setups can be consistent with the available training
data. Hence, the best model setup for a specific purpose has to be chosen
based on background knowledge. As data-driven models are often
employed when processes are ill-understood (see Kniisel et al., 2019),
the model setup of data-driven models and the resulting representa-
tional uncertainty of a target seem relevant for an analysis of predictive
uncertainty. Thus, even though the model structure, if there is an
accessible model structure, cannot directly be tied to the target system,
an approach to assess the predictive uncertainty of data-driven models
needs to be able to consider the representational uncertainty of the
model at issue. However, it is unclear how exactly representational
uncertainty and predictive uncertainty are related for data-driven
models.

The second reason existing approaches are not informative for un-
derstanding the predictive uncertainty of data-driven models is that,
similar to the model structure, model parameters are not an obvious
location of uncertainty either for many data-driven models. Some ma-
chine learning approaches like random forest are non-parametric and
hence, this location of uncertainty is simply not defined for them. Such
non-parametric approaches do have meta-parameters, e.g., the number
of trees or the number of considered variables at every split in a random
forest model, but these, again, cannot readily be interpreted in terms of
the target system. Other machine learning approaches like deep neural
networks can have an overwhelming number of parameters. As for the
model structure, many of the model parameters have no obvious inter-
pretation in terms of the target system, and the uncertainty in each
parameter does not directly translate to predictive uncertainty in the

' We note that in a given application of a model for decision support, an
additional problem can be that there are many more locations of uncertainty,
which are not directly related to the model but rather to contextual factors.
Such uncertainties are best addressed, e.g., by stakeholder engagement (for an
extensive overview of this point for water resource management, see Badham
et al., 2019). As the focus of this paper lies on uncertainty that is directly
model-dependent, we will not further discuss additional locations or sources of
uncertainty. We thank a reviewer of this journal for raising this issue.
B. Kniisel et al.

same sense. Thus, thinking about model parameters as a location of
uncertainty is not helpful for data-driven models, either.

The third reason is that machine learning is, at least sometimes,
preoccupied mainly with good predictions and less with the reasons for
these predictions. But as we will see below, for some predictions, the
reasons for the predictions can matter, too. A framework for assessing
predictive uncertainties of data-driven models should thus reflect this
purpose-dependence of the representational character of the model.

Hence, existing frameworks for uncertainty assessments seem inad-
equate as conceptual tools for a discussion of the predictive uncertainty
of data-driven models. While it can be helpful to think about uncertainty
in terms of different sources whose uncertainty can be considered ad-
ditive for data-driven models, new conceptual tools are needed to
analyze relevant sources of uncertainty.

From the above discussion, two requirements become clear for an
analytic framework for the predictive uncertainties of data-driven
models. First, such a framework needs to do justice to the fact that
under some, but not all, circumstances, a data-driven model has an
instrumental character in the sense that modelers are preoccupied only
with the predictive success of data-driven models and not with the
reasons for this success. Such cases are typically characterized by model
evaluation that focuses solely on performance metrics, such as the cross-
validation error. Second, in cases in which a modeler is not only pre-
occupied with the predictions but also with the reasons for predictive
success, the representational accuracy of data-driven models needs to be
assessed. This is so even though, as we have argued above, neither the
model structure nor model parameters can readily be interpreted in
terms of the target system, at least not for models with many degrees of
freedom. In these cases, model users will be forced to adopt a more
realistic interpretation of the model, meaning that they need to assume
that the model behaves in a way that is consistent with the actual
mechanisms of the target system even if the model structure and pa-
rameters cannot be interpreted in terms of the target system. Specif-
ically, this means that the framework needs to be able to assess to what
extent the behavior (as opposed to the structure or the parameters) of the
data-driven model is coherent with background knowledge and to what
extent it is possible to argue from the coherence with background
knowledge to representational accuracy.”

Uncertainty has also been a topic in the computer science literature
on machine learning. To the best of our knowledge, the literature on
uncertainties in machine learning has mainly discussed the role of
Bayesian approaches for quantifying uncertainty (see Blundell et al.,
2015; Ghahramani, 2015; Gal and Ghahramani, 2016). Kendall and Gal
(2017) have explicitly distinguished between epistemic and aleatory
uncertainty in the context of Bayesian deep learning for computer vision
— a distinction we will take up in later sections. These Bayesian ap-
proaches are certainly useful for some contexts, such as computer vision
and for cases in which uncertainty can be characterized easily because it
mainly comes from certain sources, such as noisy data. However, for the
present purposes, they seem insufficient as a basis of the uncertainty
assessment of data-driven environmental models as they are unable to
account for uncertainty not present in the available dataset, which be-
comes important when a relatively realistic interpretation of the model
is adopted. However, we will return to how these methods could com-
plement the ideas presented in our framework in section 6.

2 We note here that evaluating model behavior in order to assess the skill and
the uncertainty of models is not unique to data-driven models. It is also done for
process-based models. This has been discussed, for example, for climate models
by Lloyd (2009), who emphasizes the importance of model behavior (e.g., the
simulation of the tropopause) for establishing confidence in model results, and
for environmental modeling more generally by Hamilton et al. (2019) under the
term “stylized facts”. We thank a reviewer of this journal for raising this issue.

Environmental Modelling and Software 134 (2020) 104754

3. An argument-based framework for uncertainty analysis

Above, we have argued that a framework for analyzing the predictive
uncertainty of data-driven models needs to do justice to the fact that
some but not all data-driven models have a purely instrumental char-
acter. Hence, the framework necessarily has to be more general than
existing frameworks in order to allow for different representational
characters in different contexts. Here, we put forward such a framework.
The framework suggests to analyze uncertainties in three steps. The first
step consists in reconstructing what assumptions have to be made when
using a model for a specific purpose and how these assumptions can be
justified. The second step consists in evaluating how well justified the
assumptions are. The third step consists in assessing the uncertainty
based on the previous two steps. The basic assumption that has to be
justified in any modeling application is an assumption regarding the
fitness-for-purpose of the model used. As Parker (2009) has argued for
climate models, the goal of model evaluation should not be to confirm
the model itself, generally, but rather to confirm that a model is
adequate for a specific purpose. In this paper, we use the term “fit-
ness-for-purpose”, which, in contrast to adequacy-for-purpose, admits of
various degrees of fitness (meaning that models are not just
fit-for-purpose or not, but fit-for-purpose to a larger or smaller degree,
see Parker, 2020, forthcoming).

We take a model to be maximally fit for a specific predictive purpose
if it can predict the variable of interest reliably with errors always lying
in some small range, which is determined by the inherent variability of
the target system. On this way of thinking, as the fitness-for-purpose
increases, the model skill converges toward the theoretical limit of
predictability of the system as described by the model (i.e., with a
certain resolution, etc.). An example of such a fitness-for-purpose
assumption would be: Model M is fit for predicting the total precipita-
tion amount of the next 24 h at location L with errors in some small
range. According to the argument-based framework, epistemic uncer-
tainty arises due to a lack of epistemic justification. Thus, if it can be
conclusively justified that the model is maximally fit-for-purpose in this
sense, the model predictions exhibit no epistemic uncertainty, and the
remaining uncertainty of the predictions arising from the small range of
errors is aleatory in nature.

Specifically, the above considerations reveal three ways in which
model predictions can be epistemically uncertain. Namely, according to
the framework presented here, epistemic uncertainty arises because of
known factors that result in less-than-maximal fitness-for-purpose, fac-
tors that make the justification non-conclusive, or factors that make it
unclear what degree of fitness-for-purpose a model has. Essentially, all
of these three kinds of factors increase the range around model pre-
dictions in which the actual state-of-affairs is expected to lie. Thus, they
are sources of epistemic uncertainty. This conceptualization of “source
of uncertainty” is consistent with Guillaume et al. (2015, 101), who
define a source of uncertainty as “the origin of an uncertainty in the
process to acquire and use knowledge”.

In the following, we will introduce the three steps of the framework
in more detail. The first step is the reconstruction of the assumptions and
their possible justifications. In order to do this, the following two
questions have to be addressed: First, what modeling assumptions does a
prediction rely on? Second, how can these assumptions be justified? As
noted above, the basic assumption is a fitness-for-purpose assumption
concerning the model that was used to make a prediction. Depending on
the circumstances, the fitness of the model for the predictive task at hand
will be justified differently. If the justification is itself (partly) based on
assumptions that require further justification, the two questions above
need to be addressed several times in order to assess how well the basic
assumption is justified. Once this is done, an argument map (for an
introduction to argument maps, see Betz 2016b) can be drawn in which
it becomes apparent which arguments and assumptions justify the
fitness-for-purpose assumption, and how different arguments and theses
are related (an argument map is shown in Fig. 1, which is discussed in
B. Kniisel et al.

the case study in section 5, and essential elements of argument maps are
introduced in the appendix).

In the second step, based on the argument map from the first step (or
in simpler cases, based on a direct assessment of the arguments), it is
evaluated how well all the different assumptions and specifically the
fitness-for-purpose assumption are justified and, based on that, what the
actual degree of fitness-for-purpose is. For this, it is relevant to check
whether the premises used to justify a conclusion are true’ and whether
they provide sufficiently good reason to accept the truth of the conclu-
sion. Specifically, the evaluation step may reveal that some of the pre-
mises are strictly speaking false. In these cases, it should be evaluated
whether the premise may still be close enough to the truth to support the
conclusion. Furthermore, premises may be identified of which we do not
know whether they are true. Finally, the evaluation step highlights non-
conclusive justifications in the complex argumentation (arising from
non-deductively correct arguments, see below). Here, the strength of the
justification needs to be assessed based on domain-specific background
knowledge.

To what extent the premises provide good reasons to accept the truth
of the conclusion depends on whether the argument that is being eval-
uated is deductively valid or non-deductively correct. Whenever
possible, arguments should be reconstructed as deductively valid argu-
ments, i.e., arguments for which the following condition holds: if all the
premises are true, then the conclusion must be true. This way, the as-
sumptions become more explicit, which also makes more explicit why
these assumptions may act as sources of uncertainty (as will be assessed
in the third step). An example of a deductively valid argument is the
following (this example is taken from Baumberger et al., 2017, 7): “Ifa
model is adequate for projecting X for the far future, then the model
reliably indicates X for past and present. Model M does not reliably
indicate X for past and present. Hence, M is not adequate for projecting X
for the far future.” Some arguments cannot easily be reconstructed as
deductively valid, e.g., because it can be unclear which premises would
be appropriate to obtain a plausible deductively valid argument. In these
cases, the arguments can be reconstructed as non-deductively correct, i.
e. as arguments that provide sufficiently strong (but not conclusive)
reasons for the truth of the conclusion. Non-deductively correct argu-
ments are risky in the sense that even if all of their premises are true, the
truth of the conclusion is not guaranteed. An example of a non-deductive
argument is the following (this example, too, is taken from Baumberger
et al., 2017, 7): “Model M reliably indicates X and climate quantities
upon which X depends for past and present. So probably, M is adequate
for projecting X for the near future.”

Finally, the third step consists in an assessment of the uncertainty
based on the previous step of evaluating the justification of the fitness-
for-purpose assumption. By analyzing the possible epistemic justifica-
tion of assumptions, the focus of the framework presented here is on
epistemic uncertainties.’ The framework distinguishes between two
types of epistemic uncertainty that we refer to as “first-order uncer-
tainty” and “second-order uncertainty”. These two types of uncertainty
are distinguished by their objects. First-order uncertainty is the

3 We note here that in many practical situations, evaluating the premises in
terms of approximate truth may suffice. In such circumstances, the premises
and conclusions could be qualified to allow for approximate truth.

* Note that the justifications always have to rely on arguments that can be
constructed from what is known. Hence, the framework introduced here is not
helpful to identify uncertainty arising from unknown unknowns. The risk of
such genuine surprises should especially be considered if a system is complex,
ill-understood, if scientists know that they are overconfident, if surprises have
occurred in the past, or if the system is subject to new conditions, as Parker and
Risbey (2015) have argued. In practical applications of the framework outlined
here, arguments for the fitness-for-purpose could be made that aim to rule out
the risk of surprises by making explicit references to these five factors.
Depending on how well they are justified, they would be additional sources of
uncertainty that explicitly account for the risk of surprises.

Environmental Modelling and Software 134 (2020) 104754

epistemic uncertainty of the prediction. Consequently, first-order un-
certainty is defined here as the extent to which the assumptions un-
derlying a prediction are insufficiently justified. Thus, epistemic first-
order uncertainty arises if it cannot be conclusively justified that the
model is maximally fit-for-purpose. Second-order uncertainty is defined
here as the extent to which the assessment of first-order uncertainty is
impaired by context-specific factors. Specifically, second-order uncer-
tainty depends on difficulties in assessing to which extent the assump-
tions are justified, e.g. because of a lack of evidence, contradictory
evidence, or expert disagreement. Thus, epistemic second-order uncer-
tainty arises if the degree of fitness-for-purpose cannot be conclusively
determined.

The assessment of these two types of uncertainty is based on the
evaluation of the argumentation from the second step, as will be illus-
trated below. Based on the framework, the expressions of first-order and
second-order uncertainty will be purely qualitative. However, in section
6 below, we address the questions of uncertainty quantification, which is
desired for many applications.”

Before we proceed to illustrating the application of the framework,
three clarifications are in order. First, in practical applications, the
fitness-for-purpose of a model will not only depend on its predictive
accuracy. Further considerations such as practical concerns can play a
role, for example ease-of-use or computational cost (see, e.g., Haasnoot
et al., 2014, 111). However, in the present paper, we are only concerned
with the uncertainty of the predictions of a model and we will, hence,
discuss fitness-for-purpose only as it relates to predictive accuracy.

Second, maximal fitness-for-purpose is defined in terms of model
performance. Specifically, we have defined a model as maximally fit-for-
purpose if it has errors that always fall within a small range that is due to
the internal variability of the target system. In practice, it can be difficult
to specify how wide exactly this small range should be. Thus, the
specification of fitness-for-purpose can, itself, be a source of epistemic
uncertainty if it is unclear how large the errors arising from variability of
the target system are.

Third, note that if a model is found to be less-than-maximally fit-for-
purpose, this does not mean that the model is outright unfit-for-purpose.
Whether a given degree of fitness-for-purpose is sufficient to consider a
model outright fit (or adequate) for a specific purpose depends on the
context. For models that are evaluated only in terms of predictive per-
formance, a model might be considered outright fit-for-purpose if it
provides information that was previously not available. A model with a
predictive skill that is far from perfect can still provide information that
was previously not available. Thus, it can be outright fit-for-purpose also
when it is less-than-maximally fit-for-purpose (see Knutti, 2018). For
models whose fitness is evaluated in terms of criteria other than pre-
dictive skill alone, there can be well-known trade-offs between these
criteria. For example, a trade-off exists for climate models between how
many processes they represent and how intelligible they are for model
users (see Held, 2005). Due to such trade-offs, a model that is maximally
fit-for-purpose in this multidimensional sense cannot be constructed
even in principle.

The identification and evaluation of how the assumptions are justi-
fied requires expertise from domain scientists concerned with the phe-
nomenon at hand and from modelers and data scientists, but also
expertise in argument analysis (an introduction to argument analysis
can be found in Brun and Betz 2016). In the literature, it has been

> We note here that uncertainties of higher orders could be defined in analogy
to second-order uncertainty. For example, third-order uncertainty could be
defined as the uncertainty of the assessment of second-order uncertainty. As the
discussion below will highlight, second-order uncertainty is an important
concept to better understand the epistemic uncertainty of model predictions.
Uncertainties of higher order are likely irrelevant in practical applications.
B. Kniisel et al.

Table 1
Application of the conceptual framework to the toy example with maximum
daily temperature predictions.

assumption justification of the assumption

The model is fit for predicting maximum
daily temperature with a lead time of
one day.

The model has accurately predicted
many past instances of maximum daily
temperature with a lead time of one day.

recognized that an analysis of assumptions can be important for a better
understanding of uncertainties (see Kloprogge, van der Sluijs, and
Petersen, 2011). However, as will be shown below, since the framework
presented here is concerned with justifying the fitness-for-purpose, it is
not only concerned with assumptions that a modeler makes in the pro-
cess of model construction. Instead, it is concerned with assumptions
that a model user is relying on, at least implicitly, when using a given
model for a specific predictive purpose. For example, a model user may
have to assume that certain processes are accurately represented in a
data-driven model. However, this assumption is not made explicitly in
the process of building a data-driven model as the relationships between
variables are not explicitly prescribed (see Pietsch, 2015) and no pro-
cesses are therefore explicitly represented. Hence, both choices made in
the “path” (cf. Lahtinen et al., 2017) of model construction (e.g. the
choice of hidden layers in a neural network or of meta-parameters in
random forest) and assumptions made when using the fully trained
model (e.g., regarding representational accuracy) can be relevant for an
account of the predictive uncertainty of a model.

4. Toy example for illustration

In this section, we discuss a simple toy example of a data-driven
model to illustrate how the framework can be applied. Our toy model
is a random forest algorithm that is trained to predict the maximum
daily air temperature at a given location with a lead-time of one day. For
this, the current air temperature and pressure at the location, the season,
and an index on the general weather conditions are used as predictors.
Imagine that the model predictions are successful and actually measured
maximum daily air temperatures are always close to the predicted value
(within a small error range), and we are able to repeatedly use the model
and evaluate its performance.

We might generally wish to better understand the uncertainty of this
kind of prediction in order to base decisions on them. For example, the
prediction of an unusually cold or hot maximum temperature might be
relevant for public health. In order to obtain a better understanding of
the uncertainty of the predictions, we can apply the framework intro-
duced in the previous section.

First, the assumptions and their possible justifications need to be
identified and graphically arranged. As explained above, the most
fundamental assumption is a fitness-for-purpose assumption. In this
case, the fitness-for-purpose assumption states that the model is fit for
making predictions of the daily maximum temperature with a lead time
of a day for a specific location up to some range. How can this
assumption be justified? Since we have used the model and evaluated
the accuracy of its predictions repeatedly in the past, this past perfor-
mance can be used to justify the fitness of the model for the predictions
at hand. Namely, the model has predicted many past instances of
maximum daily temperature accurately.° This justification can now be
illustrated, for example in a table as shown in Table 1.

This justification can be reconstructed as a deductively valid

© Implicitly, this can be read as saying that the model does not make an
extrapolation far outside the range of values for which it has been trained.

Environmental Modelling and Software 134 (2020) 104754

argument of the following form:

P1 ‘If a model has predicted many past instances of Tmax accurately and the conditions
for the predictions remain sufficiently similar to the past instances, M is fit for
predicting Tmax.

P2 ~~ Mhas predicted many past instances of Tmg, accurately with a lead time of one day.

P3 The conditions for the predictions remain sufficiently similar to past instances.

Cc M is fit for predicting Tmax.

Now that the argumentation is reconstructed, we can proceed to the
second step, namely evaluating to what extent the fitness-for-purpose
assumption is justified. For the evaluation of the justification, we need
to assess whether the premises are true and to what extent they provide
good reasons for the fitness-for-purpose assumption. As the justification
can be reconstructed as a deductively valid argument, the conclusion
must be true if the premises are true. Thus, the evaluation consists in
determining whether all premises are true. P1 makes a conditional claim
about the fitness-for-purpose of the model. This premise, we take it, is
uncontroversial. The premise P2 is related to the evaluation of past
predictions of the model. As mentioned above, the model has been
extensively used in past cases and has been predictively successful. Thus,
P2 is true, too.’ Therefore, the uncertainty of the predictions hinges on
the truth of P3.

The truth of P3 has to be justified based on domain-specific back-
ground knowledge, namely that the past cases are sufficiently repre-
sentative to be confident about the models’ performance more generally
for cases that are similar to the ones considered thus far. The short lead-
time of the predictions makes it likely that P3 is true, too, for two rea-
sons. First, on a practical level, the short lead-time allows for repeated
evaluations of the predictions. Second and more fundamentally, the
short lead-time increases the chance that the evaluated predictions of
the past are representative of the current predictions because the system
is less likely to have experienced large changes in boundary conditions
over a short period of time.®

Hence, the evaluation of the argumentation shows that the fitness-
for-purpose assumption is justified by a deductively valid argument.
Furthermore, there are good reasons to assume that all of the premises of
the argument are true. Hence, the argument seems to be sound (a sound
argument is a deductively valid argument with true premises), which
means that there are good reasons to have confidence in the truth of the
conclusion.

Finally, in the third step, the epistemic uncertainty of the prediction
can be assessed. The first-order uncertainty depends on the extent to
which the fitness-for-purpose assumption is justified. As we have seen,
the justification of the fitness-for-purpose here seems to be a sound
argument, meaning that the model can be considered close to maximally
fit-for-purpose insofar as it has always predicted the temperatures to
within a very small margin of error, and to the extent that we are
justified in having confidence in P3. This means that epistemic first-
order uncertainty is quite small or even absent in the toy example.
Epistemic second-order uncertainty is also small or even absent as it is

” We note here that premises 1 and 2 contain vague expressions, namely the
terms “many instances” and “accurately”. It may be unclear in practical ap-
plications whether sufficiently many instances have been considered in past
model evaluation and whether the range in which errors fall is sufficiently small
to consider model predictions accurately. In real-world applications, quantita-
tive thresholds may sometimes be difficult to set. This is related to the difficulty
we have acknowledged in section 3 regarding the range of errors of a model
that is considered maximally fit-for-purpose. We thank a reviewer for raising
this point.

8 Note that the argument could as well be reconstructed without premise P3.
This would turn the argument into an inductive argument whose strength
would have to be assessed based on the representativeness of the past cases. We
choose and recommend the deductive reconstruction as it makes the uncer-
tainty more explicit.
B. Kniisel et al.

straightforward to reconstruct and evaluate the argument for the fitness-
for-purpose assumption. There may be some second-order uncertainty
related to premise P3 if it is unclear to what extent the past performance
is representative of future performance. The justification of the truth of
P3 fundamentally depends on the system understanding.

Note that in this simple toy example, the framework can also help to
identify the aleatory uncertainty. The reason for this is that the total
uncertainty of the predictions can be estimated in a straightforward way
based on the evaluated predictions. Since the epistemic uncertainty of
the predictions can be reliably estimated as a result of the low level of
second-order uncertainty, the aleatory uncertainty is simply the differ-
ence between total uncertainty and epistemic first-order uncertainty. In
the present case, aleatory uncertainty corresponds to the range that can
be estimated from the small random errors of model predictions. As
Kendall and Gal (2017) have argued, in machine learning aleatory un-
certainty is best understood as uncertainty that cannot be reduced by
collecting additional samples of data, which is exactly the kind of un-
certainty that remains here. If sufficient volumes of data have been used
for model construction and different modeling choices have been tested,
and none of the alternative choices was able to outperform the model
setup in question, then there are reasons to believe that the range of
errors arises due to aleatory uncertainty. Thus, to the extent that these
conditions hold in the toy example, the aleatory uncertainty can readily
be quantified based on records of past model performance.

In sum, all things considered, both epistemic first-order and second-
order uncertainty turn out to be very small in this example. The reason
for the small epistemic uncertainty is that the model predictions have
been evaluated repeatedly for similar cases. This allows model users to
adopt a purely instrumental view of the model, meaning that the focus of
the model evaluation lies purely on its predictive success and not on the
reasons for why the model makes certain predictions, nor on its struc-
ture. Hence, in such a case, an evaluation of uncertainty in terms of
model structure or model parameters would make little sense. This
highlights that the framework introduced here can deal with cases
where model users have a purely instrumental view of their models.

5. Case study: Long-term global selenium predictions

In this section, we present a case study from environmental science
to demonstrate the application of the framework to a long-term pre-
diction to illustrate how the framework works in more complex situa-
tions than the toy example from the previous section. Namely, we
discuss the case of predictions of changes in global soil selenium content
by Jones et al. (2017). The study used data-driven models for three
goals, namely ‘“(i) to test hypothesized drivers of soil Se [selenium]
concentrations, (ii) to predict global soil Se [selenium] concentrations
quantitatively, and (iii) to quantify potential changes in soil Se [sele-
nium] concentrations resulting from climate change” (Jones et al., 2017,
2848). For illustration, we will discuss the last of these three goals, the
impact of climate change on soil selenium.

Jones et al. (2017) used data from over 30.000 samples worldwide to
train three different models, namely, two artificial neural networks and
a random forest model. All three data-driven models relate environ-
mental variables to soil selenium concentrations. The authors performed
a variable selection procedure, after which the seven most important
predictors were retained and trained the three models using historical
data. The chosen predictors were the aridity index, the clay content,
evapotranspiration, lithology, pH, precipitation, and soil organic car-
bon. These trained models were then used to project changes in soil
selenium concentrations due to climate change, hereby using changes in
precipitation and evapotranspiration from climate models (for RCP6.0)
and an accompanying scenario for the development of soil organic
carbon.

Based on the projections performed with these models, Jones et al.
(2017) estimated that average soil selenium concentrations will decline
by 4.3% under the chosen boundary conditions. Selenium is an essential

Table 2

Environmental Modelling and Software 134 (2020) 104754

Application of the conceptual framework to the example with projections of
long-term selenium concentrations.

iter- assumption justification of the assumption

ation

1 The model is fit for projecting soil The model predicts past instances
selenium concentrations in the far of selenium concentrations well.
future for the given boundary The model relationships are
conditions. sufficiently constant over time.

2 The model relationships are The model represents most relevant
sufficiently constant over time. processes driving selenium

concentrations accurately and these
remain sufficiently constant under
changing environmental
conditions.

3 The model represents most relevant The most relevant predictors were
processes driving selenium included in the model.
concentrations accurately andthese Data representative of many
remain sufficiently constant under configurations of the target system
changing environmental was used for model training.
conditions. Sufficiently flexible machine

learning algorithms were used and
overfitting was avoided.

The model is empirically accurate
when tested with data from the
past.

Model behavior is consistent with
background knowledge.

The model results are robust to the
modeling assumptions.

4 The most relevant predictors were The variables were identified based

included in the model.

Data representative of many
configurations of the target system
was used for model training.
Sufficiently flexible machine
learning algorithms were used and
overfitting was avoided.

on domain-specific background
knowledge and a variable selection
procedure.

Data from sufficiently many regions
was considered in model training.

Neural networks and random forest
are very flexible methods, and steps
were taken to avoid overfitting.

micronutrient, which makes information on future selenium loss of this
magnitude potentially decision-relevant. For example, changes to
farming practices might be required to counter the climate impacts to
ensure nutritionally adequate crops. Such measures could include
fertilization and relying on crops that can take up selenium from the soil
even if soil selenium concentrations are lower. However, taking de-
cisions about such measures requires confidence in the predictions,
which, in turn, requires an analysis of the uncertainties of the pre-
dictions. For this, the framework introduced in this paper can be applied.

Again, in a first step, the assumptions underlying the predictions and
their possible justification need to be reconstructed and graphically
represented. An overview of all of the assumptions and their justification
is provided in Table 2. The first assumption is the fitness-for-purpose
assumption. The fitness-for-purpose assumption states that the models
constructed with the given set of drivers and historical data allows to
project (i.e., make a conditional prediction of) future selenium con-
centrations.” However, in this case, the fitness-for-purpose assumption
can no longer be conclusively justified based on the repeated evaluation
of the model predictions (as it was in the toy example) because it is
unclear whether the future cases are sufficiently similar to past cases.

° The reconstruction presented here assumes that the boundary conditions
require no further justification and can just be regarded as given. This is done
for simplicity. A different reconstruction would be possible in which the ade-
quacy of the boundary conditions, which depends on the internal consistency of
the scenario and on how informative the scenario is for the purpose at hand,
could be included. This assumption would then have to be justified based on an
independent evaluation of the respective models that were used to create these
scenarios. As this point is not essential for the present purposes, we will not
engage with this discussion in more detail.
B. Kniisel et al.

 
 
 
 

 
  
 

M underpredicts

 
   

data on selenium
sources was not fe:
available

    

the global average
selenium content

 

    
   

represents the
relevant causal
processes that

Environmental Modelling and Software 134 (2020) 104754

M accurately

          
 

Mis fit for purpose

argument 2 argument 1

 

produce S

Fig. 1. Argument map of the justification of the fitness-for-purpose of the models in the selenium case study by Jones et al. (2017).

Hence, further ways of justifying the fitness-for-purpose are required
besides past model performance.

Rather than arguing for the similarity of past and future cases, a
plausible justification for the fitness-for-purpose assumption is that the
modeled relationships can be assumed to be sufficiently constant over
time (argument 1; for this and all the other labeled arguments, see ap-
pendix). If this constancy assumption holds, the model can be used for
extrapolations into the far future (see Kniisel et al., 2019). However, this
assumption itself requires further justification. A possible justification of
the constancy assumption is that the model accurately represents the
most relevant causal processes that drive selenium concentrations and
that these mechanisms will not change in response to changing envi-
ronmental conditions (argument 2),1°

While an accurate representation of important causal processes can
be used to justify the constancy assumption, it is not clear at first glance
how strong this justification is. The reason for this is that no mechanisms
were explicitly included in the models (as has been discussed in section
2). Thus, the assumption regarding the representational accuracy of
important causal processes also needs to be justified. Furthermore, even
if the model represents important causal processes, it may be unclear to
what extent these mechanisms remain constant when extrapolated to
changing environmental conditions. This is relevant here because the
model is used to make projections for values of the variables that are
somewhat different from today’s values because of climate change.

The justification of the assumption that most of the relevant causal
processes are accurately represented leads to a third argumentation
iteration, and hence a third row in Table 2. Due to the lack of explicit
representations of processes, the assumption needs to be justified indi-
rectly. There are several ways of how this assumption could be justified.
Some of these have been suggested as conditions for the adequacy of
machine learning approaches for yielding reliable predictions or for
yielding representationally accurate models in the philosophical litera-
ture. For example, Pietsch (2015) has argued that models built with
machine learning can extract the causal structure of a phenomenon if,
among other things, (1) the most important variables are included in
model construction and (2) data representative of the relevant config-
urations of the target system are considered. Kniisel and Baumberger
(2020, forthcoming) have further emphasized that (3) the use of suffi-
ciently flexible machine learning algorithms, (4) the empirical accuracy
of the models, (5) whether model behavior is consistent with back-
ground knowledge, and (6) robustness arguments can provide
non-deductive arguments for the representational accuracy of
data-driven models.

Hence, these six points can be used to form arguments justifying the
assumption that the most important causal mechanisms are accurately
represented in the model. The points (1), (2), and (3) are best recon-
structed as the premises of one argument (argument 3). We note here

10 We note here that a different justification for the constancy of the described
relationship could also be that two factors whose relationship is modeled have a
common cause instead of being directly causally related. However, the direct
path is the more plausible one here.

that point (3) refers to the use of sufficiently flexible machine learning
algorithms. Such flexible methods have to be accompanied by measures
to avoid overfitting, which should also be considered in the argument.
The points (4), (5), and (6) are each best interpreted as premises of a
separate argument. Point (4) states that the models are empirically ac-
curate as assessed by a low cross-validation error (argument 4). Point (5)
states that model behavior, as assessed through sensitivity analysis, is
consistent with background knowledge about the system (argument 5).
Point (6), finally, states that three different machine learning algorithms
were used and largely agreed (i.e., a robustness argument) (argument 6).

Assessing the justification provided by points (4), (5), and (6) only
requires a consideration of model outputs and a test dataset. Thus, these
points require no further justification. This is not true of the points (1),
(2), and (3) because their truth cannot be directly assessed by consid-
ering model outputs. Hence, they require further justification, leading to
a fourth iteration. In order to justify them, scientists have to rely on both
background knowledge on the behavior of trace elements in the envi-
ronment and on understanding of and experience with machine
learning. Each of these three points needs to be justified individually,
resulting in three additional arguments (arguments 7, 8, and 9).

With this, the possible justification of the fitness-for-purpose has
been identified. For space reasons, we do not provide an explicit
reconstruction of the arguments here. A reconstruction of the complex
argumentation is provided in the appendix. We note here that the ar-
guments in the first two iterations can be reconstructed as deductively
valid arguments. This is not easily possible for the third iteration. Fig. 1
is an argument map showing how the different arguments relate to each
other. Note that the empirical accuracy of the model predictions now
appears twice as a justification: It is a necessary condition for consid-
ering the model fit-for-purpose in argument 1, but it also gives some
indication that the relevant processes are represented accurately in the
model in argument 4.

In the second step, the arguments for justifying the fitness-for-
purpose have to be evaluated. The truth of the individual premises
(see the reconstruction in the appendix) has to be evaluated based on
domain-specific background knowledge, the evaluation of the models
with available data, and the comparison of the behavior of the three
individual models. Based on the model evaluation as discussed by Jones
et al. (2017), all the premises of the arguments seem to be at least
approximately true. There is, however, one exception to this, concerning
the choice of variables: As Jones et al. (2017) note, their model lacks
selenium sources, which leads to an underprediction of global average
selenium values. The reason for this is that data on selenium sources like
atmospheric deposition or biomass deposition was missing.

The problems emerging from the underprediction and from missing
variables are displayed as two theses that attack some of the assump-
tions in Fig. 1. That there is an underprediction of average soil selenium
concentrations attacks the inference from the empirical accuracy of the
models to the conclusion that most relevant processes are accurately
represented in the models. That data on selenium sources was lacking
attacks the premise that most relevant predictors were considered
because the global underprediction shows that selenium sources are
B. Kniisel et al.

important for soil selenium concentrations. This means that the strength
of the argument from empirical accuracy is reduced somewhat, and the
argument about the most important variables being considered has a
premise that is strictly speaking false. These problems result in a less-
than-maximal degree of fitness-for-purpose because the resulting bias
increases the range around the model predictions in which observations
should be expected to lie.

This case study is not only characterized by a less-than-maximal
degree of fitness-for-purpose but also by a non-conclusive justification
of the actual degree of fitness-for-purpose. The reason for this is that
while several arguments can be provided to argue for the assumption
that most relevant mechanisms are accurately represented in the model,
they neither individually nor jointly guarantee that the mechanisms are
represented accurately. This is because in the third iteration, the argu-
ments are not deductively valid but provide only more or less strong
reasons for the truth of their conclusion. Hence, also the preceding as-
sumptions regarding the constancy of the identified relationships and
regarding the fitness-for-purpose cannot be justified conclusively.

Now, in a third step, we can proceed to assessing the uncertainty
based on the argumentation reconstructed above. As noted, some of the
provided premises are known to be false, strictly speaking, namely the
assumption that all relevant variables were included. This also some-
what affects the empirical accuracy of the models. Furthermore, the
justifications provided can neither individually nor jointly guarantee
that the models are really fit-for-purpose. The lack of good justification
of some of the assumptions leads to a lower-than-maximal fitness-for-
purpose, which means that there is more epistemic first-order uncer-
tainty in this case study compared to the toy example from above. The
framework introduced here does not only highlight that the epistemic
uncertainty of the prediction is comparatively large, it also highlights
which specific aspects of the justification are the source of this uncer-
tainty. Second-order uncertainty is also substantially larger than in the
toy example above. The reason for this is that it is not clear to what
extent the modeling assumptions are justified by the provided evidence.
This has to do, in part, with the opacity of the models, as it is not entirely
clear what relations they actually represent and on what grounds.''
More importantly, it has to do with the lack of background knowledge to
judge to what degree the assumptions are justified by the provided ar-
guments. This lack of background knowledge makes it difficult to assess
the strength of the non-deductive arguments in the third and fourth it-
erations shown in Table 2.

Thus, all uncertainties considered, we see that in this example, both
first-order and second-order epistemic uncertainty are present to a
larger degree than in the toy example. Substantial first-order uncertainty
arises for two reasons, namely, because the justification of the degree of
fitness-for-purpose is non-conclusive, and because of the low bias of the
models that cannot be corrected for, which reduces the actual degree of
fitness-for-purpose. The former source of uncertainty stems from non-
deductive arguments in the justification, due to which the conclusion
need not be true even if all the premises are true. This means that the
provided justification cannot guarantee that the model is generally fit
for the kind of prediction of interest. The latter source of uncertainty
arises from an underprediction of global average selenium concentra-
tions resulting from a lack of data on selenium sources. This low bias in
the predictions reduces the actual degree of fitness-for-purpose of the
models. The sources of this bias are apparent in the justification scheme
as two theses attacking two of the arguments. Namely, the lack of data
on selenium sources attacks the argument stating that the most impor-
tant variables were included in model construction (argument 8); and
the resulting underprediction attacks the argument from empirical ac-
curacy to representational accuracy (argument 4). Note again here that

11 The reason for this is that the models do not provide an explicit equation or
a set of rules that could be analyzed. This is especially true of the models used
by Jones et al. (2017), random forest and neural networks.

Environmental Modelling and Software 134 (2020) 104754

even though the degree of fitness-for-purpose is less-than-maximal, the
models are not outright unfit-for-purpose.

Second-order uncertainty in this case study arises because of certain
arguments whose strength is difficult to evaluate. For example, it is
unclear whether the models really do capture important causal pro-
cesses that are sufficiently constant under changing environmental
conditions. This is due to a lack of domain-specific background knowl-
edge. Also, the arguments provided for the statement that the model
represents important causal processes accurately are all non-deductive.
These non-deductive arguments introduce first-order uncertainty
because the justification of fitness-for-purpose becomes less conclusive.
They also introduce second-order uncertainty because the strength of
the justification is difficult to evaluate due to a lack of system
understanding.

The arguments discussed above can also be found in the paper by
Jones et al. (2017) who discussed them in order to understand the un-
certainties of the inferences. For example, they provide a discussion of
missing variables (concerning (1) above) and of the representativeness
of the available samples (concerning (2) above). Furthermore, they
discussed the empirical accuracy of the model in a cross-validation
setting (concerning (4) above) and conducted sensitivity analyses to
assess whether model behavior is consistent with background knowl-
edge (concerning (5) above). Finally, they only considered predictions
for pixels where the three data-driven models agreed on the sign of
change. Hence, they considered the robustness of the predictions (con-
cerning (6) above). This shows that the arguments provided here were
actively engaged with by the authors of the original study. Note, how-
ever, that the assumptions discussed are not explicitly made during the
process of model construction by the modelers. Rather, they are as-
sumptions that modelers need to make once they apply the models for
certain kinds of long-term predictions.

As noted, both first-order and second-order uncertainty were
considerably larger in this case study than in the toy example above. The
reason for this is that the data-driven models were constructed for the
selenium prediction but due to the long lead-time of the prediction and
the lack of evaluation of the model predictions for the desired purpose,
model users had to adopt a more realistic interpretation of model
behavior (compared to the more instrumental view of the model in the
toy example). Hence, the reason for the increase in both types of
epistemic uncertainty is not simply that data-driven models were used,
but rather that data-driven models were used in a context where the
uncertainty cannot be estimated from the past performance of the model
alone. The conclusion of this discussion is likely to hold more generally
in cases where background knowledge is insufficient to provide
conclusive justification of the fitness-for-purpose assumption. As Kniisel
et al. (2019) argue, data-driven models are often constructed when
background knowledge is insufficient for constructing process-based
models. Hence, the points about increasing second-order uncertainty
are likely to hold more generally, not just for this case study. The flex-
ibility of the framework presented here might lead to different argu-
ments being relevant in different contexts. However, the arguments
highlighted in this case study are likely to show up in different contexts
again, specifically the six reasons provided for assuming that the model
represents most of the relevant processes accurately. In some examples,
it is well possible that further iterations are required to justify some of
the six points raised above.

6. Implications for decision-making

One of the key reasons for better understanding the uncertainties of
scientific inferences is that this understanding is required for epistemi-
cally confident decision-making. Hence, more needs to be said about
how the kind of information provided by the framework presented here
can be handled in decision-making. This is specifically important
because the framework delivers information on two types of epistemic
uncertainty, first-order and second-order uncertainty and characterizes
B. Kniisel et al.

them in a purely qualitative form. In this section we address how the
information on uncertainty provided by our framework can be used
effectively for decision-making and point to areas where further
research is necessary.

As Walker et al. (2003, 8) state, quantified “statistical uncertainty
should not be accorded as much attention as other levels of uncertainty
in the uncertainty analysis” if there are more severe levels of uncertainty
present. This means that uncertainties should only be quantified when
researchers are in a position to do so confidently, which is in line with
the faithfulness requirement for uncertainty assessment discussed by
Parker and Risbey (2015). As shown by Guillaume et al. (2017), re-
searchers already effectively use various methods other than quantifi-
cation to express uncertainty. Thus, decision makers who are used to
handling scientific information may have no problems in handling the
qualitative information on uncertainty provided by the framework
introduced in this paper. Nevertheless, many formalized decision prin-
ciples require that information on first-order uncertainties be quantified.
This quantification first requires a good understanding of what the
relevant sources of uncertainty are, i.e., it requires an understanding of
which assumptions lead to uncertainty. In this sense, the framework
presented here can be used to build the groundwork for uncertainty
quantification because it highlights where uncertainties come from and
what the relevant uncertainties are.

Approaches exist to quantify uncertainties from machine learning
predictions. Some of these, such as quantile regression forests, directly
provide probabilistic information by predicting not only the best esti-
mate but also the quantiles of the probability distribution function,
which is learned from the data (Meinshausen, 2006). There are also
approaches for estimating uncertainty that are based on Bayesian
reasoning that account for the uncertainty of individual parameter
values in deep learning (see e.g. Blundell et al., 2015; Gal and Ghahra-
mani, 2016). These approaches are useful to quantify uncertainty that
can directly be inferred from the available data. We recognize that they
yield valuable information and can provide a full account of un-
certainties in some settings, e.g., in image classification tasks. However,
in cases such as the case study considered in this paper, these approaches
would not be able to quantify the full uncertainty. Namely, as the
argument-based approach introduced here reveals, some of the uncer-
tainty cannot be inferred directly from the data. For example, this
concern holds for the statement that the identified relationships remain
sufficiently constant. While this assumption leads to uncertainty, it is not
the kind of uncertainty that can be learned directly from the available
data. In examples like the case study presented in this paper, reporting
uncertainty purely based on such algorithmic approaches would result
in uncertainty assessments that are neither faithful nor complete (cf.
Parker and Risbey, 2015). Hence, the case study discussed above illus-
trates the need for additional approaches for uncertainty quantification.

One promising approach might be to rely on structured expert elic-
itation in order to estimate quantitative information on uncertainties
from the qualitative information that the framework presented here
provides (see Morgan, 2014; Thompson et al., 2016; Oppenheimer et al.,
2016). As it will generally be difficult to create exact uncertainty esti-
mates based on the framework, experts will likely be inclined to provide
imprecise probability estimates. This would require experts to consider a
graphical representation such as the argument map in Fig. 1 and assess
the strength of the arguments provided for the fitness-for-purpose
assumption at hand. The aforementioned methods for uncertainty
quantification based on the robustness of the results can provide a good
starting point here. Based on the expert assessment, the intervals ob-
tained would have to be widened or narrowed accordingly. The strength
of the arguments discussed above should be assessed by domain experts.
For some of the factors leading to uncertainty, it can suffice to specify
plausible scenarios without quantitative information on their probabil-
ity (this would be scenario uncertainty in the matrix of Walker et al.,
2003). This is for example the case for the information on boundary
conditions regarding the changing climatic conditions in the case study

Environmental Modelling and Software 134 (2020) 104754

introduced above.

The framework does not only provide information on first-order but
also on second-order uncertainty. When quantifying first-order uncer-
tainty, second-order uncertainty should be considered, too. A large
second-order uncertainty means that it is difficult to judge the degree of
fitness-for-purpose of the model. This means that first-order uncertainty
will be only weakly constrained. If first-order uncertainty is less well
constrained, a trade-off emerges. Experts can either provide narrower
estimates of first-order uncertainty and be less confident about it (i.e.,
they face more second-order uncertainty) or provide a wider estimate of
first-order uncertainty with more confidence (see Winsberg 2018b,
chap. 7). Balancing this trade-off has to be based on what is perceived to
be the most useful for decision-makers (Winsberg, 2018a).

Research into the development of decision principles that can be
used with the two-tiered information on uncertainty discussed here is
still needed (Winsberg 2018b; chap. 8). A candidate approach is the
confidence approach that considers different models depending on de-
cision makers’ risk attitude (Roussos, Bradley, and Frigg, 2020, forth-
coming). A different approach is decision-making with possibilistic
information, i.e., with information on what is and what is not consistent
with our understanding of a system (Betz, 2016a). If model-based in-
formation is handled with such a possibilistic mindset, a greater
second-order uncertainty implies that it is more difficult to distinguish
between outcomes that are consistent with our background knowledge,
outcomes that are inconsistent with our background knowledge, and
outcomes that cannot be put in either of these categories. Hence, a
possible outcome (an event with some epistemic first-order uncertainty)
with a large second-order uncertainty might have to be considered by a
risk-averse decision-maker even if its largest possible likelihood, as
estimated based on the first-order uncertainty, seems small. The reason
for this is that its (first-order) uncertainty assessment is uncertain and
might need to be revised in light of new information.

Predictions are not the only way in which models can provide
decision-relevant information. Namely, knowledge of causal connec-
tions and exploratory modeling can guide policy decisions (Weaver
et al., 2013). In such cases, models are needed that represent the pro-
cesses responsible for producing a phenomenon with sufficient accu-
racy. Data-driven models can be fit for providing this kind of
information, too. In these cases, the evaluation of the models’ fitness is
similar to the uncertainty analysis seen in section 5 (see Kniisel and
Baumberger, 2020 forthcoming).

7. Conclusions

In this paper, we have presented an argument-based framework for
assessing the uncertainties of model-based predictions. We hereby
focused on features of data-driven models and showed that the frame-
work is able to analyze the uncertainty of predictions from data-driven
models. Based on a toy example and the extensive discussion of a case
study from environmental science, we highlighted how the application
of the framework works in practice. Constructing data-driven models is
possible also when a phenomenon is comparatively ill-understood.
However, this lack of background knowledge and the opacity of data-
driven models can lead to substantial second-order uncertainty as we
have shown here. We then discussed what the framework implies for the
quantification of uncertainties and for decision-making based on infor-
mation from data-driven models. Open questions remain specifically
with respect to the quantification of uncertainties. We encourage at-
tempts at using structured expert elicitation as suggested here and
further research into decision principles.

Environmental scientists working with data-driven models are often
aware of the limitations and uncertainties of their models. However, the
lack of conceptual tools for uncertainty assessments may inhibit a clear
understanding of how large these uncertainties are. Thus, there can
potentially be overconfidence about results obtained with data-driven
models. The lack of conceptual tools can also impair a_ better
B. Kniisel et al.

understanding of the factors that lead to the uncertainty. Understanding
these factors can be useful for researchers, e.g., to identify what steps
they could take to reduce the impact of a specific factor that leads to
uncertainty. The framework presented here provides tools to perform
such uncertainty assessments and communicate the uncertainty of pre-
dictions from data-driven models more transparently. Hence, we
encourage researchers developing and working with data-driven models
to employ the framework provided here to assess the predictive un-
certainties of their models. Being more explicit about uncertainties in-
creases the usefulness of data-driven models both for scientific and
policy purposes. At the same time, explicitly discussing the representa-
tional function of models may reveal that data-driven models are more
skillful in some applications than one might have expected initially.
Hence, the argument-based framework provided here can help to make
good use of data-driven models in environmental science.

Kwakkel et al. (2010) have emphasized the importance of using a
common language in uncertainty assessments in order to provide in-
formation to decision-makers that is easier for them to compare to other
cases and contexts. We agree with this view. However, in the case of
data-driven models, it seems unlikely that locations of uncertainty
similar to the ones from other frameworks can be defined that can be
applied to data-driven models generally and are informative of their
predictive uncertainty. For example, it might be intuitive here to speak
of “model uncertainty” and “extrapolation uncertainty”. However, as
the discussion of the case study has shown, how much uncertainty the
extrapolation introduces directly depends on properties of the model.
Hence, these two terms would not refer to distinct locations of uncer-
tainty. However, we encourage future work that aims to find a termi-
nology for the information from our framework that can consistently be

Appendix

A. Reconstruction of the arguments for the case study

Environmental Modelling and Software 134 (2020) 104754

related to uncertainties from other frameworks. Future research should
also address decision principles that can handle the kind of uncertainty
that the framework presented here provides.

The considerations made in this paper are likely relevant beyond
data-driven models. The framework discussed here is quite general. It
can hence be applied to other types of models, too and could comple-
ment existing discussions of uncertainty of environmental models.
Furthermore, as the framework focuses on assumptions and how they
are justified, it can potentially reveal that some of the analyzed as-
sumptions concern value judgments and hence highlight cases of value
uncertainty.

Funding

This research was funded by the Swiss National Science Foundation
under the National Research Programme “Big Data” (NRP75), project
no. 167215.
Declaration of competing interest

The authors declare no conflicts of interest.
Acknowledgments

We thank Richard Bradley, Roman Frigg, Gertrude Hirsch Hadorn,
David Stainforth, and Lenny Winkel for discussions and/or feedback on
earlier versions of this manuscript. We further thank the participants of

the workshop Uncertainty in Data-Driven Environmental Modeling at ETH
Zurich in August 2019.

Here, we present the individual arguments that can be made to justify the fitness-for-purpose in the case study presented in section 5. It is
essentially an explicit reconstruction of the justifications shown in Table 2. The method used to reconstruct the arguments and relate them to each
other in the argument map is largely based on Betz (2016b) with the exception that we also consider non-deductive arguments here. For a general
introduction into the analysis of practical arguments, the reader is referred to Brun and Betz (2016).

In this reconstruction, the variable M denotes the model ensemble used by Jones et al. (2017), consisting of an ensemble of data-driven models, one
of which was built using random forest, and two of which were built using artificial neural networks. The variable S refers to soil selenium con-
centrations. The first argument corresponds to the first row of Table 2 and directly concerns the fitness-for-purpose of the model and is similar to the

one presented in the toy example:

Argument 1
P1.1
phenomenon in the far future.
P1.2_Mhas predicted many past instances of S accurately.
P1.3. The modeled relationships in M remain sufficiently constant.
Cl M is fit for predicting S in the far future.

If a model has predicted many past instances of a phenomenon accurately and the modeled relationships remain sufficiently constant, that model is fit for predicting the

In argument 1, premise P1.3 requires further justification. A possible justification is based on the fact that the relevant causal processes are
represented in the model in a sufficiently accurate manner. This argument can again be reconstructed as a deductively valid argument:

If a model represents the most important causal processes producing a phenomenon accurately and these processes are unaffected by changing environmental conditions, the

Argument 2
P2.1
modeled relationships remain sufficiently constant.
P2.2 The causal processes represented in M are unaffected by changing environmental conditions.
P2.3_  M accurately represents the most important causal processes that produce S.
C2 The modeled relationships in M remain sufficiently constant. (= P1.3)

10
B. Kniisel et al. Environmental Modelling and Software 134 (2020) 104754

While this argument is deductively valid, it is not clear whether its premises are true. Premise P2.1 seems uncontroversial. Premise P2.2 requires
some further justification. This can for example be justified based on background knowledge, e.g., if the processes represented are consistent with
current scientific understanding and there is reason to believe that they are not dependent on current environmental conditions. Premise P2.3 in
argument 2 also requires further justifications. There are four arguments that can be made in favor of P2.3, all of which are non-deductive. Hence, in
these arguments, even if all the premises are true, they neither individually, nor jointly guarantee the truth of the conclusion. The first of these ar-
guments refers the reasons (1), (2), and (3) presented in the main text and concerns how the machine learning algorithms were trained to construct the
ensemble of data-driven models:

Argument 3
P3.1 M was constructed using data that represents sufficiently many configurations of S.
P3.2 M was constructed using the most important variables.
P3.3 M was constructed using sufficiently flexible methods, and overfitting was avoided.
C3 M accurately represents the most important causal processes that produce S. (=P2.3)

In argument 3, the individual premises require further justification, too. This justification has to be made by referring to background knowledge.
The expertise of both domain scientists and data scientists is necessary who need to judge whether the considered samples are sufficiently diverse
(P3.1), whether relevant variables were omitted (P3.2), and whether the used methods were sufficiently flexible (P3.3).

A second argument that can be made in favor of P2.3 refers to the empirical accuracy of the model:

Argument 4
P4.1 M is empirically accurate with respect to the data from the past.
C4 M accurately represents the most important causal processes that produce S. (= P2.3)

Note here that there is a thesis that attacks argument 4. Namely, M has a low bias and underpredicts global average soil selenium concentration.
This underprediction attacks P4.1 to some extent.

A third argument considers the consistency of the model with background knowledge. The truth of P5 can be established by conducting sensitivity
analyses of the models. Furthermore, Jones et al. (2017) also use existing samples to show that the rate of change predicted by their models has
historical precedents, which also serves as evidence for the truth of P5.

Argument 5
P5 M behaves in consistency with background knowledge about S.
C5 M accurately represents the most important causal processes that produce S. (= P2.3)

Finally, a fourth argument can be made that refers to the robustness of the models because predictions were only considered for the regions in
which all three machine learning algorithms agreed.

Argument 6
P6 The predictions are only considered if the ensemble members of M agree on the sign of change of S.
C6 M accurately represents the most important causal processes that produce S. (= P2.3)

As mentioned above, the premises of Argument 3 all require further justification. For each of these, that justification has to come from background
knowledge.

Argument 7
P7 M was constructed using over 30.000 samples from different continents.
C7 M was constructed using data that represents sufficiently many configurations of S. (=P3.1)
Argument 8
P81 M was constructed using seven variables chosen based on a variable selection procedure.
P8.2 Most potentially relevant variables were included in the variable selection procedure.
C8 M was constructed using the most important variables. (= P3.2)

(continued on next page)

11
B. Kniisel et al. Environmental Modelling and Software 134 (2020) 104754

(continued )
Argument 9
P9.1 M was constructed using artificial neural networks and random forest.
P9.2 Measures were taken to avoid overfitting.
Argument 9
P9.1 M was constructed using artificial neural networks and random forest.
P9.2 Measures were taken to avoid overfitting.
c9 M was constructed using sufficiently flexible methods, and overfitting was avoided. (= P3.3)

A problem emerges with respect to argument 8. Namely, as has been noted, data on selenium sources was lacking. This is what leads the models to
underpredict global average selenium concentration (see argument 4 above). The low bias of the models shows that these sources of selenium are
important for soil selenium concentrations. Hence, that data on these sources was lacking directly attacks premise P8.2, which states that all
potentially relevant variables were included in the variable selection procedure.

The measures mentioned in argument 9 which were taken to avoid overfitting include the restriction of the number of variables considered, the
model evaluation, and that categorical variables were only considered if each class contained sufficiently many cases (as explained in the supple-
mentary material of the article by Jones et al., 2017). We also note here that a more detailed justification of the chosen algorithms could be made, in
which choices such as meta-parameters for random forest or the number of hidden layers in neural networks would be explicitly discussed to justify
that the methods are sufficiently flexible. For reasons of simplicity, we focus on the more general justification here.

All of these arguments can then be arranged in an argument map as shown in Fig. 1 in the article. This map is created as introduced by Betz
(2016b). White boxes refer to arguments, and grey boxes to theses. Solid arrows denote that the content of one box, be it an argument or a thesis,
supports the content of the other box. Dashed arrows denote that the content of one box attacks the content of the other box. Note that the solid arrows
in Fig. 1 do not differentiate between deductive and non-deductive arguments.

If an arrow goes from a thesis to an argument, this means that the thesis is a premise of the argument (support) or that the thesis contradicts a
premise of the argument (attack). If the arrow goes from an argument to a thesis, this means that the thesis is the conclusion of the argument (support)
or that the thesis is contradicted by the conclusion of the argument (attack).

References Haasnoot, M., van Deursen, W.P.A., Guillaume, J.H.A., Kwakkel, J.H., van Beek, E.,
Middelkoop, H., 2014. Fit for purpose? Building and evaluating a fast, integrated
model for exploring water policy pathways. Environ. Model. Software 60 (October),
99-120. https://doi.org/10.1016/j.envsoft.2014.05.020.

Hamilton, Serena H., Fu, Baihua, Guillaume, Joseph H.A., Badham, Jennifer,

Elsawah, Sondoss, Gober, Patricia, Randall, J. Hunt, et al., 2019. A framework for
characterising and evaluating the effectiveness of environmental modelling. Environ.
Model. Software 118 (August), 83-98. https://doi.org/10.1016/j.
envsoft.2019.04.008.

The Argumentative Turn in Policy Analysis. In: Hansson, Sven Ove, Hirsch
Hadorn, Gertrude (Eds.), 2016, Logic, Argumentation & Reasoning, first ed., vol. 10.
Springer International Publishing, Cham. https://doi.org/10.1007/978-3-319-
30549-3.

Held, Isaac M., 2005. The gap between simulation and understanding in climate
modeling. Bull. Am. Meteorol. Soc. 86 (11), 1609-1614. https://doi.org/10.1175/
BAMS-86-11-1609.

Hirsch, Hadorn, Gertrude, Brun, Georg, Riccarda Soliva, Carla, Stenke, Andrea,

Peter, Thomas, 2015. Decision strategies for policy decisions under uncertainties: the
case of mitigation measures addressing methane emissions from ruminants. Environ.
Sci. Pol. 52 (October), 110-119. https://doi.org/10.1016/j.envsci.2015.05.011.

Jones, Gerrad D., Boris, Droz, Peter, Greve, Gottschalk, Pia, Poffet, Deyan,

McGrath, Steve P., Seneviratne, Sonia I., Smith, Pete, Lenny, H., Winkel, E., 2017.
Selenium deficiency risk predicted to increase under future climate change. Proc.

Natl. Acad. Sci. Unit. States Am. 114 (11), 2848-2853. https://doi.org/10.1073/

pnas.1611576114.

Kendall, Alex, Gal, Yarin, 2017. What uncertainties do we need in bayesian deep learning
for computer vision?. In: Proceedings of the 31st Conference on Neural Information
Processing Systems, vol. 11. Long Beach.

Kloprogge, Penny, Jeroen, P., van der, Sluijs, Petersen, Arthur C., 2011. A method for the
analysis of assumptions in model-based environmental assessments. Environ. Model.
Software 26 (3), 289-301. https://doi.org/10.1016/j.envsoft.2009.06.009.

Kniisel, Benedikt, and Christoph Baumberger. under review. “Understanding climate
phenomena with data-driven models.”.

Kniisel, Benedikt, Zumwald, Marius, Baumberger, Christoph, Hirsch Hadorn, Gertrude,
Fischer, Erich M., Bresch, David N., Knutti, Reto, 2019. Applying big data beyond
small problems in climate research. Nat. Clim. Change 9, 196-202. https://doi.org/
10.1038/s41558-019-0404-1.

Knutti, Reto, 2018. Climate model confirmation: from philosophy to predicting climate
in the real world. In: Climate Modelling, Edited by Elisabeth A. Lloyd and Eric
Winsberg, vol. 59. Springer International Publishing, Cham, p. 325. https://doi.org/
10.1007/978-3-319-65058-6_11.

Kwakkel, Jan H., Walker, Warren E., Vincent, A.W.J. Marchau, 2010. Classifying and
communicating uncertainties in model-based policy analysis. Int. J. Technol. Pol.
Manag. 10 (4), 299. https://doi.org/10.1504/IJTPM.2010.036918.

Lahtinen, Tuomas J., Guillaume, Joseph H.A., Hamalainen, Raimo P., 2017. Why pay
attention to paths in the practice of environmental modelling? Environ. Model.
Software 92 (June), 74-81. https://doi.org/10.1016/j.envsoft.2017.02.019.

Badham, Jennifer, Elsawah, Sondoss, Joseph, H., Guillaume, A., Hamilton, Serena H.,
Hunt, Randall J., Jakeman, Anthony J., Pierce, Suzanne A., et al., 2019. Effective
modeling for integrated water resource management: a guide to contextual practices
by phases and steps and future opportunities. Environ. Model. Software 116 (June),
40-56. https://doi.org/10.1016/j.envsoft.2019.02.013.

Baumberger, Christoph, Knutti, Reto, Hirsch Hadorn, Gertrude, 2017. Building
confidence in climate model projections: an analysis of inferences from fit. Wiley
Interdisciplinary Reviews: Climate Change 8 (3), e454. https://doi.org/10.1002/
wec.454.

Betz, Gregor, 2016a. “Accounting for possibilities in decision making.” in the
argumentative Turn in policy analysis. In: Sven Ove Hansson and Gertrude Hirsch
Hadorn, vol. 10. Springer International Publishing, Cham, pp. 135-169. https://doi.
org/10.1007/978-3-319-30549-3_6.

Betz, Gregor, 2016b. Logik und Argumentationstheorie. In: Neues Handbuch des
Philosophie-Unterrichts, edited by Jonas Pfister and Peter Zimmermann, 1. Auflage.
UTB Philosophie, Ethik, Didaktik 4514. Bern. Haupt Verlag.

Blundell, Charles, Cornebise, Julien, Kavukcuoglu, Koray, Wierstra, Daan, 2015. Weight
uncertainty in neural networks. In: Proceedings of the 32nd International Conference
on Machine Learning, vol. 32. Lille. http://arxiv.org/abs/1505.05424.

Bradley, Richard, Drechsler, Mareile, 2014. Types of uncertainty. Erkenntnis 79 (6),
1225-1248. https://doi.org/10.1007/s10670-013-9518-4.

Brun, Georg, Gregor Betz, 2016. “Analysing practical argumentation.” in the
argumentative Turn in policy analysis. In: Sven Ove Hansson and Gertrude Hirsch
Hadorn, vol. 10. Springer International Publishing, Cham, pp. 39-77. https://doi.
org/10.1007/978-3-319-30549-3_3.

Gal, Yarin, Ghahramani, Zoubin, 2016. Dropout as a bayesian approximation:
representing model uncertainty in deep learning. In: Proceedings of the 33rd
International Conference on Machine Learning, vol. 33, p. 10. New York.

Ghahramani, Zoubin, 2015. Probabilistic machine learning and artificial intelligence.
Nature 521 (May), 452-459. https://doi.org/10.1038/naturel4541.

Gibert, Karina, Horsburgh, Jeffery S., Athanasiadis, Ioannis N., Holmes, Geoff, 2018a.
Environmental data science. Environ. Model. Software 106 (August), 4-12. https: //
doi.org/10.1016/j.envsoft.2018.04.005.

Guillaume, Joseph H.A., Kummu, Matti, Rasanen, Timo A., Jakeman, Anthony J., 2015.
Prediction under uncertainty as a boundary problem: a general formulation using
iterative closed question modelling. Environ. Model. Software 70 (August), 97-112.
https://doi.org/10.1016/j.envsoft.2015.04.004.

Gibert, Karina, Izquierdo, Joaquin, Sanchez-Marre, Miquel, Hamilton, Serena H.,
Rodriguez-Roda, Ignasi, Holmes, Geoff, 2018. Which method to use? An assessment
of data mining methods in environmental data science. Environ. Model. Software
110 (December), 3-27. https://doi.org/10.1016/j.envsoft.2018.09.021.

Guillaume, Joseph H.A., Casey, Helgeson, Elsawah, Sondoss, Jakeman, Anthony J.,
Kummu, Matti, 2017. Toward best practice framing of uncertainty in scientific
publications: a review of water resources research abstracts. Water Resour. Res. 53
(8), 6744-6762. https://doi.org/10.1002/2017WRO020609.

12
B. Kniisel et al.

Lloyd, Elisabeth A., 2009. “I—elisabeth A. Lloyd: varieties of support and confirmation of
climate models. Aristotelian Society Supplementary 83 (1), 213-232. https://doi.
org/10.1111/j.1467-8349.2009.00179.x.

Meinshausen, Nicolai, 2006. Quantile regression forests. J. Mach. Learn. Res. 7,
983-999.

Morgan, M.G., 2014. Use (and abuse) of expert elicitation in support of decision making
for public policy. Proc. Natl. Acad. Sci. Unit. States Am. 111 (20), 7176-7184.
https: //doi.org/10.1073/pnas.1319946111.

Northcott, Robert, 2019. Big data and prediction: four case studies. Studies in History
and Philosophy of Science. https://doi.org/10.1016/j.shpsa.2019.09.002 (in press).

Oppenheimer, Michael, Little, Christopher M., Cooke, Roger M., 2016. Expert judgement
and uncertainty quantification for climate change. Nat. Clim. Change 6 (5),
445-451. https://doi.org/10.1038/nclimate2959.

Overpeck, J.T., Meehl, G.A., Bony, S., Easterling, D.R., 2011. Climate data challenges in
the 21st century. Science 331 (6018), 700-702. https://doi.org/10.1126/
science.1197869.

Parker, Wendy S., 2020. “Model evaluation: an adequacy-for-purpose view. Philos. Sci.
https://doi.org/10.1086/708691. In press.

Parker, Wendy S., 2010. Predicting weather and climate: uncertainty, ensembles and
probability. Stud. Hist. Philos. Sci. B Stud. Hist. Philos. Mod. Phys. 41 (3), 263-272.
https://doi.org/10.1016/j.shpsb.2010.07.006.

Parker, Wendy S., 2009. Confirmation and adequacy-for-purpose in climate modelling.
Aristotelian Society Supplementary 83 (1), 233-249. https://doi.org/10.1111/
j-1467-8349.2009.00180.x.

Parker, Wendy S., Risbey, James S., 2015. False precision, surprise and improved
uncertainty assessment. Phil. Trans. Math. Phys. Eng. Sci. 373, 20140453. https://
doi.org/10.1098/rsta.2014.0453, 2055.

Pietsch, Wolfgang, 2015. Aspects of theory-ladenness in data-intensive science. Philos.
Sci. 82 (5), 905-916. https://doi.org/10.1086/683328.

13

Environmental Modelling and Software 134 (2020) 104754

Refsgaard, Jens Christian, Jeroen, P., van der, Sluijs, 2007. Anker lajer hgjberg, and peter
A. Vanrolleghem. “Uncertainty in the environmental modelling process — a
framework and guidance. Environ. Model. Software 22 (11), 1543-1556. https://
doi.org/10.1016/j.envsoft.2007.02.004.

Reichstein, Markus, Camps-Valls, Gustau, Stevens, Bjorn, Jung, Martin,

Denzler, Joachim, Carvalhais, Nuno, Prabhat, 2019. Deep learning and process
understanding for data-driven earth system science. Nature 566 (February),
195-204. https://doi.org/10.1038/s41586-019-0912-1.

Roussos, Joe, Bradley, Richard, Roman, Frigg, 2020. Forthcoming. “Making confident
decisions with model ensembles.” Philosophy of science. http://philsci-archive. pitt.
edu/17053/1/Roussos%20Frigg%20Bradley%20-%20Confident%20decisions%
20with%20ensembles%20(accepted%20final).pdf.

Thompson, Erica, Roman, Frigg, Casey, Helgeson, 2016. Expert judgment for climate
change adaptation. Philos. Sci. 83 (5), 1110-1121. https://doi.org/10.1086/
687942.

Walker, W.E., Harremoés, P., Rotmans, J., van der Sluijs, J.P., van Asselt, M.B.A.,
Janssen, P., Krayer von Krauss, M.P., 2003. Defining uncertainty: a conceptual basis
for uncertainty management in model-based decision support. Integrated Assess. 4
(1), 5-17. https://doi.org/10.1076/iaij.4.1.5.16466.

Weaver, Christopher P., Lempert, Robert J., Casey, Brown, John A, Hall, David, Revell,
Sarewitz, Daniel, 2013. Improving the contribution of climate model information to
decision making: the value and demands of robust decision frameworks: the value
and demands of robust decision frameworks. Wiley Interdisciplinary Reviews:
Climate Change 4 (1), 39-60. https://doi.org/10.1002/wee.202.

Winsberg, Eric, 2018a. Communicating uncertainty to policymakers: the ineliminable
role of values. In: Climate Modelling, Edited by Elisabeth A. Lloyd and Eric Winsberg.
Springer International Publishing, Cham, pp. 381-412. https://doi.org/10.1007/
978-3-319-65058-6_13.

Winsberg, Eric, 2018b. Philosophy and Climate Science. Cambridge University Press,
Cambridge, United Kingdom ; New York, NY.
