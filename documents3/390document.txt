Tanha et al. J Big Data (2020) 7:70 ; °
https://doi.org/10.1186/s40537-020-00349-y oO Jou ral of Big Data

SURVEY PAPER Oy oT-Ta waa -55 4

. . ®
Boosting methods for multi-class cat

imbalanced data classification: an experimental
review

Jafar Tanha ®, Yousef Abdi, Negin Samadi, Nazila Razzaghi and Mohammad Asadpour

 

*Correspondence:
tanha@tabrizu.ac.ir Abstract

Faculty of Electrical Since canonical machine learning algorithms assume that the dataset has equal
and Computer Engineering,

University of Tabriz, PO. number of samples in each class, binary classification became a very challenging

Box 51666-16471, Tabriz, Iran task to discriminate the minority class samples efficiently in imbalanced datasets. For
this reason, researchers have been paid attention and have proposed many meth-
ods to deal with this problem, which can be broadly categorized into data level and
algorithm level. Besides, multi-class imbalanced learning is much harder than binary
one and is still an open problem. Boosting algorithms are a class of ensemble learn-
ing methods in machine learning that improves the performance of separate base
learners by combining them into a composite whole. This paper's aim is to review the
most significant published boosting techniques on multi-class imbalanced datasets.
A thorough empirical comparison is conducted to analyze the performance of binary
and multi-class boosting algorithms on various multi-class imbalanced datasets. In
addition, based on the obtained results for performance evaluation metrics and a
recently proposed criteria for comparing metrics, the selected metrics are compared
to determine a suitable performance metric for multi-class imbalanced datasets. The
experimental studies show that the CatBoost and LogitBoost algorithms are superior to
other boosting algorithms on multi-class imbalanced conventional and big datasets,
respectively. Furthermore, the MMCC is a better evaluation metric than the MAUC and
G-mean in multi-class imbalanced data domains.

Keywords: Boosting algorithms, Imbalanced data, Multi-class classification, Ensemble
learning

 

Introduction

Imbalanced data set classification is a relatively new research line within the broader
context of machine learning studies, which tries to learn from the skewed data dis-
tribution. A data set is imbalanced when the samples of one class consist of more
instances than the rest of the classes in two-class and multi-class data sets [1]. Most of
the standard machine learning algorithms show poor performance in this kind of data-
sets, because they tend to favor the majority class samples, resulting in poor predictive
accuracy over the minority class [2]. Therefore, it becomes tough to learn the rare but

. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
GO) Springer O pen adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
— the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material
in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco

mmons.org/licenses/by/4.0/.
Tanha et al. J Big Data (2020) 7:70 Page 2 of 47

important instances. In fact, they assume equal misclassification cost for all samples for
minimizing the overall error rate.

Learning from skew datasets becomes very important when many real-world classi-
fication problems are usually imbalanced, e.g. fault prediction [3], fraud detection [4],
medical diagnosis [5], text classification [6], oil-spill detection in satellite images [7] and
cultural modeling [8]. An equal misclassification cost associated to each of the classes in
these datasets is not true. For example, in software fault prediction, if the defective mod-
ule is regarded as the positive class and non-defective module as negative, then missing a
defect (false negative) is much expensive than the false-positive error in testing phase of
software development process [9].

In recent years, several proposals have been made to deal with the class imbalance
problem that can be broadly categorized into two groups: data level and algorithm
level. Data level approaches rebalance the data distribution by resampling methods.
These methods solve the problem by either increasing the minority class or decreasing
the majority class observations. Many different sampling techniques such as random
over-sampling, random under-sampling, synthetic minority over-sampling technique
(SMOTE), direct oversampling, and other related methods have been proposed, each
with their pros and cons [10]. The algorithm level approaches modify the machine learn-
ing algorithm in such way that it accommodates imbalance data. The modification can
be considering different misclassification costs for each class, also known as cost-sen-
sitive methods, and minimizing the cost error instead of maximizing the accuracy rate
[11]. The other modification is choosing a suitable inductive bias. For example, adjusting
the probability estimate at the leaf when using a decision tree as a learner, or consid-
ering multiple minimum support for different classes in association rules. Many other
approaches have been proposed in the learning algorithms aiming to reinforce them
towards the minority class samples, such as ensemble approaches and deep-based algo-
rithms. However, the ensemble method is one of the most well-known approaches in
this group which is our main focus in this study.

Ensemble classifiers also known as multiple classifier system, improve the perfor-
mance of learning method by combining a collection of base classifiers for the classi-
fication system. The output of each base classifier is collected and used to prepare the
classification decision for new samples. Specifically, in boosting, a sequential aggregate
of base classifier is constructed on weighted versions of the training data, focusing on
misclassified samples at each stage of generating classifiers based on the sample weights
that are changed according to the performance of the classifier [12].

The imbalanced dataset problems become more complicated in multi-class imbal-
anced classification tasks, in which there may be multiple minority and majority classes
that cause skew data distribution. In this case, for example, a class may be a minority one
when compared to some other classes, but a majority of the rest of them [13]. Therefore,
many new challenges are imposed which do not exist in two-class cases. Fortunately,
some strategies such as decomposition methods and introducing new loss functions
have been proposed to deal with multi-class classification problem [14, 15].

The recent emergence of new technologies, such as internet of things (IoT), online
social network (OSN), have eased the way to originate huge amount of data called as Big
data which brings some difficulties for the extracting knowledge by learning algorithms
Tanha et al. J Big Data (2020) 7:70 Page 3 of 47

due to their specific features, i.e., volume, velocity, variety, and veracity [16, 17]. More-
over, the imbalanced data problem can be often found in this kind of datasets and by
considering multi-class property, learning from multi-class imbalanced big datasets
becomes a very challenging task that has not been explored well by the researches,
although this kind of data frequently appears in real-world classification problems [18].

Performance evaluation is another issue that should be addressed in this domain. It is
obvious that for evaluating a classifier on imbalanced data, we require such metrics that
could reflect its ability on predicting minority class(es) appropriately. Researchers have
considered this issue by proposing some metrics for imbalanced data domain, such as
area under the curve (AUC), Matthews correlation coefficient (MCC), G-mean, Kappa,
and others that some of them have been successfully extended to multi-class problems
[19, 20]. However, determining which one is more suitable for describing the perfor-
mance of the classifier, needs to be examined. In this regard, some researchers have pro-
posed frameworks for comparing evaluation metrics based on the empirical results [21,
22].

To the best of our knowledge, this is the first work to compare boosting algorithms on
various multi-class imbalanced datasets. Therefore, in this study, we examine the perfor-
mance of 14 most significant boosting algorithms on 19 multi-class imbalanced conven-
tional and big datasets and compare them with each other. A comprehensive statistical
test suite is used in order to investigate which boosting algorithm performs better in
terms of evaluation metrics. Subsequently, three most prevalent evaluation metrics used
in the imbalanced data literature are compared to determine which metric is more suit-
able for the multi-class imbalanced data domain.

Based on the aforementioned descriptions, the contributions and objectives of this

review study are:

1. To evaluate the performance of the most significant boosting classifiers on multi-
class imbalanced datasets with different imbalance ratio.

2. To determine which evaluation metric is more informative for showing the perfor-
mance of a learning algorithm in the context of multi-class imbalanced datasets.

3. To evaluate the performance of the most significant boosting algorithms on multi-
class imbalanced big datasets.

4. To analyze the computational time of the boosting algorithms.

The rest of this paper is structured as follows: "Handling imbalanced data problem"
section reviews imbalanced data handling methods, followed by a summary on the state-
of-the-art boosting algorithms in "Boosting methods" section. The experimental results
and discussions are presented in "Experimental study" section. Finally, the "Conclusion"
section remarks the findings of this study.

Handling imbalanced data problem

In many real-world situations, the data distribution among classes of a dataset is not
uniform, such that at least one class of data has fewer samples (minority class) than the
other classes (majority classes). This poses a difficulty for standard machine learning
algorithms as they will be biased towards majority classes. In this situation, they will
Tanha et al. J Big Data (2020) 7:70 Page 4 of 47

present high accuracy on majority classes and poor performance on minority class [23],
while the minority class may possess more valuable knowledge. A multi-class problem
can have different styles, including many minority and majority classes, one minority
class and many minority classes or one majority class and many minority classes [24].
In this condition, a class can be considered as a majority one compared to other classes,
but it is considered as a minority or well-balanced one for the rest of the classes [25].
Furthermore, the difficulty increases in the case of multi-class imbalanced datasets. For
handling this unwanted circumstance, researchers have proposed some methods that
can be classified into two major groups: data-level methods and algorithm-level meth-
ods. The details of these methods are explained in the following sub-sections.

Data-level methods

Data-level methods balance data among classes by re-sampling approaches. These
approaches are categorized into three major techniques: under-sampling, over-sam-
pling and hybrid-sampling. These techniques include many different forms of re-sam-
pling such as random under-sampling (samples of the majority classes are randomly
removed from the dataset), directed under-sampling (the choice of samples to eliminate
is informed), random over-sampling (generate new samples for minority classes ran-
domly), directed over-sampling (no new samples are created, but the choice of samples
to replace is informed), and combinations of the above techniques [10]. However, the
main drawbacks of these techniques are removing useful samples or introducing mean-
ingless new samples to the datasets.

Under-sampling methods remove samples from majority classes until the minority and
majority classes become balanced. Therefore, the training process can be easily imple-
mented, and it improves the problem associated with run time and storage [26]. The
most common under-sampling method is random under-sampling (RUS) that removes
samples from majority classes randomly.

Over-sampling approaches create copies of existing samples or add more samples to
the minority class [27]. They increase the training set size, thus consume extra time. The
simplest method to increase the size of the minority class is random over-sampling. This
method increases the number of minority class samples by randomly replicating, but it
is prone to overfitting [28]. For solving these problems, Chawla proposed the Synthetic
Minority Over-sampling technique (SMOTE), which creates synthetic samples from
the minority class. The SMOTE samples are linear combinations of two similar samples
from the minority class [29].

In the hybrid-sampling methods, more than one sampling technique is used together
to remove the drawback of each sampling method [30].

For multi-class datasets, the sampling strategies should be adjusted the sampling pro-
cedures to consider individual properties of classes and their mutual relations. However,
the class decomposition schemes are another most existing solution in order to handle
multi-class data and alleviate the difficulties of this type of datasets [31]. The advantage
of such approaches is simplifying the problems into multiple two-class problem. But,
there are some disadvantages with these kind of approaches, including losing the bal-
anced performance on all classes and rejecting the global outlook on the multi-class
problems. Despite the mentioned disadvantages, this direction can be considered as a
Tanha et al. J Big Data (2020) 7:70 Page 5 of 47

promising one. Two commonly used schemes for class decomposition are one-versus-all
(OVA) and one-versus-one (OVO) schemes. In the OVA strategy, the data points of a
class are considered with the data point of the other classes. In this case, for the K-class
dataset, K classification problems are introduced. While in the OVO strategy, a binary
classification problem is created between any two classes; therefore K(K—1)/2 classifica-
tion problems are introduced.

Algorithm-level methods

In the algorithm-level category, the existing learner is modified to remove its bias
towards the majority classes. The most common approach is the cost-sensitive learning
in which the learner is forced to correctly classify the minority class samples by setting
a high cost to the misclassified samples of minority class [32]. For correct classifica-
tion samples, no penalty is assigned, but the misclassifying cost for minority samples is
higher than the majority samples. The goal is to minimize the total cost of the training
dataset [33]. However, determining the cost values is not an easy task, as they depend on
multi factors that have trade-off relationships.

Hybrid methods

Hybrid methods combine previously mentioned approaches. Ensemble learning is one
of the most frequently used classifiers that combine data level and algorithmic level
methods for handling the imbalanced data problem [34]. The main goal of the ensemble
is obtaining better predictive performance than the case of using one classifier. However,
its main drawback is generating more classifiers, which increases computational com-
plexity [35]. The best-known methods of the ensemble classifiers are bagging and boost-
ing [36, 37]. In bagging, the original training set is divided into N subsets of the same
size, and then each subset is used to create a classifier. The whole classification model is
built by aggregating particular classifiers. On the other hand, boosting algorithms gener-
ates a new weak learning model, and after many rounds, the boosting algorithms com-
bine these weak learners into a single prediction model that will be much more accurate
than anyone of the weak learners.

Ensemble learning has been investigated in order to handle multiclass imbalanced prob-
lems, as well. Specifically, hybridizing ensemble methods such as boosting, bagging and
random subspace with the previously mentioned solutions (sampling or cost-sensitive)
has been proved to be effective in imbalanced data problem [38]. For example, Wang and
Yao compared the performance of Adaboost.NC and Adaboost in combination with ran-
dom oversampling with (or without) using class decomposition for multi-class imbalanced
datasets [13]. By applying ensemble learning to each sub-problem, Zhang et al. proposed a
method to improve the performance of binary decomposition used in multi-class imbal-
anced problems [39]. Krawczyk also developed a new approach to handle multi-class
imbalanced datasets by combining pairwise OVO decomposition and ensemble learning.
He divided the original set into several binary sub-problems in order to simplify the classifi-
cation task. Then, an ensemble classifier designed for handling such simplified binary prob-
lems, and outputs of individual classifiers were combined with pairwise coupling method

[40]. Feng et al. proposed a new ensemble margin-based algorithm which emphasizes on
Tanha et al. J Big Data (2020) 7:70 Page 6 of 47

the use of a large number of informative low margin samples compared to margin samples

by the aim of handling imbalanced data problem [41].

Boosting methods

Boosting as the most popular category of ensemble learning, builds a strong classifier by
combining many weak classifiers [12]. The superiority of boosting is in its serial learning
nature, which results in excellent approximation and generalization. In other words, weak
classifiers are learned sequentially, aiming to reduce the errors of the previously modeled
classifier. Many boosting approaches have been proposed so far. Each improves the clas-
sification performance by varying some steps of the general boosting scheme. Among the
various kinds of boosting approaches, we have participated 14 well-known methods in our
experimental comparisons. The methods involve AdaBoost.MH, SAMME, LogitBoost,
GradientBoost, XGBoost, LightGBM, CatBoost, SMOTEBoost, RUSBoost, MEBoost,
AdaCost, AdaC1, AdaC2 and AdaC3. We have chosen these approaches due to their high-
citation count and popularity. We have selected representative approaches from all the
categories of boosting. AdaBoost.MH, SAMME, LogitBoost, GradientBoost, XGBoost,
LightGBM and CatBoost are general boosting algorithms applicable for all applications.
Besides, SMOTEBoost, RUSBoost, MEBoost, AdaCost, AdaC1, AdaC2 and AdaC3 are
specifically developed for imbalanced data classification problem. Among these meth-
ods, SMOTEBoost and RUSBoost are data-level approaches and the rest are algorithm-
level. The detailed descriptions of these approaches are provided in “General Boosting
approaches” section. Furthermore, a brief description for some other known boosting
methods is provided in Table 1. The description of boosting approaches involves their
exclusive features, discriminations and proposal objectives. The presented techniques are
sorted based on their publication year and cover most of the methods from 1997 to 2019.

General Boosting approaches
AdaBoost.MH
AdaBoost.MH, as a boosting approach proposed in 2000, is an extension of the AdaBoost
algorithm. In order to deal with multi-class classification, AdaBoost.MH decomposes a
multi-class problem into K(K — 1)/2 binary problems (K is the number of classes) and
applies a binary AdaBoost procedure to each of the binary datasets [42]. In order to better
understand the AdaBoost.MH, binary AdaBoost algorithm is explained by details.
AdaBoost (Adaptive Boosting), which is one of the primary and well-known boosting
approaches, is proposed by Freund and Schapire in 1997. In its procedure, an ensemble of
M classifiers is learned sequentially [43]. The main idea of AdaBoost is bolding the effect
of misclassified samples by increasing their weights in the iterations of boosting. Hence, in
all iterations of the algorithm, the sample set is fixed, and only their weights are changed.
By doing so, classifiers can learn from the errors of the current classifiers and improve the
accuracy. AdaBoost uses exponential loss function as Eq. (1).

Ladaboost (Vf) = exp(—y"f) (1)

The boosting procedure in AdaBoost is as follows. At first, all 1 samples get the same
weight wi = 1/n. Then in each iteration of the boosting, a classifier f’”’(x) is selected with
Tanha et al. J Big Data (2020) 7:70

Table 1 Overview of the boosting approaches

Page 7 of 47

 

Approach

Brief description

Years

 

AdaBoost.M1 [43]
AdaBoost.M2 [43]
GentleBoost [45]

CSB1 [58]

CSB2 [58]

MAdaBoost [59]

RareBoost [60, 61]

Modest AdaBoost [62]

JOUSBoost [63]
ABC-LogitBoost [47]

AdaBoost.HM [64]

RAMOBoost [65]

AOSO-LogitBoost [48]

CD-MCBoost [66]

EUSBoost [67]

RB-Boost [68]

LIUBoost [69]

TLUSBoost [70]

A multiclass variation of AdaBoost which uses multiclass base classifier
Weight of each base classifiers is a function of error rate

A multiclass variation of AdaBoost
Weight of each base classifiers is a function of pseudo-loss

Extended version for AdaBoost which uses Newton steps
Using weighted least-squares regression for fitting the base classifiers

A Cost-sensitive variation of AdaBoost proposed for handling imbalanced data
Adding cost item into the weight update formula of AdaBoost
Removing step size coefficient from the weight update formula of AdaBoost

A Cost-sensitive variation of AdaBoost proposed for handling imbalanced data
Adding cost item into the weight update formula of AdaBoost
The step size is considered in the weight update formula, like AdaBoost

Proposed with the goal of solving the AdaBoost's sensitivity to noise
Modifying the weight update formula of AdaBoost

An improvement for AdaBoost
Using different weight update scheme for positive and negative predictions

Considering False Positive, True Positive, True Negative and False Negative in step

size calculation

An improvement of GentleBoost

Using different weight update formula for misclassified and truly classified
samples

Using inverted distribution to assign larger weights to truly classified samples

Proposed with the goal of handling imbalanced data in AdaBoost algorithm
Combining the jittering of the data and sampling techniques with AdaBoost

An improvement of LogitBoost for multiclass classification
Solving the difficulties of dense Hessian Matrix in Logistic loss

A multiclass variation of AdaBoost which uses hypothesis margin

Using multiclass base classifiers instead of decomposing the multiclass classifica-

tion problem into multiple binary problems

Proposed with the goal of imbalanced data handling
Combining Ranked Minority Oversampling with AdaBoost.M2

Using the sampling probability distribution for ranking the minority class samples

One versus one version of LogitBoost for multiclass classification

Solving the difficulties of dense Hessian Matrix in Logistic loss by utilizing vector

tree and adaptive block coordinate descent techniques

Performing coordinate descent on multiclass loss function
Concentration of each base classifier on margin maximization of a single class

An improvement of RUSBoost which uses evolutionary undersampling
Using different subsets of majority class samples in the training phase of each
base classifier to ensure diversity

Combining Random Balance with AdaBoost.M2

Using SMOTE sampling to deal with imbalanced data problem

The difference with SMOTEBoost is using random proportion of classes in each
iteration of booting to ensure the diversity of base classifiers

Proposed with the goal of imbalanced data handling
Using undersampling in order to solve the imbalanced data problem
Adding a cost term to the weight update formula of the samples

Proposed with the goal of imbalanced data handling

Using Tomek-linked and redundancy-based undersampling for removing outlier

samples

1997

1997

2000

2000

2000

2000

2001

2005

2007

2009

2010

2010

2011

2011

2013

2015

2019

2019

 

respect to the loss function and is fitted to the data points considering their current weights.

Then, the error rate of the selected classifier is computed as Eq. (2).

_ wi <(f'" (ai) # yi)

 

err

dw"

(2)
Tanha et al. J Big Data (2020) 7:70

In which f”(x;) and y; indicates the predicted value and actual output of the sample
xi, respectively. < (f ™ (xi) # yi) is the indicator function with a value equal to 1 when the
statement is true; otherwise, it is equal to zero.

Then the step size (or the coefficient of the f’’(x;)) is calculated according to the error

rate value as Eq. (3).

1 — err™

err(™)

a’” = log (3)

According to this equation, the fitted classifier of each iteration will get a high impact
in the ensemble if it has a lower error rate on the training data. After calculating the
error rate and step size of the classifier. The misclassified samples are reweighted accord-
ing to Eq. (4).

with — w}.exp(a™.<(f™(xi) # yi) i= 1,...,n (4)
When M classifiers are produced, the final ensemble model is constructed by Eq. (5).
M
F(x) = sign Ss” a™£™ (x) (5)
m=1

The pseudo-code of the AdaBoost algorithm is provided in Algorithm 1.

Algorithm1: AdaBoost algorithm
i. Input: Training set (1,71),.... vn), where x; € X, yy; € Y ={—1, +4 1},

 

Base-learner algorithm, Number of iterations M.
ii. Initialization: Weight the training samples w} = 1/n, i=1.,...,n.
iii. Iteration: For m = 1,...,M
(1) Use the Base-learner algorithm to fit a classifier f(x) to the training
data using weights w7”.
(2) Calculate the training error err of the classifier f”:

Dwi lee) # yi)

yw
(3) Calculate the weight a” for the classifier f™:
1 — err™

err™ =

a™=lo
err™

(4) | Update the weight of the training samples:
witt = w'exp (a™I(f"(x) #yi)) i=1,.,7
iv. Output: The final ensemble model:

M
F(x) = ve) “pre

m=1

 

SAMME
SAMME [44] is a multi-class variant of AdaBoost, which uses a multi-class exponential

loss function as follows.

Page 8 of 47
Tanha et al. J Big Data (2020) 7:70 Page 9 of 47

1
LsamME (yf) = exp (- at) (6)
In which the target variables are encoded using the codewords technique to the form
vi = (We. IK) i=1,2,...,N, where
1 ify,=C
Yk = ‘ __1 My ‘

K-1 otherwise (7)

In this algorithm, the boosting procedure is exactly the same as the AdaBoost with
minor differences. The error rate calculation for the weak learners and the reweighting
phase for the misclassified samples are same as the AdaBoost. The difference is in the
extra term /og(K — 1) in the step size calculation equation as following equation.

1 — err™
a” = log — + log(K — 1) (8)
err(™)
The final ensemble model has the following form
M
F(x) = ™ <(f™(x) =k).
(x) argmax ) a ( (x) k) (9)

LogitBoost

LogitBoost is one of the state-of-the-art approaches proposed by Friedman et al. in 1998
[45, 46]. The LogitBoost utilizes adaptive Newton steps to fit an additive logistic model.
So, LogitBoost is a boosting approach with logistic loss function (negative conditional
log-likelihood) as Eq. (10).

K

LrogitBoost ( S) =~ Ss” < (k — y) logP, (10)
k=1

where P;.(x) = FF expt) and 4 F(x) = 0.

The boosting procedure in K-class LogitBoost is as follows. At first, all 1 samples get
the same weight wi = 1/n. The initial probability for each class for samples is set to
P(x) = 1/K,k =1,...,K.

At each iteration of the LogitBoost, K independent classifiers that each minimizes the
overall loss function with respect to the Ath class, are fitted. So, an ensemble of classifiers

is considered for each class. The final ensemble model is chosen by Eq. (11).

F(x) = argmaxF| (x). (11)

where Fe(x) = yeh") = Der Set (fr @ — DH") }

LogitBoost algorithm was designed as an alternative solution to address the limita-
tions of AdaBoost in handling noise and outliers. It is less sensitive to outliers and noise
due to using a logistic loss function that changes the loss function linearly. In contrast,
AdaBoost uses an exponential loss function that changes exponentially with the classifi-

cation error, which makes AdaBoost more sensitive to noise and outliers.
Tanha et al. J Big Data (2020) 7:70 Page 10 of 47

LogitBoost supports both binary and multi-class classification. This algorithm can
handle multi-class problems by considering multi-class logistic loss. Also, it can address
multi-class classification problems by using a parametric method. Some other multi-
class variants of LogitBoost, have been proposed too, e.g., ABC-LogitBoost [47] and
AOSO-LogitBoost [48] (which is a one-vs-one LogitBoost for multiclass problems).

GradientBoost

GradientBoost is based on trees which is applicable for various kinds of loss functions.
Like other boosting algorithms, it builds a stage-wise additive model. Unlike traditional
boosting in which weak-learners fit a model to the output values of samples, in each iter-
ation of this algorithm, the decision trees are generated by fitting the negative gradients
[49]. Negative gradient is also called residual error that is a function of the difference
between the predicted value and the real value of the output. Generally, in Gradient-
Boost the model is initialized with a constant value y (A tree with just one leaf node) that
minimizes the loss function over all the samples as Eq. (12).

F(x) = argmin) >, L(y, y) (12)

Then, in each iteration, the negative gradient of the loss function is computed with
respect to the current ensemble as following equation.

OL (vi, F (x;))
OF (x;)

Tim =

, t=l1ton (13)
F (x)=F”—1(x)

 

 

A decision tree is fitted on the negative gradients (residuals) and then the values of the
leaf nodes are computed with the goal of loss function minimization as Eq. (14).
_ : . Em-ly,. _
where, J; is the number of the leaf nodes in the mth iteration tree, and yj is the value of
jth leaf node at iteration m. Note that the summation of loss values is calculated over the
samples that belong to that leaf nodes region Rj. Finally, the new model is added to the

ensemble as the following equation.
F(x) = FP" '(x) + vf ™ (ax) (15)

Besides the effective process of boosting in GradientBoost, a shrinkage metric v is also
for controlling overfit to training data. In other words, in each boosting iteration, the
selected model is multiplied by a coefficient between 0 and 1, which indicates the learning

rate (shrinkage value). GradientBoost supports both binary and multi-class classifications.

CatBoost

CatBoost is a novel algorithm for gradient boosting on decision trees, which has the
ability to handle the categorical features in the training phase. It is developed by Yan-
dex researchers and is used for search, recommendation systems, personal assistant,

self-driving cars, weather prediction and many other tasks at Yandex and in some other
Tanha et al. J Big Data (2020) 7:70 Page 11 of 47

companies. CatBoost has both CPU and GPU implementations which accelerates the
training process [50].

Like all gradient-based boosting approaches, CatBoost consists of two phases in build-
ing trees. The first is choosing the tree structure and the second is setting the value of
leaves for the fixed tree. One of the important improvements of the CatBoost is doing
unbiased gradient estimation in order to control the overfit. To this aim, in each itera-
tion of the boosting, for truly estimating the gradient of each sample, it excludes that
sample from the training set of the current ensemble model. The other improvement is
the automatic transformation of categorical features to numerical features without any

preprocessing phase. CatBoost is applicable for both binary and multi-class problems.

XGBoost

XGBoost is an optimized version of GradientBoost, which is introduced by Chen in
2016. XGBoost has improved the traditional GradientBoost by adding up some efficient
techniques to control overfitting, split finding and handling missing values in the train-
ing phase [51]. In order to control the overfitting, the objective (minimization) function
consists of two parts: loss function and regularization term, which controls the complex-
ity of the model as Eq. (16).

n M
Obj => lon fi + >, Ql”) (16)
i=l m=1

where, Q(f m) is the regularization term. In XGBoost, a second-order approximation is
used to optimize the objective function. Accordingly, in each iteration, the best tree is
selected by using Eq. (17).

ln G
Obj = —~= J T
j pum yat? (17)

 

where, T is the number of leaf nodes. G; and H; are the summations of the first and sec-
ond-order gradient statistics on the loss function over the samples of the j-th leaf node,
respectively. 1 and y are the regularization coefficients. Thus, in this approach, the com-
plexity of the tree is chosen and controlled separately in each iteration and the number
of the leaves is not a fixed value during all the iterations. After the best tree selection in
each iteration, the values of the leaf nodes are calculated by the use of gradient statistics
of each leaf by the use of Eq. (18).

 

Wi = Te (18)

In addition, an approximate technique is used for split finding in each node of the tree.
To this aim, for each feature, all the instances are sorted by that feature’s value, then a
linear search is done to find the best split along that feature. The best split is chosen by
comparison among the best split of all the features. Furthermore, after constructing the
tree, in each node the direction with maximum score is marked to make a default path
on the tree for classification of data with missing values.
Tanha et al. J Big Data (2020) 7:70 Page 12 of 47

XGBoost algorithm has made excellent improvements in the implementation level. It
utilizes parallel learning, out-of-core computation and cache-aware access techniques to
minimize the time complexity of learning, thus it is applicable for very large datasets.

LightGBM

LightGBM,, as an efficient implementation of GradientBoost, is proposed by Ke et al. in
order to solve the efficiency and scalability problem of GradientBoost in the case of high
dimensional feature space or large data size. In the GradientBoost, for each feature, all
the data samples are needed to be scanned and all the possible split points are examined.
Therefore, the most time-consuming part of the GradientBoost is the splitting procedure
in each tree node. In the this algorithm, two novel techniques called Gradient-based
One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) are proposed. In the
GOSS technique, only the small set of samples which have large gradients are consid-
ered in the split point selection. This works, because samples with larger gradients play a
more important role in the computation of the information gain. In the FEB technique,
in order to reduce the number of features, mutually exclusive features are bundled with
the use of a greedy approach. LightGBM only supports binary problems [52].

Boosting for imbalanced data

SMOTEBoost

SMOTEBoost, proposed by Chawla et al., is a data-level method to deal with the imbal-
anced data problem. The main steps of the proposed approach are SMOTE sampling
and boosting. This algorithm uses SMOTE technique as a data level solution. SMOTE
adds new minority class examples to a training dataset by finding the k-nearest neigh-
bors (KNN) of a minority class example and extrapolating between that example and its
neighbors to create new examples. Im addition, it adopts the AdaBoost algorithm as its
ensemble approach. In each iteration of the SMOTEBoost, the sampling is done based
on the original data, but the training process is done based on all the original and syn-
thetic data. Like AdaBoost, after training a classifier the weights of the misclassified sam-
ples are updated, but here the update is done only for the original data. Thus, after each
step of the boosting, the weights of the original data change and new synthetic data are
generated. One of the main drawbacks of the SMOTEBoost is generating a large train-
ing dataset and, therefore, relatively slow in training phase of the boosting algorithm.
SMOTEBoost supports both binary and multi-class problems [53].

RUSBoost

RUSBoost, as a data-level solution, is proposed by Seiffert et al. to reduce the com-
putational requirement of SMOTEBoost. RUSBoost combines data sampling and
the AdaBoost algorithm. It uses RUS (Random Under Sampling) sampling approach
to balance the initial data. RUS technique randomly removes the examples from the
majority class. Each iteration of the boosting consists of two phases. In the first phase
of the algorithm, random under-sampling is performed on all the classes except the
minority class. Under-sampling continues until all the classes reach the same size
(minority class size). In the second phase of the algorithm, AdaBoost is applied on

the sampled data. So, the training set is not fixed over all the iterations. One of the
Tanha et al. J Big Data (2020) 7:70 Page 13 of 47

drawbacks of the RUSBoost is its data loss in the under-sampling level. It supports
both binary and multiclass problems [54].

MEboost

MEBoost is a novel boosting approach that is proposed for handling imbalanced data
with an algorithm level solution. MEBoost mixes two different weak learners with boost-
ing to improve the performance and taking the benefits of both classifiers. In each itera-
tion of the boosting, it either uses a decision tree (which uses information entropy as
splitting metric) or an extra tree classifier (Randomized tree classification) as its learner.
In this algorithm, the ensemble model is tested according to the AUC (area under
Receiver Operating characteristic curve) score. It keeps adding weak learners to the
model until there is no significant change in the auROC on the test data. Furthermore,
MEBoost has an early stopping criterion for the case that the improvement between two

iterations of boosting is not significant. MEBoost only supports binary problems [55].

AdaC

AdaC is proposed by Sun et al. in order to improve the imbalanced data classification.
The main idea of this approach is to take an algorithm level solution to handle the imbal-
anced data by adding a cost item into the learning procedure of the AdaBoost. The cost
item C; € [0, +00] (for each sample i) is added in order to increase the impact of minor-
ity class samples in the training phase and improve the performance. To this aim, the
cost item is set to higher values for minority class samples and smaller values for major-
ity class samples. The cost value of each sample shows the importance of correctly clas-
sifying that sample [56].

AdaC algorithm has three versions, namely AdaC1, AdaC2 and AdaC3. The difference
between these three versions is in the way of adding cost to the weight update equation
of the AdaBoost. In the AdaCl1 algorithm, the cost item is embedded inside the expo-
nent part. The weight update equation of AdaC1 is provided in Eq. (19).

m+1 w}".exp(—a™Cif™ (xi)yi)

, irl wi exp(—amCjfm (5) yj) (19)

 

Moreover, the step size parameter a” is modified to the form shown in Eq. (20).

m_ _ 4

(20)
2 1— Dick oc) =y, CW; a Dit™ Ox) zy, CWE

In the AdaC2 algorithm, the cost item is embedded outside the exponent part. The
weight update equation of AdaC2 is shown in Eq. (21).

wath — Ciw!" exp (—a'™£™ (xi)yi)
© Vijen G-exp(—amf™ (xj) yj)

 

i=1,...,n (21)

The step size parameter a” is modified to the form shown in Eq. (21).
Tanha et al. J Big Data (2020) 7:70

7M
q” = My Dist (xi)=yi iW

(22)
2 Daim (xpAYi Ciw;"

AdaC3 algorithm is the combination of AdaCl and AdaC2, in which the cost item is
embedded both inside and outside the exponent part as Eq. (23).

yn Ciw'" exp (—a™Cif™ xi) yi)

= ee ayn
5 Vijer Gw-exp(—amGé™ (xj) yj) e°)

And the step size parameter a” has the form provided in Eq. (24).

2 2
1 DiCiwe + i:t™ x) =y; Ci Wi _ Dit x) yi Gi Wi

oe log Cwm C2 m C2 m
DiCiw; — Diem ox) =yi ii + Dicer oi) zy: i Wi

= 5 (24)

AdaC algorithms only support the binary classification.

AdaCost
AdaCost as another algorithm level cost-sensitive AdaBoost approach is proposed by
Fan et al. in 1999 [57]. The main idea is to emphasize on the minority class classification
accuracy and adding larger cost values to the minority class samples. In this approach,
the weight of misclassified minority class samples increases higher than misclassified
majority class samples. Moreover, the weight of a truly classified sample decreases less
if it belongs to the minority class. As a result, the weight update equation is changed by
adding a cost adjustment function f, which is a function of cost item C as Eq. (25).

ca Whhexp(—a™E™ (xi): Bsign (tex). 5) )

Ww; =

ES (25)
j=l Ww; exp(—amsm (%}) ¥iBsien(F(x).9) )

where, 6, refers to the case sign (f™ (xi).yi) =+1 and f_ refers to the case
sign (f™ (xi).yi) = —1. In the main reference, the optimum value for 6 is recommended
as B4 = —0.5C; + 0.5 and B_ = 0.5C; + 0.5. Hence, the step size parameter a” is modi-
fied to Eq. (26).

1 I+ isl win.exp (—a™F™ Oxi): Beign(t™ox).1) )

a” = —log = (26)
1— i=l wit.exp (—a™£™ Ox)yiBeign(t™ox).2) )

2

AdaCost algorithm only supports binary classification.

Experimental study

Dataset characteristics

The aforementioned boosting algorithms are evaluated on 15 conventional and 4 big
multi-class imbalanced datasets from UCI' and OpenML? repositories. The characteris-
tics of these datasets are given in Table 2.

 

' https://archive.ics.uci.edu/ml/index.php

> https://www.openmLorg/search?type=data

Page 14 of 47
Tanha et al. J Big Data (2020) 7:70 Page 15 of 47

Table 2 Characteristics of the test datasets

 

Dataset # of Attributes Instances # of classes IR

 

Conventional datasets

Wine 13 178 3 1.47
Hayes-Roth 4 132 3 1.7
Contraceptive 9 1473 3 1.89
Pen-Based 16 1100 10 2.18
Vertebral column 6 310 3 2.5
New thyroid 5 215 3 5
Dermatology 34 366 3 5.6
Balance Scale 4 625 3 5.8
Glass 9 214 7 8.44
Heart (Cleveland) 13 303 5 12.62
Car Evaluation 6 1728 4 18.61
Thyroid 21 7200 3 40.15
Yeast 8 1484 10 92.5
Page blocks 10 5473 5 175.46
Shuttle 9 58,000 7 4558.6
Big datasets
FARS 29 100,968 8 4679
KDD Cup’99 4] 494,021 5 1870
Covertype 54 581,012 7 103
Poker 10 1,000,000 10 64,212

 

Performance metrics

Using evaluation metrics is an essential factor to assess the classification performance of
a learning algorithm. Accuracy and error rate are the most widely used metrics for this
purpose. However, they are biased toward the majority class in imbalanced datasets as
described in “Handling imbalanced data problem” section. Therefore, the values of these
popular metrics do not show the ability of the classifier to predict examples from minor-
ity classes.

Alternative criteria have been used for this scenario that can be roughly categorized
into single-class and overall performance measures. The first group category of criteria
shows how well a classifier performs in one class and the second one allows to have a
clear and intuitive interpretation on the performance of the classifier on all classes.

Although in the literature, many evaluation metrics have been proposed to meas-
ure the classifier performance when dealing with imbalanced data problem, Precision,
Recall, F-measure, AUC, G-mean, Kappa, and MCC are the most prevalently used met-
rics in the two above mentioned categories. All of these metrics are calculated from the
confusion matrix. A confusion matrix is a two-dimensional matrix that presents a clear
explanation of how a classifier works in predicting new samples [71]. One dimension is
indexed by the actual class of an object and the other by the class that the classifier pre-
dicts. Figure 1 shows the confusion matrix for a multi-class classification task. The entry
Nj, with i 4 j, shows the number of samples that classified as class C; but actually belong
to class C;. The best classifier will have zero values in non-diagonal entries.

Like the previous studies, the minority class is considered as positive, while the major-
ity class is considered as negative.
Tanha et al. J Big Data (2020) 7:70 Page 16 of 47

 

 

Fig. 1 Confusion matrix for multi-class classification
Ne /

 

 

Single-class measures

Single-class metrics are calculated for each class and are less sensitive to class imbalance,
therefore, naturally are better suited to evaluate classifiers in skew data domains. In this
section, the used single-class metrics are briefly reviewed.

Precision metric measures the correctly classified positive class samples and defines as

Precisi IP (27)
recision = —————
TP + FP
where JP and FP indicate the counts of true-positive and false-positive, respectively.

Recall measures the proportion of correctly identified of all real positive samples and

computed by Eq. (28):
Recall ? (28)
ecall = ——_—_—
TP + FN
where FN indicates the counts of false-negative.

In general, there is a trade-off between precision and recall. To have an intuitive view
of the performance of the classifier based on these metrics, F-measure [72] can be used.
It is a harmonic mean of precision and recall in which the user can adjust the relative
importance to recall as to precision by the 6 parameter. Usually, the value of this param-
eter is taken as 1. The following equation show how F-measure is computed:

1+ 6)PrecisionRecall
F—measure = (1+ B)PrecisionRecall (29)
B2Precision + Recall

F-measure can be extended by micro-averaging or macro-averaging methods for
multi-class cases. In micro-averaging, F-measure is computed globally over all classes
and its elements (Precision and Recall) are obtained by summing over all classes, while
in macro-average, it is obtained for each class locally, and then by averaging those, a sin-
gle value can be computed as a F-measure value for whole classes.

Due to the independence of the imbalanced distribution, Geometric-mean (G-mean) is
another suitable single metric for imbalanced data problems [19] that is defined as Eq. (30)

C TP TN (30)
—mean = | ———_ ————__
TP + FN TN + FP

Like F-measure, this metric can be extended by micro-averaging or macro-averaging

methods for the multi-class problems, however, in this study the method that was proposed
Tanha et al. J Big Data (2020) 7:70 Page 17 of 47

by Sun et al. [19] is used to extend it to multi-class domains by the geometric mean of recall
values of all classes as defined in Eq. (31).

1
C Cc
G—mean = (TI Rea (31)

i=1

Although both G-mean and F-measure measures can be used to evaluate the perfor-
mance of a learning method in class imbalanced classification problems, F-measure may
get infinity when classifier all samples were classified as negative class. Therefore, in this

study we will use G-mean as a single class performance evaluation criterion.

Overall metrics
The area under the curve (AUC) of the receiver operating characteristic (ROC) is extremely
used as an overall evaluation technique, especially for ranking classifiers in the presence of
class imbalanced for binary classifiers. The ROC curve shows all possible conflicts between
true-positive rate (TPR) and false-positive rate (FPR) across various decision thresholds
and AUC evaluation metric converts this curve to a value in the range of [0.5, 1], where the
value 1 shows a perfect classifier and the lower value 0.5 means the classifier does not work
better than random guess [73].

Although extending the AUC to multi-class problems is an open research topic, the
MAUC metric [20] which averages the AUC value of all pairs of classes, are mostly used in

the researches multi-class imbalanced data learning and defined as
MAUC ; SAG)
= ———_ i.
cE“) J (32)

where C denotes the number of classes, A(i, j) is the AUC between class i and class j.

Matthews Correlation Coefficient (MCC) is the third overall evaluation metric used in
this study to evaluate the performance of boosting algorithms, which is especially utilized
in biomedical imbalanced data analysis [74]. MCC contributes all classes’ values in the con-
fusion matrix for computing a measure of the correlation between actual and predicted
samples. Its value is located between —1 and +1. The value +1 means perfect prediction
and — 1 inverse prediction [75]. This metric can be directly calculated from the confusion
matrix using Eq. (33)

TP x IN — FP x FN
MCC 3 aaa“ SSS (33)
/(TP + FP)(TP + FN)(IN + FP)(IN + FN)

Furthermore, MCC is a binary classification evaluation metric and how it performs in
the multi-class imbalanced dataset has not yet been well studied. However, it has been
shown that the behavior of MCC remains consistent in multi-class settings [76]. Based
on these facts and inspired by the MAUC metric, we extend the MCC metric to multi-
class case, called MMCC, by employing the OVO decomposition strategy. As MAUC,
MMCC average the MCC value of all pairs of classes and it is defined as Eq. (34).
Tanha et al. J Big Data (2020) 7:70 Page 18 of 47

MMCC = —~— S~ MCC,
~ C(C—1) » 7) (34)

i<j

Parameter settings

The Performance of the algorithms is investigated with default parameter settings,
except the nearest neighbor parameter for SMOTEboost is set to 3, and the population
of the minority class C is increased to be approximately equal to the size of the major-
ity class. In addition, since in gradient boosting methods the value between 8 and 32 is
usually used for the number of leaves in the tree, the C4.5 decision tree is employed with
depth 5 as a base learner in all boosting algorithms. Each boosting consists of 100 trees
with the learning rate of 0.3.

For binary boosting algorithms, One-Versus-One (OVO) decomposition method is
obtained to split multi-class datasets to multiple binary problems and then the voting
aggregation strategy is used in order to detect the class of new data samples from the
aggregated classifiers [77].

In the experiments, the means and standard deviations of three performance criteria,
which were described in the previous section, are calculated for the algorithms, and the
procedures are repeated 10 times by employing fivefold cross-validation to reduce any
bias because of the randomness. For creating the folds, the stratified sampling method is
used instead of simple random sampling to guarantee that each fold contains a number
of samples from each class especially minority class.

Statistical comparison of boosting algorithms

For comparing more rigorously the difference between classification algorithms, a statis-
tical test suite is carried out on the experimental results to validate their results further
and determine whether there exists a significant difference among them.

The statistical test procedure can be categorized into parametric (e.g., paired t-test and
ANOVA) and nonparametric methods (e.g., the Wilcoxon and Friedman tests) from a
methodological perspective. Although parametric tests are more robust than nonpara-
metric, they are based on strict assumptions that are often violated in machine learning
studies and make their results to be unreliable [78]. Therefore, nonparametric tests are
preferred for comparing the results of classifiers.

According to the recommendation made by DeméSar [79], in this study, the Friedman test
is used to compare multiple classifiers over multiple datasets. The null-hypothesis (there is
no significant difference between the performance of all algorithms) is rejected at a speci-
fied level of significance when Friedman test statistic ( XP) computed as Eqs. (35) and (36), is
greater than the critical value from the chi-square table. The rejection of the null-hypothesis
implies the existence of at least two classification algorithms that are significantly different.

12D M M(M + 1)?
So ape OEE

M(M +1) |4j=1 7 4 (35)

2
XE =
Tanha et al. J Big Data (2020) 7:70 Page 19 of 47

1 .
AR; = a (36)

where D and M indicate the number of datasets and classification algorithms, AR; is the
rank of j-th classifier, and r’, is the rank of jth classifier on ith dataset.

After the rejection of null-hypothesis, a post-hoc test should be conducted to determine
whether the control a(usually, the best one) presents significantly statistical differences than
the rest. In this study, the Holm post-hoc test [79] is used for this purpose. In this test, the
test statistic is computed as Eq. (37), and the Z value is used to find the p-value from the
table of normal distribution based on a level of significance of a = 0.05. The null-hypothesis
is rejected if the p-value is lower than the Holm’s critical value.

M(M +1)

6D 37)

Zi = (Ri — Rj)/
where K; and R; are the rank of ith and jth classifier, respectively.
However, the Holm test does not detect the difference between a pair of algorithms. For
pairwise comparison and determining the existence of significant differences between a
pair of classifiers, the Wilcoxon rank-sum test [80] is adopted.

Comparison of evaluation metrics

One of the key challenges in imbalanced data classification is selecting an appropriate eval-
uation metric for a better comparison of classification algorithms. To this aim, the statistical
test proposed in [21] is carried out for comparing the performance of evaluation metrics.
Firstly, the consistency of the evaluation metrics with each other are examined. Secondly,
the degree of discriminancy of the metrics is investigated. Furthermore, the degree of Indif-
ferency of metrics is tested as proposed in [22].

Degree of consistency

For a better comparison of two evaluation metrics on two algorithms, at least their man-
ner should be consistent [74]. Two evaluation metrics are consistent with each other in the
evaluation of two algorithms A and B, when metric fstates that aA is better than aB, metric
g doesn't indicate that aB is better than aA. The degree of consistency (DoC) of two evalua-
tion metrics fand g is calculated as follows.

ca IAL (38)
|R| + [S|
where R and C are defined as Eqs. (39) and (40) in the domain W.
R= {(a,b)|a,b € ¥,f (a) > f(b), g(a) > g(b)} (39)
S = {(a,b)|a,b € Wf (a) > f(b), g(a) < g(b)} (40)

The value C shows the degree of consistency of two metrics f and g on algorithms A
and B that its value is located in the interval [0, 1}.
Tanha et al. J Big Data (2020) 7:70 Page 20 of 47

Degree of discriminancy

Besides the consistency of two evaluation metrics, there exist some cases that metric f
can distinguish between two algorithms and states that for example, aA is better than aB,
but the metric g cannot distinguish between these two algorithms. In this case, metric
fis more discriminating than metric g. The degree of discriminancy (DoD) of metric f
over metric g is calculated as follows.

D= Mt (41)
|Q|
where P and Q are defined as Eqs. (42) and (43) in the domain W.
P={@,b)|a,be V, f(a) > f(b), g(a) = g(b)} (42)
Q={@b)|a,b eV, f(a) =f(b), g(a) > g(b)} (43)

The value D shows the degree of discriminancy of metric fover metric g on algorithms
A and B. In order to claim that one metric is better than another one, the condition
C > 0.5 and D > 1 should be satisfied. In this case, we can conclude that two metrics f
and g are statistically consistent with each other and metric fcan D times better tell the
difference between two algorithms.

Degree of indifferency

There exist some cases in comparison that none of the two metrics can report the differ-
ence between two algorithms A and B. The degree of Indifferency (Dol) of two evalua-
tion metrics fand g is calculated as follows.

_ (44)
||
where V and U are defined as Eqs. (45) and (46) in the domain W.
V={@b)|a,beV,a #b,f (a) =f(b), g(a = g(b)} (45)
U = {(a,b)|a,b € V,a £ b} (46)

The value J shows the degree of Indifferency for metrics fand g on algorithms A and B.

Experimental results and analysis
In this section, which consists of two parts, we empirically compare the boosting algo-
rithms that have been reviewed in “General Boosting approaches” section. In the first
part, the performance of the algorithms and their computational time are compared on
15 multi-class imbalanced conventional datasets through various statistical tests. Then,
the best evaluation metric is determined based on the obtained results. In the second
part, the performance of the algorithms is investigated on 4 multi-class imbalanced big
datasets.

The experiments were conducted using a computer system with Intel Core i3 2.13 GHz
CPU, 8 GB RAM running Microsoft Windows 10 64-bit operating system. The overall
Tanha et al. J Big Data (2020) 7:70 Page 21 of 47

results were obtained by averaging the values of evaluation criteria over 10 runs of the
algorithms (10 x 5 cross-validation). For the sake of clarity, it should be noted that the
library of all algorithms were installed using the pip Python installer, e.g., sudo pip install
xgboost, except MEBoost, SMOTEBoost, and AdaCosts, which their implemented

python source codes are freely available at GitHub’ repository.

Experiments on conventional datasets
The performance of the boosting algorithms are studied on 15 multi-class imbalanced
conventional datasets as follows.

MAUC as a measure

Table 3 shows the average and standard deviation of the results for MAUC obtained for
the boosting algorithms. The best results are in bold for each dataset. By looking at the
results in this table, although it can be seen that CatBoost presents the highest aver-
age MAUC, but for the assurance we need to conduct a non-parametric statistical test
on the results. By conducting the Friedman Test, the value of 0.043 was computed for
p-value, which rejects the null hypothesis. Moreover, the average ranks from this test are
presented in Fig. 2. It is observable that CatBoost and AdaCl achieved the lowest and
highest rank, respectively. Therefore, we select this aas a control method and continue
with the Holm post-hoc test. Based on the presented results for p-values in Table 4, it
can be seen that CatBoost statistically outperforms AdaBoost, SAMME, MEBoost, and
RUSBoost.

For comprehensive analysis and understanding of the difference between pairs of algo-
rithms the Wilcoxon signed-rank test is used. The results of this test are presented in
Tables 5 and 6. Table 5 shows the computed ranks by the test. Each number below the
main diagonal is the sum of the ranks for the datasets that the row aoutperforms the cor-
responding column a(R), while each number above the main diagonal is the sum of the
ranks for the datasets on which the column ais worse than the corresponding row a(R_).
Two classifiers are significantly different and reject the null hypothesis if the test statis-
tic, min(R+,R_), is equal or less than the critical value which is 25 for the confidence
level of a= 0.05 and N= 15, according to the table of Wilcoxon test.

The summary of the comparison results is presented in Table 5, in which the mark
@ indicates that the row asignificantly outperforms the corresponding column algo-
rithm. As can be observed from this table, CatBoost outperforms most of the algo-
rithms. SMOTEBoost outperforms four algorithms and we saw that it has the second
rank from Fig. 2. We can see that binary classification boosting algorithms could not
outperform any algorithms except AdaCost that outperforms just AdaCl1 algorithm.
Therefore, we can conclude that binary boosting algorithms do not show better per-
formance in multi-class classification problems based on the MAUC criteria. Another
interesting analytic finding is that the under-sampling method for encountering class
imbalanced problems is not a satisfactory solution based on the obtained results for
the RUSBoost algorithm, while SMOTEBoost which uses the oversampling method

 

> MEBoost: https://github.com/farshidrayhanuiu/MEBoost
SMOTEBoost, AdaC1, AdaC2, AdaC3, AdaCost: https://github.com/gkapatai/MaatPy
Page 22 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

O000'0F OLOOOF 7ZO0O0F OOODDOF SZE00F 891L00F LZ0OOOF OL000F 7Z000F 6Z0O00F 8€000F 920004 €Z000F €£00 0+
49990 0660 S0660 £9990 6688'0 71960 £9860 81260 £860 S960 69460 68860 +5260 L1860 PIOJAY
67000 LrOOO0F 9P000F 8ZZ700F Y9E000F 961004 98000 9 97000F 86000F SLOOOF SZOOOF vEOOO0F SLLOOF ZOLOOF
86960 8Zr60 9680 ILIZLO £7860 6£78'0 68560 60660  LZrg'0 S860 8660 S860 87560 87860 uolenjera Je)
SOLOOF SVLOOF ZS7TOOF CZSIOOF 68E00F vrrlOOF ZOCOOF YELOOF I18L00F LLIOOF LylOOF IELOOF ZLLOOF O€L00F
8670 €%670 18270 £5870 Z8LEO 67670 91ZE0 7Z7O0 ~— €8 L710 8E87'0 EL970 = pSOEO~—- 8870 6£87'0 yeay
69100F 9SEOOF E8LOOF PZEDOOF  7HODOOF LEVOOF Z6LOOF 88E00F I9PZ0O0F SSv00F 627O0OF rrZ00F P7EOOF 8770 0F
Zlp90 ©= 8%SS0——s«LH'O~—- LOVI'O SOzr'0 peeso 79990 SE7IO ~—- LBOL'0 SSP9'0 /8l\Z0. = ZS890~—s 990 L8eo0 sse|5)
vlOOF P6LO0F V9DD0F 98000F 61004 60700F OOLOOF Ss000F 16000 OSLOOF  66000F 97Z200F O8LO0F 8LLOOF
£6890 9950 98090 €09€0 911Z0 81990 12090 g9z90—s O70 €019'0 64090 61290 8790 LEL9O gjeos aaue|eg
8OLOOF LOLOOF Z8000F ZOLOOF IZL00F ELLOOF ZO000F L69L00#F €ZLOOF LS000#F 8000 €€000F OS0O00F COLOOF
97560 LIS60 76560 85560 y7060 8E76'0 95960 /8160 O00L60 S1L60 18960 £8460 S960 LSS60 ABojoyewUaq
ISSOOF LEVOOF SIPOOF 96Z700F G6OLO0F OLZ00F PSEOOF LIZOOF E€L700F LILOOF 90Z200F 9ZL00F 69700F L1ZOOF
cegg0 = S880 ss 8ZZB0 = L868°0 /606'0 p83'0 2060 S?790 ~=—- P6880 €906'0 86060 66880 €1Z80 6/880 PIOJAY] MAN
SVLOOF G6LIOOF SELOOF 68000F 8L700F 69100F 8LZOOF +r8l00F SSIOOF ZTOLOOF = SQLOOF_SsCOEZOOF -BLZOOF 81Z00F
IgsZ0 = LI6Z0—s« ELOL'0 £620 LS6L0 LS8Z0 69080 yl8Z0  vz8Z0 y6LL0 yi8L0 66620 8SZL0 85/20 UWIN|OD jeIQae,\
6L000F 19000F 9S000F 96000F O02000F LPO000F L7000#F LE00COF LS000F O<C00F €S000F SPO00F SSO0O0F SS00'0+
\S060 62060 8€060 SE€060 97560 76560 €0560 97460 L760 E1560 75560 7960 ~=—- 1 8S6'0 7LO60 paseq-uad
€6000F vLLOOF PrOOOF 9S000F  O09000F v7Z000F LZLOOF 150004 7ZO000F OOLOOF 9000 9000 1Z000F 87000 +
r7550 CtrvO G6L~€0 SSrS0 S00S'0 8Srr'0 Logv'0 ZEOSO =: 99ES'0 6£67'0 p06r'0 SOESO — rv6r'0 L87S°0 aAtydadeluoD
ELLOOF L6LO0F SLLOOF LOLOOF EVEOOF 6700F ESLOOF 106104 I1SLOO0F 8ELO0OF L8000F Y+SLO0F VOLOOF SSLOOF
S6080 79180 ZO|8O0 £1080 6£€50 pelo 9S6L0 ly6Z0 62620 ZE08'0 S7OsO = 88SZ0~—s«LS6Z'0 79080 Yjoy-sakey
6£L00F PELOOF ZILOOF S91O0F 7SLOOF 160004 LOLO0#F LLOOOF 6Z100F SCOOOF 86000F vS000F 8ZL00F €8lL00F
760 92160 §=—9160 ~3=— 6 Z6'0 £6960 17960 6676'0 86860 65960 £1660 p£S60 €8/60  6£€160 y£06'0 SUM
yopepy eDepy ZdePy LDePY WaDI4bIT jsOOgSNY ISOCOGFLOINS 3SOOGFIN 3SOOGDX }sOOg}UaIpesDy yjsoogybhoO7 ysoog}e> FWWVS HIN'}soogepy
wywobly yosejeqg

 

Jausea] aseq e Se 3a} UOISIDap Buisn swYyyWsObje Huljsoog Wold DNYIW 40} S}J]Nsaa paulezqo auL ¢€ a1qeL
Page 23 of 47

(2020) 7:70

Tanha et al. J Big Data

yasejep Yea JOJ D1}! U UMOYSs S| BDUeWOJad Jsaq aYyL

 

 

 

LISZ0  ©6©SZ9Z0. =: 99hZ'0—Ss ZOLL'0 7890 ZEvL0 L6LL0 9lrZ0  L9LL0 75690 Z08Z'0 68Z0 = vE9L'0 968920 abeiany
OLLOOF YSLIOOF S9LOOF SFVLOOF LISOOF S9LOOF LTLOOF G6yLOOF 8S100F pOVOOF E€l1ZOOF I1ZL00F 6LLOOF LLLOOF
80S60 9¥60 70S60 S60 Lv6€0 L760 £9860 IZ180  vLZ60 78970 g0960 ~=—s—s- L096'0-— ss PELE 6/760 3jynys
pLOOOF 8S0O00F O6000F §6200F +vlL700F S8L00F p6OlLOOF 88l00F E€1Z700F TOVO0F ELIOOF S6000F 86100F 6710 0F
SSpZ0 = S688.0~—s« L6UELO.~— LSPL'0 99550 7ZES0 S780 LII80 —- 8EZB'O 8E0¢0 LeLg0 Sp980  8S8Zz0 E8L80 syo0|q abed
LECOOF LETOOF ESLOOF I9LO0F €9000F €ElL00F QLLOOF 660004 65100 OOCOOF PLLOOF 6£L00F 86000F OL70'0F
OV8r0  SSOSO 90870 ZOLYO 6070 97670 68870 L6€v0 6£50 61970 8s6v0 LS/h0 ~60K0 880S'0 ISeOA
yopepy eDepy ZdePy LDePY WaDI4bIT jsOOgSNY ISOCOGFLOINS 3SOOGFIN 3SOOGDX }sOOg}UaIpesDy yjsoogybhoO7 ysoog}e> FWWVS HIN'}soogepy
wywobly yosejeqg

 

(panunuo>) ¢ ajqey
Tanha et al. J Big Data (2020) 7:70 Page 24 of 47

 

10

9

Ooms

©

RN

Average Rank

No Ww

  

o

 

 

 

 

 

 

™“™ Oo 82 82 8 B&B 8B Bes KR NM
a 3.3 QO m 5
L Fig. 2 Average ranks obtained from Friedman test based on the MAUC ]
Table 4 Holm post-hoc test results based on the MAUCs
Control method (CatBoost)
Algorithm Z p-value Holm Hypothesis (a= 0.05)
AdaBoost.MH 3.3387 0.0008 0.0038 Rejected
SAMME 2./277 0.0045 0.0055 Rejected
LogitBoost 1.331] 0.1831 0.0008 Not rejected
GradientBoost 2.2258 0.026 0.01 Not rejected
XGBoost 1.1783 0.2386 0.025 Not rejected
MEBoost 2.9022 0.0037 0.0041 Rejected
SMOTEBoost 1.0038 0.3154 0.05 Not rejected
RUSBoost 2.8804 0.0039 0.0045 Rejected
LightGBM 2.6186 0.0088 0.0062 Not rejected
AdaC 3.3387 0.0008 0.0038 Rejected
AdaC2 2.8368 0.0045 0.005 Rejected
AdaC3 2.4876 0.0128 0.0083 Not rejected
AdaCost 2.1167 0.0342 0.0125 Not rejected

 

to handle imbalanced data, got the second Friedman rank and could outperform four
algorithms.

By comparing a family of binary classification boosting algorithms that OVO strat-
egy was taken to them to handle multi-class problems, in addition to get higher Fried-
man rank, none of them could outperform other algorithms, except AdaCost that
outperforms just AdaC1 in terms of MAUC criterion.

From the cost-sensitive boosting approaches, in overall AdaCost shows better per-
formance; however, AdaC3 presents better average MAUC value than the others.
Moreover, AdaC3 works better than all algorithms for 3 datasets.

MMCC as a measure

Table 7 shows the average results of cross-validation for MMCC measurement. The
form of the reported results is similar to Table 3. It is clear that, except AdaBoost.MH,
SAMME, RUSBoost, and AdaCl1, the rest of the algorithms present the best MMCC
Page 25 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

— 89 08 08 6 EL cE 69 OV L9 LE O€ €9 S9 JsODepy
cS — LL 6Z BZ LL 9¢ 89 6€ LV ce SC 8S 9S cDEPV
OV eV — GOL LL 9S 6 09 VC S9 CC el LE SV COEPV
SC LV Sev — C9 OS LL 8V Sl VV 8l SCL VE OV Lepy
Lv CY ev 8S — cS cE SE SE 8S LE 6L Sv GE waoiw6ry
LV eV v9 OL 89 — Sl 8S 6€ Sv LE LL vv LV JsoOgsny
88 vO LLL 60L 88 COL — 88 c9 LL 9S LV L6 v8 JSOOGILOWS
LS cS 09 CL S8 c9 cE — ce 9S cE vl SS GE \soog3
08 S0'08 96 SOL S8 L8 8S /8 — GZ vl Lv S8 v6 \SOOgDX
cS eZ v9 QL C9 SZ eV v9 SV — 6€ SC €9 95 jsoog {Us!peJS
€8 88 86 COL 68 €8 79 EL OV L8 — vv 68 S08 \soogibo7
06 S6 ZOL SZOL LOL 601 6 901 6 S6 QL — 901 S66 \soogie>
LS c9 €8 98 GZ QL EC 59 SE LS LE vl — vV IWWYS
SS v9 SZ vl $8 EZ 9€ $8 9¢ v9 S'6E S07 L9 —  Hywrisoogepy
yopepy epepy zZePY LDePy WwadIYbI ysoOgsnY 3OOGILOWS 3SOCOGAW 3SOOgDX jsOOgUaIPeIN 3s00g}I60] jsoOg}e> JWINVS HIN'}SOOgepy = swy}40bly

 

SDNVIN 24} UO paseg }s9} UOXO>]1\\ 24} Aq payndwod syuey gs aqel
Page 26 of 47

(2020) 7:70

Tanha et al. J Big Data

 

}SOepy

cDepy

COPPV

Le&py

waoyor]

1SOOgSNY

@ JsoOgI LOWS

\SOOgAW

— \soogyyx
- e@ JSOOg}USIPeIS)
— \soogubo7

— @  ) Jso0g}e")

— AWWYS

— HW iIscogepy

 

yodepy EdePpY CDEPV

Loepy

Wa51u617

ysoogsny

ysOOgALOWS

ysoog AW

SOOGDX + ysOOgIUAIPeIy ys00g}160]7 ysoogie> JWWVS HW}soogepy swyobly

 

SDNVIN 24} UO paseg }sa} UOXO>}1\\ Jo Atewiuns 9 aqey
Tanha et al. J Big Data (2020) 7:70 Page 27 of 47

 

nan aD

©

>

Average Rank

Wo

~

 

8

C >)
0

 

 

™“™ OO 8 8 6 8B @ @ BS FNM B
> ses gg gggg& Yess
4 Y oO CO oO O Oo oO ses] we & & C3
7 nm am A m C 5 uy Uy SB
22282268 8B 2228
gvzoers xk = § 2 ® =
3 e =
< Oo n
Fig.3 Average ranks obtained from Friedman test based on the MMCC
\ /

 

for at least one dataset. However, the p-value computed by the Friedman test is 0.0439,
which rejects the null hypothesis. Furthermore, according to Fig. 3 CatBoost algorithm
achieves the lowest average rank. By selecting the CatBoost algorithm as a control
method and conducting the Holm post-hoc test, we can see that it fails to find a signifi-
cant difference between CatBoost and others, as stated in Table 8. On the contrary, the
Wilcoxon rank-sum test shows that CatBoost outperforms ten algorithms as presented
in Tables 9 and 10.

From the MMCC viewpoint, according to Table 10, both XGBoost and SMOTEBoost
can be considered as the second best algorithm due to outperforming four other algo-
rithms, although, they are significantly better than the cost-sensitive boosting methods
and the old AdaBoost.MH algorithm.

Another interesting point is that we can see the superiority of most of the gradient-
based boosting algorithms (CatBoost, GradientBoost, XGBoost) and SMOTEBoost
algorithm over other methods, therefore, integrating the used sampling method in
SMOTEBoost (Synthetic Minority Oversampling Technique) with the gradient-based
boosting algorithm will improve their performance in front of imbalanced datasets.

G-mean as a measure

The obtained results of G-mean for algorithms are reported in Table 11. It is observ-
able that CatBoost presents the best results in more datasets than the other algorithms.
Furthermore, as shown in Fig. 4, CatBoost achieves the lower Friedman average rank.
But, unlike the previous measurement criteria, RUSBoost achieves the worst rank. By
conducting Holm post-hoc test, we can see the test cannot find a significant difference
between the control method (CatBoost) and the other methods, as shown in Table 12.
However, the Wilcoxon test shows the difference between each pair of algorithms.

As presented in Tables 13 and 14, we can see that CatBoost, LogitBoost, SMOTEBoost
and XGBoost outperforms 5, 3, 3, and 1 algorithms, respectively, in terms of G-mean
criterion. In addition, like the previous assessments by using MAUC and MMCC meas-
urements, also in G-mean, no binary boosting algorithm outperforms other methods.
Page 28 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

pOOOOF 6Z2000F 6P000F O00DD0F IS00F  90E00F 8EO00F EvLLOOF €7000F S9000F 790004F S9000F 6r000F vy00'0F
SEseO 19560 70960 €EEE0 99S/'0 pLL8°0 €096'0 85660  LES60 Lev60 \Ly60 69560 £760 b6760 PIOJAY
O9LOOF 7@Z7LOOF v8000F 7@8E00F LS000F €L700F 80Z00F 7LOO0F O€Z00F 6€L00F  OZL000F S8000F ZZIO0F 9910 0F
77180 60 6EPLO.=—:*9EVO'ID-—S—s« EGE £9160 7655 0 L908°0 €y860 S90 LOL60 17660 6960 87780 LEZ60 —-uoHenjeda ued
p7COOF COLCOOF IZVOO0F LOSOOF LETROOF LSEOOF 67V00F L06Z00F €6E00F OLEOOF LZ7EOOF BLZOOF 697200F LO~OO#
lpyseO-— 8l8E0- 7S6E0- LLOVO- 90580- Y6LE0- BISEO-— 68erO- EsEerO- 90170 — bryyvO- €SLE0- 8L6E0- 7ELvO— yeay
PIEOOF 99900F 86E00F PZ900F 18000F v6800F 68E00F SO800F 68r00F SL6E00F ZISOOF 6EP00F 965004F 9670°0F
67STO = STIL'0-—s—«*dECLLZZO~—C—C~SESZN—s« BLO 56500 LLOEO LZEZ7O0 «6060 9670 SZlvyO —- €Z9E'0 LOO r6r70 sse|5)
yIZOOF OZEOOF OLLOOF 8LLIOOF Vv9EOOF 6P00F 68L00F O8L00F 8ZL00F €6700F LIZOOF 90L004F 7SE00F 6€700F
LOESO 6€€lL0 SOLO SELZ7O- 6fr'0 6LEE0 91170 91S7O = Lv7Z'0 €807'0 yS|7TO «EVOL 9570 6C770 gjeos aaueeg
ELLOOF 9OZOOF SELOOF IVLOOF 9LLO0F ¥OZ00F 6LLO0F L7EOOF FEZ70O0F OOLOOF OZLLO0F 89000F 16000 OSLOOF
€9060 +2060 12160 SLI60 7L18°0 6888'0 L1€6'0 Zly80  98€60 yev60 pye60 65560 LOg60 68060 ABo|oyeWWag
O8SOF LOLOOF O06900F 17SOOF  SOZOOF IIEOOF 66V00F S8s00F I17r00F 77COOF ©= GELDOF SC OGLO'OF  S9SPO'0F SECOOF
/8SL0 ¥g9L0 lerZ0 7r8ZL0 E780 €862'0 £S08'0 91770 76L0 ZLE80 Siz7vO. —- €008°0 98SZ'0 lpZL0 PIOJAY] MAN
pOCOOF P6LOOF ISTOOF ISLOOF V9V00F I19E00F ZLYOOF S8E00F 78Z00F IS7OOF ~6COOF 6PPO0F ZIYOOF 98f0'0F
S6090 g8r090 67790 Z9L90 87£9'0 87190 89+90 7090 7090 7Z09'0 19090 ~=—s €ZE9'0 v16S0 b68S0 UWN|OD jeIqele/
ISLOOF E€LLOOF 6OLOOF E8LOOF SEODOF 16000F ESOOOF €ZO00F YILLOOF 99000  SOlLOOF Z8000F 80100 901L0°0F
LLIgO SOl8O0 17180 61180 92060 760 9706'0 960 8980 6706'0 S7l60  7S760 8160 €Z08'0 paseq-uad
S8LOOF LEZOOF PYOOOF LLIOOF 980004  +L000F 6€C00F 66000F €vlL00F 66100 G6LIOOF 68000 ZrL0O0F 8600°0F
$7010 L7OLO-— 620E0- +680'0 81000  g9010- S6E00- 8000 £200 LLLOO— I8LOO-— = S 1900) vOLOO— 7950'0 aAydadesUoD
SCEOOF 88ZOOF 661004 SSZTOOF 78900F >¥SS00F OOS<0OF PLEOOF ZOEOOF C7COOF = STOO LOZOO Fs ZL ZO'OF pl7OO0F
LSI90 © SZEIO 7790 = LZ09'0 6790'0 LOovS‘0 9/850 91650 9650 L190 97090 677250 L16S°0 60190 YloYy-sakeH}
Z9OCOOF PSZOOF LZ7EOOF OCEOOF O8ZOOF BLIO0F SCEOOF SSIOOF LEZTOOF OSOOOF S8l00F 98000F 9E700F yS7OOF
96980 92160 g8Ego S980 7860 €76'0 E1060 y9960 £60 £7860 SS160  9FS60 8LE8‘0 yEL80 SUM
yopepy eDepy 7ZD>epy LDePY WDIYbIT JsoOgsSNY ISOCOGILOWS 3SOOgFIN 3}S00g5X JsOOg}UaIpeIy ysoog}1H07 }soogiey JWWYS HW'3}soogepy
wyywobly yosejeqg

 

Jausea] aseq e Se 391} UOISIDap Huisn swYyyWAObje Huljysoog Wod >> WW 10} S}]Nsa1 pauleyqo ayy Z ajqey
Page 29 of 47

(2020) 7:70

Tanha et al. J Big Data

yasejep Yea JOJ D1}! U UMOYSs S| BDUeWOJad Jsaq aYyL

 

 

 

le8v0-—s«LZOSO~—- «BELVO ~—- 766 0 992¢0 81970 07750 sssv0—s LLvS‘0 p71v'0 p9SS0 89250 97750 pOES0 abelany
E87OOF PSLOOF OSZOOF 97ZOOF 78Z0OOF Y9S700F 6£L00F 8rZ00F 79Z00F 86V00F vrZ700F OLIOOF O06100F ZOS00F
19/80  8Z8'0 E/go = 19980 Lv 0- 9€EL0 78960 se990—s«9S 160 £99710 — 99760 92160 S860 LSZ8'0 ajanys
SECOOF 6ZLOOF Z6LOOF ZLIOOF SZLOOF LECOOF STEOOF ELZTOOF LYEOOF €6900F  6€200F OLLOOF SZE00F py700F
€66€0 7l6SO0 LElVO Zr0r0 €901'0 76570 E7VS0 SLES 7250 EL7EO— 87750 7/790 L767'0 68150 syo0|q abed
LOSOOF E€COOF P7ZOOF €6ZO0F 86000F  69200F Pp6OLOOF 8rlOOF S6L00F 99CO0OF vr7OOF 6LO0F ZELOOF LOSO0F
E8v00—- 19000- 7rS0O- €6STO- PZLLO- 16670- LEvO0- ~7800- = S400 96400- 8?Z700- €8700- Z6Z710- 56000 — yseay
yopepy eDepy 7ZD>epy LDePY WDIYbIT JsoOgsSNY ISOCOGILOWS 3SOOgFIN 3}S00g5X JsOOg}UaIpeIy ysoog}1H07 }soogiey JWWYS HW'3}soogepy
wyywobly yosejeqg

 

(panunuo>) Z ajqey
Tanha et al. J Big Data (2020) 7:70 Page 30 of 47

Table 8 Holm post-hoc test results based on the MMCCs

 

Control method (CatBoost)

 

 

 

Algorithm Z p-value Holm Hypothesis (a= 0.05)
AdaBoost.MH 2444 0.0145 0.0038 ot rejected
SAMME 1.833 0.0667 0.0083 ot rejected
LogitBoost 0.48 0.6311 0.05 ot rejected
GradientBoost 1.1783 0.2386 0.0125 ot rejected
XGBoost 0.9383 0.348 0.025 ot rejected
MEBoost 1.9421 0.0521 0.0071 ot rejected
SMOTEBoost 1.1783 0.2386 0.0166 ot rejected
RUSBoost 1.9639 0.0495 0.0062 ot rejected
LightGBM 2.444 0.0145 0.0041 ot rejected
AdaC1 2.1385 0.0324 0.0055 ot rejected
AdaC2 1.7893 0.0735 0.01 ot rejected
AdaC3 2.2694 0.0232 0.005 ot rejected
AdaCost 2.3131 0.0207 0.0045 ot rejected

 

Computational time study

Computation time analysis was performed to determine which boosting algorithm
shows lower computation time, and the results are presented in Table 15. It should be
noted that the times are recorded just for one fold of cross-validation in the training
phase.

From Table 15, it is obvious that the LightGBM algorithm presents the lowest average
computational time. This is an interesting observation, because LightGBM is a binary
boosting algorithm and by using the OVO decomposition mechanism for handling
multi-class problems, it should be generated boosting process for K(K—1)/2 pairs of
classes (K is the number of classes). But, by comparing the computational times of algo-
rithms for each dataset, one can see that XGBoost is better than LightGBM in most of
the datasets. However, considering the classification performance of the algorithms, we
can see CatBoost shows fair computational times.

SMOTEBoost shows the highest computational time due to generating synthetic sam-
ples for minority classes. Furthermore, although it presents the best performance for the
largest dataset (Shuttle), it needs a lot of training time.

Metric evaluation study

In this section, we aim to investigate which metric would turn out to be the most suit-
able measure for evaluating the predictable performance of the algorithm on multi-class
imbalanced problems by comparing MAUC, MMCC and G-mean metrics by using the
comparison framework on the 15 imbalanced conventional datasets.

By obtaining the results of the boosting algorithms on these datasets with 5-Fold
cross-validation, 75 test sets (15 x5) are obtained for each pairwise comparison
between evaluation metrics (MAUC vs. MMCC, MAUC vs. G-mean and MMCC
vs. G-mean). In this test, all the possible pairs of 14 boosting algorithms (91 pairs)
are considered. First of all, regardless of which metric is the best, the degree of
Page 31 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

— L9 OL $8 OZ CL Le €9 LE L9 Le ol 9S L9 JSODepYy
6S — $8 68 QL L8 SC L9 SE OV Le el OS 6S ce DePV
OS SE — 88 LZ 99 L SS 91 6V vl OL cE SV COEPY
SE Le CE — LS CS LL SV vl LV 91 OL LC vv LDepyv
OS vv 6V €9 — CS 8e LE LE vS SE LZ OV SE waoiy6r)
8Vv 6€ vS 89 89 — ol SV SC SV vC Vv Of 9¢ Jsoogsny
68 S6 ell 601 C8 vol — LL 6S OL LV Le L6 SZ JsOOg4 LOWS
LS 6S S9 SZ €8 SZ eV — Of 8S 9€ Sl 8S SE JsoOogAW
€8 $8 vol 901 €8 S6 L9 SZ — OL C9 9€ LL 56 JSOOg)X
6S vl LZ 6 99 SZ OS C9 OS — LV SC 59 8G jsoOog Uslpel5
68 68 901 vOl $8 96 6 v8 8S eZ — 8e 06 v8 \soogi607
vOl LOL OLL OLL 66 9LL 68 SOL v8 S6 C8 — 801 vol jsoogie)
v9 OL £8 €6 vl 06 6C C9 eV SS Of Cl — LS AWWYS
6S L9 SZ QL C8 v8 SV C8 SC C9 9€ ol €9 —  HWiscogepy
ysopepy Edepy Zepy Ldepy WEdDIYbI] I soogsnY 3SOOGFLOWS 3SOOGAW 3SOOgDX JsOOgjUaIPeIy ys00g}I1H0] }soogieD JWIWVS HIN'}soogepy swyWwobly

 

SDD ININ 24} UO pasegq }sa} UOXOD]1M 24} Aq pajndwio> syuey 6 a1qeL
Page 32 of 47

(2020) 7:70

Tanha et al. J Big Data

 

}SOepy

cepy

COPPV

Le&py

waoiyor)
JSOOYSNY
JsOO_FlOWS
\sOogrW

@ JsOOg)X
jsoogjualpel
\soogubo7
ysooge)

— AWWYVS
—  HWisoogepy

 

yodepy EdePpY CDEPV

Loepy wadiy6r ysoogsny

ysOOgALOWS

ysoog AW

}SOOGDX

jsoog}UaIpely

ys00g}1607

ysoogye) §=AWWVS HIW'}s00gepy

swiyWiobly

 

SDI 24} UO paseg }s9} UOXO>]1M JO Asewuns OL a1qeL
Page 33 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

6ZLOOF vLOOOF LEOOOF OOODOF ZS000F 17100 OZOO'0F OLO00F +vLO00F SCOOOF vlLOOOF 9E000F LLOO0F LS000 +
ly000 =©96460 §=6 ZZ860-~S «00000 LSsg'0 £1560 9786'0 88960 68260 bl60 L9l60 60860 6L26'0 LLZ60 PIOJAY
COLOOF 68000F O9000F ZLEOOF  9€000F Z0200F 7TLOOOF ~rOOOF VSLO0F LS000F  9€000F SPOOOF S9000F 16000 +
9g9g0 6 zS880~—s EEO €09'0 78860 61780 p0€E6'0 07660 —LOPB'0 75860 £9660 19860 plv60 8/860 —-uOHJeNjeAa JeD
OLZOOF vZEOOF OOOOOF 7@8Z7O0F OO0D00F S8l0O0F LOSOOF O0000F 00000F 00000  O0000F OSZ7O0F 00000F 0000°0
99000  ZO0€00 00000 €Z100 0000°0 8500'0 £660°0 00000 00000 0000°0 00000 61100 0000°0 0000°0 yeay
LOLLOF I8ELOF LELLOF OPOLOF O0000F 90S10F C6y 04 LS600F 8LZL0F 0881: 0F LOLLOF G607LOF 1500 Spy OF
plysO ~@8l7O0 ~@BEEO 7ZSZE0 0000°0 v10Z'0 v17Z0 ElpZO © 7S77'0 LELEO Z1SS0 L6S7'0 S6070 L60€0 sse|)
9LOOF LyvOOF PLOOOF IZ7SOOF 1p900F 89010 pySOO#F O€Z00F OLEOOF OS900F SISOOF 9Z91L0F I1ELOOF 7£900F
Ig8S0 1€070 86190 E200 ZZE90 LL8v0 LOLv0 95870  8600'0 8Z€00 I8ETO0 ~—- 86870 prer0 L9v7'0 gjeos aaue|eg
SLOOOF EOLOOF S9000F OLOOO0F L60004F 601004 L0000F rS800F SZLOOF OSOOOF 96000F G6E000F O0rO000F £900°0F
97560 77560 70960 99560 0006'0 E€46'0 €5960 E7Z80 £6960 €1Z60 59960 98260 $7960 55560 ABo|oyeWWAag
8ECOOF 77EOOF LZEOOF S9TOOF 16000F I9E100F OrZOOF 6591004 9ZL00F 86000 OrlOOF 9O0LO00F ILLEZOOF OLLOOF
75880 6880 69880 64680 95160 LEL60 SS06'0 L7L7O ~—-S906'0 rb76 0 LEl60 66060 v068°0 12680 PIOJAY] MAN
p8lOOF IIOOF I9LO0F ELLOOF ~S7OOF SIZO0F 6€COOF 161004F 651004 €6l(00F €9L00F SrZOOF 6€700F L1ZOOF
BlEZ0 pZ49Z0 10820 ~ S18Z0 SE8Z'0 SOLL0 L06L0 ELZZ0.~—- €ELL'O yOLL0 96920 1Z6L0 69920 8Z9L'0 ULUNIOD |eIqaLe/\
LZ000F €9000F ESOO0F LOLOOF OZ000F 8r000F L7OOOF 8000 SS000F LS000+ LSOOOF 9P000F 95000 LS00°0#
/7060 €7060 S060 1¢€060 y7S60 98560 9676'0 ¢7L6 0 7£6'0 60560 7SS60 = S L960 9/560 9006'0 paseq-uad
88000 6ZlIO0F OPZOO0F 1S000F v9000F 18000 OTLOOF €9000F 080004F LOlOOF S8000F SSO00F 64000 8700°0F
6650 878sd ILLZI0 vLESO 97670 L7Ev0 yOLr'0 \l6v0 sSeEso 678r'0 618v0 75050 lysr'0 ey7S'0 aAydadesUoD
pOCOO#F S7ZOOF 6Z1IO0F LZLOOF L8LLOF S7Z00F POLOOF v6L00F 9SL00F ZCOLOOF =: OLLOOF SC HZLOO FS BLLO'OF 6LL00F
7860 ©=66S080~=—s €008'0~=—s-: 968Z0 €OZE0 €6ZL0 E780 96820 9820 S16Z'0 L0o6Z0 ~©=—-8SSZ0 67820 8r6L0 YloYy-sakeH}
TELOOF CELOOF IZIOOF ELIOOF BElLO0F 16000 SOLOOF LLOOOF O9LLOF 9COOOF ZOLOOF 8S000F SZLOOF OLLOOF
Lye60 94/80 6160 Sze60 8896'0 796'0 LOS60 ZE860 $9960 80660 S9S60 89260 67160 8506'0 SUM
yopepy eDepy 7ZD>epy LDePY WDIYbIT JsoOgsSNY ISOCOGILOWS 3SOOgFIN 3}S00g5X JsOOg}UaIpeIy ysoog}1H07 }soogiey JWWYS HW'3}soogepy
wyywobly yosejeqg

 

Jausea] aseq e Se 391} UOISIDap Huisn swYyyAObje Huljysoog Wod UDaUI-D 10} S}J{Nsa1 paule}qo aYUL LL 2IqeL
Page 34 of 47

:70

7.

(2020)

Tanha et al. J Big Data

yasejep Yea JOJ D1}! U UMOYSs S| BDUeWOJad Jsaq aYyL

 

 

eZZS0 96590 8le90 ZLILSO LvcS 0 0SS9°0 v689 0 96850 S099°0 c€SS0 7v690 LL69°0 £5890 L699°0 abelony
L6SOO0F 9CZO0F LZ8004F ZZSO0F O0000F O80004F 88000+F  LcéSOOF SO0800+F v9000 + S9LOOF LE600F Y9C800F 99400 +
cEv60 SO0C60 6€060 00760 0000'0 SL68°0 18860 069€'0 9Vv6 0 8c00°0 L0L6'0 66680 CCV60 56160 3/}INYS
O0O0OOO0+F S6O00+F e870O0F OOODDO0F SEO0D0F LSEOOF c9LOOF B8lLLOOF ELLOOF LS800 + c6000F O8000F LElOOF 6LLOOF
00000 c9LL0 LeLEO 00000 eZLv00 C6690 O818°0 06780 LLv8'0 O01 COE80 £6580 CV 180 SLCE80 syD0|q abed
99L00+F 6c600+ ZS600+ OO0D00F 000004 00000+F evSOOF O0000+ O00000+ 0000'0 + 80v00F  O0000+F 000004 66700 +
SSLO  LE600 67910 00000 0000'0 0000'0 e€LS00 0000'0 0000'0 0000'0 ©6100 0000'0 0000'0 9€c0'0 Sed,
yopepy = Edepy ZdepY = LDePY WdDIYHIT JsoO_gSNH IsOCO_FLOWS 3SO0OgAW 3sOOgDX JsOOg}UaIPeID ysoog}IHbO] }Jsoogiey JWWVS HW'}soogepy

 

wyywobly yosejeqg

(panunuo>s) LL aqey
Tanha et al. J Big Data (2020) 7:70 Page 35 of 47

rr)

 

9

8
7
0

ON

Average Rank

Ke NO WH Bf hn

   

|
t
t

l
2

st
2
3

 

 

 

 

 

 

AMV eee eS OZ SF “
L Fig. 4 Average ranks obtained from Friedman test based on the G-mean
7
Table 12 Holm post-hoc test results based on the G-means
Control method (CatBoost)
Algorithm Z p-value Holm Hypothesis (a= 0.05)
AdaBoost.MH 2.1385 0.0324 0.0062 ot rejected
SAMME 1.9857 0.047 0.0071 ot rejected
LogitBoost 0.6328 0.5268 0.05 ot rejected
GradientBoost 1.7675 0.0771 0.01 ot rejected
XGBoost 1.1565 0.2474 0.0166 ot rejected
MEBoost 2.1385 0.0324 0.0055 ot rejected
SMOTEBoost 0.7637 0.445 0.025 ot rejected
RUSBoost 2.5531 0.0106 0.0038 ot rejected
LightGBM 2.2912 0.0219 0.005 ot rejected
AdaC1 2.5313 0.0113 0.0041 ot rejected
AdaC2 1.7021 0.0887 0.0125 ot rejected
AdaC3 2.3349 0.0195 0.0045 ot rejected
AdaCost 1.833 0.0667 0.0083 ot rejected

 

 

consistency of evaluation metrics are examined. The results of the DoC comparison
between metrics are reported in Table 2. The C value shown in the table for each pair
of metrics is resulted by averaging the C values of 91 pairs of algorithms. As discussed
in “Comparison of evaluation metrics” section, the value of C larger than 0.5 indicates
the consistency of the two metrics. According to Table 16, The C value obtained for
all the metric pairs is larger than 0.9, which satisfies the condition of being larger than
0.5. Thus, it can be claimed that MAUC is consistent with MMCC with a consistency
degree of 0.9853. Also, MAUC is consistent with G-mean with a consistency degree
of 0.9334 and finally MMCC and G-mean are constant with each other with the con-
sistency degree of 0.9403. But among these three pairwise comparisons, MAUC and
MMCC are more consistent with each other.

In order to select the best metric, the degree of discriminancy (DoD) of each metric
over another metric is examined. The results of the DoD comparison between metrics

are reported in Table 17. Exceptionally, in Table 17, instead of average D values, the ratio
Page 36 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

— LZ 79 vl 9 L9 8p 89 Ly vS OE Cy 8p SQ \soDepy
6p — GS ZL LL 99 61 EZ Ly L9 GE ve LT OS EDepy
9S 9 — QL OZ L9 Ov 9 EE 6S 67 GE Ov vS CDePV
LE 8p ag — 6p O¢ LZ GGS 7 BE Zl 61 EE CS L>epy
GG eV GE 9S — OE L€ G6E Sty GSE 97 El Sp LE wao1yorq
EG vS 6S 69 69 — 0c LS O€ Gv EC CC c€ Cy ysoogsny
eZ LOL 08 66 68 00L — 8 Srl EZ 09 LS Q/ 6L  —- SOOGALOWS
ZS Ly Ov S79 S08 vS 9€ — SSP G9 GE 0c 8p GE \sooga\
6Z EL ZL 78 GLL GL SSy Srl — S79 OE O€ 6Z 19 1SOOgDX
99 6S OV L9 S18 09 Ly S7S CLS — OE EC ES ly soogjualpely
v8 8 QL EOL 62 78 09 OL GZ GZ — ES 8 6Z \soogi607
BZ 98 8 98 76 €8 9 8 GL 8 L9 — EZ 68 ysoog}e)
GIL £6 G9 ZL SCL tL bb CLL S Ob G99 0c cE — LS AWWYS
GG OL LS 89 bl BZ lv OZ BE 89 97 LE Sy — Hwrsoogepy

ysopepy eDepy z>epy LDePyY Wa5DIYybI] 3ysOOgSNY 3sOOgAIOWS 3S0OGRW 3sO0OgDX 3sOOgjUaIpeIy yso0g}I607 ysoog}ey JAWWWS HIN}SOOgepy = swiywobjy

 

SUBAUU-D BY} UO paseg }S9} UOXOD]I\ 24} Aq payndwod syuey €1 aqeL
Page 37 of 47

(2020) 7:70

Tanha et al. J Big Data

 

}SOepy

cDepy

COPPV

Le&py

waoyor]

JSOOYSNY

JsOO_FlOWS

\sOogrW

— \SOOgDYX
— jsoogjualpel
— @ \soogubo7

e@ — ysooge)
— AWWYVS

—  HWisoogepy

 

ysopepy Edepy Z>epy Ldepy WADI4bI7 ysoogsny

ysOOgALOWS

ysoog AW

SOOGDX + ysOOgIUAIPeIy ys00g}160]7 ysoogie> JWWVS HW}soogepy swyobly

 

SUBAUU-D BY} UO pasegq }S9} UOXOD]IM JO Asewuins yL ajqeL
Page 38 of 47

(2020) 7:70

Tanha et al. J Big Data

yasezep Yded JOJ D1Je}! Ul UMOYS S| ALU} |EUOI}AWOD }S9aq UL

 

 

 

£60 Sv0 rv 0 cf0 LEO 9S | SINOH SL6C LE0 LS9 vil ES" Or'| ZL abelany
90'0 90'0 90'0 900 S10 ZOOL SINOH BE LEE LEE bbl LO’8s 79'8 0v'6 rl 0 3jynys
678 99'| €9'| LS LEO ZI LO9LL 99'r| a0) SOV 671 SLT vOE 687 syo0|q abed
lv'0 ZE0 E70 LZ0 060 160 E£0Z pSrz ZE0 6L'y seg ZL 6Z0 97 | \SeaA
L67 £60 857 Sel 870 aa SINOH 98'| SEO 87 809 877 697 £96 PIOJAY
800 080 110 400 E10 660 8168 €6S 800 Sl L6'l Sv'0 0Z0 Lv'0 uolenjera Je)
E10 07 0€0 Z10 600 690 O16 L8vl €5°0 00! rll rv'0 rv0 ZL'0 weay
90'0 90'0 90'0 S00 LO S90 ore S67 S00 /80 GO'| LZ0 9€0 S00 sse|o)
SSO or'0 SSO 790 LO 680 LOV| 65S 00 ZL ELL €9'0 a0) cL0 gjeos aaue|eg
800 cf0 90'0 S00 LS0 BLL OL tv 400 ZOOL E97 Lv 0 LZ'0 800 ABojoyewUaq
100 700 100 100 So 750 lov 950 700 8E0 £80 870 100 100 ploJAYy MAN
700 100 700 700 E10 7Z0 AS B0'e £00 750 LZ S90 yS0 CO'O = UWNIOD |eIQeLe/\
lv'0 ZE0 9€0 ZE0 Or pl 97 0627 0z0 ZOE 6L'Z S97 0'l LTO paseq-uad
LZ0 LS0 LS0 Sv'0 910 LVL LLL lv 800 Rom ES'y 6€0 LS0 LLL anndasenuoy
100 100 700 100 90'0 0S0 Z0'L Srl 700 99'0 180 L10 ve LOO Yloy-sakeH
100 100 100 700 90'0 7Z0 700 LEL £00 SEO S80 6L:0 100 100 SUM
ysopepy eDepy zDepy LDePY WaDIY4bIT 3sOOgSNY ISOOGFLOWS 3SOOG_W 3OO_DX soOg}UaIPeIy ysoog}bH07 ysoog}eyD JWWVS HW'}soogepy
wyywiobly yosejeqg

 

swyWiobye buijysoog ay} Jo (spuoras) aw} ;EUOIeyNdWOD S| aBqQeL
Tanha et al. J Big Data (2020) 7:70 Page 39 of 47

Table 16 Experimental results for verifying degree of consistency (C) of evaluation metrics

 

 

MAUC MMCC G-mean
MAUC — 0.9583 0.9334
MMCC — — 0.9403

G-mean — — —

 

Table 17 Experimental results for verifying degree of discriminancy (D) of evaluation

 

 

metrics
MAUC (%) MMCC (%) G-mean (%)
MAUC - 12 100
MMCC 7 — 100
G-mean 0 0 —

 

Table 18 Experimental results for verifying degree of indifferency (E) o evaluation metrics

 

 

MAUC MMCC G-mean
MAUC — 0.0872 0.0853
MMCC — — 0.0829

G-mean — — —

 

of the cases with D>1 are reported. The reason for taking this approach is the exist-
ence of NaN and Infinite values, hence the averaging would not indicate the real results.
According to the table, in the comparison between MAUC and G-mean, in all the cases
(100% of cases), the value of D for MAUC over G-mean is larger than 1. So, MAUC
always acts better than G-mean and in none of the 91 pairs of algorithms, G-mean wins
(0%). Also, in the comparison between MMCC and G-mean, in all the cases (100% of
the cases), the value of D for MMCC over G-mean is larger than 1 and for G-mean over
MMCC, D> 1 is not observed in any of the 91 pairs. Until now, the results prove that
both MAUC and MMCC are more discriminant than G-mean. The final step in selecting
the best metric is comparing the D values between MAUC and MMC metrics. Accord-
ing to Table 3, in only 12% of the cases (11 out of 91), the DoD of MAUC over MMCC is
larger than 1. But in 70% of the cases (64 out of 91), the DoD of MMCC over MAUC is
larger than 1. Thus, it can be claimed that MMCC is more discriminant than MAUC and
can better distinguish between classification algorithms. The conclusion of the DoC test
between evaluation metrics is that MMCC and MAUC are totally better than G-mean
and MMCC act slightly better than MAUC.

Finally, the degree of indifferency (Dol) of evaluation metrics is tested. The results of
Dol comparison are reported in Table 18 by averaging the results of 91 pairs of algo-
rithms. According to this table, by comparing MAUC and MMCC, in nearly 8% of the
cases, none of the metrics can distinguish between the classification algorithms. Fur-
thermore, by comparing MAUC and G-mean, the average DoC is equal to 0.0853, so,
in 8% of the cases, both MAUC and G-mean cannot tell the difference between clas-

sification algorithms. This result is nearly true for the comparison between MMCC and
Tanha et al. J Big Data (2020) 7:70 Page 40 of 47

G-mean. Also, in this case, in 8% of the comparisons, they both cannot distinguish the
difference between classification algorithms. In all the pairwise comparisons of Dol, the
value of E is approximately 8%.

Experiments on big datasets

According to study [16], a dataset consisting at least 100,000 instances can be considered
as a big dataset. Therefore, 4 big multi-class imbalanced datasets, as shown in Table 2,
were used to investigate the performance of the boosting algorithms [17, 18]. The
obtained results for the used three performance metrics on these datasets are presented
in Table 19.

Based on the previous experiments that were determined MMCC is the best evalu-
ation metric, we performed Friedman test on the obtained results of this metric for
ranking the algorithms. Figure 5 shows the outcome of this statistical test. It is obvious
that the LogitBoost algorithm performs better than the other algorithms on big data-
sets in overall. However, in particular big datasets with high imbalanced ratio (FARS
and Poker), the LogitBoost could not present good results. For instance, according to
Table 19, AdaC3 performs well in FARS dataset based on the MMCC metric. Moreo-
ver, all of the boosting methods perform worse than random prediction in Poker dataset
and most of them present very low performance on FARS dataset. By considering the
obtained values for G-mean metric, we can conclude that all of the algorithms could not

learn one or some of the classes.

Result discussion

If we rank the algorithms based on all performance metrics, then CatBoost is the first
due to outperforming more algorithms than the others. The second best algorithm is
SMOTEBoost that outperforms 4, 4, and 3 other boosting methods in terms of MAUC,
MMCC, and G-mean, respectively. This observation indicates that oversampling is an
effective method in imbalanced data classification, while we can see RUSBoost, which
uses under-sampling method, could not outperform any other algorithm.

Considering all evaluation metrics, XGBoost and LogitBoost outperform 7 and 6
algorithms, respectively. Thus, XGBoost can be considered as the third best algorithm.
It should be noted that it is possible to get better results from XGBoost by finding the
optimal values of its parameters, but it is not an easy task and having many parameters
increases the complexity of this issue.

One of the interesting observations is that no binary boosting algorithm with decom-
position mechanism is not better than the others, except AdaCost which is better than
AdaC1 in terms of MAUC. Thus, multi-class boosting algorithms can deal with imbal-
anced data problem in a more satisfactory way due to using a proper margin loss func-
tion for multi-class problems. However, from Tables 3, 7 and 11, it can be seen that there
are some datasets in which binary boosting methods show the best results.

In addition, according to the obtained ranks from the Freidman test, the algorithm
with lower rank (CatBoost) was selected as a control algorithm for post-hoc test,
but it failed to reject the null hypothesis, especially in the results for MMCC and
G-means metrics. Therefore, there are some cases that the Holm post-hoc test fails
Page 41 of 47

(2020) 7:70

Tanha et al. J Big Data

 

 

9LZOOF CILOOF SELOOF I1vOOOF LSIOO0F SHOOOF 8L000F L10~@O0OF €7000F 9LLO0OF  SOOOOF ~@rOOOF ELO00F vL000F
77900 = L800» LSE600~—s ZL ZZ'0 £6570 ZLLLO 65910  S8S70 OSEL‘O 89110 SOLL0 9ZETO LSS10 b7S0'0 oNVW
190d
bySOOF 600004 O0000F LZ000F vSl00F CELOOF 68000 €l000F O0Z000F T8000F = pCOOOF_Ss ELOD'OF 9SLO'0F 8100 0F
99900  €€0S0 00000  1SS50 8180 ELESO ZE~LO ~—- 96Z'00-—Ss BENIN €00Z'0 76580 £9690 G88e0 6£89°0 UeSW-5)
O<000F OLOOOF SEOD0F EZOOOF IS100F Lrz00F 6SLO0F 7E0O00F ZLOOOF LILOOF E€€000F YvOO00F ZS000F 7LOO0F
OVLZ7O S667O rELO-— S6EEO €6SV0  SEElLO- /SL00 £€96€0 L600 bL870 68150 9L£@7O LSELO- 9707'0 WW
OZO00F 90000 O00D00F VIOOOF OZLOOF CZLLIOOF LOLOOF 7~OO0OF 80000F 78660 7ZO00 §=99Z000F EC000F 00000
6Z690 84690 10€70 9Y9LPZ0 98820 ZOOV'0 LELvyO  CZLLO TELS €00Z'0 L9E80 7990 S800 E£S90 oNVW
adAoy
CCCOOF PCLOOF LEVOOF v6Z00F O0000F SLED00F 76COOF 85000 000007 6Z100F LOLOOF 00000 SSO00F 95100
p9S00 v6s90 179€0 LE9TO 0000°0 7Z0S°0 96980  S€S6'0 0000°0 67900 87260 00000 L1v6'0 86£6'0 UesW-5
87VO0F O6000F 6OLD0F I8L00F I96hK00F E7POOF 89700F 99000F 601007 TLEOOF LOLOOF O8lLO0F PZLOOF 00000
0s0v0 §©=6:L0@90-—ssLvHO~—- 890 LZL00 COLE O 7ZLSO ~—- EZ88'0 07120 9€70'0 61160 90180 OZE8'0 E7S8'0 WW
8900 ~rOO0F OSZOOF 69100F PL700F v6Z00F CLLOOF 9E000F 80007 8IZ@OOF CLLOOF SSLOOF 60L00F 8S100F
81990 Ory60 €SS80 O8€8°0 LOo9v'0 pZ8L0 S8r60 8960 GSZ8°0 LSO€0 ZE960  €€E60 £9760 S760 oNVW
66,dn> day
6SlLO0F 625004 00000 O00Z00F O00000F l7r00F 00000 00000 00000F CLOOOF 00000 00000 00000F 00000
80900 S!Zl!0 O00000  €1010 0000°0 89€0'0 00000  0000°0 0000°0 L€00'0 00000  0000°0 0000°0 0000°0 UesW-5)
EvcOOF CEOOOF FLIZOO OLOOOF 99100F 7OZ00F LOCOOF veod0F LE000F lopvOOF =69CSOD0F = EVOOOF ~PELOOF LC00'0F
@8vpLO-— =1ZLE0 §=6S67K0—Ss LOLZO p80l0  g8Szlo- 08s00 82070 p8El0 SLEEO— 79610 ZZLL0 £060°0 E0810 WW
O€LOF €6000F FSELOO0 ZIIOOF 68000F  S6l100F ESLOOF 62000F 62000F 8~7COF 6€000F O£000F 6721007 7COO'O +
9060 06190 LZEEO 0L650 SSLV0 8020 OVSV0 S50 80150 ZLLZO 87vS0  O€ES0 bSLv0 78ES0 oNVW
SuV4
ysopepy epepy 7>epy LdePY WaDIYHI] JsoogsnY ISOOGFLOWS 3OOGAW 3OOgDX JsOOgUIPeID 3ys00gHO7 ysOOg}eD FWWYVS HIN'}soogepy
wywobly yosejeqg

 

sjasejep Big paduejequil ssej>-1jNW UO sWYIObye Huljsoog Huluuns WoOdy S14}9W UOIZEN]eAS 10} $}]NSa4 paule}qo SUL 6L 2IGeL
Page 42 of 47

(2020) 7:70

Tanha et al. J Big Data

yasejep Yea JOJ D1}! U UMOYSs S| BDUeWOJad Jsaq aYyL

 

 

 

WW
87v00- ~=—1Z970 ~S8Z0O- ~— SEL SLZ00 = 8E90'0- 09910  600¢0 y760°0 07600 — S60E0 98770 yy0l'0 LS61°0 Jano Belany
00000 Ov00'0#F OO0000#F CO00DD0#F 00000 00000 00000 00000 00000F 00000F  00000F O00000F 00000F 0000°0 =
00000 g1000 00000 00000 0000°0 0000°0 00000  0000°0 0000°0 0000°0 00000  0000°0 0000°0 00000 UedUU-5)
SSLOOF 86000F 7@ZZ00F 68000F 1rZOOF 980004 QLOO0#F Z9ILOOF 78000F €Z000F 9ZL00F 8E000F OFLOOF S00 0F
StyvO— P87ZO—- 69S80—- O06ZE0- O0-SL7O ZSEE0- l787O-— —62870  66550- 9SSEO- 688E0- LLOSO- 969E0- SvS7'0— WW
yopepy Eepepy 7ZePy LdePY WaEdIYbI] IsoCOgsNY 3sOOg_IIOWS 3OOg_W 3OOgGDX JsOOg}UAaIpeIy ysoog}HO7] jsoog}eyD JAWWYVS HIN'}soOgepy
wywobly yosejeqg

 

(panunuo>) 6L aqeL
Tanha et al. J Big Data (2020) 7:70 Page 43 of 47

r

 

Average Rank
kL DH o

NO

  

l
2

LightGBM
AdaC
AdaC
AdaC

AdaCost

r > - r r r , 7 ) C

aC3

SAMME
CatBoost
XGBoost
MEBoost

LogitBoost

RUSBoost

AdaBoost.MH
GradientBoost
MOTEBoost

S

 

 

Fig.5 Average ranks obtained from Friedman test based on the MMCC
LL

in detecting differences between algorithms and it is necessary to use pairwise sta-
tistical tests.

The average computational times of algorithms on all datasets indicate that the
LightGBM which belongs to the binary boosting set is faster than the other algo-
rithms, although it should be run for each pair of classes. The reason for this obser-
vation is that binary methods show the lower computational time in large datasets.
However, among the multi-class boosting methods, XGBoost has the lowest com-
putation time, which is due to designing for speed and performance. Eventually,
SMOTEBoost has the highest computation time where in large datasets, it takes lots
of hours. This is due to generating extra samples for minority classes.

Finally, by comparing the metrics through a comparison framework, it can be seen
that MMCC is the most suitable metric among the used evaluation metrics in the
classification task with multi-class imbalanced datasets. The conducted test shows
that although all 3 metrics are consistent with each other and have the same degree
of indifferency, MAUC is more discriminant than G-mean and MMCC is more dis-
criminant than both MAUC and G-mean. The better discriminacy of MMCC can be
explained by its computational formula. MMCC contributes all elements of confu-
sion matrix in its formula, but G-mean is calculated by using just the recall values of
classes in its formula and MAUC is computed by the area of the ROC curve, which
plots the true-positive rate against false-positive rate of a classifier.

The obtained results for big datasets are different. In overall, although the Logit-
Boost algorithm performs well, but no algorithm can present better performance in
highly imbalanced big datasets. By presenting zero or very small values for G-mean
metric, it can be observed that these boosting methods cannot learn some classes in
this kind of datasets. Therefore, the boosting algorithms studied in this work are not
suitable for multi-class imbalanced big datasets with higher imbalanced ratio.

Conclusion

In this paper, the most significant boosting ensemble algorithms to deal with multi-class
imbalanced data have been reviewed. Many real-life problems are naturally multi-class
imbalanced and in recent years, this issue has been handled by using ensemble learning
Tanha et al. J Big Data (2020) 7:70 Page 44 of 47

techniques. However, there was a lack of study in the literature to investigate and com-
pare the performance of boosting algorithms as a class of ensemble learning methods on
this type of datasets.

According to the analysis, 14 binary and multi-class boosting algorithms investigated
on 19 multi-class conventional and big datasets with various imbalanced ratios. By
comparing multiple algorithms on multiple datasets, it is observed that CatBoost algo-
rithm outperforms most of the other boosting methods in terms of MAUC, G-mean,
and MMCC on 15 conventional datasets. SMOTEBoost can be considered as a second
best algorithm that outperforms 4, 4, and 3 algorithms in terms of MAUC, MMCC and
G-mean metrics, respectively in conventional datasets. Therefore, it can be concluded
that oversampling is an effective approach for tackling the imbalanced data problem.
Both XGBoost and LogitBoost algorithms can be expressed as a third algorithm that
could perform other algorithms. One of the interesting observations was that just multi-
class algorithms could outperform others in which the training process took place with-
out using class decomposition.

The computational time was performed as a part of this study to show which algo-
rithm is faster. The results indicate that binary algorithms present low computational
time although they should run for each pair of classes.

The used evaluation metrics were compared together based on the results of algo-
rithms in conventional datasets and it was observed that MMCC is the most suitable
metric in the multi-class imbalanced data domain. However, G-means metric gives this
useful information that the learner could not classify the samples of one or more of the
classes if it gets the zero value.

Finally, the results for big datasets were different. It was observed that LogitBoost
algorithm performs better than the other algorithms in overall. But generally, boosting
methods cannot present good prediction performance in multi-class big datasets with
high imbalanced ratio. Therefore, one open research issue is how to modify the studied
boosting methods so that they can perform well on highly imbalanced big datasets.

The other potential future works of this study involve the following: (1) An investi-
gation of the effect of the preprocessing operators on the performance of the boosting
algorithms. In this study, no preprocessing operator such as normalization and stand-
ardization was applied to the data, but it is most likely to obtain better results with
experiences by using this preprocessing operation. (2) The study of parameter values
and determining under which values a boosting algorithm, XGBoost for example, could
improve the performance of multi-class imbalanced data. (3) In this study, decision tree
was employed as a base learner under the boosting procedure, but any other learning
model can be used. Hence, the performance of boosting approaches with other base
learners can be investigated on multi-class imbalanced datasets. (4) Comparing the per-
formance of binary boosting algorithms in multi-class data using other decomposition
ways such as one-versus-all and all-versus-all to decompose the multi-class problem into
multiple binary decision problems. (5) Comparing the other evaluation metrics which
are suitable for the multi-class imbalanced domain, like Kappa, Weighted Accuracy, and

other measures.
Tanha et al. J Big Data (2020) 7:70 Page 45 of 47

Abbreviations

AUC: area under the curve; MAUC: multi-class area under the curve; ROC: receiver operating characteristic; MCC: Mat-
thews correlation coefficient; MMCC: multi-class Matthews correlation coefficient; SMOTE: synthetic minority over-sam-
pling technique; OVA: one-versus-all; OVO: one-versus-one; TP: true positive; FP: false positive; TN: true negative; FN: false
negative; TPR: true positive rate; FPR: false positive rate; DoC: degree of consistency; Dol: degree of indifferency.

Acknowledgements
Not applicable.

Authors’ contributions

YA: conceptualization, methodology, software, writing, designing the experiment. NS: software, writing, designing the
experiment. NR: writing, designing the experiment under the supervision of JT and MA as academic supervisors. All
authors read and approved the final manuscript.

Funding
Not applicable.

Availability of data and materials
The datasets used during the current study are available https://archive.ics.uci.edu/ml/index.php.

Competing interests
The authors declare that they have no competing interests.

 

Received: 27 May 2020 Accepted: 14 August 2020
Published online: 01 September 2020

References

1. Japkowicz N. Learning from imbalanced data sets: a comparison of various strategies. In: AAAI workshop on learn-
ing from imbalanced data sets, Vol. 68. 2000. p. 10-5.

2. Batista GE, Prati RC, Monard MC. A study of the behavior of several methods for balancing machine learning training
data. ACM SIGKDD Explorations Newsl. 2004;6(1):20-9.

3. Shatnawi R. Improving software fault-prediction for imbalanced data. In: 2012 international conference on innova-
tions in information technology (IIT); 2012. p. 54-9.

4. Di Martino M, Decia F, Molinelli J, Fernandez A. Improving electric fraud detection using class imbalance strategies.
In: ICPRAM; 2012. p. 135-41.

5. Majid A, Ali S, Iqbal M, Kausar N. Prediction of human breast and colon cancers from imbalanced data using nearest
neighbor and support vector machines. Comput Methods Programs Biomed. 2014;113(3):792-808.

6. Liu Y, Loh HT, Sun A. Imbalanced text classification: a term weighting approach. Expert Syst Appl.
2009;36(1):690-701.

7. Kubat M, Holte RC, Matwin S. Machine learning for the detection of oil spills in satellite radar images. Mach Learn.
1998;30(2-3):195-21515.

8. Su P Mao W, Zeng D, Li X,Wang FY. Handling class imbalance problem in cultural modeling. In: 2009 IEEE interna-
tional conference on intelligence and security informatics; 2009. p. 251-6.

9. Abdi Y, Parsa S, Seyfari ¥. A hybrid one-class rule learning approach based on swarm intelligence for software fault
prediction. Innovations Syst Softw Eng. 2015;11(4):289-301.

10. Ganganwar V. An overview of classification algorithms for imbalanced datasets. Int J Emerg Technol Adv Eng.
201 2;2(4):42-7.

11. Kotsiantis S, Kanellopoulos D, Pintelas P Handling imbalanced datasets: a review. GESTS Int Trans Computer Sci Eng.
2006;30(1):25-36.

12. Ferreira AJ, Figueiredo MA. Boosting algorithms: a review of methods, theory, and applications. In: Ensemble
machine learning. Boston: Springer; 2012. p. 35-85.

13. Wang S, Yao X. Multiclass imbalance problems: analysis and potential solutions. IEEE Trans Syst Man Cybern.
201 2;42(4):1119-30.

14. BiJ, Zhang C. An empirical comparison on state-of-the-art multi-class imbalance learning algorithms and a new
diversified ensemble learning scheme. Knowl-Based Syst. 2018;15(158):81-93.

15. Wuk, Zheng Z, Tang S. BVDT: A boosted vector decision tree algorithm for multi-class classification problems. Int J
Pattern Recognit Artif Intell. 2017;31(05):1750016.

16. Leevy JL, Knoshgoftaar TM, Bauder RA, Seliya N. A survey on addressing high-class imbalance in big data. J Big Data.
2018;5(1):42.

17. Abu-Salih B, Chan ky, Al-Kadi O, Al-Tawil M, Wongthongtham P Issa T, Saadeh H, Al-Hassan M, Bremie B, Albahlal A.
Time-aware domain-based social influence prediction. J Big Data. 2020;7(1):10.

18. Sleeman IV WC, Krawczyk B. Bagging Using Instance-Level Difficulty for Multi-Class Imbalanced Big Data Classifica-
tion on Spark. In2019 IEEE International Conference on Big Data (Big Data) 2019 (pp. 2484-2493). IEEE.

19. Sun Y, Kamel MS, Wang Y. Boosting for learning multiple classes with imbalanced class distribution. In: Sixth interna-
tional conference on data mining (ICDM’06); 2006. p. 592-602.

20. ZhenL, Qiong L. A new feature selection method for internet traffic classification using ml. Phys Procedia.
201 2;1(33):1338-455.

21. Ling CX, Huang J, Zhang H. AUC: a statistically consistent and more discriminating measure than accuracy. ljcai.
2003;3:519-24.
Tanha et al. J Big Data

22.
23.
24.
25,
26.
2/.
28.
29,

30.

31.
32.

33.
34.
35.
36.
37.
38.
39.

40.

 

51.

52.

53.

54,

55,

56.

57.
58.

59.
60.

(2020) 7:70 Page 46 of 47

Huang J, Ling CX. Using AUC and accuracy in evaluating learning algorithms. IEEE Trans Knowl Data Eng.

2005;1 7(3):299-310.

Singh A, Purohit A. A survey on methods for solving data imbalance problem for classification. Int J Computer Appl.
2015;127(15):37-41.

FernaNdez A, LoPezV, Galar M, Del Jesus MJ, Herrera F. Analysing the classification of imbalanced data-sets with
multiple classes: Binarization techniques and ad-hoc approaches. Knowl-Based Syst. 2013;1(42):97-110.

Krawczyk B. Learning from imbalanced data: open challenges and future directions. Prog Artif Intell.

2016;5(4):22 1-32.

Tahir MA, Asghar S, Manzoor A, Noor MA. A classification model for class imbalance dataset using genetic program-
ming. IEEE Access. 2019;8(7):71013-377.

Ramentol E, Caballero Y, Bello R, Herrera F. SMOTE-RSB*: a hybrid preprocessing approach based on oversam-

pling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory. Knowl Inf Syst.

201 2;33(2):245-65.

Liu A, Ghosh J, Martin CE. Generative oversampling for mining imbalanced datasets. In: DMIN; 2007. p. 66-72.
Kumari C, Abulaish M, Subbarao N. Using SMOTE to deal with class-imbalance problem in bioactivity data to predict
mTOR inhibitors. In: Proceedings of the international conference on adaptive computational intelligence (ICACI),
Mysuru, India; 2019. p. 1-12.

Colton D, Hofmann M. Sampling techniques to overcome class imbalance in a cyberbullying context. J Computer-
Assist Linguistic Res. 2019;3(3):21-40.

Esteves VM. Techniques to deal with imbalanced data in multi-class problems: a review of existing methods.

Ling CX, Sheng VS. Cost-sensitive learning and the class imbalance problem. Encyclopedia Mach Learn.
2008;2011:231-5.

Maheshwari S, Agrawal J, Sharma S. New approach for classification of highly imbalanced datasets using evolution-
ary algorithms. Int J Sci Eng Res. 2011;2(7):1-5.

Btaszczynski J, Stefanowski J. Neighbourhood sampling in bagging for imbalanced data. Neurocomputing.
2015;20(150):529-42.

Rokach L. Ensemble-based classifiers. Artif Intell Rev. 2010;33(1-2):1-39.

Schapire RE. A brief introduction to boosting. ljcai. 1999;99:1401-6.

Breiman L. Bagging predictors. Mach Learn. 1996;24(2):123-40.

Galar M, Fernandez A, Barrenechea E, Bustince H, Herrera F. A review on ensembles for the class imbalance problem:
bagging-, boosting-, and hybrid-based approaches. IEEE Trans Syst Man Cybern. 201 1;42(4):463-84.

Zhang Z, Krawczyk B, Garcia S, Rosales-Pérez A, Herrera F. Empowering one-vs-one decomposition with ensemble
learning for multi-class imbalanced data. Knowl-Based Syst. 2016;15(106):251-63.

Krawczyk B. Combining one-vs-one decomposition and ensemble learning for multi-class imbalanced data. In:
Proceedings of the 9th international conference on computer recognition systems CORES 2015. Cham: Springer;
2016. p. 27-36.

Feng W, Huang W, Ren J. Class imbalance ensemble learning based on the margin theory. Appl Sci. 2018;8(5):815.
Schapire RE, Singer Y. BoosTexter: A boosting-based system for text categorization. Mach Learn.
2000;39(2-3):135-68.

Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an application to boosting. In:
European conference on computational learning theory. Heidelberg: Springer; 1995. p. 23-37.

Hastie T, Rosset S, Zhu J, Zou H. Multi-class adaboost. Stat Interface. 2009;2(3):349-60.

Friedman J, Hastie T, Tibshirani R. Additive logistic regression: a statistical view of boosting (with discussion and a
rejoinder by the authors). Ann Stat. 2000;28(2):337-407.

Sun P, Reid MD, Zhou J. An improved multiclass LogitBoost using adaptive-one-vs-one. Mach Learn.
2014;97(3):295-32626.

Li PR Abc-logitboost for multi-class classification. arXiv preprint: arXiv:0908.4144. 2009.

Sun P. Reid MD, Zhou J. Aoso-logitboost: Adaptive one-vs-one logitboost for multi-class problem. arXiv preprint:
arXiv:1110.3907. 2011.

Friedman JH. Greedy function approximation: a gradient boosting machine. Ann Stat. 2001;1:1189-232.,
Prokhorenkova L, Gusev G, Vorobev A, Dorogush AV, Gulin A. CatBoost: unbiased boosting with categorical features.
In: Advances in neural information processing systems. 2018. p. 6638-48.

Chen T, Guestrin C. Xgboost: A scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining; 2016. p. 785-94.

Ke G, Meng Q, Finley T, Wang T, Chen W, Ma W, Ye Q, Liu TY. Lightgbm: A highly efficient gradient boosting decision
tree. In: Advances in neural information processing systems; 2017. p. 3146-54.

Chawla NV, Lazarevic A, Hall LO, Bowyer KW. SMOTEBoost: Improving prediction of the minority class in boosting. In:
European conference on principles of data mining and knowledge discovery. Springer: Berlin; 2003. p. 107-19
Seiffert C, Khoshgoftaar TM, Van Hulse J, Napolitano A. RUSBoost: A hybrid approach to alleviating class imbalance.
IEEE Trans Syst Man Cybern Syst Hum. 2009;40(1):185-97.

Rayhan F, Ahmed S, Mahbub A, Jani MR, Shatabda S, Farid DM, Rahman CM. MEBoost: mixing estimators with boost-
ing for imbalanced data classification. In: 2017 11th international conference on software, knowledge, information
management and applications (SKIMA); 2017. p. 1-6.

Sun Y, Kamel MS, Wong AK, Wang Y. Cost-sensitive boosting for classification of imbalanced data. Pattern Recogn.
2007;40(1 2):3358-78.

Fan W, Stolfo SJ, Zhang J, Chan PK. AdaCost: misclassification cost-sensitive boosting. Icml. 1999;99:97-105.

Ting KM. A comparative study of cost-sensitive boosting algorithms. In: Proceedings of the 17th international con-
ference on machine learning. 2000.

Domingo C, Watanabe O. MadaBoost: A modification of AdaBoost. In: COLT; 2000. p. 180-9.

Joshi MV, Agarwal RC, Kumar V. Predicting rare classes: can boosting make any weak learner strong? In: Proceedings
of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining; 2002. p. 297-306.
Tanha et al. J Big Data

(2020) 7:70 Page 47 of 47

61. Joshi MV, Kumar V, Agarwal RC. Evaluating boosting algorithms to classify rare classes: Comparison and improve-
ments. In: Proceedings 2001 IEEE international conference on data mining; 2001. p. 257-64.

62. Vezhnevets A, Vezhnevets V. Modest AdaBoost-teaching AdaBoost to generalize better. Graphicon.

2005;1 2(5):987-97.

63. Mease D, Wyner A, Buja A. Cost-weighted boosting with jittering and over/under-sampling: Jous-boost. J Mach
Learn Res. 2007;8:409-39.

64. Jin X, Hou X, Liu CL. Multi-class AdaBoost with hypothesis margin. In: 2010 20th international conference on pattern
recognition. 2010. p. 65-8.

65. Chen S, He H, Garcia EA. RAMOBoost: ranked minority oversampling in boosting. IEEE Trans Neural Netw.
2010;21(10):1624-42.

66. Saberian MJ, Vasconcelos N. Multiclass boosting: theory and algorithms. In: Advances in neural information process-
ing systems; 2011. p. 2124-32.

67. Galar M, Fernandez A, Barrenechea E, Herrera F. EUSBoost: enhancing ensembles for highly imbalanced data-sets by
evolutionary undersampling. Pattern Recogn. 2013;46(12):3460-71.

68. Diez-Pastor JF, Rodriguez JJ, Garcia-Osorio C, Kuncheva LI. Random balance: ensembles of variable priors classifiers
for imbalanced data. Knowl-Based Syst. 2015;1(85):96-111.

69. Ahmed S, Rayhan F, Mahbub A, Jani MR, Shatabda S, Farid DM. LIUBoost: locality informed under-boosting for imbal-
anced data classification. In: Emerging technologies in data mining and information security. Singapore: Springer;
2019. p. 133-44.

70. Kumar S, Biswas SK, Devi D. TLUSBoost algorithm: a boosting solution for class imbalance problem. Soft Comput.
2019;23(21):10755-67.

71. Deng X, Liu Q, Deng Y, Mahadevan S. An improved method to construct basic probability assignment based on the
confusion matrix for classification problem. Inf Sci. 2016;1(340):250-61.

72. Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in
binary classification evaluation. BMC Genomics. 2020;21(1):6.

73. Saito T, Rehmsmeier M. The precision-recall plot is more informative than the ROC plot when evaluating binary clas-
sifiers on imbalanced datasets. PLoS ONE. 2015;10:3.

74. Halimu C, Kasem A, Newaz SS. Empirical Comparison of Area under ROC curve (AUC) and Mathew Correlation
Coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In:
Proceedings of the 3rd international conference on machine learning and soft computing; 2019. p. 1-6.

75. Rahman MS, Rahman MK, Kaykobad M, Rahman MS. isGPT: An optimized model to identify sub-Golgi protein types
using SVM and Random Forest based feature selection. Artif Intell Med. 2018;1(84):90-100.

76. Jurman G, Riccadonna §S, Furlanello C. A comparison of MCC and CEN error measures in multi-class prediction. PLoS
ONE. 2012;7:8.

77. Zhang ZL, Luo XG, Garcia S, Tang JF, Herrera F. Exploring the effectiveness of dynamic ensemble selection in the
one-versus-one scheme. Knowl-Based Syst. 2017;1(125):53-63.

78. Singh PK, Sarkar R, Nasipuri M. Significance of non-parametric statistical tests for comparison of classifiers over
multiple datasets. Int J Comput Sci Math. 2016;7(5):410-42.

79. Demiésar J. Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res. 2006;7:1-30.

80. Wilcoxon F, Katti SK, Wilcox RA. Critical values and probability levels for the Wilcoxon rank sum test and the Wilcoxon
signed rank test. Selected Tables Math Stat. 1970;1:171-259.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen”®

journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

 

Submit your next manuscript at > springeropen.com

 

 

 
