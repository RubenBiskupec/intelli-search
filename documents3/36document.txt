Araka et al. Research and Practice in Technology Enhanced Learning Research and Practice in
(2020) 15:6 .
https://doi.org/10.1186/s41039-020-00129-5 Technology Enhanced Learning

REVIEW Oy else =e

Research trends in measurement and ®
intervention tools for self-regulated learning ~~
for e-learning environments—systematic

review (2008-2018)

Eric Araka' , Elizaphan Maina’, Rhoda Gitonga’ and Robert Oboko*

 

 

* Correspondence: araka.eric@ku.ac.ke
"Kenyatta University, P.O. Box 43844,

Nairobi 00100, Kenya ; ; ;
Full list of author information is For the last one decade, research in self-regulated learning (SRL) and educational

available at the end of the article psychology has proliferated. Researchers and educators have focused on how to
support leaners grow their SRL skills on both face-to-face and e-learning
environments. In addition, recent studies and meta-analysis have greatly contributed
to the domain knowledge on the use of SRL strategies and how they contribute and
boost academic performance for learners. However, there is little systematic review
on the literature on the techniques and tools used to measure SRL on e-learning
platforms. This review sought to outline recent advances and the trends in this area
to make it more efficient for researchers to establish the empirical studies and
research patterns among different studies in the field of SRL. The findings from this
study are concurrent with existing empirical evidence that traditional methods
designed for classroom supports are being used for measuring SRL on e-learning
environments. Few studies have used learner analytics and educational data mining
(EDM) techniques to measure and promote SRL strategies for learners. The paper
finally points out the existing gaps with the tools presently used to measure and
support SRL on learning management systems and recommends further studies on
the areas of EDM which can support SRL.

Abstract

Keywords: Self-regulated learning, Learning management systems, Measuring,
Promoting, Educational data mining, Learner analytics

 

Introduction

Educational environments fall along a continuum from physical classroom where face-
to-face interactions are common to fully online learning environments where asyn-
chronous interactions are the default. This continuum of educational environments
has provided opportunities for blended and web facilitated courses where learning ma-
terials and student-instructor interactions are delivered online with little or no face-to-

face meetings (Allen & Seaman, 2013).

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which
GQ) Springer Open permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the
— original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or
other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit
line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by
statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a
copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 2 of 21

Presently, there is a recognized shift towards technology supported learning com-
monly known as e-learning with most of institutions of higher learning adopting e-
learning for fully online courses or complementary to the face-to-face sessions in
blended learning approach in order to curb the challenge of large backlog of students
to be admitted (Hadullo, Oboko, & Omwenga, 2018; Luna, Castro, & Romero, 2017).
As a result, there is increased number of students undertaking online learning courses
(Bogarin, Cerezo, & Romero, 2018; Broadbent & Poon, 2015). The term “online learn-
ing” or as commonly known as e-learning refers to the web-based systems such as
massive open online courses (MOOCs) and learning management systems (LMS) that
are used by instructors to deliver learning materials and allow students to access the
content and interact and obtain support during a learning episode (Delen & Liew,
2016). MOOCs are defined as open education systems for open and distance learning
where students register for courses with limited admission restrictions such as pre-
requisite courses and selection criteria.

Despite the benefits to online learning, existing literature indicates challenges that
need to be addressed. First is offering adequate support and guidance to learners
undertaking online learning (Kizilcec, Pérez-Sanagustin, & Maldonado, 2017; Terras &
Ramsay, 2015). Offering individualized support and guidance may not easily be
achieved because of large number of students enrolling on e-learning. The increased
number of students taking online courses is likely to face the challenge of having
enough human capacity to offer adequate support. To provide effective support and
guidance to online students, we need to tap into the potential opportunities offered by
educational data mining (EDM) and learner analytics (LA) tools. EDM is described as
the approach of applying data mining algorithms on datasets generated from educa-
tional environments in order to understand learners and learning environments. The
datasets which is in form of logs generated when learners engage to various online
learning activities can be analyzed to produce inferences that can be used as indicators
to provide interventions that reduce dropout rates and increase retention rates, profile
learners, develop learner models, and recommender systems (Arnold & Pistilli, 2012;
Romero, Lopez, Luna, & Ventura, 2013; Romero & Ventura, 2007). LA on the other
hand involves integration and analysis of data collected from educational environments
for insights and patterns on how students engage on various learning activities during
online learning. The main objective is to support students by providing interventions to
improve on undesirable learning behaviors and reinforce positive learning (Lodge,
Panadero, Broadbent, & De Barba, 2019). Lodge and Corrin (2017) opine that LA pro-
vide opportunity for monitoring students’ learning in order to understand their behav-
ioral patterns and provide real-time interventions especially in online learning
envrionments. According to Naif, Ayman, & Saeed-ul (2019), the outcome from LA
helps in understanding the behaviour of learners with a view of providing early inter-
vention mechanisms that enhances learning engagement which has been found to be
positively correlated to academic performcance. This is likely to lead to reduced stu-
dent dropout and increase the retention rates especially in higher education. In com-
parison, while EDM is concerned about techniques that can be used to explore data
from educational environments and using the techniques to understand learners and
learning environments, focusing on automated discovery of information, LA is more

about analyzing and reporting of insights hidden in the data about learners and
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 3 of 21

learning environments, focusing on insights to “inform and empower instructors and
students” (Siemens & Baker, 2012).

Compared to physical classroom teaching where learners are confined together at
certain periods, online learners are not restricted in managing their own schedules and
learning process—what time to study and how long to engage in learning. The success
of e-learning depends on the learner’s ability to take control of their own learning
process (Nikolaki, Koutsouba, Lykesas, Venetsanou, & Savidou, 2017). The theory
through which learners take control of the learning process is referred to as “self-regu-
lated learning (SRL)”. Self-regulated learners are those who have the ability to take
charge in managing their own learning while assuming an active role in achieving their
academic goals (Zimmerman, 1990).

SRL is grounded on different theoretical models that provide frameworks on which re-
search studies on SRL are carried out. According to Carlos Nufiez, Romera, Magno, and
Panadero (2017), the popular and commonly referred models include Zimmerman’s,
Boekaerts’,, Winne and Hadwin’s, Pintrich’s, Efklides’, and Hadwin, Jarvela, and Miller’s
models. Each of these models describes phases, processes, and components that can be
summed up into SRL strategies that are measured in a learning process. The strategies in-
clude time management, metacognition, effort regulation, critical thinking, rehearsal, elab-
oration, organization, peer-to-peer learning, and help seeking. Leaners who employ some
or all of the identified strategies perform better than those with low level SRL skills and
hence the need for supporting SRL on e-learning environments especially LMS which are
majorly used by higher institutions of learning (Broadbent & Poon, 2015; Kizilcec et al.,
2017; Littlejohn, Hood, Milligan, & Mustain, 2016). These strategies can be measured be-
fore, during, or after a learning process using instruments and methods specially designed
for each of the SRL model.

According to Panadero, Klug, and Jarvela (2016), measurement of SRL is believed to
have undergone through what is described as “three waves.” These waves are identified
as first wave where self-regulation was conceptualized in terms of traits or characteris-
tics that are inherent in learners and therefore measured using self-report tools such as
interviews and questionnaires. In the second wave, SRL is viewed as events or processes
that take place within a learner while being influenced by external environment
through which learning takes place. The online SRL methods, which are used to meas-
ure SRL in the second wave, allow measurement without the learner being aware. This
is achieved through the use of log data collected when learners are interacting with
learning environments, instructors, and fellow students. The third wave is perceived as
the “current wave” where SRL measurement approaches also serve as tools to promote
or reinforce the self-regulatory skills in learners. Since the interest of this study was to
outline the SRL measurement advances and trends, it is worthwhile to review the
current studies in relation to the “three waves.” More importantly, this will be helpful
to SRL researchers and educators to know the direction of these forms of SRL

measurements.

Previous review studies
Recent studies and meta-analysis have greatly contributed to the domain knowledge
that the use of SRL strategies boosts academic performance for learners. In this
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 4 of 21

subsection, we highlight some of the recent systematic reviews that have been carried
out and are related to the current study.

In their study, Wong et al. (2018) carried out a systematic review on the studies that
have been carried out to support SRL in MOOCs and other online learning environ-
ment and investigated the role human factors play in SRL. The paper review focused
on various levels of education from primary, secondary, and higher education and
working adults.

In their review Lee, Watson, and Watson (2019) highlights the SRL strategies and in-
terventions that have been employed from various studies with main focus on MOOCs.
The SRL strategies reviewed in the study include self-efficacy and face value, goal set-
ting, help seeking, time management, and effort regulation. Of the studies reviewed, only
four employed some kind of interventions for promoting SRL, and these include soft-
ware programs integrated in MOOCs, automated feedbacks, and prompts.

Garcia and his colleagues presents another review paper on e-learning tools and plat-
forms that are used to support SRL strategies especially for computer science students
using a taxonomy developed by Barry Zimmerman and Manuel Martinez Pons (1986).
The study also sought to determine if other strategies have emerged that were not ori-
ginally captured by the taxonomy. The categories being supported by various e-learning
environments include self-evaluation, organizing and transferring, goal setting and
planning, seeking information, keeping records and monitoring, environmental structur-
ing, self-consequences, rehearsal and monitoring, seeking social assistance, and reviewing
records. Of these categories, environmental structuring and seeking social assistance are
not captured in the original taxonomy but have been identified and investigated by
modern technologies employed by other e-learning tools (Garcia, Falkner, & Vivian,
2018).

Roth, Ogrin, and Schmitz (2016) highlight the self-report instruments used to meas-
ure SRL in higher education while capturing psychometric properties and characteris-
tics. The self-report tools identified in the review include structured questionnaires
such as Motivated Strategies for Learning Questionnaire (MSLQ), Learning and Study
Strategies Inventory (LASSI), and Situational Judgment Tests (SJT); and interviews such
as Self-Regulated Learning Interview Schedule (SRLIS), think-aloud protocols, and
learning diaries.

Schraw (2010) reviewed four articles and highlighted the methods for measuring self-
regulated learning in online learning environments. Schraw (2010) presents taxonomy
of the tools and methods for measuring SRL—offline and online measures. Offline tools
and methods are those that measure SRL before or after a learning period. Such
methods include self-reports, current abilities, and expected performance. Online tools
are those that are used during the learning process for example the use of analysis from
data generated from an educational environment. Online techniques are unobtrusive as
the measurements are taken while learners are unaware and therefore do not affect stu-
dents’ engagement behaviors and performance.

These studies have greatly contributed to our understanding that use of SRL skills
positively enhances academic performance for learners (Adam, Alzahri, Cik Soh, Abu
Bakar, & Mohamad Kamal, 2017; Garcia et al., 2018; Lee et al., 2019; Roth et al., 2016;
Wong et al., 2018). Despite these developments in SRL, our understanding on the

measurement tools and instruments to measure SRL for online learning environments
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 5 of 21

is limited as evidenced on the use of traditional tools that were developed for face-to-
face still being used for measuring SRL online. Most of these studies focus on measur-
ing SRL in MOOCs and only analyze SRL strategies with limited attention to the inter-
ventions to improve self-regulated learning. The last review on SRL scaffolds on
computer-based learning environments was published in 2012 (Devolder, van Braak, &
Tondeur, 2012).

In a recent study by Panadero et al. (2016), various measurement and intervention
tools have been developed over the last decade. In what they term as “wave” of evolu-
tion of different SRL measurement tools, they argue that SRL measurement has under-
gone through “three waves” which can also be viewed as stages. This study sought to
establish the status of these “three waves” in reference to the tools developed and be-
longing to each wave of evolution.

In this study, various methods and techniques that have been used to measure and
promote SRL since its emergence in 1990s are reviewed. We take into account the his-
torical context: the tools that have been used in the past and how they have fashioned
the design of the tools used in measuring and promoting SRL presently and in future.

The following research questions guided this study:

RQ1: What methods and instruments are being used to measure SRL on e-learning en-
vironments in higher education?

RQ2: What methods are being used to measure and promote SRL at the same time in
on e-learning environments in higher education?

RQ3: What is the trend in terms of measurements and interventions that are being
used to promote SRL skills for learners on e-learning environments in higher
education?

RQ4: What EDM tools are being used to measure and promote SRL on e-learning en-

vironments in higher education?

Methodology

The review process followed the five-step methodology by Khan, Kunz, Kleijnen, and
Antes (2003) for conducting systematic review which involves (a) framing the questions
for the review, (b) literature identification, (c) assessing the quality of articles, (d) sum-
mary of the studies reviewed, and (e) result interpretation. Based on this methodology,
we first discuss how the literature identification was done, secondly the criteria used to
do quality assessment of the articles, thirdly summary of the studies reviewed, and fi-

nally the discussion of the results.

Identification of literature

Meta-search on papers with focus on SRL measurement tools and interventions on e-
learning environments in higher education such as LMS and MOOCs was carried out.
The keywords used include Self-Regulated Learning AND e-learning AND Learning
Management Systems AND Assessing AND Measuring AND Supporting AND Pro-
moting AND SRL Interventions. The databases incorporated in the search for articles
include ERIC, PsycINFO, PsycARTICLES, Google Scholar, ACM, Research Gate, and
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 6 of 21

IEEE Xplore digital library. A total of 158 articles published between 2008 and 2018
were identified.

Quality assessment and selection criteria

After reviewing the abstracts of the 158 papers, we identified 42 papers that were rele-
vant to measuring and promoting SRL in higher education. Twelve more papers were
removed from the analysis based on the inclusion criteria. The final set of 30 papers
were reviewed, analyzed, and discussed in the “Results” section. After analysis of the
content for each paper, we also identified eight studies that described SRL measure-
ment approaches that also served to promote SRL in online learning environments, and
the summary is presented in Table 2.

Inclusion criteria

The following inclusion criteria were used to obtain the relevant papers for this study:

e Articles that addressed the measurement or promotion of SRL

e Articles that described original work with actual SRL measurement or promotion
tool developed

e Articles whereby SRL measurement or promotion tool was validated through

experiments in institutions of higher learning

Results

This section describes the various tools and instruments used to measure and pro-
mote SRL on the various online learning environments in higher education that
were identified from the review. Out of the 30 studies that were reviewed, 10 stud-
ies were carried out on LMS, 8 studies on MOOC environments, and 12 studies
on other learning environments such personal learning environments (PLEs) while
in four studies, the e-learning environment was not specified as presented in
Table 1. In terms of the type and source of data used for SRL measurement or
intervention, 16 out of the 30 studies used self-report tools such as structured
questionnaires and interviews to measure the level of SRL skills in learners while 9
studies analyzed log data extracted from educational learning environments such as
MOOCs or LMS to establish the levels of SRL for each learner. There were only 5
studies where both self-report tools and log data were used to measure SRL as
presented in Table 1.

RQ1: What methods and instruments are being used to measure SRL on e-learning
environments in higher education?
The following SRL measurement tools and instruments were identified from the litera-

ture reviewed in this study:
a. Online Self-Regulated Learning Questionnaire (OSLQ)

The Online Self-Regulated Learning Questionnaire (OSLQ) is a tool developed by
Barnard et al. (2009) to assess students’ use SRL strategies in online or blended
 

 

analytics

Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 7 of 21
Table 1 Summary of the reviewed studies
No. Article reference SRL Learning Type of — Instrument used to Analysis method
measurement environment data collect data
method collected
1 Alharbi, Henskens, & — Self-report Personalized Log data Learning system logs ©§ ANCOVA
Hannaford, 2014 questionnaire — learning and self and questionnaire
and learning — object report
analytics system
2 Arnold & Pistilli, Learning LMS Log data Learning system logs — Pearson
2012 analytics correlation
coefficient and
chi-square
3. = Azevedo et al., 2009 Learning MetaTutor Log data Learning system logs — Not specified
analytics
4 Barnard, Lan, To, Self-report Not Self- OSLQ Confirmatory
Paton, & Lai, 2009 questionnaire specified report factor analysis
5 Chaves-Barboza, Self-report PLE Self- Questionnaire Pearson
Trujillo-Torres, Anto- questionnaire report correlation
nio Lépez-Nunez, & coefficient and
Sola-Martinez, 2017 ANOVA
6 Chen, 2009 Self-report PLE Self- Questionnaire ANOVA and t
questionnaire report test
7 ~~ Cho & Shen, 2013 Self-report LMS Self- MSLQ Pearson
questionnaire report correlation
coefficient and
chi-square
8 Cho & Cho, 2017 Self-report Not Self- OSROQ Exploratory
questionnaire specified report factor analysis
(EFA)
9 Cicchinelli et al., Self-report LMS Self- Motivational Beliefs Pearson
2018 questionnaire report and Self-Regulation correlation
and learning and log Strategies (MBSRS) and__ coefficient
analytics data learning system logs
10 Davis, Chen, Jivet, Learning MOOC Log data Learning system logs __A/B testing
Hauff, & Houben, analytics
2016
11. Dawson et al., 2015 — Learning MOOC Log data Learning system logs — Not specified
analytics
12 Delen et al, 2014 Self-report MOOC Self- Self-Regulation Pearson
questionnaire report Strategy Inventory correlation
and learning and log (SRS) coefficient
analytics data
13. Gaupp, Fabry, & Self-report LMS Self- Questionnaire Pearson
Korner, 2018 questionnaire report correlation
coefficient
14. Hashemyolia et al, Selreport LMS Self- MSLQ t tests and
2015 questionnaire report standard
deviation
15 Jansen, van Self-report MOOC Self- Self-regulated Online — Exploratory
Leeuwen, Janssen, & questionnaire report Learning factor analysis
Kester, 2017 Questionnaire (SOL-Q) and
confirmatory
factor analysis
16 Kizilcec, Pérez- Self-report MOOC Self- Questionnaire Covariate-
Sanagustin, & questionnaire report adjusted OLS
Maldonado, 2016 regression
analyses
17 Kizilcec et al, 2017. Self-report MOOC Self- Questionnaire Regression
questionnaire report coefficient
18 Lee & Recker, 2017. ~~ Learning LMS Log data Learning system logs = Means
Araka et al. Research and Practice in Technology Enhanced Learning

(2020) 15:6

Table 1 Summary of the reviewed studies (Continued)

Page 8 of 21

 

 

No. Article reference SRL Learning Type of — Instrument used to Analysis method
measurement environment data collect data
method collected
19 Lee, 2008 Self-report LMS Self- Questionnaire Convergent
questionnaire report validity analysis
and correlation
of latent
variables
20 Muller & Seufert, Self-report Not Self- MSLQ and LIST Means, ANOVA,
2018 questionnaire specified report and standard
deviation
21 Nussbaumer, Self-report LMS Log data Learning system logs = Mean, standard
Hillemann, & Albert, questionnaire and self and questionnaire deviation, and
2015 and learning report median
analytics
22  Onah & Sinclair, Self-report MOOC Self- Modified-OSLQ Means
2017 questionnaire report
23 Rodriguez Groba, Learning LMS Log data Learning system logs — Not specified
Vazquez Barreiros, analytics
Lama, Gewerc, &
Mucientes, 2014
24  Siadaty, 2016 Learning MOOC Log data Learning system logs — Regression
analytics analysis
25 Song, Kalet, & Plass,  Selfreport Not Self- Self-Regulation Regression
2011 questionnaire specified report Measure for analysis
Computer-based learn-
ing (SRMC)
26 Winne & Hadwin, Learning Web-based Log data Learning system logs — Not specified
2013 analytics application
27. Yamada etal, 2017 = Self-report Not Log data MSLQ t test, means,
questionnaire specified and self- and standard
and learning report deviation
analytics
28 Yen et al., 2016 Self-report PLE Self- OLSQ t test and
questionnaire report regression
analysis
29 Zarouk & Khaldi, Learning LMS Log data Learning system logs — Not specified
2016 analytics
30 Zhao Li Chen, Zhao,  Self-report Web2.0 Self- Distance learners’ self- ANOVA, t test,
& Chen, 2016 questionnaire technology — report regulated learning abil- and means

ity selfrating scale

 

learning environments. The tool contain 24 items under five categories that are

used to measure six SRL strategies which include goal seeking, help seeking, time

management, task strategies, environment structuring, and self-evaluation. Onah
and Sinclair (2017) also used a modified OSLQ tool to develop another tool used
to measure SRL on massive open online courses systems known as MOSLQ.

Similar to OSLQ is the Online Self-Regulation Questionnaire constructed by Cho

and Cho (2017) to examine how learners interact with content, teachers, and other

learners in an online course. The tool is a 19-item scaled tool that also assesses
the six SRL strategies identified by Barnard et al. (2009). The tool was also used
by Yen et al. (2016) during their study that investigated the impacts of SRL in on-

line learning.

b. Motivated Strategies for Learning Questionnaire (MSLQ)
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 9 of 21

The Motivated Strategies for Learning Questionnaire (MSLQ) has been in existence
since the 1980s. Developed by Pintrich, the MSLQ is an 81-itemized self-report instru-
ment which has been used for a longtime to measure SRL strategies mostly for physical
classroom educational settings. In the studies surveyed in this study, the MSLQ instru-
ment was administered online. The studies carried out by Hashemyolia et al. (2015),
Miller and Seufert (2018), and Yamada et al. (2017) used the MSLQ tool to measure SRL
strategies in online learning which included both the MOOCs and LMS environments.

c. Learner analytics

Learner analytics (LA) is a recent and fast growing field that focuses on the use of
learner data generated from various learning environments. Once log data is collected,
it is analyzed for making inferences that can be used to inform and understand learners’
engagement behavior during online courses. Learning analytics is “the measurement,
collection, analysis and reporting of data about learners and their contexts, for purposes
of understanding and optimizing learning and the environments in which it occurs”
(Long & Siemens, 2011). The log data collected during an online learning episode can
also be analyzed using data mining algorithms to provide inferences on how students
self-regulate and generate visualized SRL reports to teachers and students. According
to Roll and Winne (2015), LA provides a new dimension to better SRL research for
learners and learner environments. Unlike the use of self-report tools to collect infor-
mation how students regulate based on their opinions about themselves, LA rely on the
“traces” that students leave behind when studying an online course. Current online
learning environments record and store all the logs capturing how and what activities
students engaged while studying online. The data is analyzed to provide evidence for
self-regulation for each learner. The advantage of using LA or EDM over other ap-
proaches of measuring and promoting SRL is that they are unobtrusive in nature as op-
posed to the obtrusive methods such as self-report methods. With obtrusive methods,
students are aware that SRL is being measured about them and are likely to change be-
haviors for being aware that measures about themselves are taking place (Schraw,
2010). EDM or LA methods are also advantageous over self-report tools as allow both
SRL measurement and interventions to co-occur.

In this review, some studies employed LA in SRL measurements through the use of
student logs and traces from LMSs, MOOCs, and PLE. Alharbi et al. (2014) used LA to
design a learning object system based on SRL and developed a software agent to meas-
ure and scaffold SRL strategies. Arnold and Pistilli (2012) employed LA on both online
and offline data to provide visual reports to teachers about students’ SRL levels. Lec-
turers will then submit interventions feedback and reports to students manually. The
use of LA in this project helped the lecturers to easily monitor SRL for students and
avail reports to students. In other studies such as Cicchinelli et al. (2018), Davis, Chen,
Jivet, et al. (2016), Lee and Recker (2017), and Nussbaumer et al. (2015), LA is used to
identify SRL strategies for learners through the use of LMS and MOOC data and pro-
vide learner feedback through dashboards. In their study, Rodriguez Groba et al. (2014)
used LA to develop a software agent known as SoftLearn tool used by teachers to assess
SRL skills in students. The tool provides visualized learner activities which are made
available to teachers for measuring SRL levels on LMS students.
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 10 of 21

d. Other online survey tools

In this study, it was discovered that there are several other new online survey instru-
ments that have been developed and implemented. The survey tools are self-report in-
struments which were administered online in the various projects. The studies in which
online survey tools were used include Chaves-Barboza et al. (2017), Gaupp et al. (2018),
Jansen et al. (2017), Kizilcec et al. (2016), and Kizilcec et al. (2017).

Other self-report tools developed also include the Self-Regulation Measure for Com-
puter based Learning (SRMC) which was developed by Song et al. (2011) by modifying
the Zimmerman’s Self-Regulated Learning Interview Schedule (SRLIS). The SRMC was
used to assess medical students’ self-regulation as aptitude in computer-based learning

environment.

RQ2: What methods are being used to measure and promote SRL at the same time in on
e-learning environments in higher education?

Self-regulated learning promotion or intervention is described as an activity or event
that can “trigger SRL development” within an online student during learning episode.
While SRL strategy measures seeks to establish SRL levels for learners, SRL interven-
tions aim at strengthening or stimulating the growth of the inherent SRL skills in lea-
ners Triquet, Peeters, and Lombaerts (2017). After content analysis of the 30 studies
reviewed in this study, eight studies that described SRL measurement approaches that
also served to promote SRL were identified and the summaty is provided in Table 2.

a. Learner analytics and dashboard visualizations: The findings from the reviewed
literature indicate that LA is used as a tool to measure SRL strategy and also used
to promote SRL at the same time by enhancing learner activities through provision
of insights using dashboards visualization. The data used for the LA is generated
from the various learning environments such as LMSs, MOOCs, and PLEs. These
studies used learner analytics by offering dashboard feedback to students (Arnold
and Pistilli, 2012, Davis, Chen, Jivet, et al., 2016, Davis et al., 2016, and
Nussbaumer et al., 2015).

b. Software agents: The use of artificial intelligent software agents was also identified
in the study. Software agents are intelligently powered to offer assistive activity
guidelines that stimulate the growth of SRL skills for the students. When the
learners engage on the suggested SRL tasks by agents, the SRL skill growth is
triggered. The software agents identified in this review include ProSOLO software
by Dawson et al. (2015), eLDa tool by Onah and Sinclair (2017), SoftLearn tool by
Rodriguez Groba et al. (2014), and Learn-B software by Siadaty (2016).

c. Learner feedback: As presented in Table 2, we established that a number of studies
employed web-enabled prompts to provide feedback to learners. The feedback pro-
vided to learners was in most studies facilitated through the use of LA reports sub-
mitted to teacher who then used the feedback to assess learners’ use of SRL
strategies (Cho & Shen, 2013; Dawson et al., 2015; Onah & Sinclair, 2017; Winne
& Hadwin, 2013)

d. Other systems and applications
Araka et al. Research and Practice in Technology Enhanced Learning

Table 2 SRL measurement tools that also acted as intervention tools

(2020) 15:6

Page 11 of 21

 

 

No Reference SRL Type of interventions provided
measurement Feedback Hint Prompt S/W Description
agent
1 Azevedo, Yes Yes No Yes Yes MetaTutor is an environment that
Witherspoon, enhances SRL for biology
Chauncey, students
Burkett, & Fike,
2009
2 Barnard, Lan, To, Yes Yes No No No Interventions from lecturers
Paton, & Lai, based on the analytics via email
2009
3 Cho & Shen, Yes Yes Yes Yes No Uses an offline survey to get
2013 feedback on learners level of
satisfaction in using the system—
use of dashboard visualization
4 Dawson, Yes Yes Yes No No Uses learner analytics to provide
Joksimovié, learner feedback through
Kovanovicé, dashboard
Gasevic, &
Siemens, 2015
5 Delen, Liew, & Yes Yes No No ProSOLO Software used to unravel learner
Willson, 2014 software autonomy
6 Onah &Sinclair, Yes Yes No No Yes Learning analytics tracker that
2017 provide different learning
methodologies to learners not
too specific to SRL strategies
7 Siadaty, 2016 Yes Yes No No SoftLearn Not specified
tool
8  Winne & Hadwin, Yes Yes Yes Yes No SRL support inform of hints that
2013 guide students on what activity

to do next

 

i. The Student Relationship Engagement System (SRES)

The SRES, a standalone learner analytics tool that enhances teacher and student in-
teractions, developed by Liu and his colleagues, allows data from different sources to be
fed into the system for analysis. The teachers use the information to contextualize and
understand each learner. The learners are notified through automated feedback and
therefore are able to receive support from teachers through data-driven personalization.
The analyzed data is used by teachers to align the needs of the students of each student
through the use of educational data from the various sources. The only drawback to
the system is that it is a standalone and not integrated to LMS or e-learning system
(Liu, Bartimote-Aufflick, Pardo, & Bridgeman, 2017).

ii. SoftLearn

The SoftLearn was developed by Groba, Barreiros, Lama, Gewerc, and Mucientes
(2015) and is based on learning analytics. The tool is used by instructors to evaluate
students’ SRL skills. The preferred learning environments where the tool can be de-
ployed are PLEs, e-portfolios, and social networks. The tool presents to the instructor
the learning path being followed by learners for assessment through a graphical
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 12 of 21

interface. The limitation to this tool is that the instructor is placed at the center of the
evaluation process, and when the number of students is large, the challenge will be if
the instructor will be able to provide the required assessment for each learner.

iii. OnTask

OnTask is an open source platform implemented by (Pardo et al., 2018). Instruc-
tors use data collected from learning environments and other sources to provide
personalized learning student actions. The instructors are able to use the tool to
manage data to understand the learner and then specify the actions for learners
through automated feedback that enable the student reflect on various learning ac-
tivities and apply them in the learning process. Like the SoftLearn tool, the
OnTask model presents analyzed information to the instructors inform of visual-
ized feedback. Instructors will then provide personalized support to students based
on the information presented to them.

RQ3: What is the trend in terms of measurements and interventions that are being used
to promote SRL skills for learners on e-learning environments in higher education?

This study sought to establish the status of the “three waves” in reference to the tools
developed and belonging to each wave of evolution (Panadero et al., 2016). In taking
account of the historical context, the tools that have been used in the past and how
they have fashioned the design of the tools used in measuring and promoting SRL pres-
ently and in future are presented in this section. The SRL measurements tools and
methods therefore were analyzed in relation to the “three waves” that are explained
below:

a. The first wave: SRL measured through self-report tools

This is believed to be the first era of SRL measurement where SRL was viewed as trait
based. Self-regulated learning was conceptualized as an individual inclination that based
on traits of the learner without contextual considerations of the learning environment.
This led to the development of tools and measurement methods that were trait-based.
These methods and tools include the self-report instruments such as structured ques-
tionnaires and interviews that were used to assess SRL before or after a learning
process. When self-reports measures are used, there is tendency for learners overesti-
mating the use of their SRL skills (Winne & Perry, 2000). Self-reports are user-oriented
and therefore depends on how learner perceives themselves as far as SRL level skills are
concerned. The measures are also deployed outside the learning environment (Roth
et al., 2016). According to Lee (2008), SRL can only be measured in the context of an
actual learning environment where it occurs. Since the self-report tools analyze SRL be-
fore or after a learning period and are designed to only measure SRL, they do not pro-
vide interventions for SRL. Such tools include structured questionnaires, interviews,
current abilities, and expected performance (Schraw, 2010). The trait-based measures
could later be replaced by domain-based measures where SRL was measured during
learning process (Boekaerts & Cascallar, 2006).
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 13 of 21

b. The second wave: online measures of SRL

During this wave, SRL was conceptualized as a process-oriented and measured SRL
as a series of events or processes. As a result, tools that assess SRL during the learning
process started emerging. SRL started to be perceived as event-based where approaches
such as the use log data (traces) and observation of learner behaviors are used to meas-
ure SRL. The objective was to be able to measure SRL without students being aware
while considering the contextual learning environment. This approach is unobtrusive
since SRL is measured during learning process without triggering change of behaviors
of learning (Schraw, 2010).

c. The third wave: measurement approaches that provide SRL interventions

In this wave, the SRL measurements and interventions are designed so that they can
co-occur. According to Panadero et al. (2016), we are now in the phase of the third
wave where tools used to measure SRL also provide interventions for enhancing
learners’ self-regulatory skills. They argue that we have started to witness development
and deployment of SRL tools that not only measure SRL but also provide interventions
for supporting SRL skills. According to Triquet et al. (2017), SRL measurement is car-
ried out to establish the levels SRL skills for learners while SRL interventions stimulate
the growth of SRL skills within learners hence becoming beneficial to learners when in-
terventions are rendered during the learning episode. While the “three waves” are inde-
pendent to each other, the usage is expected to overlap.

From the findings, it can be noted that the traditional SRL measurement
methods and tools that were designed for physical classroom setting are still used
for measuring SRL on e-learning environments. Although the self-report tools and
instruments that include questionnaires and interviews have been found to be reli-
able and effective in measuring SRL strategies in learners (Roth et al., 2016), there
are challenges that have been raised by researchers that need to be considered.
The self-report tools have been found to be are bias and only record SRL based
on students’ perceptions about themselves. Additionally, researchers argue that the
tools were effectively designed for use in face-to-face classroom settings and may
not therefore apply in online learning environments. The challenges are presented
in Table 3.

The findings from this review also indicate that while the self-report tools have
continued to dominate in the SRL measurements, there is a recognizable shift to-
wards the use of approaches that are able to provide SRL measurements and inter-
ventions as well. All the studies that provided SRL interventions relied on log data
that was analyzed to provide levels of SRL and at the same time provided mecha-
nisms for SRL improvement (see Table 3). These studies used learner analytics
and/or dashboard feedback to students (Arnold & Pistilli, 2012; Davis, Chen, Jivet,
et al., 2016; Davis, Chen, van der Zee, Hauff, & Houben, 2016; Nussbaumer et al.,
2015). The reviewed studies indicate that there are two types of SRL interventions
that are provided in online learning environments: (a) interactive feedback through
visualized dashboards and (b) metacognitive and behavioral prompts and hints that
aim at engaging students to enhance their SRL capability. The use of visualization
Araka et al. Research and Practice in Technology Enhanced Learning

(2020) 15:6

Table 3 Challenges of measuring SRL on e-learning platforms

 

Challenge

Description

Reference

 

Use of traditional tools/methods on
e-learning

The traditional tools and methods
are obtrusive

Existence of many models and
many constructs to be measured

Lack of a tool(s) for both SRL
measurements and interventions

Lack of framework that describes/
guides how to establish learners’
levels of SRL and describe at what
level to start and stop issuing
scaffold within an e-learning
system.

The traditional instruments such as
questionnaires and interviews are
trait-based and user-oriented;
learners respond to SRL items de-
pending on how they perceive
themselves leading to learners over-
estimating their use of SRL skills.
The tools are also deployed outside
the learning environment before or
after a learning episode and there-
fore not able to measure SRL during
an actual learning episode when
skills are being employed by
students.

The learners are normally aware of
SRL being measured and therefore
affect their engagement and
performance

There is no generalized model that
describes or conceptualizes all SRL
constructs. Additionally, each of the
existing models is grounded on
different aspects of learning.

What's next after establishing one’s
level of SRL? So far, we have had
separate tools for measuring and
promoting SRL. Authors now
recommend a tool for both.

The existing theoretical models only
provide frameworks that describe
the different phases, processes, and
constructs to be measured. When it
comes to actual measurement and
provision of scaffolds, there is no

(Broadbent & Poon, 2015; Lee,
2008; Roth et al., 2016; Saks &
Leijen, 2014; Siadaty, 2016; Winne &
Perry, 2000)

(Schraw, 2010)
(Siadaty, 2016)

(Carlos NUfez et al., 2017)

(Panadero et al., 2016)

(Panadero et al., 2016)
(Araka, Maina, Gitonga, & Oboko,
2019)

Page 14 of 21

defined framework to follow for
guidelines.

 

dashboards allow learners to see their learning behaviors without stimulating devel-
opment of SRL skills. The effect of visualized dashboard is therefore passive in na-
ture as they only provide one-way and non-interactive feedback. From the studies
reviewed, it was established that in some studies, visualized reports regarding
learners’ behaviors with respect to SRL are delivered first to instructors for their
synthesis and interpretation so as to know how to provide individualized support
to learners. For example, the OnTask model by Pardo and his colleagues is de-
signed to provide students’ support actions based on analyzed data from different
educational sources. With this model, the instructors are presented with analyzed
information from various sources inform of visualized feedback on learners. This
information then guides the instructors on how to offer personalized support
(Pardo et al., 2018). The other model which is similar to OnTask is the SRES (Liu
et al., 2017). SRES is standalone learner analytics tool that is intended to enhance
instructor-student interactions. It allows data from different sources to be fed into
the system for analysis. Instructors rely on the information to contextualize and
understand each learner’s learning behaviors. In essence, the analyzed information
is used by instructors to align each of the students’ needs through the use of edu-

cational data from the various sources.
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 15 of 21

RQ4: What EDM tools are being used to measure and promote SRL on e-learning
environments in higher education?

In this section, we present the findings on how EDM and LA are being used to measure
and promote SRL on e-learning environments in higher education settings. From the
review, it is evident that data mining techniques are being used in measuring and pro-
moting SRL. Educational data mining methods and learner analytics are now being ap-
plied on data collected from educational environments such LMS, PLEs, and MOOCS.
The datasets from the educational learning environments is collected for analysis in
order to understand learners, tutors, and learning environments. EDM also helps the
researcher and educators to understand how students engage during learning process
with respect to self-regulation and to what extent to which the students employ self-
regulating skills on e-learning (Kizilcec et al., 2017).

In their study, Cavalcanti et al. (2018) developed a model to predict the performance
of students based on self-regulated learning skills by use of EDM methods in identify-
ing SRL indicators from datasets collected from an e-learning system. The prediction
was based on SRL indictors and learners’ behaviors, motivation, and application of cog-
nitive abilities. Other software tools that have employed the use of educational data
mining include MetaTutor metacognitive tool and nStudy tool (Azevedo et al., 2009;
Winne & Hadwin, 2013). The MetaTutor software is a learning environment that pro-
motes self-regulatory skills to high school and college students through the provision of
feedback inform of prompts that guides students on the activities to engage on during
learning. The nStudy tool also provides learning hints to students. Prompts and hints
provided by these tools are geared towards enhancing SRL learning in students and
hence qualify as intervention approaches.

Notably, there is progress on the SRL measurement tools used in measuring SRL in
online learning environments—from the self-report tools that measured SRL before or
after learning to analytic tools that are employed to measure SRL during a learning

episode.

Discussion

The purpose of the current study was to investigate the research trends in terms of
SRL measurement and promotion for SRL online and establish research gaps between
the period 2008 and 2018. From the review, it can be noted that measurement and pro-
motion of SRL has greatly advanced from the use of traditional methods which relied
on learner perceptions on their SRL skills and use of offline data to online measures
such as the use of learner logs from e-learning environments. The findings from the re-
view indicate that there is noticeable evidence of a recognizable shift from using tools
that only measures SRL to tools that measure SRL while providing interventions that
stimulates growth of SRL in learners during the learning process (Azevedo & Wither-
spoon, 2009; Barnard et al., 2009; Cho & Shen, 2013; Dawson et al., 2015; Delen et al.,
2014; Onah & Sinclair, 2017; Siadaty, 2016; Winne & Hadwin, 2013).

The study provides an understanding on the research trends in terms of the tools
and instruments used to evaluate and promote SRL online learning environments. In
some studies, instructors were presented with processed information through data ana-
lytics dashboards. The instructors could utilize the analyzed information in offering
support to students based of the processed information (Liu et al., 2017; Pardo et al.,
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 16 of 21

2018). The visualized reports about learners’ behaviors are first delivered to instructors
for their synthesis and interpretation so as to know how to provide individualized sup-
port to learners. While these tools support instructors to gain insights on how learning
is taking place and allow them to customize SRL support to students, one issue need to
be addressed: the large number of students enrolled on online courses and reliance on
human judgments in the provision of interventions. This approach requires that in-
structors know how to interpret data and make human judgments before offering feed-
back to students. The increased number of students taking online courses could be an
hindrance to human capacity of offer adequate support. Offering individualized feed-
back to e-learning students may no longer be tenable as the number of learners is be-
coming large for tutors to guide them individually (Nussbaumer et al., 2015).
Researchers and educators there need to shift from instructor-centered support to
data-centered applications based data collected from various e-learning sources such as
LMSs, MOOCs, PLEs, social education networks, and e-portfolios.

From the review, the SRL measurement and promotion approaches can be catego-
rized into two. First category is approaches that extend the decision making capability
for teachers to be able to offer data-driven and personalized support to learners. These
approaches take advantage of teachers’ knowledge and augment its information from
the analyzed data. The second type of approaches are those that offer metacognitive
feedback by making learners stop learning and reflect on the learning process and then
proceed. Most of the existing studies focus on the first model. The increase in number
of online learners however implies that it may not be easy for online instructors to
interact with every student and provide individualized guidance and support. Re-
searchers therefore argue that the effective way on how EDM can help promote SRL is
the provision of individualized interventions through metacognitive feedback such as
hints and prompts (Lodge et al., 2019).

It can also be noted that there is developing potential of using EDM tools to provide
measurement and interventions concurrently. Interventions, when implemented within
the measurement tools, could play a significant role in stimulating the growth of SRL
skills. Although this study identified an observational trend that the use of learning ana-
lytics and EDM in measuring and promoting SRL has started to emerge and now ad-
vancing, literature indicates that there is continued use of self-report tools that were
originally created for the traditional face-to-face classroom set-up. According to Winne
and Baker (2013), EDM can been used to identify, model, and predict learners’ behav-
ior. It can also be noted that that the SRL measurement + intervention tools have
started to emerge. So far, we have had separate tools for measuring and promoting
SRL. Some of the studies that used LA as an alternative approach to measuring SRL
also used the dashboard results from LA to promote SRL skills for learners on e-
learning environments. This review indicates that EDM and LA are now being applied
to establishing learner behaviors in online learning. Their guided implementation could
lead to the development of various tools that are used specifically to mine education
data generated from various learning environments including web-based systems such
as LMS. The EDM tools accomplish various aspects of data preparation, modeling, pro-
cessing, analysis, and visualizations. The tools ensure availability of visualized and inter-
active feedback on learner styles to both students and instructors. Real-time access to
visualized feedback will enhance continuous monitoring and support for the benefit of
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 17 of 21

learners to self-regulate based on the personalized feedback received on LMS dash-
board. External agents such as instructors will also be able to use the feedback to pro-
vide personalized support/scaffolding to learners. The impact will increase learner
motivation, satisfaction, and better learning outcome. This will also enable researchers
and instructors to detect, isolate, and engineer changes on e-learning environments that
impact learners. Early interventions and support that can be offered to learners via
EDM tools will lead to improved performance for learners and reduce drop-out rates
and reduce the time learners take to graduate especially through online courses. The
real-time visualized feedback from actual datasets can also be accessed by instructors
to monitor how SRL skills for learners change over time. Additionally, the reviewed lit-
erature indicates some challenges experienced when measuring SRL on online learning
environments as presented in Table 3.

Despite the challenges identified in this review, it has been established that self-
report methods that were designed to measure SRL in face-to-face classroom setup
are continuously being used to measure SRL in online learning environments. Al-
though researchers argue that the popularity of self-report tools could be their reli-
ability and validity that has been proven over the years (Roth et al., 2016), self-
report tools are obtrusive in nature. When learners are prompted to provide their
perceptions on their SRL skills, they may not only overestimate their responses but
also fail to capture their actual study behaviors. Literature also indicates that self-
report tools are usually modified or enhanced to fit the context of online learning
environment while the items still remain same as those that were designed for
face-to-face classroom settings. This denotes that continued use of self-report tools
is likely to lead to situations where the measured SRL levels do not represent ac-
tual learner behaviors (Broadbent & Poon, 2015). According to Winne and Baker
(2013), measuring metacognition and motivation faces the challenge of identifying
the constructs that can be modeled especially when self-report tools are used. They
argue that the instruments used to gather and process metacognition and motiv-
ation are unreliable and erroneous in noticing change of state of learners’ skills
and that in most research studies, experimental data is usually manipulated in
order to improve reliability of the instruments used.

It would also be important to observe that there is a lack of a model that can be used
to implement the “third wave” of SRL measurement and promotion in higher education
especially in online learning environments. The existing studies did not describe an
EDM model for implementation except for one study that proposed a conceptual
model (Araka et al., 2019). To address the challenges encountered when using self-
report tools for measuring SRL in online learning environments, we propose the use of
EDM techniques for measuring and promoting SRL as EDM relies on the use log or
trace data collected from educational environments as indicators of SRL. Given the ob-
served challenges, there is a need to develop a framework that helps to integrate the
SRL measurement and promotion tools with EDM tools.

Conclusion

This study presents various tools and methods that have been used to measure
and promote SRL for online learning environment for the last one decade. The po-
tential of EDM in measuring and promoting SRL on e-learning environments has
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 18 of 21

also been established. While there are challenges in measuring and promoting SRL
strategies in online learning environments, some of them may be addressed
through the implementation of EDM techniques. Effectively, the techniques need to
be deployed on e-learning systems such as the popular learning management sys-
tems which are being used by most of institutions of higher learning to offer both
blended and online course to students. Similarly, the EDM tools have the potential
of capturing and analyzing real-time learner traces and present visualized feedback
to learners and hence allowing continuous assessment of SRL. Learner scaffolds
can also be provided through learner dashboard. This implementation allows stu-
dents to be supported and guided while studying online without the limitation of
numbers of students enrolled (Araka et al., 2019). However, we found out that
there is lack of a framework on how the EDM measurement and intervention
models can be conceptualized and deployed on LMSs.

Additionally, this systematic review indicates SRL interventions have been im-
portant in improving or stimulating growth of SRL. Nonetheless, one critical issue
that researchers need to address is the lack of a model that captures all the SRL
strategies from the existing SRL conceptual models and maps them to the LMS
data indicators from which SRL strategies for each learner can be inferred. In
order to design a model that can be generalized, there is a need for further study
on existing frameworks that are used to provide SRL interventions, nature of inter-
ventions, and indicators that were used in each study. This will then act as guide
to understanding the nature and effectiveness of SRL interventions provided for
each learner. According to Lodge et al. (2019), it is indispensable that e-learning
environments provide tools to evaluate students, establish the levels of their indi-
vidual engagements, and identify those who need interventions and reinforce each
learner with the right kind of interventions.

Researchers in the field of SRL have started to embrace the use EDM and LA in
measuring and providing SRL interventions. Future research should now focus on
implementing EDM approaches that measure and provide SRL interventions in real-
time. More interesting will be the integration of EDM tools into existing LMS for
higher institutions of learning and MOOCs. This will enable collection, analysis, and
provision of feedback to learners in real-time ensuring individualized support to
learners. The educational data mining tools must provide solutions that should make
use the rich and actual datasets from online learning environments to provide infer-
ences on students’ levels of SRL skills while at the same time providing interventions
for promoting SRL skills among learners.

Currently, most institutions of higher learning have adopted e-learning for online
courses to curb the increased demand of higher education (Hadullo et al., 2018). How-
ever, the numbers of instructors are not enough to provide the support in terms of
self-regulated learning (Muuro, Wagacha, Oboko, & Kihoro, 2014). Consequently, there
is a need to include other tools such as EDM which can be used to promote SRL with
little human intervention. Even though such tools have been developed, from this re-
view, there is a need to carry out more empirical evidence on the effectiveness of using
EDM measurement and intervention models compared to human interventions in pro-
moting learning in institutions of higher learning given the increased demand of higher

education.
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 19 of 21

Acknowledgements
Not applicable.

Authors’ contributions
All the authors, EA, EM, RG, and RO, read and approved the final manuscript for submission.

Funding
This research was supported by the National Research Fund 2016/2017 grant award under the multidisciplinary-multi-
institutional category involving Kenyatta University, University of Nairobi, and The Cooperative University of Kenya.

Availability of data and materials
Not applicable.

Competing interests
The authors declare that they have no competing interests.

Author details
"Kenyatta University, P.O. Box 43844, Nairobi 00100, Kenya. “University of Nairobi, Nairobi, Kenya.

Received: 28 June 2019 Accepted: 18 February 2020

 

References

Adam, N. L, Alzahri, F. B., Cik Soh, S., Abu Bakar, N., & Mohamad Kamal, N. A. (2017). Selfregulated learning and online
learning: A systematic review. In Lecture notes in computer science (including subseries lecture notes in artificial intelligence
and lecture notes in bioinformatics). https://doi.org/10.1007/978-3-319-70010-6_14.

Alharbi, A., Henskens, F., & Hannaford, M. (2014). Personalised learning object system based on self-regulated learning
theories. Journal of Engineering Pedagogy, 4(3), 24-35. https://doi.org/10.3991/ijep.v4i3.3348.

Allen, |. E., & Seaman, J. (2013). Changing course: Ten years of tracking online education in the US. Babson Survey Research Group
Retrieved from https://onlinelearningconsortium.org/survey_report/changing-course-ten-years-tracking-online-education-
united-states/.

Araka, E., Maina, E., Gitonga, R., & Oboko, R. (2019). A conceptual model for measuring and supporting self-regulated learning
using educational data mining on learning management systems. In P. Cunningham & M. Cunningham (Eds.), /ST-Africa
2019 (pp. 1-11). Nairobi: IEEE. https://doi.org/10.23919/ISTAFRICA.2019.8764852.

Arnold, K. E., & Pistilli, M. D. (2012). Course signals in Pudu: Using learning analytics to increase student success. In Proceedings
of the 2nd International Conference on Learning Analytics and Knowledge - LAK ‘12 (pp. 267-270). New York: ACM Press.
https://doi.org/10.1145/2330601.2330666.

Azevedo, R., Witherspoon, A.,, Chauncey, A., Burkett, C, & Fike, A. (2009). MetaTutor: A MetaCognitive tool for enhancing self-
regulated learning. In Cognitive and metacognitive educational systems: AAAI Fall Symposium (FS-09-02) (pp. 14-19).

Azevedo, R,, & Witherspoon, A. M. (2009). Self-regulated learning with hypermedia handbook of metacognition in education.
In Handbook of metacognition in education (pp. 319-339). https://doi.org/10.4324/9780203876428.ch7.

Barnard, L., Lan, W., To, Y., Paton, V., & Lai, S.-L. (2009). Measuring self-regulation in online and blended learning environments.
Internet and Higher Education, 12(1), 1-6. https://doi.org/10.1016/j.iheduc.2008.10.005.

Boekaerts, M., & Cascallar, E. (2006). How far have we moved toward the integration of theory and practice in self-regulation ?,
199-210. https://doi.org/10.1007/s10648-006-901 3-4.

Bogarin, A., Cerezo, R., & Romero, C. (2018). Discovering learning processes using inductive miner: A case study with learning
management systems (LMSs). Psicothema 2018, 30(3), 322-329. https://doi.org/10.7334/psicothema2018.1 16.

Broadbent, J., & Poon, W. L. (2015). Self-regulated learning strategies & academic achievement in online higher education
learning environments: A systematic review. The Internet and Higher Education, 27, 1-13. https://doi.org/10.1016/j.ineduc.
2015.04.007.

Carlos Nufez, J., Romera, E. M., Magno, C.,, & Panadero, E. (2017). A review of selfregulated learning: Six models and four
directions for research. Frontiers in Psychology, 8, 422. https://doi.org/10.3389/fpsyg.201 7.00422,

Cavalcanti, A, Dourado, R., Rodrigues, R., Alves, N., Silva, J., & Ramos, J. L. C. (2018). An analysis of self-regulated learning
behavioral diversity in different scenarios in distance learning courses. Anais Do XXIX Simpésio Brasileiro de Informatica Na
Educacdo (SBIE 2018), 1(Cbie), 1493-1502. https://doi.org/10.5753/cbie.sbie.2018,1493.

Chaves-Barboza, E., Trujillo-Torres, J. M., Antonio Lopez-Nufez, J., & Sola-Martinez, T. (2017). Actions and achievements of self-
regulated learning in personal environments. Research on students participating in the Graduate Program in Preschool
Education at the University of Granada. Journal of New Approaches in Educational Research, 6(2), 135-143. https://doi.org/
10.7821/naer.2017.7.236.

Chen, C. M. (2009). Personalized E-learning system with self-regulated learning assisted mechanisms for promoting learning
performance. Expert Systems with Applications, 36(5), 8816-8829. https://doi.org/10.1016/j.eswa.2008.1 1.026.

Cho, M-H., & Cho, Y. (2017). Distance education self-regulation in three types of online interaction: A scale development.
Distance Education, 38(1), 70-83. https://doi.org/10.1080/01587919.2017.1299563.

Cho, M. H., & Shen, D. (2013). Self-regulation in online learning. Distance Education, 34(3), 290-301. https://doi.org/10.1080/
01587919.2013.835770.

Cicchinelli, A. Veas, E., Pardo, A., Pammer-Schindler, V., Fessl, A., Barreiros, C, & Lindstadt, S. (2018). Finding traces of self-
regulated learning in activity streams. In International Conference on Learning Analytics and Knowledge (p. 10). Sydney.
https://doi.org/10.1145/3170358.31 70381.

Davis, D., Chen, G,, Jivet, |, Hauff, C, & Houben, G. J. (2016). Encouraging metacognition and self-regulation in MOOCs
through increased learner feedback. In CEUR Workshop Proceedings (Vol. 1596, pp. 17-22). https://doi.org/10.1067/mtc.
2002.1 20730.

 
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 20 of 21

Davis, D., Chen, G, van der Zee, T., Hauff, C., & Houben, G. J. (2016). Retrieval practice and study planning in MOOCs:
Exploring classroom-based self-regulated learning strategies at scale. LNCS, 9891, 57-71. https://doi.org/10.1007/978-3-
319-45153-4_5.

Dawson, S., Joksimovic¢, S., Kovanovi¢, V., Gasevi¢, D., & Siemens, G. (2015). Recognising learner autonomy: Lessons and

reflections from a joint x/c MOOC. In HERDSA Conference (pp. 1-13). https://doi.org/10.1007/s11270-007-9477-y.

Delen, E., & Liew, J. (2016). The use of interactive environments to promote self-regulation in online learning: A literature

review. European Journal of Contemporary Education, 15(15), 24-33. https://doi.org/10.13187/ejced.2016.15.24.

Delen, E., Liew, J., & Willson, V. (2014). Effects of interactivity and instructional scaffolding on learning: Self-regulation in online

video-based environments. Computers in Education, 78, 312-320. https://doi.org/10.1016/j.compedu.2014.06.018.

Devolder, A., van Braak, J., & Tondeur, J. (2012). Supporting self-regulated learning in computer-based learning environments:
Systematic review of effects of scaffolding in the domain of science education. Journal of Computer Assisted Learning,
28(6), 557-573. https://doi.org/10.1111/).1365-2729.201 1.00476.x.

Garcia, R., Falkner, K., & Vivian, R. (2018). Systematic literature review: Self-regulated learning strategies using e-learning tools
for computer science. Computers in Education. https://doi.org/10.1016/j.compedu.2018.05.006.

Gaupp, R., Fabry, G., & Korner, M. (2018). Self-regulated learning and critical reflection in an e-learning on patient safety for
third-year medical students. International Journal of Medical Education, 9, 189-194. https://doi.org/10.5116/ijme.5639.d5a8.

Groba, A. R., Barreiros, B. V., Lama, M., Gewerc, A., & Mucientes, M. (2015). Using a learning analytics tool for evaluation in self-
regulated learning. In Proceedings - Frontiers in Education Conference, FIE, 2015-Febru (February). https://doi.org/10.1109/FIE.
2014.7044400.

Hadullo, K., Oboko, R., & Omwenga, E. (2018). Status of e-learning quality in Kenya: Case of Jomo Kenyatta University of
agriculture and technology postgraduate students. International Review of Research in Open and Distance Learning, 19(1),
138-159. https://doi.org/10.19173/irrodl.v19i1.3322.

Hashemyolia, S., Asmuni, A., Fauzi, A. Ayub, M., Daud, S. M., & Shah, J. A. (2015). Motivation to use self-regulated learning
strategies in learning management system amongst science and social science undergraduates. Asian Social Science,
11(3). https://doi.org/10.5539/ass.v11n3p49.

Jansen, R. S., van Leeuwen, A., Janssen, J., & Kester, L. (2017). Validation of the self-regulated online learning questionnaire.
Journal of Computing in Higher Education, 29, 6-27. https://doi.org/10.1007/s12528-016-9125-x.

Khan, K. S., Kunz, R., Kleijnen, J., & Antes, G. (2003). Five steps to conducting a systematic review. Journal of the Royal Society of
Medicine. https://doi.org/10.1258/jrsm.96.3.1 18.

Kizilcec, R. F., Pérez-Sanagustin, M., & Maldonado, J. J. (2016). Recommending self-regulated learning strategies does not
improve performance in a MOOC. In Proceedings of the Third (2016) ACM Conference on Learning @ Scale - L@S ‘16 (pp.
101-104). https://doi.org/10.1 145/2876034.2893378.

Kizilcec, R. F., Pérez-Sanagustin, M., & Maldonado, J. J. (2017). Self-regulated learning strategies predict learner behavior and
goal attainment in Massive Open Online Courses. Computers in Education, 104, 18-33. https://doi.org/10.1016/j.compedu.
2016.10.001.

Lee, D., Watson, S. L, & Watson, W. R. (2019). Systematic literature review on self-regulated learning in massive open online
courses. Australasian Journal of Educational Technology, 35(1), 28-41. https://doi.org/10.14742/ajet.3749.

Lee, J-E., & Recker, M. (2017). Measuring students’ use of sel-regulated learning strategies from learning management system
data: An evidence-centered design approach.

Lee, J-K. (2008). The effects of self-regulated learning strategies and system satisfaction regarding learner ' s performance in
e-learning environment. Journal of Instructional Pedagogies, 1, 30-45 Retrieved from https://files.eric.ed.gov/fulltext/EJ1
056334.pdf.

Littlejohn, A, Hood, N., Milligan, C,, & Mustain, P. (2016). Learning in MOOCs: Motivations and self-regulated learning in
MOOCs. Internet and Higher Education, 29, 40-48. https://doi.org/10.1016/j.ineduc.201 5.1 2.003.

Liu, D. Y. T., Bartimote-Aufflick, K., Pardo, A., & Bridgeman, A. J. (2017). Data-driven personalization of student learning support
in higher education. Studies in Systems, Decision and Control, 94, 143-169. https://doi.org/10.1007/978-3-319-52977-6_5.

Lodge, J. M., & Corrin, L. (2017). What data and analytics can and do say about effective learning. Npj Science of Learning, 2(1),
1, https://doi.org/10.1038/s41539-017-0006-5.

Lodge, J. M., Panadero, E., Broadbent, J., & De Barba, P. G. (2019). Supporting self-regulated learning with learning analytics.
Learning analytics in the classroom: Translating learning analytics research for teachers, (October), 45-55.

Long, P., & Siemens, G. (2011). Penetrating the fog: Analytics in learning and education Retrieved from https://er.educause.
edu/~/media/files/article-downloads/erm1 151 .pdf.

Luna, J. M., Castro, C, & Romero, C. (2017). MDM tool: A data mining framework integrated into Moodle. Computer
Applications in Engineering Education, 25(1), 90-102. https://doi.org/10.1002/cae.21 782.

Muller, N. M., & Seufert, T. (2018). Effects of self-regulation prompts in hypermedia learning on learning performance and self-
efficacy. Learning and Instruction, 58(February), 1-11. https://doi.org/10.1016/j.learninstruc.2018.04.011.

Muuro, M. E., Wagacha, W. P., Oboko, R., & Kihoro, J. (2014). Students’perceived challenges in an online collaborative learning
environment: A case of higher learning institutions in Nairobi, Kenya. International Review of Research in Open and
Distance Learning, 15(6), 132-161. https://doi.org/10.19173/irrodl.v15i6.1768.

Naif, A., Ayman, F., & Saeed-ul, H. (2019). Predicting at-risk students using clickstream data in the virtual learning environment.
Sustainability 2019, 11(7238), 1-12.

Nikolaki, E., Koutsouba, M., Lykesas, G., Venetsanou, F., & Savidou, D. (2017). The support and promotion of self-regulated
learning in distance education. European Journal of Open, Distance and E-Learning, 20(1) Retrieved from http://www.
eurodl.org/?p=current&sp=brief&article=746.

Nussbaumer, A., Hillemann, E-C., Gutl, C., & Albert, D. (2015). A Competence-based Service for Supporting Self-Regulated
Learning in Virtual Environments. Journal of Learning Analytics, 2(1), 101-133. https://doi.org/10.18608/jla.2015.21.6.

Onah, D. F. O., & Sinclair, J. E. (2017). Assessing self-regulation of learning dimensions in a stand-alone MOOC platform.
International Journal of Engineering Pedagogy (UEP), 7(2), 4. https://doi.org/10.3991 /ijep.v7i2.6511.

Panadero, E., Klug, J., & Jarveld, S. (2016). Third wave of measurement in the self-regulated learning field: When measurement
and intervention come hand in hand. Scandinavian Journal of Educational Research. https://doi.org/10.1080/00313831.
2015.1066436.

 

 

 

 
Araka et al. Research and Practice in Technology Enhanced Learning (2020) 15:6 Page 21 of 21

Pardo, A., Bartimote-Aufflick, K., Buckingham Shum, S., Dawson, S., Gao, J., Gasevi¢, D., et al. (2018). OnTask: Delivering data-
informed, personalized learning support actions. Journal of Learning Analytics, 5(3), 235-249. https://doi.org/10.18608/jla.
2018.53.15.

Rodriguez Groba, A., Vazquez Barreiros, B., Lama, M., Gewerc, A, & Mucientes, M. (2014). Using a learning analytics tool for
evaluation in self-regulated learning. In 2014 IEEE Frontiers in education conference (pp. 2484-2491).

Roll, |, & Winne, P. H. (2015). Understanding, evaluating, and supporting self-regulated learning using learning analytics.
Journal of Learning Analytics, 2(1), 7-12. https://doi.org/10.18608/jla.201 5.21.2.

Romero, C,, Lopez, M. |, Luna, J. M., & Ventura, S. (2013). Predicting students’ final performance from participation in on-line
discussion forums. Computers in Education, 68, 458-472. https://doi.org/10.1016/j.compedu.2013.06.009.

Romero, C.,, & Ventura, S. (2007). Educational data mining: A survey from 1995 to 2005. Expert Systems with Applications,
33(2007), 135-146. https://doi.org/10.1016/j.eswa.2006.04.005.

Roth, A, Ogrin, S., & Schmitz, B. (2016). Assessing self-regulated learning in higher education: A systematic literature review of
self-report instruments. Educational Assessment, Evaluation and Accountability. https://doi.org/10.1007/s11092-015-9229-2,

Saks, K., & Leijen, A. (2014). Distinguishing self-directed and self-regulated learning and measuring them in the E-learning
context. In International Conference on Education & Educational Psychology 2013 (ICEEPSY 2013) (Vol. 112, pp. 190-198).
Science Direct). https://doi.org/10.1016/j.sbspro.2014.01.1155.

Schraw, G. (2010). Measuring self-regulation in computer-based learning environments. Educational Psychologist, 45(4), 258-266.
https://doi.org/10.1080/00461520.2010.515936.

Siadaty, M. (2016). Trace-based microanalytic measurement of self-regulated learning processes Moray House School of
Education and School of Informatics. Journal of Learning Analytics, 3(1), 183-214.

Siemens, G,, & Baker, R. (2012). Learning analytics and educational data mining: Towards communication and collaboration. In
ACM International Conference Proceeding Series, (April 2012) (pp. 252-254). https://doi.org/10.1145/2330601.2330661.

Song, H. S., Kalet, A. L., & Plass, J. L. (2011). Assessing medical students’ self-regulation as aptitude in computer-based learning.
Advanced Health Science Education Theory Practice, 16(1), 1-15. https://doi.org/10.1007/s10459-010-9248-1.

Terras, M. M., & Ramsay, J. (2015). Massive open online courses (MOOCS): Insights and challenges from a psychological
perspective. British Journal of Educational Technology, 46(3), 472-487. https://doi.org/10.1111/bjet.12274.

Triquet, K., Peeters, J. & Lombaerts, K. (2017). Se/f-regulated learning online: Benefits, empirical foundations, multi-level, and
multi-modal promotion & evaluation thereof for teacher professional development Contributing SRL Part to Teach-UP. A
policy experimentation co-funded by Erasmus+. Deliverab.

Winne, P. H., & Baker, R. S. J. D. (2013). The potentials of educational data mining for researching metacognition, motivation and
self-regulated learning. JEDM - Journal of Educational Data Mining, 5(1), 1-8. https://doi.org/10.1037/1082-989X.2.2.131.

Winne, P. H., & Hadwin, A. (2013). nStudy: Tracing and supporting self-regulated learning in the Internet. In International
handbook of metacognition and learning technologies (pp. 293-308). New York: Springer International Handbooks of
Education. https://doi.org/10.1007/978-1-441 9-5546-3_20.

Winne, P. H., & Perry, N. E. (2000). Measuring selfregulated learning. In Handbook of self-Resulation 5 Retrieved from http://
cachescan.bcub.ro/e-book/E1/580704/531-599.pdf.

Wong, J., Baars, M., Davis, D., Van Der Zee, T., Houben, G-J., & Paas, F. (2018). Supporting self-regulated learning in online
learning environments and MOOCs: A systematic review. International Journal of Human Computer Interaction, 1-18.
https://doi.org/10.1016/j.cadmos.2010.1 1.005.

Yamada, M., Shimada, A., Okubo, F., Oi, M., Kojima, K, & Ogata, H. (2017). Learning analytics of the relationships among self-
regulated learning, learning behaviors, and learning performance. Research and Practice in Technology Enhanced Learning.
https://doi.org/10.1 186/s41039-017-0053-9.

Yen, C-J., Tu, C-H., Sujo-Montes, L., Sealander, K,, Yen, C-J., Tu, C-H., & Sujo-Montes, L. (2016). A predictor for PLE
management: Impacts of self- regulated online learning on students’ learning skills. Journal of Educational Technology
Development and Exchange UJETDE), 9(1), 29-48. https://doi.org/10.18785/jetde.0901.03.

Zarouk, M. Y., & Khaldi, M. (2016). Metacognitive learning management system supporting self-regulated learning. In 2076 4th
IEEE International Colloquium on Information Science and Technology (CIST) (pp. 929-934).

Zhao Li Chen, H., Zhao, H., & Chen, L. (2016). How can self-regulated learning be supported in E-learning 2.0 environment:

A comparative study. Journal of Educational Technology Development and Exchange UJETDE), 9(2), 1-20. https://doi.org/10.
18785/jetde.0902.01,

Zimmerman, B. J. (1990). Self-regulated learning and academic achievement: An overview. Educational Psychologist, 25(1), 3-17.
https://doi.org/10.1207/s15326985ep2501.

Zimmerman, B. J., & Pons, M. M. (1986). Development of a Structured Interview for Assessing Student Use of Self-Regulated
Learning Strategies. American Educational Research Journal, 23(4), 614-628. https://doi.org/10.3102/000283 12023004614.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
