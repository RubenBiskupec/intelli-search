Annals of Mathematics and Artificial Intelligence (2020) 88:1119-1123
https://doi.org/10.1007/s10472-020-097 13-3

®

Check for
updates

Cognition and Neurocomputation

Larry M. Manevitz '? - Alex Frid?

Published online: 28 September 2020
© Springer Nature Switzerland AG 2020

Cognition and Neurocomputation is a subject that can be thought of either as a narrow
specialty area of neuroscience, psychology and psychiatry or egotistically even the main area
of which those other subjects are sub-areas. The underlying principle of the subject is that the
emergence of the phenomenon of human cognition is approachable and even explainable as
the natural emergence of a macro-phenomenon from an extensive micro-phenomenon in
analogy as, for example, classical Newtonian physics can explain the motions of planets.

Historically when there is a break-through in neurocomputation there are attempts to explain
cognitive effects by analogy. Such was the case 70 years ago with the McCullough-Pitts neuron
and simple neural networks, 60 years ago with the Perceptron, 40 years ago with neural
networks and backpropagation, and now during the past 10 years with deep neural networks.
Usually these “explanations” are somewhat naive from a cognitive viewpoint and can occa-
sionally be even a bit embarrassing. It is fair to say that the explanations of cognitive behavior
directly from the neurocomputation model has not yet been an overwhelming success.

Nonetheless, great advances have occurred; an example is the work of McClelland et al.
[14]. Here the lesson is that when one can see the produced model and the human model as
two instances of a common mechanism, where the interpretation of the mechanism can be
completely different in each. The simpler the artificial model is then results in the largest
strengthening of belief in the correctness of the mechanism in the human. For good examples,
see McClelland et al. [14] and Peleg, Hazan et al. [13].

As a subject, we see that Neurocomputation and Cognition has at least three main
components, “Cognition”, “Neuro” and “Computation”. “Cognition” can be thought of as (1)
the goal, that is, neurocomputation is there to explain by computational means human
cognitive abilities, or alternatively (11) as the source, being taken as a well-known strong

><] Larry M. Manevitz
manevitz @cs.haifa.ac.il

Alex Frid
alex.frid@ gmail.com

Department of Computer Science, Ariel University, Ariel, Israel

The Neurocomputation Laboratory, Department of Computer Science, University of Haifa, Haifa,
Israel

The Laboratory of Clinical Neurophysiology, Technion Faculty of Medicine, The Technion, Israel
Institute of Technology, Haifa, Israel

g) Springer
1120 Manevitz L.M., Frid A.

human ability, and then trying to abstract from it artificial reproductions of it, which can be
used for “practical” tasks. In both cases, the “Neuro” part indicates that the successful artificial
construct should be an emergent property of many neurons, either artificial or natural. The
precise choice of the model of the basic neuron and the basic method of modifying the model
(e.g. weights, or architecture) reflect the “Computational” constraints. On the one hand, to
emphasize the explanatory effect of the model, it should be as simple as possible in
implementing the abstractions of the components of the theory. On the other hand, if the
implementation is mainly for using the abstraction from the cognitive source for a practical
device, one can diverge from the biological principles as needed.

In recent years, with the “deep learning revolution” (and actually prior to it) [1] it has become
clear that the emergence requirement indicates that these neurocomputational models are often
driven by “learning” from sample data. From this point of view, each model after learning is a
statistical representation of the data. In general, more complex (“deep”’, “convolutional”, “recur-
rent’ and so on) models seem to have an ability to represent more complex data distributions. This
richness in representation seems to be more comprehensive than standard statistics, and perhaps
should be thought of as the statistics of our time. However, this richness comes with a price—these
new Statistics are dependent on having sufficient data. (Occasionally, other “tricks” or method-
ologies are used to somewhat compensate for this lack of data (e.g. drop-out or transfer learning)
and to avoid what is commonly known as the “overfitting” phenomena.)

This is particularly important for many of the cognitive modeling tasks, wherein the data is
usually collected by conduction of complex neurophysiological experiments resulting in
relatively small and high dimensional data sets. This problem is usually called the
curse-of-dimensionality [2] or a small-n-large-p [3]. For example, a typical {MRI experiment
will contain less than 100 participants (i.e. data-points), having at least 30 min with 2 s
sampling rate per participant. Thus, in this case, assuming each fMRI scan contains 40,000
voxels and the raw dimensionality of a single data point is 36 x 10°.

Looked at in this way, the listed papers in this collection can also be seen as essentially
dealing with the problem of computational approaches to biomarker discovery. In this
collection of works, biomarkers could be defined generally as bits of measurable information
whose presence alone, or in a combination with other measures, is indicative of some
phenomenon. However, these biomarkers can sometimes be problematical in the
neurocomputation setting, as these discovered biomarkers, while statistically indicative can
be difficult or impossible to interpret. That is, the direct techniques can be very good in
classification or predictive analysis, but more lacking in explanatory ability. On the other hand,
the flexibility of the artificial neural modeling allows the possibility of testing neurophysio-
logically based connectivity hypotheses in a controlled setting. Observing this hypothesis in
the artificial setting adds (or denies) support to the theory.

These are just some of the issues struggled with and arising in work in this field. In this
collection of articles, we see many of these issues confronted in different settings.

This issue grew out of a special session on Cognition and Neurocomputation organized by
Larry Manevitz, Alex Frid and Bernadette Ribeiro as part of the ICNN/WCII 2018 meeting in
Rio de Janeiro. Some of these papers are substantial expansions of work presented there while
others are completely new papers. All of the papers presented here have undergone the usual
stiff reviewing process of AMAI. Below we give a brief summary of the issues to which each
paper relates.

Analyzing cognitive processes from complex neuro-physiologically based data: some
lessons by Alex Frid and Larry M. Manevitz [4] considers in varying settings, the application

g) Springer
Cognition and Neurocomputation 1121

and adaptation of machine learning techniques specifically to a variety of cognitive tasks.
These include memory retrieval, discovery of biological markers in the speech signal for
Parkinson Disease differential diagnosis (i.e. to level of progression of the disease), biological
markers and classification for Dyslexia diagnosis, visual classification, grammatical identification
during reading) over a variety of biological signals (EEG, ERP, voice signal, fMRI). One of the
main “lessons” is that the relative paucity of data (almost always the case in human cognitive data)
to make subtle distinctions from data can be overcome by emphasizing both the feature selection
and the fusion methods or combining the information from the chosen features.

The machine learning methodologies adaptations put forth include a novel
(“expansion-projection”’) deep autoencoder architecture, the generation of an artificial exhaus-
tive dictionary of potential responses to stimuli, adapting a convolution deep neural network to
the one dimensional raw speech signal (and automatic discovery of “standard” signal process-
ing filters), and boosting methods to find multivariate combinations of univariate chosen
features.

The next paper, Data-driven Koopman operator approach for computational neurosci-
ence by Natasza Marrouch, Joanna Slawinska, Dimitrios Giannakis and Heather L. Read [5],
deals with a specific representation of the EEG data, The show that the use of a “Koopman”
operator on the EEG data (instead of a FFT or Wavelet, for example) can result in a
representation that is good for presenting mechanisms that underlie natural spatial and
temporal coordination of neural activities across various brain areas and during different steps
of cognitive processing.

The two papers Deep learning models for brain machine interfaces by Lachezar Bozhkov
and Petia Georgieva [6] and Human-In-The-Loop active learning via brain computer
interface by Eitan Netzer and Amir B. Geva [7] are interested in Brain-Computer Interfaces,
however from quite different approaches. The first paper’s main approach to obtaining a
quality user-independent encoding of brain-motor encoding is to use a stacked autoencoder,
with each level trained separately. They show how to balance the accuracy/generalizability
conundrum (over different users) in their setting. The second paper shows how one can add a
human supervisor to the loop of learning by combining the appropriate choice of a clustering
algorithm and then selection of an appropriate labeling request to the human. That is, the
system requests a useful new label implementing the concept of “active learning”. Although
used here to make a human in the loop feasible, this concept of clustering and then requesting a
label for an indicative member of the cluster can be adapted to other mechanisms as well.

Learning non-convex abstract concepts with regulated activation networks by R. Sharma,
B. Ribeiro, A. M. Pinto, and F. A. Cardoso [8] deals with an interesting methodology (called
Regulated Activation Networks [9]) appropriate to growing a representation for an arbitrary
“concept”. Here the authors distinguish between “‘convex” and “non-convex” representations.
Given n-dimensional data, they define the basic concepts as being n-dimensional balls chosen,
for example, as clusters, and then develop a way to combine clusters in this space, based on
similarity of the centroids of the clusters. The number of such concepts (both “convex” and
“non-convex’’) are learned from the data; with a heuristic cut-off methodology. The system is
automatic, unlike the Netzer-Geva [7] paper in this issue. They show how this method can be
thought of as analogous to human concept building and apply it to a variety of data bases.

The paper Lattice map spiking neural networks (LM-SNNSs) for clustering and classifying
image data by H. Hazan, D. J. Saunders, D. T. Sanghavi, H. Siegelmann, and R. Kozma [10]
is the latest in a series of works exploring the importance of computational advantage of
adding additional biological features to complex artificial neural networks. In particular, in this

g) Springer
1122 Manevitz L.M., Frid A.

work, the basic artificial model (McCullogh-Pitts) of a neuron is replaced by the computa-
tionally more complex (but still simple from the neuroscience perspective) spiking neurons in
a lattice map. Moreover, the authors explore biologically related adaptations such as inhibition
schedules in global STDP and selective mechanisms in the neurons. Amongst the results is an
adaptation of self-organizing maps, and showing a reduction of computational resources
needed as compared to comparable deep neural networks in a different topological setting.

The final paper Classifying the valence of autobiographical memories from fMRI data
[11] by Alex Frid, Noberto Eiji Nawa and Larry M. Manevitz investigates how to classify from
{MRI data freely retrieved autobiographical memories as to their valence (i.e. “happy” versus
“sad” memories). The method used is a combination of strong univariate feature selection
together with a boosting algorithm. It is interesting to note that in the paper of Ando and Nawa
[12], the authors (who designed the clever experiment and gathered the data), tried an opposite
philosophy of using most of the brain features (under the assumption that memory retrieval is
very dispersed in the brain) and did indeed show that such autobiographical retrieval can be
distinguished from other cognitive tasks (specifically counting backwards). On the other hand,
they could not make the delicate distinguishment of valence. Since that work was on the same
data set as in this paper, this gives an indication of the importance of matching the appropriate
statistical learning methodology to the problem.

Acknowledgements We wish to thank all the referees of these articles, the co-organizer of the special session of
the conference, Prof. Bernadette Ribeiro, the efficient staff of Annals of Mathematics and Artificial Intelligence
and especially the Editor-in-Chief Prof. Martin Golumbic.

References

1. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature. 521(7553), 436-444 (May 2015). https://doi.
org/10.1038/nature 14539

2. Bellman, R.E.: Adaptive Control Processes: a Guided Tour. Princeton University Press, Princeton, New
Jersey (1961)

3. Fort, G., Lambert-Lacroix, S.: Classification using partial least squares with penalized logistic regression.
Bioinformatics. 21(7), 1104-1111 (Apr. 2005). https://doi.org/10.1093/bioinformatics/btil 14

4. A. Frid and L. M, Manevitz, “Analyzing Cognitive Processes from Complex Neuro-Physiologically Based
Data: Some Lessons,” Ann. Math. Artif. Intell., (2019), doi: https://doi.org/10.1007/s10472-019-09669-z

5. N. Marrouch, J. Slawinska, D. Giannakis, and H. L. Read, “Data-Driven Koopman Operator Approach for
Computational Neuroscience,” Ann. Math. Artif. Intell., Nov. (2019), doi: https://doi.org/10.1007
/s10472-019-09666-2

6. L. Bozhkov and P. Georgieva, “Deep Learning Models for Brain Machine Interfaces,” Ann. Math. Artif.
Intell., Oct. (2019), doi: https://do1.org/10.1007/s10472-019-09668-0

7. E. Netzer and A. B. Geva, “Human-in-the-Loop Active Learning Via Brain Computer Interface,” Ann.
Math. Artif: Intell., Mar. (2020), doi: https://doi.org/10.1007/s10472-020-09689-0

8. R. Sharma, B. Ribeiro, A. M. Pinto, and F. A. Cardoso, “Learning Non-convex Abstract Concepts with
Regulated Activation Networks,” Ann. Math. Artif. Intell., Mar. (2020), doi: https://doi.org/10.1007
/S10472-020-09692-5

9. A.M. Pinto and L. Barroso, “Principles of Regulated Activation Networks,” in Graph-Based Representation
and Reasoning, Cham, (2014), pp. 231-244, doi: https://doi.org/10.1007/978-3-319-08389-6_ 19

10. H. Hazan, D. J. Saunders, D. T. Sanghavi, H. Siegelmann, and R. Kozma, “Lattice Map Spiking Neural
Networks (LM-SNNs) for Clustering and Classifying Image Data,” Ann. Math. Artif. Intell, Sep. (2019),
doi: https://doi.org/10.1007/s10472-019-09665-3
11. A. Frid , N. E. Nawa and L. M. Manevitz, “Classifying the valence of autobiographical memories from

fMRI data’, Ann. Math. Artif. Intell. (2020). doi: https://doi.org/10.1007/s10472-020-09705

g) Springer
Cognition and Neurocomputation 1123

12. Nawa, N.E., Ando, H.: Classification of Self-Driven Mental Tasks from Whole-Brain Activity Patterns.
PLoS ONE. 9(5), e97296 (2014). https://do1.org/10.1371/journal.pone.0097296

13. Peleg, O., Manevitz, L., Hazan, H., Eviatar, Z.: Two Hemispheres — two networks: a computational model
explaining hemispheric asymmetries while reading ambiguous words. Ann. Math. Artif. Intell. 59(1), 125—
147

14. McClelland, J.L., McNaughton, B.L., Lampinen, A.K.: Integration of new information in memory: new
insights from a complementary learning systems perspective. Phil. Trans. R. Soc. B. 375, 20190637 (2020).
https://doi.org/10.1098/rstb.2019.0637

Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.

g) Springer
