Multimodal Technologies Za
ee and Interaction (mpPt|

Article
Multimodal Mixed Reality Impact on a Hand
Guiding Task with a Holographic Cobot

Andoni Rivera Pinto 1*©, Johan Kildal 1® and Elena Lazkano 2®

1 TEKNIKER, Basque Research & Technology Alliance (BRTA), C/ Ifaki Goenaga 5, 20600 Eibar, Spain;
johan.kildal@tekniker.es

Computer Sciences and Artificial Intelligence Department, University of the Basque Country /Euskal
Herriko Unibertsitatea (EHU/UPV), 48940 Leioa, Spain; e.lazkano@ehu.es

Correspondence: andoni.rivera@tekniker.es

check for
Received: 17 September 2020; Accepted: 28 October 2020; Published: 31 October 2020 g updates

Abstract: In the context of industrial production, a worker that wants to program a robot using the
hand-guidance technique needs that the robot is available to be programmed and not in operation.
This means that production with that robot is stopped during that time. A way around this constraint
is to perform the same manual guidance steps on a holographic representation of the digital twin
of the robot, using augmented reality technologies. However, this presents the limitation of a lack
of tangibility of the visual holograms that the user tries to grab. We present an interface in which
some of the tangibility is provided through ultrasound-based mid-air haptics actuation. We report
a user study that evaluates the impact that the presence of such haptic feedback may have on a
pick-and-place task of the wrist of a holographic robot arm which we found to be beneficial.

Keywords: human-robot interaction; cobot; augmented reality; mid-air haptics; hologram; multimodal

1. Introduction

Industry 4.0 [1] is the term used for an ongoing fourth industrial revolution, according to which
factories of the future will comprise machinery, warehousing and production facilities in the shape of
cyber-physical systems.

Collaborative robots, also known as ‘cobots’, are one such type of technological assets that interact
and cooperate with workers within the same shared workspace.

In order to tag a robot as collaborative, it needs to meet some key requirements [2]. The first
requirement is trust, which is essential for collaboration and ensures the safety of the human worker
during the task. A robot with this feature also needs to match or improve the efficiency of the task
performance (e.g., the time spent doing the work). Related to trust, it is also necessary for the user to
feel a sense of control. In case of feeling uncomfortable, the human worker must be able to intuitively
stop the robot. Finally, the robot has to be predictable in order to avoid unexpected movements that
could endanger the human worker.

The ability of cobots to be programmed easily by technically naive workers is an important
requirement for their easy adoption by manufacturing industries [3]. ‘Manual guidance’ (the technique
to program a robot by freely moving it by hand within the workspace [4]) is a very useful and widely
used technique to program robots in an easy and technically non-demanding way. This is also known
as programming by demonstration (PbD) [5]. Moving a robotic arm by hand is intuitive for workers,
who are able to transfer their job expertise to the robot without the need of technical knowledge of
robot kinematics or programming languages.

Making use of hand guidance for programming imposes, however, the requirement is that the
robot has to be available (and not engaged in any other automation-related task) for a worker to have

Multimodal Technol. Interact. 2020, 4, 78; doi:10.3390 /mti4040078 www.mdpi.com/journal/mti
Multimodal Technol. Interact. 2020, 4, 78 2 of 21

exclusive access to the robot while programming it for a new task. This non-productive time for robot
re-training has an evident negative impact on productivity.

As a way around this limitation, we propose performing manual guidance on the holographic
representation of the digital twin [6] of the cobot that needs to be programmed, by using current
off-the-shelf mixed-reality (MR) equipment. Creating a hologram of the robot that is only visual,
however, has the limitation that is lacking the tangibility of grabbing and moving a physical robot.
In order to replicate the experience of physical hand-guidance more closely, in this paper we propose
that some aspect of the tangibility of the robot might be reproduced by providing tactile feedback to
the hand that holds the robot. To do so in a seamless way (i.e., without attaching any equipment to the
workers hand), we explore the possibilities offered by a mid-air haptics technology that makes use of
ultrasound-based actuation. The goal is that the spatial and temporal combination of visual and tactile
augmentations may convey a more realistic representation of the robot, supporting manipulation
actions that resemble interacting with the physical robot.

Relevant prior work aiming towards manipulating a hologram does exist in the literature, as it is
reviewed in the next section. Such examples from the state of the art made use of different kinds of
augmented reality and virtual reality devices in order to program a robotic arm movement. One of the
outcomes from such work was the lack of naturalness of the manipulation, which further supports our
approach of creating a visuo-tactile multimodal interface.

In preliminary earlier work [7], we prototyped this concept by creating an application with which
the user could perform a pick-and-place task of the wrist of a holographic robot. This task represents a
part of the hand guidance programming process. We also reported an initial pilot evaluation with a
small group of users, with the aim to obtain early indication of the impact that using haptic feedback
in performing the task might have on task execution and user experience. Five inexperienced users
collaborated in the preliminary study work where we obtained a first impression related to the user
experience and distance error and total time spent in the task values. While results suggested that
the interaction might be promising, that pilot study was non-conclusive, given the small user group
and the limited amount of data collected. In this paper, we report a full study with 16 naive users
performing the same task and procedure, but collecting a larger set of results with the employment of
additional research methodologies. With the goal to obtain more solid insight into performance and
user experience aspects, we employed a broader range of both quantitative and qualitative methods
and metrics.

We monitored and analysed additional quantitative metrics, and we collected subjective
experience data with the use of additional user research methods: extended versions of the validated
SEQ questionnaire and the NASA-TLX index. The same four-question ad hoc questionnaire as in the
previous paper was also still used for comparison (all questionnaires used in the studies are included
in Appendix A) and we monitored some data related to the user performance to compare performance
with visual only feedback and the multimodal experience.

With the present study, we wanted to shed light on two aspects of the interaction proposed.
First, we wanted to know how feasible it was for a user to perform hand guidance of a virtual robot
using off-the-shelf mixed reality devices. The devices that we focused on were (i) a Hololens HMD
device to create visual holograms and (ii) an Ulrahaptics Stratos Explore device to create the tactile
feedback on the user’s hand. In addition, and more specifically, we wanted to learn about the role
that adding ultrasound-based tactile stimulation might play in the user’s performance and in the UX
obtained. Thus, we formulated the following two research questions:

e Isit possible to program a robotic arm through hand guidance using current mixed reality devices?
e What is the impact of the Ultrahaptics Stratos device in manual guidance through the application?

To adapt the experimental task to naive users (with no prior experience collaborating with robots),
we simplified a programming-by-demonstration task to a task of pick-and-place of the wrist of
the robot.
Multimodal Technol. Interact. 2020, 4, 78 3 of 21

In this way, the user could grab the holographic robot end effector, displacing it into a
new position.

The simplification of the task comes from not taking into account the kinematics constraints of
the physical robot, which is an important part in a PbD solution because it defines the rotation and
movement limits of the robot. However, we put the research focus on the impact that the tangibility of
the hologram has during the execution of the task.

The rest of the paper is structured as follows. Section 2 reviews the prior work. Section 3 describes
the design of the experimental user study reported in this paper. In the Section 4, we describe the
experimental user study procedure and the results are reported in Section 5. These results are analysed
in Section 6. Finally, in Section 7, we present the conclusions and future work.

2. State of the Art

As we know it today, augmented reality almost exclusively addresses the visual channel, with
some examples of auditory augmentation as well. However, as far back as 1962, the concept of multiple
sensory modalities was proposed by Morton Heilig’s Sensorama [8]. This device is one of the first
machines related to the multimodal virtual reality (VR), with stereoscopic 3D view, stereo sound,
wind and smell effects and mobile seat.

In 2002, visual virtual reality was introduced in medicine [9] in order to train and, thus, improve
surgeon’s skills. During the following years, a number of studies continued to focus on this area [10-12].

Head-mounted display (HMD) technology has advanced in these recent years, allowing the
representation of complex 3D scenes rendered in real time and with a good image quality and frame
ratio per second.

Oculus Rift [13], PlayStation VR or HTC Vive are examples of HMDs which allow a realistic
immersive simulation of a virtual world.

Regarding augmented reality HMDs, Google presented in 2012 the advanced prototype
“Google Glasses” in 2012, a device which had the aim of coexisting in the society. In 2015 Google
decided not to sell their device commercially to the public and, in 2017, they presented a new release
aimed for business and industrial environment.

Human-robot interaction (HRI) has made profilic use of augmented reality, as seen in recent
review publications, such as in [14], which identifies a roadmap for the field. In the same way, in [15],
a review of advanced robot programming approaches is presented where the authors explain the
current augmented reality-based programming approaches.

Augmented reality and virtual reality fit perfectly in the manufacturing industry as support
for operators. A good example is found in Makris et al. [16] for the programming of welding robots,
or in [17] for industrial robots supported by virtual reality. Using different methodologies, algorithms
and tools like in [18], it is possible to improve training, programming, maintenance and process
monitoring. Defining fiducial markers in the 3D world [19] that are directly linked to the real robot
close spots working with augmented reality or defining the path that the robot has to follow [20]
could also simplify the difficulty of the task. Related to the safety requirements for human-—robot
collaboration in industrial settings, in [21], authors present a depth-sensor based model for workspace
monitoring and an interactive AR user interface for safe human-robot collaboration.

Through these kinds of augmented reality devices, the virtual representation of a robot makes
the safety factor easy to comply with. Moreover, due to the possibility of overlapping additional
information over the real world, it is possible to improve time to complete the task using marks and
labels [22]. In 2017 [23], Ni et al. developed an augmented reality system that mixes haptic and visual
experiences for the programming of welding robots using a haptic PHANToM device. In this case,
authors used a screen for visual augmented reality.

Puljiz et al. [24] presented a work on programming a collaborative robot using Microsoft
Hololens mixed reality HMD with a KUKA KR-5 collaborative robot and ROS as an operating system.
Luebbers et al. [25] presented a novel augmented reality interface for the visualization and directed
Multimodal Technol. Interact. 2020, 4, 78 4 of 21

control of robot skill learning from demonstration (LfD). Rosen et al. [26] proposed a mixed-reality
HMD visualisation of the intended robot motion, allowing users also to adjust the goal pose of the end
effector through hand gestures.

In addiction, [27-29] are examples of systems in which systems that use mixed reality HMD were
used for the interactive programming of industrial robots.

A limitation common to all the prior work is the lack of haptic feedback that can increase the
realism of the pick-and-place task. An exception is [23], where the autors did use a PHANToM haptic
device, but the visual experience was limited to a screen. If the application does not contain predefined
target points to perform the task, a naive user would have difficulties to move the robot and have
spatial perception. Moreover, we decided to use a mid-air haptic device as an alternative.

With the currently available mixed reality (MR) head-mounted displays, the capability of
representing a hologram of a robot and its workspace allows creating a better context to perform
a task [30]. With hand tracking technologies, the worker could perform the pick-and-place task
in a similar way as with a physical robot. In the examples from the literature mentioned above,
the movement is done in the air without grabbing any physical object, and consequently with a lack
of tangibility, which could be addressed by using a haptic device, making the execution of the task
potentially more intuitive and natural [31]. In the work presented here, the haptic device selected is an
Ultrahaptics Stratos device, which is able to present tactile sensation on the skin in mid-air, without
the intrusion of a physical interface.

The main goal of the present study was to combine the use of a haptic device with a MR
virtual environment to measure the feasibility and the improvement introduced by the Ultrahaptics
Stratos device.

3. Design of the Experimental User Study

The goal was to investigate the research questions mentioned above, regarding the programming
of the robots through manual guidance of the holographic representation of its digital twin, in the
sub-task of performing a pick-and-place operation in the context of a virtual scene and robot.

For this implementation, we used a Microsoft Hololens mixed reality device to provide visual
augmented reality. This device consists of 2 transparent lenses in 16:9 aspect ratio, sensors including
an inertial measurement unit, depth camera, environment understanding cameras and microphones
among others, and input/output connections (Bluetooth, Wi-Fi, etc.) which allow communication
between this head-mounted display with the haptic device. As mentioned, tactile feedback was
generated using an Ultrahaptics Stratos Explore ultrasound actuator [32]. This option of mid-air
rendering of tactile sensations was deemed more appropriate than alternatives such as haptic gloves,
or an ‘AIREAL’ [33] air vortex emitter device. The Ultrahaptics Stratos device consists of 256 ultrasound
transducers that are combined to create tangible sensations on specific points in space that the hand
occupied at each moment in time. It presents the advantage that it does not require wearing any
physical device on the hand, although it has the limitation that the interaction space is small (up to
less than 70 cm above the actuating panel). This device integrates a Leap Motion hand tracker that
permits the localization of the user’s hand within the interaction space. Through the position of the
hand and the capacity to create local air disturbances, it is possible to create a range of sensations on
the user’s skin. When the user places his/her hand in the interaction region, a holographic hand is
superimposed with the real hand, which corresponds to the reconstruction of the user’s hand as it is
tracked by the Leap Motion device.

Using these devices and the Unity cross-platform game engine, we created a 3D
environment which simulated a collaborative robot that moved within the interaction space of the
Ultrahaptics device.

The setup to run the application is shown in Figure 1. A PC managed the creation of the 3D
environment and, with a script, it also collected all the data during the task execution. The Ultrahaptics
Stratos Explore device was connected through a USB port to the same PC. The Microsoft Hololens
Multimodal Technol. Interact. 2020, 4, 78 5 of 21

HMD device was connected to the application through WiFi connection in order to render the 3D
visual representation of the scenario.

   
   
 
  

Ethernet cable

2 USB cables Ultrahaptics Stratos
PC
Explore

for Leap Motion (haptic + Leap Motion)
and haptic matrix

Microsoft Hololens

(AR Visual) Unity Application

    
   

Figure 1. Setup scheme to run the application with the connected devices.

Figure 2 shows an overview of the interactive scene. Through the visual holograms, the haptic
device can be distinguished as a rectangular panel placed on a desk. Above the haptic device,
the hologram of a 6 degrees of freedom robot is shown. A sphere is displayed as the wrist of the
robot, which the used could grab and move by hand for the execution of the pick-and-place task.
As to the end effector, the cobot had a two-finger gripper. For this experimental study, when the
program was launched, the grasping tool was always pointing towards the target, which was the centre
of the handle (marked in red) of a holographic tool box located on the left side of the Ultrahaptics
Stratos device. In Figure 2, we can also see a linear slider (composed by a white bar and a small blue
box which indicates the rotation angle), with which the user could rotate the gripper through the
Z axis. This slider was not used during the study. The user could view this scene from any angle,
obtaining any perspective that could aid with the task.

 

Figure 2. Overview of the scene through Microsoft Hololens AR glasses.

When the user’s hand entered the scene, a holographic version of his/her real hand appeared,
superimposed with it. This was indication that the system was tracking the hand successfully. If the
hand stopped being detected by the Leap Motion device, the holographic hand disappeared from
the scene. Once in the scene, the hand could grab the spherical wrist of the robot. If grabbed
successfully, the user felt a continuous tactile sensation on his/her hand, and the sphere (plus the
whole kinematic chain of the robot) would follow the user’s hand as it displaced. If for any reason the
sphere dropped from the hand, the tactile sensation disappeared, and the robot would not displace
following the hand anymore. The tactile sensation served, thus, as a non-visual confirmation that the
robot was grabbed by the hand. To voluntarily release the sphere, the user extended the fingers of
the hand, opening it completely.
Multimodal Technol. Interact. 2020, 4, 78 6 of 21

The interaction with the scene also includes voice commands in order to stop (stop robot) or
resume (move robot) the mobility of the robot. When the user activated the ‘stop robot’ voice command,
the user could not move it even if he/she tried to grab the end effector. In order to allow the robot
movement again, the user had to activate the ‘move robot’ voice command. This was the mechanism
used to carry out different repetitions of the pick-and-place task during the study. Each time the
movement was activated, the target toolbox appeared in a different place in the scene, traversing
three possible position in a triangle. Activating and deactivating the robot in this way would be
used, during the study, for a participant to initiate and finish each execution of the experimental task.
The decision to use voice commands was made because working with interactive menus with a narrow
field of view would be more damaging than beneficial. Menus take up a lot of vision space that the
current Hololens device version, with a narrow field of view, cannot fit in easily without occluding the
scene and diverting the user’s attention from the main objective. With a ‘show help’ voice command,
a list of all the available voice commands was displayed in text next to the robotic arm.

4. User Study

A group of 16 inexperienced participants volunteered to take part in the study. Most of the
participants (12) were in the 18-30 age range, two participants in 31-50 age range and another two
participants in the 51-65 age range. Before starting the study, all participants were notified that their
performance data would be collected and that they should complete a questionnaire. They were
informed that they could drop out at any point if they decided to do so, and that any data collected up
to that point would be discarded. No instance of a drop out took place.

As we Stated before, the objective of this study was to answer the research questions proposed:

e Isit possible to program a robotic arm through hand guidance using current mixed reality devices?
e What is the impact of the Ultrahaptics Stratos device in manual guidance through the application?

To investigate these questions, we designed a single factor, repeated measures user study in
which the factor was the presence of tactile feedback from the Ultrahaptics Stratos device (i.e., in one
condition the user would feel tactile feedback while grabbing the robot, but not in the other condition).
Participants executed the pick-and-place task three times in each condition (more details about the
task are provided below). The order of presentation of the conditions (interacting with tactile feedback
in the first or in the second place) was fully counterbalance between the participants.

The research methodology employed included the measurement and assessment of a substantially
larges set of variables that were used in the prior pilot study [7]. During the study, data were collected
by two means: (i) validated and ad hoc questionnaires filled in by each participant at different
points during the session, and (ii) observed (i.e., objective) data about the interaction logged by the
computer system managing the session. The second category (logged objective observations) included
time to complete the task, the number of times the user’s hand lost grasp of the sphere, the total
amount of time spent moving the cobot, the distance error between the gripper and the target in the
final positioning, the ratio between the time moving the cobot and the total amount of time spent on
the task. Participants thus generated two kinds of results. The quantitative variables which had been
collected during the users executions, and qualitative variables collected through the questionnaires.

Taking into account that the users performed the task in two conditions (with and without
tactile feedback), and in order to answer the research questions, any differences between conditions
found in the data from this controlled study would be attributed to the effect of the condition.

Statistically, we analysed quantitative data using t-tests [34] as well as the Wilcoxon test [35],
depending on the normality of the data distribution (for which we used the Shapiro—Wilk test). All data
analysis and production of figures were done using the R language [36]. The p-value used was 0.05.

The quantitative metrics mentioned could not capture differences in UX and other
subjective aspects. For this reason, we also make use of research methods that participants would
assess or respond to subjectively. These included ‘Raw NASA-TLX’ [37] and ‘SEQ’ [38] questionnaires.
Multimodal Technol. Interact. 2020, 4, 78 7 of 21

The Raw NASA-TLX [39] was extended with the categories “sense of control” and “physicality”
(i.e., the in what degree it was perceived that the hologram grabbed with the hand was physical).
We added these extensions because we wanted to analyse these two aspects explicitly. They were,
of course, not added in the calculation of the TLX Task Load Index. Similarly, the SEQ questionnaire
was also extended with an additional category, “satisfaction with the result obtained” on the same
scale format. The preliminary pilot study [7] did not make use of any of these methodologies, but it
did include a brief ad hoc questionnaire. For the sake of comparison of results between the preliminary
pilot study and this full study, we did also include the same ad hoc questionnaire in this case.

Each participant started with filling in a demographic questionnaire that included prior experience
with augmented /mixed/virtual reality. Whether or not they had experience with those technologies
and related devices, all the participants underwent a 15 min training session where they learned how to
interact with the devices and the 3D scene in order to perform the experimental task. Training included
sampling different examples of haptic feedback from the Ultrahaptics device, as well as different
kinds of interaction with Leap Motion and the elements in the scene. One of these interactions was
moving a sphere. This task was the same one they completed later with the collaborative robot’s end
effector but, in this case, the interaction was out of any context.

In the experimental part of the study, participants performed the hand guiding task with the
holographic cobot. The goal of the task was to grab by the hand the sphere on the wrist of the robot
and to bring it to the holding point on the handle of the tool box (marked in red), thus indicating
to the robot where it should place its end effector in order to lift up the tool box (see in Figure 3).
The lifting of the toolbox did not follow as part of the study. Participants were asked to trade-off
between speed of execution and accuracy: they were instructed to carry out the task in as short an
amount of time as possible, providing a positioning of the end effector that was as accurate as possible.
However, there was no time limit for each execution. To execute this task, the participant said the
voice command ‘move robot’, which started the task execution timer. The participant then moved
his/her hand to the ball, grabbed it (curving down the five fingers on the hand, as if surrounding
the sphere with them) and dragged it (keeping the hand in that same semi-closed position), until the
end effector had been brought to the target position next to the tool box. The user could release the
ball (by opening the hand) and grab it again repeatedly as desired, e.g., to achieve a best positioning.
In the moment that the user thought that the tool was pinching correctly the tool case mark (i.e.,
when the participant was satisfied with the execution of the task), the user stopped dragging the
end effector, released the sphere (by opening the hand) and indicated that the execution had ended
with the “stop robot” voice command, which stopped the timer and logged the performance values
corresponding to that execution.

Each participant made three executions of the task with tactile feedback from the Ultrahaptics
Stratos haptic device and another three executions without any tactile feedback. As stated before,
the order of the conditions was fully counterbalanced between participants. Half of the participants
started with the haptic feedback enabled, while the other half started without any haptic feedback.
After every execution, the participants completed the extended version of the single ease question
(SEQ) that asked about the task difficulty in each execution and about satisfaction obtained with each
execution. Both categories made use of the same 7-point Likert scale, so that providing first-impression
responses (which was the aim of this post-task questionnaire) as easy as possible. After participants
performed the three executions of a condition, they completed the post-condition raw NASA-TLX
questionnaire [39] that rated, for that condition, the perceived workload in order to assess the task.
In the same way as with the SEQ test, we extended this NASA-TLX with two additional categories:
perceived control and physicality. These two values are not standard in the NASA-TLX, and do not
contribute to the calculation of the index of this test. However, we included them because they might
capture a difference between the conditions.
Multimodal Technol. Interact. 2020, 4, 78 8 of 21

With the purpose of knowing more precisely the participants’ opinions, we also administered the
same post-study ad hoc questionnaire as in the preliminary pilot study, with the same four questions
on a 7-point Likert scale (see details about the questionnaire in the Appendix A of this paper).

 

Figure 3. User performing the pick and place task grabbing the holographic end effector.
5. Results

The results shown below contain the early work variables as well as the new measured variables.
First we show the quantitative variables, which are distance error, time spent on the task, time moving
the cobot, hologram movement time ratio and number of times the user lost the ball. Then, the results
related to the questionnaires are shown (extended SEQ and extended RAW NASA TLX).

5.1. Quantitative Results

These key variables can measure objectively if there exists a real difference in the task performance.
These variables are related to the distance, the time and the times that the user paused his/her activity.

5.1.1. Distance Error

The distance error refers to the distance measured between the target point, and the gripper
tool center point (TCP) is shown in Figure 4. Both data sets appeared to be similarly distributed,
with apparently small differences between distributions ((My+y = 2.706 mm, SDy44 = 2.351 mm);
(My = 2.259 mm, SDy = 1.998 mm)).

Distribution of distance error commited (meters)

0.009

0.006

0.003

 

0.000

V+H V

Figure 4. Data distribution of distance error made with haptic feedback (V = H) and without haptic
feedback (V).
Multimodal Technol. Interact. 2020, 4, 78 9 of 21

The Wilcoxon test gave a p-value greater than 0.05 (0.4183), which means that there is no significant
difference between the mean distance error in both cases.

5.1.2. Time Spent on the Task

We analysed 3 values related to the time needed to complete the task. The first one measures the
full time spent on an execution since the user first grabbed the robotic hologram of the end effector of
the robot, until the user released it.

When we checked the results through the box plots, we found an outlier value from User 11.
This value corresponds to the first execution with tactile feedback and the first try of this user during
the whole of his/her participation. The time value that we considered as an outlier is 307.2 s, the gap
with the next higher value (69.5) is 237.7. Consequently, we also discarded the task execution value of
that same participant value without tactile feedback. With 47 executions in total for each condition
(instead of 48), we obtained the data obtained in Figure 5, left, showing similar distributions in both
conditions ((Myin = 24.027, SDyiq = 21.483); (My = 26.614, SDy = 27.473)).

The Wilcoxon test returned a p-value (p = 0.962), which showed no statistically significant
difference between conditions.

Distribution of total time spent (Seconds) Distribution of time moving the cobot (Seconds)
150 150
e@
e
100 100
e@
e@
e@
e@
50 50 8

 

V+H V V+H V

Figure 5. Box plots of time spent during the execution and time moving the cobot with (V + H) and
without (V) haptic feedback.

5.1.3. Time Moving the Cobot

The second time-related variable analysed is the total amount of time the user spent moving the
hologram with the hand (i.e., discarding the time from the execution during which the hologram was
not being moved). Like with the total amount of time spent on the task, we omitted the first executions
of User 11 in each condition. In Figure 5, right, shows in a box plot the distribution of the data
obtained. Also, in this case, the distributions from each condition were very similar ((My+y = 16.338,
SDy+H = 12.638); (My = 16.242, SDy = 11.747)) and far from significant according to a Wilcoxon test
(p = 0.987).

5.1.4. Hologram Movement Time Ratio

The last variable related to time that we analysed is the proportion between the time that the user
was actually moving the robot and the total amount of time spent on the task. With this information we
Multimodal Technol. Interact. 2020, 4, 78 10 of 21

wanted to know if the percentage of time with the holographic cobot grew using the Ultrahaptics Stratos
Explore device. The inspection of the data represented in Figure 6 suggested that the distributions
were very similar ((My+y = 0.748, SDyiy = 0.165); (My = 0.715, SDy = 0.197)).

Distribution of the ratio between time moving the cobot and the time spent on the task

1.00

0.75

 

0.50

0.25

0.00

V+H Vv

Figure 6. Distribution of the number of times users lost grip of the holographic ball in each execution
with and without haptic feedback.

In this case, we used the t-test because of the normal distribution of both data sets. The obtained
p-value is 0.578, suggesting there was no difference in this case either.

5.1.5. Lost Ball

We analysed the number of times the user’s hand lost grip of the ball that was fetched to drag
the hologram (leaving the hologram behind immobile until it was fetched again). We got the results
plotted in Figure 7.

Number of times the user lost the robot control

30 e e

20

10

 

V+H V

Figure 7. Distribution of the number of times users lost grip of the holographic ball in each execution
with and without haptic feedback.
Multimodal Technol. Interact. 2020, 4, 78 11 of 21

Like with the previous parameters, the distributions of the data obtained were very similar in both
conditions ((My+H = 6.681, SDy.4 = 7.553); (My = 7.319, SDy = 8.577)), and no statistically significant
difference was obtained from a Wilcoxon test (p = 0.698), suggesting that the presence of tactile
feedback might not influence the number of times the hologram dropped off from the user’s hand.

Summarising all the above analysis, none of the observed quantified variables showed significant
differences between conditions.

The following two sub-sections present subjective data captured and quantified with two standard
questionnaire-based methods, extended SEQ questionnaires and the 210 extended NASA-TLX test.

5.2. Qualitative Results

Now, we are going to analyse the users’ opinions extracted from the extended SEQ test,
the extended RAW NASA-TLX test and the customized questionnaire.

5.2.1. SEQ—Task Difficulty

The Single Ease Question is a test that consists of only one question about the difficulty of the task
that the user has performed in the test. After each task execution, users register on a 7-point Likert
scale how easy they found it to execute that task (three times for each test condition) where a low
value means high difficulty and high value means a lower difficulty. Figure 8 shows the distribution of
the answers provided. The Wilcoxon test (p = 0.147) found no perceived difference in difficulty that
was Statistically significant. The distribution of the obtained data shows the similarity ((My+ = 6.291,
SDy+H = 1.031); (My = 6.104, SDy = 0.973)).

SEQ Difficulty answers distribution

  

V+H V

Figure 8. Distribution of users’ answers about the task difficulty after each execution, where a high
value represents a difficult task and low value an easy task.

5.2.2. SEQ—Satisfaction with the Result

We extended the SEQ questionnaire with a second question that participants responded to in the
same way (same Likert scale) and immediately after the first one. In this scale, participants rated how
satisfied they were with the result they had obtained in that task execution, meaning the higher the
value, the better the performance. Results are plotted in Figure 9. In this case, the data distribution
Multimodal Technol. Interact. 2020, 4, 78 12 of 21

obtained is [((My.y = 6.354, SDy.y = 6.0); (My = 0.887, SDy = 1.167)], and the Wilcoxon test returned a
value (p = 0.033), corresponding to a statistically significant difference between series.

SEQ Satisfaction with the result

 

4 e
2 e
V+H V

Figure 9. Distribution of participants’ answers about the satisfaction with the obtained result.

5.2.3. NASA-TLX

The index obtained from the “Raw NASA-TLX (Task Load Index)” questionnaire is a measure of
the work load experienced by participants when executing the experimental task. The questionnaire
consists of six dimensions, all of which are rated on a 21-point (0 to 20) Likert-like scale: mental
demand, physical demand, temporal demand, performance level achieved, effort expended and
frustration experienced. Keeping the same format, we added two dimensions to the questionnaire:
sense of control and perceived physicality. The sense of control dimension has been used as part of
an extended TLX questionnaire (in e.g., [40]), and is highly relevant in the task chosen in this study.
We added also the perceived physicality [41] dimension to assess the subjective degree of realism that
tactile feedback might be adding to the manipulation of the hologram. To calculate the index, we used
the raw version of NASA-TLX [39], which does not make use of weighted pairwise comparisons.

Figure 10 shows the distribution of the Raw NASA-TLX values obtained from the 16 users,
suggesting a lower TLX with tactile feedback ((Myiy = 5.646, SDysy = 3.999); (My = 6.271,
SDy = 4.484)). However, after performing the t-test, we obtained a p-value greater than 0.05 (0.096),
which means that no significant difference between the mean of both conditions was found from
our data. Still, we went on to examine each dimension of TLX individually, to see which dimension(s)
the difference in mean TLX value originated from.

Figure 11 plots the data distribution of the six constituent dimensions of TLX.

As a result, we found a highly significant statistical difference in the temporal demand
(p-value < 0.01). Participants reported a lower temporal demand with tactile feedback from the
Ultrahaptics Stratos explore device.

In addition, regarding the perception of control while carrying out the task, data from the
experiment revealed ((My+y = 14.625, SDy,4 = 4.470); (My = 12.25, SDy = 4.524)) a difference that was
also statistically significant in their mean difference (p-value = 0.04), with perception of control being
superior when the user performed the task with tactile feedback from the Ultrahaptics Stratos Explore
device (Figure 12).
Multimodal Technol. Interact. 2020, 4, 78 13 of 21

Task load indexes distribution

20

—_

V+H V

   

o

Figure 10. Distribution of the task load indexes of the task performance with and without haptic

 

   

feedback respectively.
Mental demand Physical demand Temporary demand
20 20 20
15 15 15
10 10 10
5 5 5
0 V+H V 0 V+H V 0 V+H V
Effort expended Performance achieved Frustartion experienced
20 20 20
e
15 15 15
10 10 10
5 5 5 =
0 V+H V 0 V+H V 0 V+H V

Figure 11. Distributions of responses in both conditions (with and without tactile feedback) in the six
scales that form the NASA-TLX questionnaire. Notice that, in all cases, the polarity of the scales reflect
a better outcome the lower the values, including the performance scale.

Control sensation data distribution

20
15
10

5

V+H V

Figure 12. Distribution of the answers to the control sensation while performing the task.
Multimodal Technol. Interact. 2020, 4, 78 14 of 21

Similarly, perceived physicality increased when the users performed the task with haptic
feedback as opposed to without it. This difference (p-value = 4.751 x 104) can be seen in Figure 13
((Myin = 15.437, SDyiy = 6.375); (My = 3.521, SDy = 4.843)).

Physicality distribution data

20

10
5

V+H V

 

Figure 13. Distribution of the answers to the physicality while performing the task.
5.2.4. Ad Hoc Questionnaire

Finally, we administered the same ad hoc questionnaire as in the preliminary study [7].
This questionnaire consisted of four statements that the users had to answer to on a Likert 7-point
scale, indicating whether they agreed with them (3) or not (—3):

e Qi—With tactile feedback, I have perceived a certain advantage to carry out the task.

e Q2—Thaven’t done complete the task faster when I felt the ball.

e Q3—TI have achieved better accuracy with tactile feedback.

e 4 —The perception that I was handling the robot was the same with and without tactile feedback.

The distribution of the answers is shown in Figure 14.

Subjective perception of condition comparison

2 e
1 °
0 e
-1 °
-3
Q2 Q3 Q4

Q1

 

  

 

Figure 14. Results of the subjective opinion from the participants about the multimodal robot manipulation task.
Multimodal Technol. Interact. 2020, 4, 78 15 of 21

Some consensus was shown for Q1 and Q4, with opinions regarding Q3 and Q4 distributed
around the neutral region of the scale (with a rather even split of opinions). Thus, for Q1, 13 out of the
16 users perceived quite a clear advantage in carrying out the task with tactile feedback. As for Q4,
participants largely supported the opinion that handling the hologram with or without tactile feedback
felt different.

6. Discussion of the Results

With the results obtained, we can evaluate the impact of the presence of tactile feedback from an
Ultrahaptics Stratos device on the task performance and on the user experience.

Regarding the objective quantitative metrics measured from the participants’ task executions
(distance error, time spent on the task, net time grabbing the ball, ratio between both time values,
number of times the participant lost the ball), we observed that the effect of introducing tactile
feedback was small and, in fact, we found no statistically significant differences between conditions.
An immediate conclusion that can be drawn from this is that the presence of the tactile feedback from
an Ultrahaptics did not affect performance on the task of the experiment, either positively or negatively.

In contrast, with the analysis of quantified subjective data obtained through the various
questionnaires administered, some results emerged that showed positive effects from the presence of
tactile feedback, with no negative effects detected. Starting with the extended SEQ questions (ease of
task execution and satisfaction with result obtained), mean values of the distributions suggested
positive effects from the presence of feedback, although those differences were not found to be
statistically significant. A similar trend was found for the TLX index and for its six constituent
dimensions, where, in all cases, mean values of response distributions were numerically lower with
tactile feedback, suggesting positive effects from its presence. Statistical comparison of distributions
showed that the difference was significant in the case of temporal demand, where it was reported to be
lower (with a high level of statistical significance) when tactile feedback was present. This same trend
showing a positive effect was also present in the categories that extended the NASA-TLX questionnaire
(not included in the calculation of the TLX index): sense of control and perceived physicality. In both
cases, not only were distribution means higher in the condition with tactile feedback, but the differences
were statistically significant in both cases, with a particularly high level of significance in the case of the
perceived physicality. Further confirmation of the positive impact from the presence of tactile feedback
was found in the data collected from the ad hoc questionnaire (the four statements that participants
could agree or disagree with). Regarding the two questions gathering most consensus in the responses,
O4 showed that a clear difference was perceived when handing the hologram with or without tactile
feedback and, according to the broad consensus around Q1, the presence of tactile feedback offered an
advantage for the execution of the experimental task.

We hypothesize that the positive impact of having tactile feedback (reflected in the data from
several of the metrics) might have been due to the effectiveness of feedback as a mechanism to confirm
to the participant that he/she was grasping the hologram, as long as the feedback was felt. If such
grasp got lost and the hologram was left behind in the process of dragging it, participants could
notice immediately the change in sensation on the palm of their hand and react quickly to fetch the
hologram again and resume the task. This could account for the significant differences found in
lowering temporal demand, improving the sense of control, and providing an enhanced perception
of physicality of the hologram. This interpretation of the results was reinforced by discussions with
the participants about their experience during the study, who provided comments stating that it
was helpful to receive tactile feedback, mostly because they could know better if they were holding
the robot at each moment or not. Based on the data presented above, our interpretation is that the
presence of tactile feedback was fulfilling the expectation of participants to be feeling in their hands the
(virtual) object that they were holding. Tactile feedback increased the naturalness of that experience,
and the sensation felt was reassuringly familiar, paving the way for an interaction that did not need to
be learned.
Multimodal Technol. Interact. 2020, 4, 78 16 of 21

7. Conclusions and Future Work

In response to the first research question in the introduction, this paper presents the
implementation of a functional demonstrator based on a Hololens head mounted display, with which
a user can manipulate (move) a holographic robot, for the execution of a pick and place task,
achieving positioning precision that remains under 3 mm. This functionality is a building block
of interactions in more complex programming-by-demonstration scenario. The second research
question (impact of tactile feedback from mid-air haptics actuators on a pick and place task) motivated
the main contribution of this paper. The paper reports a user study in which one such task was
performed by participants in two conditions, with and without tactile feedback. As discussed above, the
results obtained suggest that feedback does not affect observed performance (no significant numerical
differences were recorded, although the trend was for better scores obtained with tactile feedback).

As for the qualitative results, subjective scores from participants on a set of questionnaire-based
methods employed supported also the trend that tactile feedback was noticeable and with an impact
that was positive. The strongest evidence of that was found on the reduction of temporal demand,
improved sense of control, enhanced perceived physicality of the hologram, plus the consensus among
participants that the noticeable effect introduced by tactile feedback gave an advantage in the successful
execution of a pick-and-place task of a holographic robot arm.

Building on the results obtained in this paper, our next steps are focused on implementing
a programming-by-demonstration method and scenario, based on the handing of a holographic
robot arm. We will reproduce a realistic context in which a real robot can reproduce the procedure
demonstrated to the hologram. Alongside this process, we will investigate further haptic actuation
techniques and multimodal interaction that can lead to improved performance and user experience, as
well as individual the impact of each device used in the interaction task.

Author Contributions: Conceptualization, A.R.P., J.K. and E.L.; methodology, A.R.P. and J.K.; software, A.R.P.;
validation, A.R.P., J.K. and E.L.; formal analysis, A.R.P. and J.K; investigation, A.R.P. and J.K.; resources, J.K.;
data curation, A.R.P.; writing—original draft preparation, A.R.P.; writing—review and editing, J.K. and E.L.;
supervision, J.K. and E.L; project administration, J.K. and E.L. All authors have read and agreed to the published
version of the manuscript.

Funding: This research received no external funding.
Acknowledgments: We would like to thank the people who participated in this study for the time they dedicated.

Conflicts of Interest: The authors declare no conflict of interest.

Appendix A. Evaluation Questionnaire

All the data related to this paper can be found in the following link: https://drive.google.com/
drive/folders/12HaSSw9czind]jjUhUptYFsSgGqphuofn?usp=sharing.
Multimodal Technol. Interact. 2020, 4, 78

DEMOGRAPHIC QUESTIONNAIRE

Gender: Male

Age:

<18 18-30

Female

31-50

Previous experience with Augmented Reality: Yes _  No__

- AR games:

OOaadgg

Pokemon Go

Invisimals

Geocaching

Fombies, Run!

Indoor decoration apps
Other:

- AR devices:

oO

Ood

- Other:

Google Glass
Hololens
Magic Leap
Other:

Previous experience with Virtual Reality: Yes No

OOadadg

Figure Al. Demographic questionnaire some data about the participant.

VR visor for smartphones
Oculus Rift

HTC Vive

PlayStation VR

Virtual Boy

Other:

Extended SEQ
With Ultrahaptics — 1* try

Very difficult © Oo Oo O Oo Oo

Not satisfied

Figure A2. Extended SEQ. The user had to fill out one of these papers for each execution.

Perform this task was:

Satisfaction with the obtained result:

O oO oO oO O O

51-65

© Very easy

65

© Very satisfied

17 of 21
18 of 21

Multimodal Technol. Interact. 2020, 4, 78

Extended NASA-TLX

Mental demand

o o0 8 0 6 69 6 O&O 6 © High

oo GC Oo 0 6 lhU68mUmUCUC OUCOCUCUCOOWUCUOD
o o© Oo o9 80 89 6 6 GO 8 8 High

o o© 6 0 8 69 8 © 6

Low
Lev

0

Physical demand

o 09 89 © © © 8 ©& © © High

oo 600000006 6 6
6 © 0 0 © © 0 GO O © © High

o 60 6 0 68 6 6 6 6

Low
Litt

0

Temporary demand

o o© 0 60 6 6 6 60 6 © © High

6 © 0 0 2 © 0 © O © © High

o 09 9 0 6 6 80 8 6 6
o o 8 0 8 609 ©@ © 6

Low
Lic

0

quired effort

o 0 89 © 8 &® 8 ©& © © High

oo 6000 000 6 6 6
6 © 6 0 © 6 6 G GO © © High

o 60 6 0 6 6 6 6 6

Low
Lids

0

Performance

a

o o®0 © 60 6 6 6 © 6 ©8 © Low

High © © O© 60 6 6 O@ GO 6 6

o 46 © 0 0 © 0 © 6 6 oO © Low

High © © OO © 6 60 8 6 Oo

Frustation level

o 0 89 0 6 0&9 6 O&O 6 © High

oo 80 0000 00 0 9
© 60 0 © © 0 © O © © High

o o© 6 60 6 6 6 6 6

Low
Litt

0

a

o Oo Oo O80 0 0 6

o o © © © High

o Oo 6 6OlUOOmlUCUOWUCUCOC

low o © 6 6 6 6 6 6 6

low © 0

o 0 8 0 6 69 6 O&O 6 © High

o 9 © 09 6 09 09 9 G6 & © © High
°

o 0 969 0 6 6 6 0 6 6
o o© o®© 0 68 6 6 6 6

Low
Levey

6 © 60 0 © © 0 © O © © High

0

Figure A3. Extended RAW NASA TLX questionnaire. The users had to complete one row after

the three executions with haptic feedback. The same procedure after the three executions without

haptic feedback.
Multimodal Technol. Interact. 2020, 4, 78 19 of 21

Custom questionnaire

€Not agree Agree?
With tactile feedback, | have perceived O Oo O O O O O
certain advantage to carry out the task.
| haven't done the task faster when | felt
the ball.
| have achieved better accuracy with O
tactile feedback.
The perception that | was handling the
robot was the same with and without © oO Oo O oO O O
tactile feedback.

O O O O O O O

Figure A4. Customized questionnaire. The users had to complete it at the end of the evaluation.

References

1. Henning, K. Recommendations for Implementing the Strategic Initiative Industrie 4.0; Forschungsunion: Berlin,
Germany, 2013.

2. Michalos, G.; Makris, S.; Tsarouchi, P.; Guasch, T.; Kontovrakis, D.; Chryssolouris, G. Design considerations
for safe human-robot collaborative workplaces. Procedia CIRP 2015, 37, 248-253. [CrossRef]

3. Kildal, J.; Tellaeche, A.; Fernandez, I.; Maurtua, I. Potential users’ key concerns and expectations for the
adoption of cobots. Procedia CIRP 2018, 72, 21-26. [CrossRef]

4. Massa, D.; Callegari, M.; Cristalli, C. Manual guidance for industrial robot programming. Ind. Robot Int. J.
2015, 42, 457-465, [CrossRef]

5. Billard, A.; Calinon, S.; Dillmann, R.; Schaal, S. Survey: Robot programming by demonstration. In Handbook
of Robotics; Springer: Berlin/Heidelberg, Germany, 2008; Volume 59.

6. El Saddik, A. Digital Twins: The Convergence of Multimedia Technologies. IEEE Multimed. 2018, 25, 87-92.
[CrossRef]

7. Rivera-Pinto, A.; Kildal, J. Visuo-Tactile Mixed Reality for Offline Cobot Programming. In HRI ’20:
Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction; Association for
Computing Machinery: New York, NY, USA, 2020; pp. 403-405, [CrossRef]

8. Heilig, M.L. Sensorama Simulator. U.S. Patent 3,050,870, 28 August 1962.

9. Seymour, N.; Gallagher, A.; Sanziana, R.; O’Brien, M.; Vipin, B.; Andersen, D. Virtual Reality Training
Improves Operating Room Performance. Ann. Surg. 2002, 236, 458-463. [CrossRef] [PubMed]

10. Gallagher, A.G.; Ritter, E.M.; Champion, H.; Higgins, G.; Fried, M.P.; Moses, G.; Smith, C.D.; Satava, R.M.
Virtual reality simulation for the operating room: Proficiency-based training as a paradigm shift in surgical
skills training. Ann. Surg. 2005, 241, 364. [CrossRef] [PubMed]

11. Grantcharov, T.P.; Kristiansen, V.; Bendix, J.; Bardram, L.; Rosenberg, J.; Funch-Jensen, P. Randomized clinical
trial of virtual reality simulation for laparoscopic skills training. Br. J. Surg. 2004, 91, 146-150. [CrossRef]
[PubMed]

12. Aggarwal, R.; Ward, J.; Balasundaram, I.; Sains, P.; Athanasiou, T.; Darzi, A. Proving the effectiveness of
virtual reality simulation for training in laparoscopic surgery. Ann. Surg. 2007, 246, 771-779. [CrossRef]
[PubMed]

13. Luckey, P. Oculus Rift. 2012. Available online: https://en.wikipedia.org/wiki/Oculus_Rift (accessed on 30
October 2020).

14. Makhataeva, Z.; Varol, H.A. Augmented Reality for Robotics: A Review. Robotics 2020, 9, 21, [CrossRef]

15. Zhou, Z.; Xiong, R.; Wang, Y.; Zhang, J. Advanced Robot Programming: A Review. Curr. Robot. Rep. 2020,
[CrossRef]

16. Makris, S.; Karagiannis, P.; Koukas, S.; Matthaiakis, A.S. Augmented reality system for operator support in
human-robot collaborative assembly. CIRP Ann. 2016, 65, 61-64, [CrossRef]

17. Burghardt, A.; Szybicki, D.; Gierlak, P.; Kurc, K.; Pietrus, P.; Cygan, R. Programming of Industrial Robots

Using Virtual Reality and Digital Twins. Appl. Sci. 2020, 10, 486, [CrossRef]
Multimodal Technol. Interact. 2020, 4, 78 20 of 21

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.
35.

36.

37.

38.

39.

Andersson, N.; Argyrou, A.; Nagele, F.; Ubis, F; Campos, U.E.; de Zarate, M.O.; Wilterdink, R. AR-Enhanced
Human-Robot-Interaction—Methodologies, Algorithms, Tools. Procedia CIRP 2016, 44, 193-198, [CrossRef]
Pettersen, T.; Pretlove, J.; Skourup, C.; Engedal, T.; Lokstad, T. Augmented reality for programming
industrial robots. In Proceedings of the Second IEEE and ACM International Symposium on Mixed and
Augmented Reality, Tokyo, Japan, 10 October 2003; pp. 319-320.

Ong, S.; Yew, A.; Thanigaivel, N.; Nee, A. Augmented reality-assisted robot programming system for
industrial applications. Robot. Comput. Integr. Manuf. 2020, 61, 101820, [CrossRef]

Hietanen, A.; Pieters, R.; Lanz, M.; Latokartano, J.; Kamdarainen, J.K. AR-based interaction for human-robot
collaborative manufacturing. Robot. Comput. Integr. Manuf. 2020, 63, 101891, [CrossRef]

Akan, B.; Ciiriiklui, B. Augmented reality meets industry: Interactive robot programming. In Proceedings
of the SIGRAD 2010: Content Aggregation and Visualization, Vasteras, Sweden, 25-26 November 2010;
Link6éping University Electronic Press: Link6ping, Sweden, 2010; pp. 55-58.

Ni, D.; Yew, A.W.W.;; Ong, S.K.; Nee, A. Haptic and visual augmented reality interface for programming
welding robots. Adv. Manuf. 2017, 5, [CrossRef]

Puljiz, D.; Stohr, E.; Riesterer, K.S.; Hein, B.; Kroger, T. Sensorless Hand Guidance using Microsoft Hololens.
In Proceedings of the 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),
Daegu, Korea, 11-14 March 2019; pp. 632-633.

Luebbers, M.B.; Brooks, C.; Kim, M_J.; Szafir, D.; Hayes, B. Augmented Reality Interface for Constrained
Learning from Demonstration. In Proceedings of the 2nd International Workshop on Virtual, Augmented,
and Mixed Reality for HRI (VAM-HRI), Daegu, Korea, 11-14 March 2019.

Rosen, E.; Whitney, D.; Phillips, E.; Chien, G.; Tompkin, J.; Konidaris, G.; Tellex, S. Communicating and
controlling robot arm motion intent through mixed-reality head-mounted displays. Int. J. Robot. Res.
2019, 38, 1513-1526. [CrossRef]

Ostanin, M.; Klimchik, A. Interactive Robot Programing Using Mixed Reality. IEAC-PapersOnLine
2018, 51, 50-55, [CrossRef]

Rickert, P.; Meiners, F.; Tracht, K. Augmented Reality for teaching collaborative robots based on a physical
simulation. In Tagungsband des 3. Kongresses Montage Handhabung Industrieroboter; Schtippstuhl, T., Tracht, K.,
Franke, J., Eds.; Springer: Berlin/ Heidelberg, Germany, 2018; pp. 41-48.

Rosen, E.; Whitney, D.; Phillips, E.; Chien, G.; Tompkin, J.; Konidaris, G.; Tellex, S. Communicating
robot arm motion intent through mixed reality head-mounted displays. In Robotics Research; Springer:
Cham, Switzerland, 2020; pp. 301-316.

Brooks, F.P. Grasping Reality Through IllusionéMdash;Interactive Graphics Serving Science. In CHI ’88:
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems; ACM: New York, NY, USA, 1988;
pp. 1-11, [CrossRef]

Ikits, M.; Brederson, J.D. The Visual Haptic Workbench. In Visualization Handbook; Hansen, C.D., Johnson,
C.R., Eds.; Butterworth-Heinemann: Burlington, MA, USA, 2005; pp. 431-447, [CrossRef]

Carter, T.; Seah, S.A.; Long, B.; Drinkwater, B.; Subramanian, S. UltraHaptics: Multi-point mid-air haptic
feedback for touch surfaces. In Proceedings of the 26th Annual ACM Symposium on User Interface Software
and Technology, St Andrews, UK, 8-11 October 2013; pp. 505-514.

Sodhi, R.; Poupyrev, I.; Glisson, M.; Israr, A. AIREAL: Interactive tactile experiences in free air.
ACM Trans. Graphics 2013, 32, 134. [CrossRef]

Student. The probable error of a mean. Biometrika 1908, 6, 1-25. [CrossRef]

Wilcoxon, F. Individual comparisons by ranking methods. In Breakthroughs in Statistics; Springer:
Berlin/ Heidelberg, Germany, 1992; pp. 196-202.

R Core Team. R: A Language and Environment for Statistical Computing; R Foundation for Statistical Computing;
R Core Team: Vienna, Austria, 2018.

Byers, J.; Bittner, A.; Hill, S. Advances in Industrial Ergonomics and Safety; Taylor & Amp: London, UK, 1989;
pp. 481-485.

MeasuringU: 10 Things To Know About The Single Ease Question (SEQ). Available online: https://
measuringu.com/seql0/ (accessed on 19 September 2020).

Hart, S.G. NASA-task load index (NASA-TLX); 20 years later. In Proceedings of the Human Factors and
Ergonomics Society Annual Meeting; Sage Publications: Los Angeles, CA, USA, 2006; Volume 50, pp. 904-908.
Multimodal Technol. Interact. 2020, 4, 78 21 of 21

40. Kildal, J.; Lucero, A.; Boberg, M. Twisting touch: Combining deformation and touch as input within the
same interaction cycle on handheld devices. In MobileHCI ’13: Proceedings of the 15th International Conference
on Human-Computer Interaction with Mobile Devices and Services; Association for Computing Machinery:
New York, NY, USA, 2013; pp. 237-246. [CrossRef]

41. Reeves, S. Physicality, spatial configuration and computational objects. In First International Workshop
on Physicality; Lancaster University: Lancaster, UK, 2006; Volume 94.

Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional
affiliations.

@) © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
(cc) article distributed under the terms and conditions of the Creative Commons Attribution
BY

(CC BY) license (http: //creativecommons.org/licenses/by /4.0/).
