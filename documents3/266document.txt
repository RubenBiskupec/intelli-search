Ducange et al. J Big Data (2020) 7:19
https://doi.org/10.1186/s40537-020-00298-6

© Journal of Big Data

SURVEY PAPER Oy oT-Ta waa -55 4

. oi, ®
An overview of recent distributed chek for

algorithms for learning fuzzy models in Big Data
classification

Pietro Ducange!, Michela Fazzolari? and Francesco Marcelloni'

 

*Correspondence:
pietro.ducange@unipi.it Abstract
] ° ° ° ° . . .
saements el tee Nowadays, a huge amount of data are generated, often in very short time intervals

zione, . . . .
Lucio Lazzarino, 1, 56122 Pisa, and in various formats, by a number of different heterogeneous sources such as social
Italy networks and media, mobile devices, internet transactions, networked devices and
Full list of author information sensors. These data, identified as Big Data in the literature, are characterized by the
is available at the end of the . . . .
article popular Vs features, such as Value, Veracity, Variety, Velocity and Volume. In particular,

Value focuses on the useful knowledge that may be mined from data. Thus, in the last
years, a number of data mining and machine learning algorithms have been proposed
to extract knowledge from Big Data. These algorithms have been generally imple-
mented by using ad-hoc programming paradigms, such as MapReduce, on specific
distributed computing frameworks, such as Apache Hadoop and Apache Spark. In the
context of Big Data, fuzzy models are currently playing a significant role, thanks to their
capability of handling vague and imprecise data and their innate characteristic to be
interpretable. In this work, we give an overview of the most recent distributed learning
algorithms for generating fuzzy classification models for Big Data. In particular, we first
show some design and implementation details of these learning algorithms. Thereafter,
we compare them in terms of accuracy and interpretability. Finally, we argue about
their scalability.

Keywords: Big Data, Fuzzy models, Data mining, Classification algorithms, Distributed
computing

 

Introduction

In the Big Data Era |1], a huge Volume of information is generated at very high speed.
In most cases, such data are collected from different sources, may have different formats
(Variety) and need to be elaborated in almost real time (Velocity) [2]. This is the so-called
three-V’s model of Big Data and it has been used for the first time by Douglas Laney in
2001 [3], to describe the data management in three-dimensions. This original three-V
paradigm is still valid, but it has been recently enriched by additional Vs. In fact, Big
Data may be poorly accurate or truthful (Veracity). Moreover, the added- Value that the
analysis of Big Data may offer is already exploited in several contexts such as industrial
applications [4], marketing strategies [5], Cloud Computing and Internet of Things [6, 7],
and health care [8].

. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
GO) Springer O pen adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
— the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material
in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco

mmons.org/licenses/by/4.0/.
Ducange et al. J Big Data (2020) 7:19 Page 2 of 29

Dealing with Big Data to extract useful knowledge and value is not a trivial task. This
is mainly due to volume, diversity, noisiness, redundancy and complexity features, which
characterize this kind of data. In particular, due to their huge volume, it is impossible
to load all the data into the memory of a single machine. This prevents the execution of
classical sequential algorithms, including data mining and machine learning procedures
[9].

To this aim, novel distributed implementations of data mining and machine learning
algorithms for Big Data have been proposed, mainly based on the MapReduce paradigm
[10]. For example, Ludwig in [11] and Kim et al. in [12] discuss the design, the imple-
mentation and the experimentation of distributed clustering algorithms. As regards clas-
sification algorithms, very interesting results, in terms of accuracy and scalability, have
been discussed by Bechini et al. in [13] and by Maillo et al. in [14]. Highlights on the
recent advances, challenges and objectives in designing, developing and using data min-
ing and machine learning algorithms for Big Data can be found in the work of Zhou et al.
discussed in [15].

As stated before, the classical data storage and elaboration paradigms are not suitable
for handling Big Data. Thus, in the last years practitioners and researchers have experi-
mented new distributed frameworks, specifically developed for large-scale data storage
and processing over a large number of computers, called nodes, which communicate
over a network. Nodes interact in order to achieve a common goal, i.e. to solve a prob-
lem or give insights on a set of data. Each node is an independent unit, with its own CPU
cores, memory and network interface. In distributed computing the components are
located on these networked computers and communicate and coordinate their actions
through messages. Some important characteristics of distributed systems are concur-
rency of components, lack of a global clock and independent failure of components [16].
The most popular distributed frameworks to manage Big Data are Apache Hadoop [17]
and Apache Spark [18], which are described in detail in "Ihe MapReduce paradigm and
distributed computing frameworks" section.

The strategies presented so far are useful to address the Big Data issues connected with
Volume, Velocity and Value. On the other hand, other issues arise due to data Variety
and Veracity, as already mentioned above. To deal with these additional problems, Fer-
nandez et al. in [19] and Hariri et al. in [20] highlighted that fuzzy models are particularly
suitable for handling the variety and veracity of Big Data. Indeed, fuzzy models are based
on the concept of fuzzy logic, which is a many-valued logic in which the truth value of a
variable may assume any real number between 0 and 1. Fuzzy logic is often exploited to
express the concept of partial truth, in contrast with the Boolean logic, which assumes
that the truth values of a variable may only be 0 and 1. Thus, fuzzy models have the
ability to deal with data and information that are vague, uncertain and imprecise. These
characteristics are often typical of Big Data.

Since 2014, a number of contributions on fuzzy models for Big Data have been pro-
posed in the literature, focusing on different application fields. Most of them regard the
design, implementation and experimentation of fuzzy models for classification [21-30].
There also exist some contributions regarding regression tasks [31-33] and descriptive
models, such as fuzzy clustering [34—36], subgroup discovery [37] and the generation of
fuzzy association rules [38].
Ducange et al. J Big Data (2020) 7:19 Page 3 of 29

In this paper, we aim to give an overview of distributed algorithms for learning
fuzzy models from Big Data, focusing in particular on classification applications. We
describe and discuss the most relevant algorithms designed and implemented for
generating fuzzy classification models, specifically Fuzzy Rule-Based Classifiers and
Fuzzy Decision Trees. We focus on the works in which actual big datasets were used
in the experiments. As expected, only distributed versions of algorithms for generat-
ing fuzzy models were actually able to deal efficiently with dataset sizes larger than at
least 0.5 GB. Thus, in our analysis we considered the distributed versions of the fol-

lowing algorithms:

¢ The Chi et al. algorithm for generating Fuzzy Rule-Based Classifiers (FRBCs) [21-
24].

« Fuzzy Associative Classifiers (FACs) [26].

¢ Evolutionary Fuzzy Classifiers (EFCs) [27—29].

« Fuzzy Decision Trees (FDTs) [25, 30].

In order to evaluate the performance of the aforementioned algorithms, we selected
four popular Big Data classification datasets, whose sizes span up to 8 GB, and ran the
algorithms on these datasets in a computer cluster located at the University of Pisa.
We compared the achieved results considering not only the accuracy of the models,
but also their interpretability. Indeed, although most of the discussed fuzzy classifiers
are very accurate, their complexity, in terms of number of rules or number of nodes
of the fuzzy trees, is very high. As discussed by Gacto et al. in [39], the greater the
complexity, the lower the interpretability. Interpretability is a very important feature
that characterizes fuzzy models, and assumes a special significance in the context of
Big Data, as stated by Fernandez et al. in [19] and by Wang et al. in [40]. Finally, we
discuss some scalability properties of the different distributed learning algorithms.

In conclusion, the main objectives and motivations which support this overview are:

¢ To describe and discuss the most relevant algorithms designed and implemented
for generating fuzzy classification models, specifically Fuzzy Rule-Based Classifi-
ers and Fuzzy Decision Trees;

« To highlight both the strengths and the weaknesses of the discussed algorithms;

¢ To allow the reader to appreciate the fast evolution of theses algorithms, in terms
of both their design schemes and frameworks used for implementing and experi-
menting them in classification tasks for big data;

« To compare these algorithms in terms of the performance obtained on a set of
selected real big datasets;

« To give the opportunity to future works of exploiting the discussed approaches
in specific engineering and technological applications, by focusing on the most
promising ones;

¢ To suggest possible improvements of the discussed algorithms.

The paper is organized as follows. First, we present the MapReduce paradigm and
some recent distributed computing frameworks for handling Big Data. Then, we
Ducange et al. J Big Data (2020) 7:19 Page 4 of 29

describe preliminary concepts about fuzzy systems and we introduce the different
fuzzy classification models considered in this paper. Moreover, we briefly analyze the
most relevant state-of-the-art sequential algorithms for learning fuzzy classification
models. The central sections of the manuscript describe the details of the distributed
fuzzy versions of these classification models. The last part of the manuscript includes
three sections: in the first section, we discuss the experimental results in terms of
accuracy and interpretability. In the second section we point out an in-depth analytic
discussion and give some envisions on future research directions. Finally, in the last

section, we draw some conclusions.

The MapReduce paradigm and distributed computing frameworks
In this section, we provide a snapshot of the MapReduce paradigm and the main frame-
works employed in distributed data mining.

The issues related to Big Data require the adoption of new strategies for elaborating
such data. To this aim, the MapReduce programming paradigm [10, 41] was introduced
and adopted by Google in 2004, becoming the de facto standard for dealing with Big
Data analysis. The MapReduce paradigm has been proposed to process huge volumes
of data in a scalable way, according to a divide and conquer strategy, that consists in
breaking down a problem into simpler sub-problems of the same type. ‘The solutions to
the sub-problems are then combined in some way to provide a solution to the original
problem.

At high level, the paradigm divides the computational flow into two main phases,
namely Map and Reduce, organized around (key, value) pairs.

When the MapReduce execution environment runs a user program, it automatically
partitions the data into a set of independent chunks that can be processed in parallel by
different nodes. In the Map phase, each task is fed by one chunk of data and, for each
(key, value) pair as input, it generates a list of intermediate (key, value) pairs as output.
In the Reduce phase, all the intermediate results are grouped together according to a
key-partitioning scheme, so that each Reduce task processes a list of values associated
with a specific key as input for generating a new list of values as output. Developers are
able to implement parallel algorithms by simply defining Map and Reduce functions. The
result of the whole MapReduce process is a set of (key, value) pairs produced by all the
executed Reduce tasks.

In the last years, several projects have been developed to deal with distributed data
storage and elaboration. The most popular is Apache Hadoop [17],' which is an open
source project created in 2005 and currently maintained by a global community of con-
tributors. The Hadoop framework consists of several modules, among which the most

important ones are:

¢ A distributed storage system, called Hadoop Distributed File System (HDFS), which

supports the storage of large datasets on distributed nodes in a cluster;

 

' Apache Hadoop, https://hadoop.apache.org/, accessed: December 2019.
Ducange et al. J Big Data (2020) 7:19 Page 5 of 29

« A parallel processing engine, called Hadoop MapReduce, which was originally
intended to implement the MapReduce paradigm, whereas currently it supports
multiple processing schemes;

¢ A set of libraries and utilities, included in Hadoop Common and used by other
Hadoop modules;

« A resource-management platform, called Hadoop Yet Another Resource Negotiator
(YARN), developed to manage computer resources in clusters and to use them for

scheduling users’ applications.

As regards data mining tools for Big Data, the library Apache Mahout [42] can be con-
sidered as the main contribution based on Apache Hadoop. Mahaout has been the first
attempt of producing free implementations of distributed and/or scalable machine
learning algorithms for Big Data. It includes many algorithms for machine learning
tasks, such as classification, clustering and pattern discovery.

Although Hadoop is widely spread, it suffers from various limitations that make it not
suitable for certain types of applications. In particular, it supports batch processing only
and employs a two-stage disk-based MapReduce computation engine, which makes it very
inefficient in managing real-time data processing and iterative algorithms. In the recent
years, to overcome these limitations, Apache Spark and Apache Flink have been proposed.

Apache Spark has been developed to overcome the limitations of Hadoop. It is an
open-source distributed general-purpose cluster computing framework with mostly in-
memory data processing engine. This key feature allows running most of the computa-
tions in memory. Thus, Zaharia et al. in [18, 43] proved that Apache Spark performs
faster than Hadoop at least in some specific applications, where the use of iterative algo-
rithms or interactive data mining is required. The core of Spark is the Resilient Distrib-
uted Dataset (RDD). RDD is an immutable distributed collection of objects, partitioned
across nodes in the cluster, that can be operated in parallel.

Spark is based on a master/slave architecture. A Spark application runs a set of inde-
pendent processes that use as input the RDD: it consists of a coordinator, the Driver, that
communicates with multiple distributed workers, the Executors. The Driver is in charge
of processing the user’s main function and activates the tasks, which are distributed to
the Executors. Executors run the individual tasks in parallel and send the results back to
the Driver.

The most popular machine learning library running on Spark is the MLIib library [44],
which implements several machine learning and data mining algorithms for clustering,
classification, regression, recommendation systems, pattern mining, etc.

Spark is not real-time, but near real-time. This is one of its main limitations, together
with problems in dealing with small files, the lack of a dedicated file management system
and high memory consumption to run in-memory.

Apache Flink [45]® tries to overcome these issues. It reduces the complexity, which
has been faced by other distributed data-driven frameworks, by integrating query
optimization, concepts from database systems and efficient parallel in-memory and
out-of-core algorithms, with the MapReduce paradigm.

 

2 Apache MLIib, http://spark.apache.org/mllib/, (accessed: December 2019).
> Apache Flink, https://flink.apache.org/, accessed: September 2019.
Ducange et al. J Big Data (2020) 7:19 Page 6 of 29

 

  
   

be1=Ap2 Chy=bp2=a-3 Ch2=be3

 

 

Fig. 1 An example of a strong fuzzy partition
L

Fuzzy classification models: preliminary concepts, architectures and classical
learning algorithms

In this section, we briefly introduce some preliminary concepts regarding fuzzy parti-
tions and the classification model structure of Fuzzy Rule-Based Classifiers (FRBCs),
Fuzzy Associative Classifiers (FACs) and Fuzzy Decision Trees (FDTs). Moreover,
we also provide an overview of the classical sequential model learning methods. Let
X = {Xj,...,XF} be the set of attributes and Xf be the output of a fuzzy classifi-
cation model. The output Xr is a categorical variable assuming values in the set
I = {Cj,...,Cx,...,Cx} of K possible classes Cx. Let Uy, with f = 1,...,F, be the uni-
verse of the fth attribute X;. In the following, we assume to have available a training

set TS = {(X1,%F41,1)---» (XN, XF+41,N)} composed of N input-output pairs..

Fuzzy partitions

Most of the approaches discussed in the next sections employ fuzzy partitions. Let
Pr = {Apir-- + Ap,jr-»-»Ap,T,} be a partition of X¢ consisting of Ty fuzzy sets Af;.
Although there exist several types of fuzzy partitions, strong fuzzy partitions [46] are
widely used, because they require few parameters for their definition, thus simplifying
the modeling process. A strong fuzzy partition is an ordered collection of fuzzy sets,

such that:
Ty
Wa ec Xp: S- Ap j(*) =1 (1)
j=l

In Fig. 1, we show an example of a strong fuzzy partition composed of three triangular
fuzzy sets Ay ;, whose membership function is defined by the tuples (a, ;, by ;, cy), where
af and cy; correspond to the left and right extremes of the support of A,,, and by to the
core.

The approaches described in the following adopt strong fuzzy partitions of the
attributes.

Fuzzy Rule-Based Classifiers
An FRBC includes a Rule Base (RB), a Data Base (DB) containing the definition of the
fuzzy sets used in the RB, and a reasoning method. An RB is composed of M rules

expressed as:

RmAF X] is Ajj,,, AND...AND Xf is AF,,, -

THEN Xp 41 is Cj, with RWn (2)
Ducange et al. J Big Data (2020) 7:19 Page 7 of 29

where C;,, is the class label associated with the mth rule, and RW, is the rule weight, i.e.,
a certainty degree of the classification in the class C;,, for a pattern belonging to the sub-
space delimited by the antecedent of rule R,,.

Given an input pattern x € R*, being R* an F-dimensional real space, the strength of
activation (matching degree of the rule with the input) of the rule R,, is usually computed
as:

F
Wn(®) = | [Ap ing Ep) (3)
f=l

where A; ;_, f (xf) is the membership value of xp associated with the fuzzy set A,, re In
this case, we have considered the product as t-norm for implementing the logical con-
junction in the antecedent of the rule expression in (2).

Two different definitions of rule weight RW,, are commonly found in the literature
[47]:

1. The certainty factor:

x, ECin Wm (xz)

CFEy, = ; (4)
yo Wm (Xt)
2. The penalized certainty factor:
» _ Wm(Xt)
PCF, = CFm — 2x EGim (5)

ya Wm (Xt)

As regards the algorithms discussed in the following, we highlight that the different ver-
sions of the distributed Chi et al. algorithm adopt the penalized certainty factor, while
the Distributed Fuzzy Associative Classifiers adopt the certainty factor. Finally, the dis-
tributed multi-objective evolutionary classifiers adopt no weights. A specific reasoning
method uses the information from the RB and DB to determine the class label for a given
input pattern. Details on different types of rule weights and reasoning methods used in
the literature can be found in the contribution of Cordon et al. in [47].

The RB and the DB of an FRBC can be generated adopting different algorithms, such
as the Chi et al. algorithm [48] and the Antonelli et al. evolutionary-based algorithms
[49].

Specifically, the RB design process aims to determine the optimal set of rules for man-
aging the classification problem. The DB design process consists of finding the appropri-
ate number of fuzzy sets for each attribute and their parameters. The objective of the
design process is to concurrently maximize the classification accuracy and, possibly, the
model interpretability.

As regards the Chi et al. algorithm [48], it is one of the first heuristics adopted for
generating the RB of an FRBC: given a pre-defined DB describing the fuzzy partitions of
each attribute, the algorithm generates a rule for each training pattern. The antecedent
of a rule is generated considering the list of fuzzy sets, which have been activated by a

certain training input pattern with the highest membership degree. The consequent is
Ducange et al. J Big Data (2020) 7:19 Page 8 of 29

directly specified by the class of the training pattern. Duplicated rules are removed and
appropriate strategies have been defined for handling rules with the same antecedent
and different consequents and for associating a weight with each rule.

As discussed by Fernandez et al. in [50], Evolutionary Fuzzy Systems (EFSs) are well
known hybrid models, which exploit evolutionary algorithms (EAs) for learning the
parameters of fuzzy models. EAs are able to solve optimization tasks by imitating some
aspects of natural evolution [51]. Learning the RB and the DB of an FRBC can be con-
sidered as an optimization process, where the accuracy of the final model is usually the
fitness function to be optimized. However, in the last decades, interpretability of the
fuzzy models has been often taken into account concurrently with the accuracy, leading
to the so-called Multi-Objective Evolutionary Fuzzy Systems (MOEFSs): more details on
MOEFSs can be found in the works of Ducange et al. and of Fazzolari et al. [52, 53].
MOEFSs adopt multi-objective evolutionary algorithms [54], which aim to concurrently
maximize both the accuracy and the interpretability during the evolutionary learning
process of fuzzy models. In the last decade, several MOEFSs have been successfully
experimented for selecting (Ishibuchi et al. in [55]) or learning (Cococcioni et al. in [56])
the set of rules, for optimizing the fuzzy partitions (Botta et al. in [57]) and for concur-
rently learning the RB and the DB of FRBCs (Antonelli et al. in [49] and Fazzolari et al. in
[58]). We recall that MOEFSs return a set of solutions characterized by different trade-
offs between accuracy and interpretability.

Interpretability regards the capability of explaining how decisions have been taken,
using terms understandable to humans. Thus, the simplicity of the fuzzy reasoning
method, adopted to deduce conclusions from facts and rules, assumes a special impor-
tance. Moreover, the intepretability is strictly related to the transparency of the model,
namely to the capability of understanding the structure of the model itself. Fuzzy mod-
els, especially FRBCs, can be characterized by a high transparency level, whenever the
linguistic RB is composed of a reduced number of rules and conditions and the fuzzy
partitions have a good integrity. The integrity of fuzzy partitions depends on some
properties, such as coverage, distinguishability and normality [59]. Several measures
have been proposed in the specialized literature for evaluating the interpretability of an
FRBC, taking into consideration semantic and complexity aspects of both the RB and
the DB (check the contribution of Gacto et al. in [39]).

Fuzzy Associative Classifiers
As discussed by Baralis et al. in [60] and by Abdelhamid et al. in [61], Associative Classi-
fiers (ACs) integrate a frequent pattern mining algorithm and a rule-based classifier into
a single system. Specifically, first, frequent patterns are extracted from the dataset using
an appropriate mining algorithm. In the classification context, a pattern consists of set
of items, where one item is a class. Thereafter, classification rules are generated from the
frequent patterns, and pruned according to their support, confidence, and redundancy.
In the fuzzy context, for each fuzzy partition Py of an attribute X;, the single item is
defined as the couple (X;, Ay,;), where A,,; is the j-th fuzzy set defined in Py.

The mth Fuzzy Classification Association Rule (FCAR,,) out of the Rule Base
RB = {FCAR},..., CAR} is expressed as
Ducange et al. J Big Data (2020) 7:19 Page 9 of 29

FCAR:FAnty > C;,, with RWin (6)

where the consequent Cj, is the class label selected for the rule, the antecedent FAnt,,
is equal to the antecedent of the rule expression in (2) and RW,,, is the weight of the
rule. Thus, the FCARs that can be generated by using a fuzzy association rule mining
approach are the same as the ones of classical FRBCs. Examples of classification mod-
els based on FCARs can be found in [62, 63]. In [62], Alcala et al. discuss the use of a
fuzzy version of the Apriori algorithm [64] for generating an initial set of FCARs. Then, a
single-objective genetic algorithm is adopted for selecting the most relevant rules along
with the optimization of the fuzzy set parameters. In [63], Segatori et al. introduce a
fuzzy extension of the well known FP-Growth [65], which allows them to quickly mine a
set of FCARs characterized by a high confidence level. In order to reduce the number of
rules in the final RB, an FCARs pruning process, based on redundancy and training set
coverage, is also applied after the first mining step.

Fuzzy Decision Trees

A decision tree is a directed acyclic graph, where each internal (non-leaf) node denotes
a test on an attribute, each branch represents the outcome of the test, and each leaf (or
terminal) node holds one or more class labels. The topmost node is the root node. In
general, each leaf node is labeled with one or more classes Cy € I’ with an associated
weight wz: weight w;, determines the strength of class Cz, in the leaf node [66]. Given a
training set TS, the structure of a decision tree, in terms of nodes, branches and leaves,
is usually generated using a recursive scheme. First of all, one of the attributes is selected
in the decision node corresponding to the root, taking the overall TS into considera-
tion. The attribute selection algorithm returns also a set of branches and corresponding
nodes. For each node, a new attribute is selected from the set of the attributes, consider-
ing only the instances of the TS, which satisfy the test associated with the branch. When
no attribute can be selected, the node is denoted as a leaf node. The attribute selection
algorithm is usually based on a specific metric, such as Gini Index and Information Gain.
More details regarding decision trees can be found in the contribution of Quinlan in
[66].

In [67], Altay et al. discuss some fuzzy extensions of the classical ID3 and SLIQ algo-
rithms [68]. Recently, an incremental algorithm for learning FDT, based on the concept
of Fuzzy Hoeffding Bound and Fuzzy Information Gain, was presented by Pecori et al. in
[69]. Some preliminary experiments have been discussed by the authors: very promising
results in the context of data streams classification have been achieved.

In this paper, we adopt the distributed implementation proposed by Segatori et al. in
[25] of an algorithm for learning a decision tree based on fuzzy information gain: each
attribute is preliminarily partitioned by using strong fuzzy partitions. Figure 2 shows an
example of multi-way FDT, in which we consider two attributes. Each attribute is parti-
tioned with three fuzzy sets. A test branch is always generated for each fuzzy set of the
input variable involved in a test node.

Once the tree has been generated, a given unlabeled instance x is assigned to a class
C, €T by following the activation of nodes from the root to one or more leaves. In
classical decision trees, each node represents a crisp set and each leaf is labeled with a
Ducange et al. J Big Data (2020) 7:19 Page 10 of 29

 

 

 

 

 

 
  

FINAL RB

 

FUZZY PARTITIONS
Fig. 3 MapReduce scheme of the Chi-FRBCS-BigData algorithm
Ne

 

 

unique class label. It follows that x activates a unique path and is assigned to a unique
class. In FDT, each node represents a fuzzy set. Thus, x can activate multiple paths in the
tree, reaching more than one leaf with different strengths of activation, called matching
degrees. Details on how determine the output class label of a given unlabeled instance,
using an FDT, can be found in the work of Segatori et al. [25].

Local and global implementations of the distributed Chi et al. algorithm

Lopez et al. discussed in [22] the first attempt of extending an algorithm for generat-
ing FRBCs for Big Data to a distributed computing venue. Here, the authors discuss the
design and the implementation of a distributed version of the well-known Chi et al. algo-
rithm [48]. This algorithm has been developed adopting the MapReduce programming
paradigm under the Hadoop framework. Figure 3 gives a snapshot of the implementa-
tion scheme: according to the MapReduce paradigm, the training dataset is split into
chunks which feed the mappers. Each mapper generates an RB from the specific chunk
of the training data, using the classical procedure of the Chi et al. algorithm. Then, a
single reducer combines these RBs for generating the final RB. Two strategies have
been proposed for solving the problem of conflicting rules: both strategies search for
rules with the same antecedent. For each set of rules with the same antecedent, the first
Ducange et al. J Big Data (2020) 7:19 Page 11 of 29

 

<ant,cons>

>

 

<ant, list(cons)>

INITIAL RB
NO WEIGHTS

FUZZY PARTITIONS
Fig. 4 MapReduce scheme of the CHI_BD Algorithm: Stage 1
Ne

 

 

approach retains the rule with the highest weight. The second one calculates the aver-
age weight of the rules that have the same consequent. Finally, the rule with the high-
est average weight is kept in the final RB. The algorithm discussed above is labeled as
Chi-FRBCS-BigData and represents a /ocal distributed implementation of the Chi et al.
algorithm. Indeed, each mapper generates an RB using only the subset of instances pro-
cessed by that specific mapper. Thus, the rule weights widely depend on the proportion
and distribution of the classes in the specific subset of training instances. Different RBs
can be generated considering different training dataset partitions and different number
of mappers. Lopez et al. and Fernandez et al., also experimented the Chi-FRBCS-Big-
Data considering imbalanced classification datasets and analyzing the effects of different
granularities of the fuzzy partitions, respectively, in [23, 70].

Recently, an optimized version of the distributed Chi et al. algorithm, denoted as CHI_
BD, has been proposed by Elkano et al. in [24]. The optimization regards both the gen-
eration of the rules and the architecture of the distributed execution scheme. Figures 4
and 5 resume the MapReduce stages adopted for the improved implementation of the
distributed Chi et al. algorithm.

In Stage 1, an initial RB, which can contain rules with the same antecedent and dif-
ferent consequents, is generated without rule weights. Each mapper generates a pair
< antecedent, consequent > (< ant,cons > in Fig. 4) for each pattern included in its
own TS chunk. The pairs generated by all the mappers feed the reducers, which group
together all the pairs for generating the initial RB without weights.

Stage 2 generates the final RB, composed of weighted classification rules. To this aim,
each mapper loads its training data chunk and also the initial RB generated in Stage 1.
Each mapper calculates the matching degree of each training pattern of the chunk. In
the reduce phase, for each rule, a reducer sums up the matching degrees generated for
the specific rule by the different mappers. Thereafter, the weights for each consequent
are calculated, ensuring that the overall TS contributes to their values. Only the conse-
quent associated with the highest weight is retained in the final RB.

CHI_BD represents the global counterpart of the local implementation discussed by
Lopez et al. in [22]. Indeed, as stated before, the CHI_BD algorithm ensures that the rule
Ducange et al. J Big Data (2020) 7:19 Page 12 of 29

 

<rule,, matching degrees>

    

FINAL RB

 

INITIAL RB
NO WEIGHTS

Fig. 5 MapReduce scheme of the CHI_BD Algorithm: Stage 2
Ne

 

 

weights are calculated considering the overall T'S and that their values do not depend on
the data partitions and on the number of adopted mappers. Also the CHI_BD algorithm
has been developed under the Hadoop framework.

Distributed Fuzzy Associative Classifiers

The recent contribution discussed by Segatori et al. in [26] introduces a novel distributed
algorithm for generating FRBCs based on ACs. The algorithm discussed in the follow-
ing, labeled as Distributed Fuzzy Associative Classifier based on Fuzzy Frequent Pattern
(DFAC-FFP) mining, adopts the fuzzy definition of support and confidence to determine
the strength of a classification rule. For a generic FCAR,,, fuzzy support and confidence
are defined as:

N (7)

fuzzySupp (FAnt, > C;j,,) =

» xn € TS), Wm (Xn)

fuzzyConf (FAnt,, > Cin) ~S € TS weant, (Xn) (8)
Xn m

where TS;,, = {Xn | (Xn, Jn) € TS, yn = G,,} is the set of TS instances labelled with
class Cj,,,, Wm(Xn) is the matching degree, as defined in formula (3), of rule FCAR,,, and
WEAnt,, (Xn) is the matching degree of all the rules whose antecedent is equal to FAnt,,.

In order to generate a set of FCARs, the following procedures are sequentially exe-
cuted during the DFAC-FFP learning process:

¢ Distributed fuzzy partitioning: A strong fuzzy partition is directly generated on each
continuous attribute using a distributed approach based on fuzzy entropy;

¢ Distributed Fuzzy Classification Association Rule (FCAR) Mining: A distributed
fuzzy frequent pattern mining algorithm extracts frequent FCARs with confidence
and support higher than a given threshold;

¢ Distributed FCAR pruning: The mined FCARs are pruned by means of two dis-
tributed rule pruning phases based on redundancy and training set coverage.
Ducange et al. J Big Data (2020) 7:19 Page 13 of 29

For the sake of brevity, in this work we omit the description of the implementation of
the distributed fuzzy partitioning algorithm. Details on this algorithm can be found in
[26]. However, the distributed FCAR mining and pruning procedures may be applied
whenever an initial partition Py for each attribute X¢ has been previously defined.

Each stage of the DFAC-FFP has been implemented using the MapReduce paradigm
on the Apache Spark framework.

Figure 6 shows the three MapReduce stages of distributed FCAR mining, namely
distributed fuzzy counting, distributed fuzzy FP-growth and distributed rule selection
stages.

The first MapReduce stage takes as inputs the fuzzy partitions and the TS chunks, and
outputs a list of fuzzy sets Ay; whose fuzzy support is larger than threshold minSupp. In
detail, each mapper produces, for each fuzzy set Ay; of each fuzzy partition Pr, the list
of membership degrees pf j(x;,¢) calculated for each r” input pattern of the specific TS
chunk. Each reducer receives in input a fuzzy set Ay; and the corresponding list of mem-

bership degrees, calculated by each mapper, and calculates the fuzzy support as follows:

fuzzySupp (Ay j) = N

where N is the total number of instances of the T'S. Only the fuzzy sets whose fuzzy sup-
port is higher than minSupp are retained and included in the list of frequent fuzzy items.

The second MapReduce stage is based on a distributed version of the Fuzzy FP-growth
algorithm. FP-growth is a well known frequent pattern mining algorithm, introduced
by Han et al. in [65], which allows handling high dimensional datasets. Indeed, it first
extracts the frequent items, and sorts them by descending frequencies. Thereafter, such
a dataset of frequent items is compressed into a frequent pattern tree, called FP-tree.
Finally, frequent patterns are recursively mined by extracting from the FP-tree a set of
projected datasets, each one associated with a frequent item or a pattern fragment. In
the work discussed in [26], Segatori et al. proposed a distributed implementation of the
Fuzzy FP-growth algorithm introduced by the same authors in [63]. Each mapper takes
in input a chunk of the training set and the list of frequent fuzzy items Ay; and outputs
item-projected objects. These objects feed reducers, which first build the item-projected
datasets, and then generate local conditional FP-trees. From the local conditional FP-
trees, FCARs are mined, retaining only the ones whose support, confidence and x? val-
ues exceed the relative thresholds.

The last MapReduce stage is in charge of selecting, from the set of very specialized
FCARs mined by the Fuzzy FP-growth algorithm, the top H non-redundant FCARs per
class. To this aim, each mapper is fed by a block of FCARs previously generated and
outputs pairs containing the rule and the consequent class. Each reducer processes all
the rules with the same class label, outputting the most relevant ones. Details on item-
projected objects and datasets and on the FCAR relevance measures can be found in the
work of Segatori et al. in [63].

Figure 7 shows the two MapReduce stages of the distributed FCAR pruning.

In the first stage, the pruning of the set of FCARs is carried out on the basis of fuzzy
support and fuzzy confidence thresholds. Each mapper is fed by a TS chunk and by the
list of FCARs generated during the distributed FCAR mining approach. For each FCAR,,
Page 14 of 29

(2020) 7:19

Ducange et al. J Big Data

 

 

syvod
payejas

jays» |

NOILDATAS F1NY GALNEIYLSId

 

 

 

 

 

 

 

<palqo payafoid-wa}!
HLMOUD-d4 AZZNA GALNAIYLSIG

 

sway
Azznj quanba.y
94} 40 3s]
yoddns Azzn4

Bulull YD Peinqliasip ay} JO awayds sdnpsydey 9 ‘Bry

 

 

SNOILILYVd AZZNA

 

 

 

< (x) nts >

ONILNNOD AZZNA GALNEIYLSIG

 

 

Ww
Ducange et al. J Big Data (2020) 7:19 Page 15 of 29

and for each input pattern x; in the TS chunk, the matching degree w,,(%,) is calculated
and produced as output of each mapper. Each reducer calculates the actual fuzzy sup-
port and confidence of a specific FCAR,, given the list of the w,,(x;) computed by the
different mappers. Only the rules, whose values of fuzzy support and confidence are
higher than specific thresholds, are retained and taken into consideration for the next
stage. In the second MapReduce stage, only the rules characterized by a training cover-
age higher than a threshold are inserted into the final RB. Each mapper is fed by a train-
ing data chunk and by the set of rules previously generated: it calculates the training set
coverage and returns the most covered rules (see [26] for more details). The reducers
generate the final RB considering only the rules, identified by the mappers, which satisfy
the coverage criteria.

Distributed Evolutionary Fuzzy Systems

To the best of our knowledge, the first distributed version of an EFS can be found in
[28], where Fernandez et al. discuss a distributed evolutionary rule selection approach.
Specifically, this approach is carried out inside each mapper of the Chi-FRBCS-BigData,
previously introduced by the authors and discussed in "Local and global implementa-
tions of the distributed Chi et al. algorithm" section. Indeed, in each mapper, after the
generation of rules by means of the classical Chi et al. algorithm, a single objective evo-
lutionary algorithm selects the most relevant rules. The optimized fitness function is a
linear combination of the accuracy and the measure of complexity of the RB. The algo-
rithm is implemented under the Apache Spark framework and experimented consider-
ing imbalanced classification datasets.

As discussed by Fernandez et al. in [19] and by Wang et al. in [40], in the context of Big
Data, the interpretability of fuzzy models assumes a special relevance. If an interpret-
able and transparent model can be derived from a big dataset, the model itself may be
considered as a sort of “visualization tool’, which may allow us to understand the phe-
nomena hidden behind the data. Thus, in 2017, the first distributed MOEFS for Big Data
classification was discussed by Ferranti et al. in [27]. The algorithm, denoted as Distrib-
uted Pareto Archived Evolution Strategy with Rule and Condition Selection (DPAES-
RCS) is a distributed implementation on the Apache Spark environment of the Pareto
Archived Evolution Strategy with Rule and Condition Selection (PAES-RCS), introduced
by Antonelli et al. in [49]. PAES-RCS learns the RB of a set of FRBCs through a Rule and
Condition Selection (RCS) strategy: an initial set of rules is generated by means of heu-
ristics, such as the Chi et al. algorithm or a decision tree, and the most relevant rules and
conditions are selected during the evolutionary learning process. In DPAES-RCS the ini-
tial set of rules is generated exploiting a distributed version of the C4.5 algorithm [66],
available in the MLib of Spark, which is a well known algorithm for the generation of
classical decision trees. The parameters of the fuzzy sets are learnt concurrently with the
RB. Recently, a novel version of DPAES-RCS, called DPAES-FDT-GL has been presented
by Barsacchi et al. in [29]. Here, the initial rule set is generated adopting the distributed
FDT discussed in "Distributed Fuzzy Decision Trees" section and introduced by Segatori
et al. in [26]. Moreover, during the evolutionary learning process also the granularity of
the fuzzy partitions is concurrently learnt with the RB and the parameters of the fuzzy
sets.
Page 16 of 29

(2020) 7:19

Ducange et al. J Big Data

 

 

au |eul4

    
 

(<avod>

ADVYFAOD LAS ONINIVYL AZZNA NO GASVd ONINNYd GALNAlYlsid

 

      

sayn4 AzZzn}
peunid

Bulunid yyD4 Pelnquisip ay} Jo awayds sdnpaydeywy Z ‘Bly

SUIUIW YYD4
paynqiiisip Woy
SYVD4 papayas

 

 

  

ANOD GNV ddNsS AZZN4 NO G3SV4 DNINNUd GALNAlaLSid

 
Ducange et al. J Big Data (2020) 7:19 Page 17 of 29

Figure 8 shows the scheme of the distributed learning process by means of PAES-RCS.
Once the initial set of rules has been learned, for instance using a distributed FDT learn-
ing algorithm as discussed in "Distributed Fuzzy Decision Trees" section, two solutions
are randomly generated and inserted into an archive of non-dominated solutions. The
RBs of the two initial solutions contain a random number of rules. Moreover, for each
rule, a random number of conditions is selected. The fuzzy partitions are strong fuzzy
partitions composed of a random number of fuzzy sets (between a minimum and a max-
imum value) and a random distribution of the cores along the universe of definition. At
each iteration, two solutions are randomly selected from the archive and mating opera-
tors, namely crossover and mutation operators, are applied for generating two offspring
solutions. A two dimensional fitness function is calculated for each offspring, consider-
ing the interpretability, in terms of total number of conditions in the RB, and the accu-
racy of the FRBCs associated with each solution. All the previously discussed steps of the
algorithm and the calculation of the interpretability can be easily executed by a sequen-
tial driver program. On the other hand, the accuracy needs to be calculated by using a
distributed approach, when the amount of data is very huge. Indeed, the overall TS must
be scanned for computing the accuracy. Thus, the TS is divided into chunks and a num-
ber of Computing Units (CUs) are in charge of calculating the output of the classifier,
associated with each offspring, and return the number of patterns correctly classified.
A driver program collects the results provided by each CU and calculates the accuracy.
Finally, the driver program updates the archive of non-dominated solutions consider-
ing the two offspring solutions. The multi-objective evolutionary learning scheme ter-
minates when a stopping condition is reached (usually, a maximum number of fitness
evaluations is fixed and adopted as stopping condition). The final archive contains a set
of non-dominated FRBC characterized by different trade-offs between accuracy and
interpretability.

More details regarding the chromosome coding, the mating operators and the PAES-
RCS learning scheme can be found in the contributions of Ferranti et al. in [27] and of

Barsacchi et al. in [29].

Distributed Fuzzy Decision Trees

As discussed in "Fuzzy classification models: preliminary concepts, architectures and
classical learning algorithms" section , an FDT can be recursively generated consid-
ering an initial partition Py for each input variable X;. In the work of Segatori et al.
in [25], authors discuss a distributed implementation of an FDT learning scheme,
considering the MapReduce paradigm under the Apache Spark framework. Similar
to DFAC-FFP, the proposed algorithm includes an initial distributed fuzzy discre-
tization step for each input variable, based on fuzzy entropy. Also in this case, we
skip the description of the initial fuzzy discretization. On the other hand, the dis-
tributed FDT learning scheme discussed in the following can be applied whenever an
initial strong fuzzy partition is available for each input variable.

Two types of FDTs are considered by the authors, namely multi-way and binary
decision trees. As regards multi-way decision trees (see Fig. 2), the splitting points
are generated considering a branch for each fuzzy set of the selected attribute. On
the other hand, binary trees consider just a two-way splitting point for the specific
Page 18 of 29

(2020) 7:19

Ducange et al. J Big Data

 

 

SDY-SIVd UO paseg sways Hulwed| AJEUOIINJOAS dAIDa/QO-1]jNW peinquisig g "B14

   

au
leniuy

 

 

 

     

SNOILILYVd
AZZNA TVILINI

WwW

 

 
Ducange et al. J Big Data (2020) 7:19 Page 19 of 29

 

 

 

 

Fig.9 An example of fuzzy binary splitting

selected variable. In this case, as shown in Fig. 9, for the selected attribute the split-
ting point is identified considering two fuzzy sets created by applying the union
operator among the fuzzy sets of the specific partition. The best splitting point
is directly generated by the attribute selection algorithm. Both for multi-way and
binary decision trees, the attribute selection algorithm is based on the fuzzy infor-
mation gain. More details on fuzzy entropy, fuzzy information gain and the distrib-
uted fuzzy discretization can be found in the paper of Segatori et al. [25].

Figure 10 shows the scheme of the MapReduce implementation of the FDT learn-
ing approach. The scheme is valid both for multi-way and binary decision trees. A
MapReduce stage is re-iterated for the identification of the attributes to be selected
and of their splitting points (in the case of binary splitting). At each iteration the
set of nodes identified at the previous iteration is taken into consideration (set R
of nodes) and, for each of them, the selected attribute and the splitting points are
returned. Each mapper is fed by a TS chunk and by the list of nodes to be split. For
each node, each mapper calculates a vector of statistics, considering the contribu-
tion of the handled TS chunk. The statistics calculated by the mappers are then used
by the reducers for calculating the fuzzy information gain, selecting the new input
variables and determining the splitting points. The algorithm stops when the set R
of current nodes is composed only by leaves. A node is identified as a leaf if the fol-
lowing conditions are satisfied [25]: (i) the node contains only instances of the same
class, (ii) the node contains a number of instances lower than a fixed threshold, (iii)
the tree has reached a maximum fixed depth and (iv) the fuzzy information gain is
lower than a fixed threshold. Recently, an improved fuzzy partitioning algorithm,
which exploit the probability integral transform, has been introduced in [30]. This
new partitioning algorithm allows reducing the complexity of the multi-way decision
trees generated using the distributed FDT learning scheme discussed above. This
approach has been labeled as FMDT), where / is the number of partitions considered
for each attribute.
Ducange et al. J Big Data (2020) 7:19 Page 20 of 29

   
  
   

New nodes

FUZZY PARTITIONS

List of nodes
to split (R)

  
   
   

Fuzzy
Decision
Tree

 

 

Fig. 10 MapReduce scheme of the distributed FDT learning approach
XX

Experimental results: some discussions

The distributed implementations of the fuzzy classification models discussed in the pre-
vious sections have been experimented by their authors on a number of public bench-
mark datasets. Most of these datasets can be retrieved from the UCI* and the LIBSVM°
repositories.

In order to compare the results achieved by the different distributed learning algo-
rithms discussed so far, we performed a number of experiments, adopting the source
codes publicly available and the ones developed at the University of Pisa. Table 1 sum-
marizes the algorithms analyzed in this work, specifying their names, their acronyms
and the reference papers. We skipped the single-objective EFS because it was designed
for imbalanced binary datasets. As regards the values of the parameters of each specific
algorithm, we adopted the best setup suggested in the paper in which each algorithm
was introduced and experimented.

We executed all the algorithms on the same cluster located at the University of Pisa.
The cluster consists of one master equipped with a 4-core CPU (Intel Core i5 CPU 750 x
2.67 GHz), 8 GB of RAM and a 500 GB Hard Drive, and four slave nodes equipped with
a 4-core CPU with Hyperthreading (Intel Core i7-2600K CPU x 3.40 GHz, 8 threads),
16 GB of RAM and a 1 TB Hard Drive. All nodes are connected by a Gigabit Ethernet (1

 

* Available at https://archive.ics.uci.edu/ml/datasets.php.

> Available at www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.
Ducange et al. J Big Data (2020) 7:19 Page 21 of 29

Table 1 Algorithms used in the experimental comparison

 

 

 

 

Name Acronym References
Local distributed Chi for Big Data Chi-FRBCS-BigData [22, 23, 70]
Global distributed Chi for Big Data CHI_BD [24]
Distributed Fuzzy Associative Classifier based on Fuzzy Frequent Pattern DFAC-FFP [26]
Distributed PAES with Rule and Condition Selection DPAES-RCS [27]
Distributed PAES with Fuzzy Decision Tree and Granularity Learning DPAES-FDT-GL [29]
Multi-way Fuzzy Decison Tree Multi-way FDT [25]
Multi-way Fuzzy Decison Tree with Improved Fuzzy Partitioning FMDT, [30]

 

Gbps) and run Ubuntu 12.04. The training sets were stored in the HDFS (Hadoop Dis-
tributed File System).

In Table 2 we show four datasets that have been considered in our experimental analy-
ses and that have been also discussed in most of the papers where the models have been
proposed. The chosen datasets represent three different categories, namely datasets
with only real-valued attributes (Susy and Higgs), with both real-valued and categorical
attributes (KDD) and with only categorical values (Poker Hand). Moreover, these data-
sets are characterized by different numbers of input/output instances (up to 11 millions)
and input variables (from 10 to 41). Furthermore, for each dataset, the size in terms of
memory occupancy (up to 8.04 GB) is also reported in Table 2.

Tables 3 and 4 show the results achieved by the classifiers considered in this paper
on the four selected big datasets. We recall that both SUS and HIG datasets are bal-
anced binary classification datasets. KDD is a slightly imbalanced classification data-
set, where the minority class is distributed in more or less 20% of the instances, thus
the percentage of correctly classified instances can be considered as a good accuracy
measure. We also experimentally verified that the minority class instances in the
training set folds are enough to induce the generation of classifiers able to accurately
distinguish the two classes. As regards the interpretability measure, we adopt the
number of rules for the FRBCs, generated by means of Chi-FRBCS-BigData, Chi_BD,
DFAC-FFP, DPAES-RCS and DPAES-FDT-GL, and the number of leaves for the FDTs
(we considered just multi-way FDTs).

In the tables, we show the average results of a fivefold stratified cross validation.

As shown in Table 3 and Figure 11, on the KDD dataset all the algorithms perform
well in terms of accuracy. On the other datasets, the FDTs perform always better than
the FRBCs. Regarding FRBCs, both the local and global versions of the distributed
Chi et. al algorithm achieve the worst results. On Sus dataset, the accuracies achieved

Table 2 Datasets used in the experimental comparison

 

 

 

Datasets

Name # Instances # Attributes # Classes # Size
Higgs (HIG) 11,000,000 28 (real: 28) 2 8.04 GB
Kddcup 2 (KDD_2) 4,856,151 A] (real: 26, cat: 15) 2 476 MB
Susy (SUS) 5,000,000 18 (real: 18) 2 2.4 GB
Poker Hand (POk) 10,000,000 10 (cat: 10) 10 24.6 MB

 
Ducange et al. J Big Data (2020) 7:19 Page 22 of 29

Table 3 Experimental comparison: average accuracies on the test set

 

 

 

Algorithm Dataset

HIG SUS KDD POK
Chi-FRBCS-BigData 55.89 55.75 99.93 51.78
CHI_BD 57.81 65.36 99.91 52.22
DFAC-FFP 66.00 78.26 99,99 76.65
DPAES-RCS 65.00 78.12 99,94 60.22
DPAES-FDT-GL 65.03 78.60 99.88 61.80
Multi-way FDT 71.25 79.63 99.98 77.17
FMDTs 72.32 78.97 99.98 76.21

 

Table 4 Experimental comparison: average complexities

 

 

 

Algorithm Dataset

HIG SUS KDD POK
Chi-FRBCS-BigData 24,058 678 1020 813,193
CHI_BD 624,358 9355 5498 52,652
DFAC-FFP 9365 10,970 890 5712
DPAES-RCS 30.2 28 21.8 50
DPAES-FDT-GL 14 14.6 10.8 41.6
Multi-way FDT 920,942 758,064 630 28,561
FMDTs 2987 2865 96 1873

 

by the remaining algorithms are similar. On HIG dataset, DFAC-FFP and DPAESs
achieve accuracies up to 5% lower than the ones achieved by FDTs. On POK dataset,
DFAC-FFP achieves accuracies similar to FDTs, while the accuracies of the DPAESs
are considerably lower than the ones obtained by FDTs, although higher than the
ones achieved by Chi-FRBCS-BigData and CHI_BD algorithms.

As regards the complexities, we have to consider that the high accuracies of the
FDTs are supported by very complex models, constituted by a huge number of param-
eters (number of leaves). Especially for the multi-way FDT, the generated models are
up to four orders of magnitude more complex than the one generated by DPAES-
RCS and DPAES-FDT-GL. Even though the complexities associated with the mod-
els generated by FMDTs are lower than the ones generated by the multi-way FDT,
DPAES-RCS and DPAES-FDT-GL result to be the most interpretable models for Big
Data classification tasks. Moreover, DPAES-RCS and DPAES-FDT-GL achieve results
similar to DFAC-FFP (except for the POK dataset), with a complexity smaller by two
orders of magnitude. More details on the interpretability of DPAES-RCS and DPAES-
FDT-GL can be found in the works of Ferranti et al. [27] and of Barsacchi et al. [29].
In these two papers, some examples of actual interpretable and transparent RBs and
DBs, regarding real world classification problems, are shown and discussed in depth.

It is worth noticing that, in some of the papers in which the algorithms discussed
in this paper have been introduced, some comparisons with non-fuzzy classifica-
tion models have been carried out. For instance, Segatori et al. in [26] demonstrate

that DFAC-FFP generates models characterized by accuracies similar to the ones
Ducange et al. J Big Data (2020) 7:19 Page 23 of 29

 

 

 

- >)
Accuracy Comparison (Test Set)

120
S© 400
”
®
©
S 80 § Chi-FRBCS-BigData
-—
2 ™ CHI BD
o = DFAC-FFP
= 60 ™ DPAES-RCS
. = DPAES-DT-GL
a @ Multiavay FDOT
=> 40 m= FMDT5
~~
©
®
<
8 20

0
HIG SUS KDD POK
Datasets
Fig. 11 Accuracy comparison among the different fuzzy classification models
X y

 

 

generated by two distributed non fuzzy ACs, namely MRAC and MRAC4, intro-
duced by Bechini et al. in [13]. On the other hand, the fuzzy AC models are more
compact than MRAC and MRAC-+-. Segatori et al. in [25], compared the distributed
FDTs with non-fuzzy Distributed Decision Trees (DDTs), available in the Spark Mlib.
Results show that, in most datasets, the distributed FDTs achieve better accuracies
than DDTs. As regards the complexities, the binary FDT are comparable with DDTs
in terms of number of leaves and nodes, while multi-way FDTs are the most com-
plex classification models. As regards DPAES-RCS, Ferranti et al., in [27], show that
this algorithm achieves performances, in terms of accuracy, comparable with the ones
achieved by DDTs. Obviously, the interpretabilty of the classification models gener-
ated by DPAES-RCS is much higher than the one of DDTs, both at complexity and
semantic levels. Indeed, the rules that can be derived from DDTs are not linguistic
rules, thus they are very hard to read and interpret.

Another important aspect to take into consideration when dealing with distributed
algorithms is the scalability. To this aim, Chu et al. in [71] suggest to adopt the speedup
o as the main metrics for evaluating the scalability in parallel and distributed computing.
According to the speedup definition, the efficiency of a program using multiple CUs is
calculated comparing the execution time of the parallel implementation against the cor-
responding sequential version. For most of the distributed fuzzy classification models
discussed in this work, authors carried out a scalability analysis. Specifically, for CHI_
BD, DFAC-FFP, DPAES-RCS, and the Multi-way FDT, authors calculated different val-
ues of speedups, varying the number of CUs from 4/8 to 24/32. These algorithms have
shown an almost linear behavior for the speedup of the classification model learning
process. This means that, whenever needed, additional CUs can be used to effectively
Ducange et al. J Big Data (2020) 7:19 Page 24 of 29

 

40

32

24

Speedup

16

0 8 16 24 32 40
Number of CUs

 

 

Fig. 12 The speedup trend of DPAES-RCS

XX 7

reduce the runtimes. As an example, in Fig. 12, we show the speedup trend of the
DPAES-RCS algorithm, extracted from the paper of Ferranti et al. [27].

Discussion and future directions

After the analysis that we have provided till now, we can state that a set of effective algo-
rithms and tools are available for approaching the problem of generating fuzzy classifi-
cation models from Big Data. In Table 5, for each discussed algorithm, we highlight its
strengths and its weaknesses.

The analysis of the results has highlighted that FDTs are the most accurate classifica-
tion models. However, these models are characterized by a high complexity level. On
the other hand, the FRBCs generated by a distributed multi-objective learning scheme,
based on the DPAES-RCS algorithm, are characterized by an optimal trade-off between
their interpretability and their accuracy. As counterpart, these interpretable models are
generated by means of EAs, which are, in general, characterized by a quite long execu-
tion time. Finally, the fuzzy classification models discussed in this work are not able to
deal with streaming data. Indeed, once a specific fuzzy classification model has been
generated, it cannot be adapted with new training data, which may reflect some changes
of the domain context.

We envision that the future directions in the context of fuzzy classification models for
Big Data will regard: (i) enhancing the interpretability of the rules and of the fuzzy parti-
tions, both at semantic and complexity levels, (ii) handling data streams |72] moving
towards a more general granular computing framework [73, 74]; and (iii) reducing the
computation efforts for generating compact and accurate solutions. The three aforemen-
tioned challenges should be conducted in parallel as much as possible. Indeed, inter-
pretable models, able to extract knowledge in almost real-time from huge amount of
streaming and heterogeneous data, will be the actual added values for future research
activities on classification tasks for Big Data.

Additional efforts can also be done with respect to the fields of application of fuzzy
classification models. In fact, recent developments in several fields such as, cyber-phys-
ical systems [75, 76], cyber-security [77], and learning analytics [78], have increased
the amount of collected data to an enormous scale. These data are inherently uncer-

tain due to noise, incompleteness, and inconsistency, thus they require the adoption of
Ducange et al. J Big Data

(2020) 7:19

Page 25 of 29

Table 5 Algorithms used in the experimental comparison, strengths and weaknesses

 

Algorithm Strengths

Weaknesses

 

Chi-FRBCS-BigData The first distributed algorithm proposed in
the literature for learning a fuzzy model in

big data classification

CHI_BD Global search: unlike Chi-FRBCS-BigData,
employs a global search, thus the struc-
ture of the final model does not depend

on how data chunks are generated

DFAC-FFP Includes a fuzzy discretization algorithm

The generated models are very accurate

DPAES-RCS Optimizes concurrently the rule bases and

the parameters of the fuzzy sets

Generates solutions characterized by good
trade-off between accuracy and interpret-
ability

Even the most accurate solutions are char-
acterized by a reduced number of rules

Adds to the strengths of the PAES-RCS algo-
rithm the capability of optimizing also the
number of fuzzy sets for each attribute

DPAES-FDT-GL

Multi-way FDT Includes a fuzzy discretization algorithm
Is very fast for generating the models
The fuzzy classification models are very
accurate
FMDT Adds to the strengths of the Multi-way FDT

algorithm the capability of reducing the
model complexity

Employs a local search, thus the structure
of the final model depends on how data
chunks are generated

Adopts a single reducer for fusing the rules
generated by a distributed mapping stage

Generates a large number of rules

Generally achieves accuracies lower than the
comparison algorithms

Generates a large number of rules

Generally achieves accuracies lower than the
comparison algorithms

Generates a large number of rules

The input variables may be partitioned with
a large number of fuzzy sets, thus the
interpretability of the fuzzy partitions may
be low

Adopts a pre-fixed number of fuzzy set for
each input variable

Is very slow with respect to the other
algorithms (it is based on evolutionary
optimization)

Is very slow with respect to the other
algorithms (it is based on evolutionary
optimization)

ls characterised by a low interpretability
of the final models because of the large
number of rules generated

 

The final models are still characterised by a
low interpretability because of the large
number of rules

 

appropriate techniques to manage them. With the increase of the amount, variety, and
speed of data, also the inherent uncertainty increases consequently. Moreover, the inter-
pretability of fuzzy models may accomplish with one of the most recent and relevant
requirements of Artificial Intelligence (AI)-based applications, namely the explainabil-
ity. Indeed, eXplainable AI (XAI) [79, 80] refers to all methods and techniques in the
application of AI that allow users to understand how, given specific inputs, AI systems
produce the corresponding outputs. Several application domains consider model inter-
pretability to be fundamental and require appropriate trade-offs between accuracy and

interpretability.

Conclusions
In this work, we have briefly discussed the main design and implementation issues
regarding the most recent fuzzy models for handling classification tasks on Big Data.

Specifically, we have analyzed different distributed implementations of learning
Ducange et al. J Big Data (2020) 7:19 Page 26 of 29

algorithms for generating the model structure of FRBCs and FDTs. Most of the discussed
learning algorithms, specifically the ones regarding FRBCs, are extensions to the parallel
and distributed environment of well-known sequential approaches for generating the RB
and the fuzzy set parameters from data. In particular, we have discussed the distributed
versions of the classical Chi algorithm, of an FAC and of some EFCs. As regards FDT,
we have briefly resumed the steps of a novel distributed learning process, which exploits
an attribute selection and splitting algorithm based on fuzzy information gain. We have
drawn a comparison among the discussed distributed fuzzy classification algorithms, by
considering the results obtained on four popular classification datasets for Big Data, in
terms of accuracy and scalability. Moreover, for each algorithm, we identified its benefits
and limitations.

In conclusion, through this work we have provided a clear description of the current
background in the field of fuzzy models for big data. Moreover, we have carried out an
accurate analysis on research challenges and gaps. Finally, we have suggested areas for

further investigation for supporting researchers in positioning their works.

Abbreviations

AC: Associative Classifier; CU: Computing Unit; DB: Data Base; DDT: Distributed Decision Tree; DFAC-FFP: Distributed
Fuzzy Associative Classifier based on Fuzzy Frequent Pattern; DPAES-RCS: Distributed Pareto Archived Evolution Strategy
with Rule and Condition Selection; EA: Evolutionary Algorithm; EFC: Evolutionary Fuzzy Classifier; EFS: Evolutionary Fuzzy
System; FAC: Fuzzy Associative Classifier; FCAR: Fuzzy Classification Association Rule; FDT: Fuzzy Decision Tree; FRBC:
Fuzzy Rule-Based Classifier; HDFS: Hadoop Distributed File System; MOEFS: Multi-Objective Evolutionary Fuzzy System;
PAES-RCS: Pareto Archived Evolution Strategy with Rule and Condition Selection; RB: Rule Base; RDD: Resilient Distributed
Dataset; YARN: Yet Another Resource Negotiator.

Acknowledgements
Not applicable.

Authors’ contributions
PD, MF and FM contributed equally to the analysis and the selection of the materials for the survey and to writing of the
manuscript. All authors read and approved the final manuscript.

Funding

This work was partially supported by Tuscany Region, in the context of the projects Talent and Sibilla in the framework of
regional program “FESR 2014-2020" and by the Italian Ministry of Education and Research (MIUR), in the framework of the
CrossLab project (Departments of Excellence).

Availability of data and materials
The datasets adopted in this study are available in the UCI repository, at https://archive.ics.uci.edu/m|/datasets.php.

Competing interests
The authors declare that they have no competing interests.

Author details
Dipartimento di Ingegneria dell'Informazione, Largo Lucio Lazzarino, 1, 56122 Pisa, Italy. * Istituto di Informatica e Tele-
matica - Consiglio Nazionale delle Ricerche (IIT-CNR), Via Giuseppe Moruzzi, 1, 56124 Pisa, Italy.

 

Received: 17 September 2019 Accepted: 21 February 2020
Published online: 10 March 2020

References

1. John Walker S. Big data: a revolution that will transform how we live, work, and think. London: Taylor & Francis; 2014.

2. Anuradha J, et al. A brief introduction on big data 5vs characteristics and hadoop technology. Procedia Comput Sci.

2015;48:319-24,

Laney D. 3-d data management: controlling data volume, velocity, and variety. META Group Res Note. 2001;6:6.

4. Wan J, Tang S, LiD, Wang S, Liu C, Abbas H, Vasilakos AV. A manufacturing big data solution for active preventive
maintenance. IEEE Trans Ind Inform. 2017;13(4):2039-47.

5. Ducange P, Pecori R, Mezzina P. A glimpse on big data analytics in the framework of marketing strategies. Soft Com-
put. 2018;22(1):325-42.

6. Al-Ali A, Zualkernan IA, Rashid M, Gupta R, Alikarar M. A smart home energy management system using iot and big
data analytics approach. IEEE Trans Consum Electron. 2017;63(4):426-34.

Ww
Ducange et al. J Big Data (2020) 7:19 Page 27 of 29

10.
11,

16.
17.
18.

20.

21.

22.

23.

24,

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

39.

40.

Stergiou C, Psannis KE. Recent advances delivered by mobile cloud computing and internet of things for big data
applications: a survey. Int J Netw Manage. 2017;27(3):1930.

Wang Y, Kung L, Wang WYC, Cegielski CG. An integrated big data analytics-enabled transformation model: applica-
tion to health care. Inf Manage. 2018;55(1):64-79.

Han J, Kamber JPM. Data Mining. Concepts and techniques. In: Data management systems, 3rd edn. Burlington:
Morgan Kaufmann; 2012.

Dean J, Ghemawat S. MapReduce: simplified data processing on large clusters. Comm ACM. 2008;51(1):107-13.
Ludwig SA. Mapreduce-based fuzzy c-means clustering algorithm: implementation and scalability. Int J Mach
Learn Cybern. 2015;6(6):923-34.

Kim Y, Shim K, Kim M-S, Lee JS. DBBCURE-MR: an efficient density-based clustering algorithm for large data using
mapreduce. Inf Syst. 2014;42:15-35.

Bechini A, Marcelloni F, Segatori A. A MapReduce solution for associative classification of big data. Inf Sci.
2016;332:33-55.

Maillo J, Ramfrez S, Triguero |, Herrera F. KNN-IS: an iterative spark-based design of the k-nearest neighbors classi-
fier for big data. Knowl Based Syst. 2017;117:3-15.

. Zhou L, Pan S,Wang J, Vasilakos AV. Machine learning on big data: opportunities and challenges. Neurocomput-

ing. 201 7;237:350-61.

Coulouris G, Jean Dollimore TK. Distributed systems: concepts and design. London: Pearson Education; 2009.
Apache Hadoop. https://hadoop.apache.org/. Accessed Jan 2016.

Zaharia M, Chowdhury M, Franklin MJ, Shenker S, Stoica |. Spark: cluster computing with working sets. In: Pro-
ceedings of the 2nd USENIX conference on hot topics in cloud computing, vol. 10. 2010. p. 10.

Fernandez A, Carmona CJ, del Jesus MJ, Herrera F. A view on fuzzy systems for big data: progress and opportuni-
ties. Int J Comput Intell Syst. 2016;9(sup1):69-80.

Hariri RH, Fredericks EM, Bowers KM. Uncertainty in big data analytics: survey, opportunities, and challenges. J
Big Data. 2019;6(1):44.

Lopez V, del Rio S, Benitez JM, Herrera F. On the use of mapreduce to build linguistic fuzzy rule based classifica-
tion systems for big data. In: Fuzzy systems (FUZZ-IEEE), 2014 IEEE international conference on, IEEE. 2014. pp.
1905-12.

del Rio S, L6pez V, Benitez JM, Herrera F. A MapReduce approach to address big data classification problems
based on the fusion of linguistic fuzzy rules. Int J Comput Intell Syst. 2015;8(3):422-37,

Lopez V, del Rio S, Benitez JM, Herrera F. Cost-sensitive linguistic fuzzy rule based classification systems under
the MapReduce framework for imbalanced big data. Fuzzy Sets Syst. 2015;258:5-38.

Elkano M, Galar M, Sanz J, Bustince H. CHI-BD: a fuzzy rule-based classification system for big data classification
problems. Fuzzy Sets Syst. 2017;348:75-101.

Segatori A, Marcelloni F, Pedrycz W. On distributed fuzzy decision trees for big data. IEEE Trans Fuzzy Syst.
2018;26(1):1 74-92.

Segatori A, Bechini A, Ducange P, Marcelloni F. A distributed fuzzy associative classifier for big data. IEEE Trans
Cybern. 2018;48(9):2656-69.

Ferranti A, Marcelloni F, Segatori A, Antonelli M, Ducange P. A distributed approach to multi-objective evolution-
ary generation of fuzzy rule-based classifiers from big data. Inf Sci. 2017;415:319-40.

Fernandez A, Almansa E, Herrera F. CHI-SPARK-RS: an spark-built evolutionary fuzzy rule selection algorithm in
imbalanced classification for big data problems. In: 2017 IEEE international conference on fuzzy systems (FUZZ-
IEEE), IEEE. 2017. pp. 1-6.

Barsacchi M, Bechini A, Ducange P, Marcelloni F. Optimizing partition granularity, membership function param-
eters, and rule bases of fuzzy classifiers for big data by a multi-objective evolutionary approach. Cogn Comput.
2019;11:367-87.

Elkano M, Uriz M, Bustince H, Galar M. On the usage of the probability integral transform to reduce the complex-
ity of multi-way fuzzy decision trees in big data classification problems. In: 2018 IEEE international congress on
Big Data. 2018. pp. 25-32.

Marquez A, Marquez F, Peregrin A. A scalable evolutionary linguistic fuzzy system with adaptive defuzzification
in big data. In: 2017 IEEE international conference on fuzzy systems (FUZZ-IEEE), IEEE. 2017. pp. 1-6.

Lopez S, Marquez AA, Marquez FA, Peregrin A. Evolutionary design of linguistic fuzzy regression systems with
adaptive defuzzification in big data environments. Cogn Comput. 2019;1 1:388-99.

Cézar J, Marcelloni F, Gamez JA, de la Ossa L. Building efficient fuzzy regression trees for large scale and high
dimensional problems. J Big Data. 2018;5(1):49.

Bharill N, Tiwari A, Malviya A. Fuzzy based scalable clustering algorithms for handling big data using apache
spark. IEEE Trans Big Data. 2016;2(4):339-52.

Wu J, Wu Z, Cao J, Liu H, Chen G, Zhang Y. Fuzzy consensus clustering with applications on big data. IEEE Trans
Fuzzy Syst. 2017;25(6):1430-45.

Hidri MS, Zoghlami MA, Ayed RB. Speeding up the large-scale consensus fuzzy clustering for handling big data.
Fuzzy Sets Syst. 2018;348:50-74.

Pulgar-Rubio F, Rivera-Rivas A, Pérez-Godoy MD, Gonzalez P, Carmona CJ, del Jesus M. MEFASD-BD: multi-
objective evolutionary fuzzy algorithm for subgroup discovery in big data environments-a mapreduce solution.
Knowl Based Syst. 2017;117:70-8.

Fernandez-Bassso C, Ruiz MD, Martin-Bautista MJ. Fuzzy association rules mining using spark. In: International
conference on information processing and management of uncertainty in knowledge-based systems. Springer.
2018. pp. 15-25.

Gacto MJ, Alcala R, Herrera F. Interpretability of linguistic fuzzy rule-based systems: an overview of interpretabil-
ity measures. Inf Sci. 2011;181(20):4340-60.

Wang H, Xu Z, Pedrycz W. An overview on the roles of fuzzy set techniques in big data processing: trends, chal-
lenges and opportunities. Knowl Based Syst. 2017;118:15-30.
Ducange et al. J Big Data (2020) 7:19 Page 28 of 29

 

AA,

 

50.

51.
52.

53.

54.

55.

56.

5/7.

58.

59.

60.

61.

62.

63.

64.
65.

66.
67.
68.

69.

70.

71.

72.

73.

74.

75.
76.

Dean J, Ghemawat S. Mapreduce: a flexible data processing tool. Commun ACM. 2010;53(1):72-7.

Lyubimov D, Palumbo A. Apache Mahout: Beyond MapReduce. 1st ed. South Carolina: CreateSpace Independ-
ent Publishing Platform; 2016.

Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin MJ, Shenker S, Stoica |. Resilient distrib-
uted datasets: a fault-tolerant abstraction for in-memory cluster computing. In: Proceedings of the 9th USENIX
conference on networked systems design and implementation. NSDI'12. Berkeley, CA, USA: USENIX Association;
2012. p. 15-28.

Meng X, Bradley J, Yavuz B, Sparks E, Venkataraman S, Liu D, Freeman J, Tsai D, Amde M, Owen S, Xin D, Xin

R, Franklin MJ, Zadeh R, Zaharia M, Talwalkar A. Milib: machine learning in apache spark. J Mach Learn Res.
2016;17(1):1235-41.

Carbone P, Katsifodimos A, Ewen S, Markl V, Haridi S, TZoumas K. Apache flink: stream and batch processing in a
single engine. Bull IEEE Comput Soc Tech Comm Data Eng. 2015;36(4)28-38.

Guillaume S, Charnomordic B. Fuzzy inference systems: an integrated modeling environment for collaboration
between expert knowledge and data using FisPro. Expert Syst Appl. 2012;39(10):8744-55.

Cordén O, del Jesus MJ, Herrera F. A proposal on reasoning methods in fuzzy rule-based classification systems.
Int J Approx Reason. 1999;20(1):21-45.

Chi Z, Yan H, Pham T. Fuzzy algorithms: with applications to image processing and pattern recognition. In:
Advances in fuzzy systems—applications and theory. vol. 10. World Scientific, Singapore. 1996.

Antonelli M, Ducange P, Marcelloni F. A fast and efficient multi-objective evolutionary learning scheme for fuzzy
rule-based classifiers. Inf Sci. 2014;283:36—54.

Fernandez A, Lopez V, del Jesus MJ, Herrera F. Revisiting evolutionary fuzzy systems: taxonomy, applications,
new trends and challenges. Knowl Based Syst. 2015;80:109-21.

Khan GM. Evolutionary computation. In: Evolution of artificial neural development. 2018. pp. 29-37.

Ducange P, Marcelloni F. Multi-objective evolutionary fuzzy systems. In: International workshop on fuzzy logic
and applications. Springer. 2011. pp. 83-90.

Fazzolari M, Alcala R, Nojima Y, Ishibuchi H, Herrera F. A review of the application of multi-objective evolutionary
fuzzy systems: current status and further directions. IEEE Trans Fuzzy Syst. 2013;21(1):45-65.

Deb K. Multi-objective optimization. In: Burke EK, Kendall G, editors. Search methodologies. Berlin: Springer;
2014. p. 403-49.

Ishibuchi H, Yamamoto T. Fuzzy rule selection by multi-objective genetic local search algorithms and rule evalu-
ation measures in data mining. Fuzzy Sets Syst. 2004;141(1):59-88.

Cococcioni M, Ducange P, Lazzerini B, Marcelloni F. A pareto-based multi-objective evolutionary approach to the
identification of mamdani fuzzy systems. Soft Comput. 2007;11(11):1013-31.

Botta A, Lazzerini B, Marcelloni F, Stefanescu DC. Context adaptation of fuzzy systems through a multi-objective
evolutionary approach based on a novel interpretability index. Soft Comput. 2009;13(5):437-49.

Fazzolari M, Alcala R, Herrera F. A multi-objective evolutionary method for learning granularities based on

fuzzy discretization to improve the accuracy-complexity trade-off of fuzzy rule-based classification systems:
D-MOFARC algorithm. Appl Soft Comput. 2014;24:470-81.

Antonelli M, Ducange P, Lazzerini B, Marcelloni F. Learning knowledge bases of multi-objective evolution-

ary fuzzy systems by simultaneously optimizing accuracy, complexity and partition integrity. Soft Comput.
2011;15(12):2335-54.

Baralis E, Garza P. I-prune: Item selection for associative classification. Int J Intell Syst. 2012;27(3):279-99.
Abdelhamid N, Ayesh A, Thabtah F, Ahmadi S, Hadi W. MAC: a multiclass associative classification algorithm. J Inf
Knowl Manage. 2012;11(02):1250011.

Alcala-Fdez J, Alcala R, Herrera F. A fuzzy association rule-based classification model for high-dimensional prob-
lems with genetic rule selection and lateral tuning. IEEE Trans Fuzzy Syst. 2011;19(5):857-72.

Antonelli M, Ducange P, Marcelloni F, Segatori A. A novel associative classification model based on a fuzzy
frequent pattern mining algorithm. Expert Syst Appl. 2015;42(4):2086-97.

Zhang C, Zhang S. Association rule mining: models and algorithms. Berlin: Springer; 2002.

Han J, Pei J, Yin Y. Mining frequent patterns without candidate generation. In: SIGMOD Rec. vol. 29. New york:
ACM. 2000. pp. 1-12.

Quinlan JR. Induction of decision trees. Mach Learn. 1986;1(1):81-106.

Altay A, Cinar D. In: Kahraman C, Kabak O, editors. Fuzzy decision trees. Cham: Springer; 2016. pp. 221-61.
Witten IH, Frank E, Hall MA, Pal CJ. Data mining: practical machine learning tools and techniques. Burlington:
Morgan Kaufmann; 2016.

Pecori R, Ducange P, Marcelloni F. Incremental learning of fuzzy decision trees for streaming data classification.
In: 2019 conference of the international fuzzy systems association and the European society for fuzzy logic and
technology (EUSFLAT 2019). Paris: Atlantis Press. 2019/08.

Fernandez A, del Rio S, Bawakid A, Herrera F. Fuzzy rule based classification systems for big data with MapRe-
duce: granularity analysis. Adv Data Anal Classif. 2016;11:711-30.

Chu C-T, Kim SK, Lin Y-A, Yu Y, Bradski G, Olukotun K, Ng AY. Map-reduce for machine learning on multicore. In:
Advances in neural information processing systems. 2007. pp. 281-8.

Pecori R, Ducange P, Marcelloni F. Incremental learning of fuzzy decision trees for streaming data classification.
In: 2019 conference of the international fuzzy systems association and the European society for fuzzy logic and
Technology (EUSFLAT 2019). Atlantis Press. 2019.

Pedrycz W. Granular computing: analysis and design of intelligent systems. Boca Raton: CRC Press; 2016.
Antonelli M, Ducange P, Lazzerini B, Marcelloni F. Multi-objective evolutionary design of granular rule-based
classifiers. Granul Comput. 2016;1(1):37-58.

Xu LD, Duan L. Big data for cyber physical systems in industry 4.0: a survey. Enterp Inf Syst. 2019;13(2):148-69.
Mohammadi M, Al-Fugaha A, Sorour S, Guizani M. Deep learning for iot big data and streaming analytics: a
survey. IEEE Commun Surv Tutor. 2018;20(4):2923-60.
Ducange et al. J Big Data (2020) 7:19 Page 29 of 29

77.

78,

79.

80.

Kayes A, Rahayu W, Dillon T, Chang E, Han J. Context-aware access control with imprecise context characteriza-
tion through a combined fuzzy logic and ontology-based approach. In: OTM confederated international confer-
ences “On the move to meaningful internet systems”. Springer. 2017; pp. 132-53.

Pecori R, Suraci V, Ducange P. Efficient computation of key performance indicators in a distance learning univer-
sity. Inf Discov Deliv. 2019;47:96-105.

Adadi A, Berrada M. Peeking inside the black-box: a survey on explainable artificial intelligence (xai). IEEE Access.
2018;6:52138-60.

Fernandez A, Herrera F, Cordon O, del Jesus MJ, Marcelloni F. Evolutionary fuzzy systems for explainable artificial
intelligence: why, when, what for, and where to? IEEE Comput Intell Mag. 2019;14(1):69-81.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen”®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

 

Submit your next manuscript at > springeropen.com

 

 

 
