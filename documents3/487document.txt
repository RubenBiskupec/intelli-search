Journal of Real-Time Image Processing (2020) 17:1757-1768
https://doi.org/10.1007/s11554-020-01002-w

SPECIAL ISSUE PAPER

-)

Check for
updates

Fast automatic camera network calibration through human mesh

recovery

Nicola Garau! - Francesco G. B. De Natale’ - Nicola Conci'

Received: 2 December 2019 / Accepted: 11 July 2020 / Published online: 4 September 2020

© The Author(s) 2020

Abstract

Camera calibration is a necessary preliminary step in computer vision for the estimation of the position of objects in the 3D
world. Despite the intrinsic camera parameters can be easily computed offline, extrinsic parameters need to be computed
each time a camera changes its position, thus not allowing for fast and dynamic network re-configuration. In this paper we
present an unsupervised and automatic framework for the estimation of the extrinsic parameters of a camera network, which
leverages on optimised 3D human mesh recovery from a single image, and which does not require the use of additional
markers. We show how it is possible to retrieve the real-world position of the cameras in the network together with the floor
plane, exploiting regular RGB images and with a weak prior knowledge of the internal parameters. Our framework can also
work with a single camera and in real-time, allowing the user to add, re-position, or remove cameras from the network in a

dynamic fashion.

Keywords Camera calibration - Pose estimation - Human mesh recovery - 3D matching

1 Introduction

In computer vision and 3D reconstruction, many works over
the years have tried to automate the process of camera resec-
tioning and calibration. Having the possibility to minimise
the manual intervention within the calibration pipeline could
simplify its deployment in many contexts and in a signifi-
cant way. However, there is still a lack for fully unsuper-
vised and markerless approaches for camera calibration in
literature. The manifoldness of camera sensors and lenses
present in the market hinders any generalization attempt.
Another aspect that plays an important role in increasing the
difficulty of automatic calibration is the dynamic nature of
the environments, in which camera networks are generally
being installed. For example, in many scenarios, including
video surveillance, Ambient Assisted Living (AAL) and

b<] Nicola Garau
nicola.garau @ unitn.it

Francesco G. B. De Natale
francesco.denatale @ unitn. it

Nicola Conci

nicola.conci @unitn.it

University of Trento, Via Sommarive, 9, Povo,
38123 Trento, TN, Italy

environmental monitoring, the reconfiguration and conse-
quent re-calibration of the camera network 1s a common pro-
cess, often due to the re-positioning or addition of pieces
of furniture, or, more in general, the presence of obstacles
that can partially or fully limit the visibility of the observed
environment. [In addition, cameras with pan-tilt-zoom (PTZ)
capabilities are often used. A big issue linked to the usage
of PTZ cameras is that they are capable of changing their
internal configuration, making it necessary to re-calibrate
the whole network whenever these changes occur. In addi-
tion, wind or other weather conditions may also further
complicate the scenario, introducing noise, and making it
difficult to accomplish even the simplest vision tasks, such
as keypoints extraction, motion detection and tracking.

Generally speaking, the internal configuration of a cam-
era usually remains fixed, unless when zooming, refocus-
ing or changing the lens parameters. Many good solutions
to estimate the intrinsic parameters of a camera have been
provided in the literature, and they usually require the usage
of a checkerboard or other calibration tools.

On the other hand, extrinsic parameters model the rela-
tion between the camera coordinates and the real-world
coordinates. Ideally, extrinsic parameters remain unaltered
if both the camera and world long-term steadiness can be
guaranteed. For this reason, even a slight movement of the

Q) Springer
1758

camera can cause a loss in calibration precision, leading to
the need of re-calibrating the whole system. This 1s problem-
atic because the standard calibration procedures, though not
complex, are rather time consuming and require the usage of
third party calibration instruments by an expert technician
who needs to be on the spot to perform the task. Another
issue is that whenever a calibration is in progress, the camera
network remains busy and inoperable.

A few approaches in literature [8] try to simplify the cali-
bration process by increasing the accuracy of the calibra-
tion pattern detectors, thus reducing the number of required
checkerboard images. However, despite being fast, they still
require manual intervention. For example, although in a dif-
ferent application scenarios, the adoption of markerless solu-
tions has been explored in the autonomous driving context,
exploiting visual odometry [21], SLAM [6], and optical flow
[19] for feature tracking; however, they are often not suitable
for surveillance scenes, since such methods require a fixed
camera configuration with respect to the vehicle, in order to
exploit the car movement information for calibration. Other
methods use SIFT/SURF feature matching between camera
views to estimate the extrinsic camera parameters [11], but
they usually require additional data from other sensors, such
as active range sensors.

A recent trend in computer vision is pedestrian-based
camera calibration, which focuses on finding how to estimate
both intrinsic and extrinsic camera parameters by exploiting
the cues provided by walking humans. In particular, these
approaches are usually based on:

e Manhattan World Assumption
e Planar trajectories
e Skeleton data from 3D sensors

The approaches based on the Manhattan World Assump-
tion [4] are usually adopted in city-like environments due to
their geometric homogeneity, but may fail in other scenar-
10s, when no such geometric cues are being found. Human
detection and tracking have been explored in literature as a
support for vanishing point estimation and to estimate the
ground plane from multiple camera views [30]. However,
these methods often require a prior knowledge of the cam-
eras’ vertical position or of the people height, they are not
robust to occlusions, noise, and can be fooled by uncon-
ventional human poses. Recently, RGBD sensors such as
the Microsoft Kinect V2! and the Intel RealSense’ allowed
obtaining a better understanding of the scene through depth
and 3D skeleton pose estimation [23, 26, 27]. However, there

' https://developer.microsoft.com/en-us/windows/kinect.

> https://www.intel.com/content/www/us/en/architecture-and-techn
ology/realsense-overview.html.

Q) Springer

Journal of Real-Time Image Processing (2020) 17:1757-1768

are many issues linked to the usage of RGBD sensors such
as Kinect and RealSense to calibrate a camera network from
the skeleton information. Among them, the most relevant
ones are:

e Small range (usually 4m) of the depth sensor; this con-
straint is not suitable for large environments.

e Low precision; occlusions, ambiguities and reflections in
the scene are an important factor for the skeleton extrac-
tion precision.

e High infrastructural and processing cost; multiple com-
puters and GPUs are usually required to process data
coming from a network of RGBD sensors in real-time.

A recent trend in computer vision concerns the area of
human pose estimation from monocular images. There have
been many successful examples, such as [24, 33] and [2].
Many of the good results have been made possible thanks to
the availability of very large datasets, in particular CMU’s
Panoptic Studio [12], which contributed to speed up the
development of many popular and open source frameworks,
such as OpenPose [3].

Amongst the different kinds of 3D monocular human
pose estimation techniques used in literature, we can dis-
tinguish between:

e two-stage approaches
e end-to-end approaches

Two-stage approaches, such as [31], first estimate 2D joints
and then recover the depth component. On the other hand,
end-to-end approaches try to recover the 3D skeleton or
mesh in one shot. Kanazawa et al.’s work [13] is one of the
most recent ones, which takes as input an image, encod-
ing it into body pose, shape and weak camera pose via a
CNN encoder; then, a discriminator is used as supervisor
to encourage a better loss, by comparing the produced 3D
model with a pool of real scanned 3D human poses (Fig. 2).
Despite it being a very good approach to estimate the 3D
mesh of a person, it may still fail, especially when deal-
ing with unusual viewpoints and in time-constrained sce-
narios. Kolotouros et al. in SPIN (SMPL oPtimization IN
the loop) [16] provide a fix to these issues by employing
an hybrid top-down and bottom-up approach that aims at
optimising the human mesh recovery (HMR) phase. Their
method is based on the iterative application of optimisation
and regression-based approaches (such as [13]) to further
improve human mesh recovery, by mixing the advantages
of both the approaches, in particular the accuracy of the first
one with the speed of the second one.

Starting from Kanazawa’s work, we extend it in a similar
fashion as the one described in [16], and re-purpose in order
to work with multiple views and with a more realistic camera
Journal of Real-Time Image Processing (2020) 17:1757-1768

model, that allows us to better estimate the extrinsic param-
eters for each camera. Our results show that, starting from a
single frame, the retrieved human skeleton alone can provide
a sufficient number of keypoints to estimate the real-world
3D position of cameras in a network, thus achieving fully
unsupervised camera calibration. We show results in differ-
ent scenarios and with different cameras configurations and
discuss on how our method can be further extended for bet-
ter accuracy. This work is an extension of our previous work
[7]. The main contribution, compared to the work in [7]
consists of the capability of the system to obtain real-time
camera network calibration, at comparable accuracy. This is
achieved thanks to the adoption of a faster SNWBP network
[9] and a more precise human mesh recovery pipeline [16].
These improvements allow for an even easier deployment in
real-world scenarios, and are particularly helpful when deal-
ing with large camera networks and real-time constraints.

2 Related work
2.1 Human Pose Estimation (HPE)

Before the advent of deep learning, classical HPE
approaches were based on the so-called pictorial structures
framework [1]. Later on, this kind of hand-crafted features,
as well as customised hardware solutions (e.g., RGBD-based
sensors) became less popular, making room for HPE algo-
rithms based on deep learning paradigms.

Many human pose estimation techniques [3, 33] are based
on bottom-up 2D skeleton estimation to guarantee good
performances. Recent contributions [31] explore two-stage
approaches, in which the 2D pose is first estimated and then
used as a baseline to infer the corresponding 3D pose.

2.1.1 Bottom-up approaches

Estimating the human pose in a bottom-up fashion means
first estimating all the joints in a frame and then linking
them together in a meaningful, structured hierarchy. Cao
et al.’s Realtime multi-person 2D pose estimation using part
affinity fields [2] is one of the most popular multi-person
real-time 2D pose estimation works in literature. It combines
the architecture of a CNN-based variation of Pose Machines,
called Convolutional Pose Machines [33], leveraging on part
affinity fields. Part affinity fields can be defined as a group
of oriented vectors linking different joints. In other words,
part affinity fields can be seen as confidence maps identify-
ing bones, while joint confidence maps identify joints and
articulations.

The solution, presented in [2], is very robust to large scale
occlusions and self occlusions. Its dual-branch architecture
for CNN-based joint parts and pairs estimation is optimised

1759

to run in real-time on consumer hardware, making it suitable
for many research applications, and known as OpenPose [3].
However, it is still not faster than many top-down approaches
when dealing with low density scenarios. Recently, it has
been extended with a single track architecture [9], rendering
it much faster than before, also embedding the hand and face
joint information.

2.1.2 End-to-end solutions for 3D human pose estimation

End-to-end recovery of human shape and pose [13] is one of
the most popular works in joint 3D human shape and pose.
From a single RGB image of a person, the human pose 0
and body shape f are regressed, together with camera scale
s, rotation R and translation T.

An issue with this approach is that it is not suitable for
run-time application and it is highly affected by viewpoint
changes and flickering between frames, due to the lack of
temporal coherence. Other works (see [14, 16, 22]) inspired
by [13] try to solve the flickering issue by using temporal
cues or predicting future poses.

2.2 Automatic calibration

Most of the automatic extrinsic calibration works in litera-
ture leverage on the so called Manhattan World Assumption
[4], which assumes that the geometry typical of urban areas
makes it easier to discover vanishing points from images
taken in those kind of environments. As an example, Zhang
et al. in [35] propose a solution that exploits the geometry of
solar panels to estimate orthogonal vanishing points. While
such assumption is valid and works well in city scenarios,
it does not generalize sufficiently, especially when indoor
scenes are taken into consideration. Many methods in lit-
erature deal with the problem of finding the parameters of a
single camera which is being plugged in to an existing and
already calibrated camera network. Vasconcelos et al. in [32]
exploit sets of pairwise correspondences among images in
order to estimate the pose of the new camera. Despite the
high deployability, this method only works when the extrin-
sic parameters of the other cameras are known. In literature,
methods for self-calibration of pan-tilt [15] and tilt-zoom
[25] cameras can also be found. Traditionally, such kind
of self calibration problems are handled using geometrical
constraints; however, with the growing popularity of deep
learning, some approaches tried to solve the problem, as in
Hold-Geoffroy et al.’s work [10], which aims at estimating
pitch, roll and focal length of a single camera employing
convolutional neural networks. Of particular interest from
this viewpoint, some very recent works focus on embedding
CNN capabilities directly into camera sensors. One of the
first works to achieve such results is Bose et al.’s A Cam-
era That CNNs: Towards Embedded Neural Networks on

Q) Springer
1760

Fig. 1 An overview of our cam-
era network calibration pipeline.
In the figure, SNWBP refers to
a Single-Network Whole-Body
Pose Estimation, the filters
prepare the inputs for the SPIN
human mesh recovery module.
The matcher simply matches the
obtained 3D skeletons in order
to compute the output poses

   
 

  

Lreproj = ||az ~~ &\|5

*
Sy
=

    

Projection

Regression

os DB *
=
s&s
2
lb
by

&>

  

Fig. 2 Joint pose, shape and camera estimation of End-to-end recov-
ery of human shape and pose [13] pipeline

Pixel Processor Arrays, an interesting proposal that could
open new possibilities for in-camera self-calibration. With
the growing popularity of omnidirectional cameras, Miyata
et al. in [20] show how to exploit their large field of view
to anchor non-overlapping views. However, such solution
are not employable in some scenarios, such as AAL, since
occlusions may play an important role for the failure of the
keypoints detectors. Augmented reality (AR) also played an
important role for refreshing the field of camera calibra-
tion. Zhao et al. in [36] employ augmented reality markers
placed directly on top of cameras in order to perform camera
calibration. However, the method requires the usage of an
additional dedicated camera just for the recognition of the
AR markers.

Perhaps the most popular approaches for automatic cam-
era calibration are the ones employing vanishing points esti-
mation. Tang et al. in ESTHER [30] propose a complete
pedestrian trajectory-based solution for joint intrinsic and

[o,]

[By]

  

A. Single frame

B. SNWBP

I O1

[
B O ’ ’
a ay aH

SNWBP
Ce Bh (On, Bn, En|

 

Journal of Real-Time Image Processing (2020) 17:1757-1768

By Oi, 1, €1

      

 

   
    
   

6 06

extrinsic parameters estimation, especially focusing on
intrinsic calibration for distortion correction. However, their
method requires pedestrian to walk in a standard upright
position, as well as a prior knowledge of the cameras verti-
cal height.

To our best knowledge, few other works exploit human
pose cues for camera calibration, and most of them exploit
3D sensors data, such as depth maps or cameras disparity
information.

Desai et al. in [5] propose a skeleton-based method for
semi-automatic continuous calibration of Kinect V2 sen-
sors. In their work, they also explore some issues related
to working with depth sensors, such as low range of vision,
skeleton flipping and high computational costs. Among the
many recent 3D human pose estimation works, some also try
to jointly retrieve human pose and weak camera parameters.
Kanazawa et al.’s approach [13] provides an estimation of
the subject in terms of mesh, shape and pose representation,
as well as some shallow cues of the camera pose.

3 Method overview

In this section, we refer to Fig. 3 to provide a red thread to
explain our method’s pipeline. During phase A, for each
camera C; in the network we acquire a single frame in a
synchronous fashion. Then, each frame is forwarded to a
Single-Network Whole-Body Pose Estimation (SNWBP)
[9] network (phase B), which is a very fast convolutional

]

     
   

[ U, 6, &

  

C. Optimised
SMPL model

D. 3D human pose
+ 3D camera pose

Fig.3 Illustration of the proposed pipeline: from a single RGB image to the estimation of the extrinsic parameters of the camera. In case of a
camera network, the same pipeline is applied for each camera, before a 3D matching phase, as described in Fig. 1

D) Springer
Journal of Real-Time Image Processing (2020) 17:1757-1768

 

Fig.4 2D pose estimation for bounding box extraction

network that is able to infer the 2D skeleton (o) of multiple
people inside the image in real-time. In this phase we also
use the 2D skeleton information to compute a 2D bounding
box B; for each detected person in each frame. Then, dur-
ing phase C, we use our joint human mesh recovery and
camera pose estimation network, which is based on [16].
Starting from the monocular human mesh recovery network
described in [16], we extend it by modifying the underlying
camera model, providing a full perspective camera model.
This addition, makes it possible, during phase D, to exploit
the information acquired in the previous steps, such as the
bounding boxes, 2D and 3D skeletons, body shape and pose
parameters, to retrieve a good estimation of the camera pose
for each camera in the network. All the four phases can run
in parallel for each camera in the system, and in a continuous
loop, in order to maximise both performance and precision
by refining the calibration results over time.

4 The proposed model

In this section, we propose our one-shot method for fully
automatic and unsupervised camera network calibration that
leverages on monocular 3D human pose estimation from
single images. In Figs. 1 and 3 we describe the pipeline and
the different steps of our architecture. Looking at the bigger
picture, our framework receives as input a single RGB frame
Ij. ,fromn > | camera video streams Cy __,,. We then apply
fast, single network 2D pose estimation [9] for each frame,
in order to filter matching subjects across frames and obtain
the corresponding bounding boxes Bo__,, (Fig. 4). We then
apply our custom optimised human mesh recovery method
based on [13, 16] to infer the 3D position of skeletal joints
together with their real-world scale. Finally, when dealing
with n > 1 cameras, we align the skeletons centroids and
use a least squares approach to find a set of rigid transfor-
mations 7; ;_,o;<1,...n; from each skeleton to another one in
3D world space. After minimizing the displacement error
between skeletons in 3D space, we can exploit the epipolar
geometry as well as the world-space and image-space posi-
tion of joints to retrieve both the extrinsic parameters for

1761

rotation and translation R | t and the fundamental matrices
F (;_,o\i=1,....n}- 10 case of a single camera, the matching step
and the fundamental matrix calculations are being ignored
and we simply retrieve the camera matrix as well as the 3D
human pose and shape.

Whenever the framework detects a difference in the
detected 3D-space joints, which is bigger than a threshold,
it triggers a new re-calibration cycle, in order to keep the
network calibrated over time, progressively refining its accu-
racy. Another big advantage of our method is its flexibility.
In fact, it can work even with a single camera and it allows
for new cameras to be plugged into the system in a dynamic
fashion.

In the next sections, we refer to the camera matrix as P.
The intrinsic matrix is defined by K, where f, = fm, and
f, = fm, represent the focal length values in pixels, scaled
along x and y by a scaling value m. The principal point of
the camera is represented by (xo, yp). Extrinsic parameters
are modelled by [R | t], where R is the rotation matrix and f¢
identifies the translation vector.

K

Ar [R |
Ix SX0) 7 ple (1)
00 1

4.1 2D pose matching

As a first step, we provide a module that handles fast multi-
person 2D pose estimation and filters detected skeletons to
ensure good pose-based subject matches across the views.
This first part of the architecture takes as input n RGB
frames and outputs a bounding box for the target person in
each image in terms of 2D pose, together with the overall
highest detection confidence score among all the views. To
ensure real-time performances, we employ an improved ver-
sion of the method described by Cao in [2] for joint parts
and pairs detection, namely Single-Network Whole-Body
Pose Estimation (SNWBP) [9]. Alternatively, under particu-
lar conditions such as fixed, single-person, noise-free and
occlusions-free scenarios, it is possible to employ classic
background subtraction methods or more advanced versions
such as [29] to extract the bounding boxes.

At this point the skeleton joints information is already
sufficient to calculate the fundamental matrices that link the
views. However, we decided to further reinforce this estima-
tion by providing additional points obtained from the re-
projection of the 3D skeleton onto the image planes, as we
will explain later on. By doing so, we observe an increment
in the accuracy of the final fundamental matrices. Therefore,
at this phase we only keep a reference to the displacement
of the central point [D?", D}""] and the pixel-size of each

D) Springer
1762

bounding box, as well as an unscale factor, which serves
as a parameter that can be used to reverse the scaling of the
bounding boxes.

The 3D mesh recovery module based on [13] is able to
retrieve an estimation of the person height, which can be
used as a substitute to the real height. However, we provide
as an option the possibility to give as additional input the
real height of the considered subject in order to maximise
the accuracy of the calibration.

4.2 Mesh recovery

Once we recovered the matching bounding boxes across all
the different views together with the optional joint informa-
tion, we need to recover 3D joints information that we will
use to calculate the extrinsic parameters. At this point, each
scaled bounding box Bp __,, 1s configured as a crop of the
frames containing the subject chosen by 2D pose-similarity,
as seen from different viewpoints. We now need to retrieve
the 3D skeleton joints from each viewpoint, in a monocular
fashion without relying on information from the other views.

To achieve this, we employ our modified version of the
method described in [13] and [16] (SPIN). By feeding each
bounding box B; to the network, we obtain the vector 0,
corresponding to the SMPL (Skinned Multi-Person Linear
Model) [18] human body model parameters, which is con-
figured as follows:

camera pose shape
mae OOO (2)
O=[s,t.t, 0 , B |

From each human mesh M(@, f), it is possible to obtain
a set of J = 19 world-scale 3D joints €; (in meter coordi-
nates). The 10 body shape parameters f# encode different
deformations of the mesh shape, and are used to refine the
weak 2D pose matching described in Sect. 4.1 as well as
removing remaining outliers. We discard the original camera
parameters s, t,, ¢, since they model a weak perspective pin-
hole camera model with its principal point shifted by [¢,, 4,]
(Fig. 5). In the original model described in [13], the world
translation of the mesh is computed as z = F’/s, where s is
a scaling factor.

The weak perspective model is not accurate enough for
retrieving real-world mesh displacements because it does
not take into account perspective transformations. In fact, in
weak perspective geometry, perspective transformations are
modeled via a simple scaling in the subject size, proportion-
ally to its distance from the camera. In practice, if we take
into account the manifold of commercially available sen-
sors and lenses, employing a weak camera model is a strong
generalisation, which can lead to substantial errors. For this
reason, we substitute the original weak camera model with

D) Springer

Journal of Real-Time Image Processing (2020) 17:1757-1768

 

Fig.5 Principal point offset for mesh positioning in the weak per-
spective camera model does not correspond to a real-world mesh
translation [28]

a fully-fledged perspective one, to recover the real-world
mesh displacement A™” in millimeters, as shown in Eq. 3:

Amm aye
x Amn
as as »
Amm APx Amn APx mm mm (3)
Amm _ z x z y f wB
— pix ° pen ° WBpix
x y

where f?* = [f?", py corresponds to the focal length val-
ues in pixels, w is the image width in pixels and W is the
sensor width in millimeters. B?'* and B™™ are the image-
coordinates and world-coordinates sizes of the bounding
boxes retrieved from 4.1. At this point, 4°" contains the
real-world relative translation going from the camera C; to

the 3D skeleton &,.
4.3 Skeleton matching

At this stage, in presence on an arbitrary number > | cam-
eras, we have obtained n camera-centric systems each one
referring to a 3D skeleton. The next step is setting each skel-
eton’s centroid c, as the pivot point for each corresponding
camera C;. Thus, we need to find the rotation matrices R, that
map each skeleton ¢, __,, to ¢y. We achieve this by moving
towards a skeleton-centric system, in which each skeleton
centroid c 1s positioned in the center of coordinates (0, 0, 0).
In this space, we can find the relative skeleton-to-skeleton
transformations in terms of rotations and translations using
a single value decomposition (SVD) approach, as explained
in Eqs. 4 and 5.

H=&-é& , U,S,V' =SVD(A) (4)

R=V-U' , t=c,-(R-o)) (5)

More in detail, we calculate H as the dot product of a pair
of 3D point sets of joints € and €,. We then apply an SVD to
#to find the matrices U, S, V, as explained in Eq. 4. Finally,
Journal of Real-Time Image Processing (2020) 17:1757-1768

   

ey [& ] — H=f0-& 1 & J
e°%

 

Fig.6 Our SVD approach for 3D skeleton matching

 

Fig.7 Visualisation of the final result of our automatic calibration
pipeline

we find the rotation matrix R and the translation vector ¢ as
detailed in Eq. 5. A simple representation of the 3D skeleton
match can be seen in Fig. 6.

Then, we move back to the camera-centric space and find
the transformation that maps ¢) to 4,"". We finally find the
inverse transformations A", starting from Eq. 3.

By applying this procedure, we obtain a 3D space, in
which the first camera Cp is positioned at the center of the
coordinate system, the n skeletons are in AD and the rela-
tive position of all the other virtual cameras is known. An
example of the final output of the whole pipeline can be
seen in Fig. 7.

4.4 Fundamental matrix

With the skeletons ¢) _; correctly positioned in the 3D
world, we calculate X as the merged 3D skeleton contain-
ing the mean values of all the joints coming from €9 __ ; in
world-space coordinates. Since we also know the position of
each camera in the 3D world, we can project X to each image

1763
Table 1 Reprojection errors in pixels for the four test scenarios
Scenarios
Kitchen Gym Laboratory Apartment
MRE 12.77 6.15 12.03 8.38

Bold indicates the lowest error for each metric

plane of C;, obtaining o;. We then build a vector o; contain-
ing 2D skeleton joints values for a batch of frames coming
from C;, and use it as ordered keypoints to find the funda-
mental matrices F’,,_,, that match camera C; with camera C>.

This allows us to find the epipolar lines and corresponding
matching points between pairs of camera views. Moreover,
since the extrinsic matrices have been previously retrieved, it
is possible to describe, how points in world coordinates map
to each camera coordinate system, and viceversa.

5 Results

To test our framework, we conducted seven real-world
experiments in different scenarios, as listed in Table 2. The
first four experiments were carried out in a real living lab
consisting of three rooms, which is equipped with a network
of identically-configured HD and FullHD cameras monitor-
ing all the rooms. The last three experiments serve as a com-
parison of the proposed pipeline with our previous method,
which employed video sequences instead of single frames, as
well as slower and less precise human pose estimators. Our
results are comparable with the ones provided by [5], both in
terms of spatial configuration and precision, even if we rely
on just monocular information from simple RGB cameras
and not on depth or triangulation. Experiments 2, 3, 7 show
how our method is robust against important occlusions in the
scene. In experiment 5 we demonstrate how our method can
also work with very distant and little overlapping cameras.
In experiment 6 we employ two handheld smartphones (not
stabilized) and successfully retrieve a good estimation of
their pose in the 3D world.

5.1 Quantitative results

The main results of our experiments are listed in Tables |
and 2. As can be seen, they are comparable with the results
provided by [5], particularly taking into consideration that
we only employ monocular cameras and no additional
depth sensors. The metrics MinSDE, ASDE and MaxSDE,
describe the minimum, average, and maximum displace-
ment of skeletal joints, respectively, after the matching in
3D space, in meters, calculated by the Euclidean distance:

D) Springer
Journal of Real-Time Image Processing (2020) 17:1757-1768

1764

60°0
See
9C'E

910

e10

O10

9 CV
wu / ..8°C/T

SL8
Cc

juounredy

L

e10
6L'S
99°¢

80°0

90°0

10°0

LO’ OV

wu €°9 ..8°C/T wu £°¢ 0 e/T

OSC
Cc

Aroyeroge’]

9

(10110 JUOWMSORTdsIp ouvyd ‘ouvyd yemata ‘oueyd [eor) ouvyd oy} SuoTe yUoUTOOR]dsIp ‘(xe ‘ASRIOAR ‘UIUT) JOLIN JUOUMIORTd
-SIP UOJIOYS CE ‘ISU [edoj ‘sozIs JOsUas ‘soweIJ JO JoquINU ‘seIOWIvS JO JaquuNU :sMmOYy ‘([/] POUJSW snoTAdId INO WOIF S}[NSOI 91e JOIY} ISP] OY}) SOLIVUIDS }S9} JUdIOJJIP USAVS oY) :suUUNTOD

1c 0
COL
CLL

VIO

80°0

vO'0

gs Oe Oe
wu [°6 ..€/T

OSC
€

WAS ITBYoTOOU AA,
g

s0°0
OS's
SVs
‘pu
‘pu
‘pu

O'S

WOOI SUIATT

Vv

80
vo9
98°¢
80°0
v0'0
80-°FC°T

O'S OT

D usyoIryy
€

OINOU YORI IOJ JOLII SAMO] BY) SoyeOIpUI Plog

LO'0
vO'L
L69
O10
LO'0
80-980 ¢

O'S OV

g uoyIry
Z

Le 0 Hdd
99 ddA
60°9 dda
juawmaovjdsip pj4OM-]DAM
90°0 xew 4dS
r0°0 BAB ACS
80-9Pe'€ ural Hd$S
SUIYIIDU Ge
0's OV yrsusy [e00,J
WU 87°9 ..¢/T OZIS JOSUdS
I sowely JO “WINN
Z seIowes Jo “WNnN
uoYDANSYUO)
V usyory
I
sjuowiiadx7q

synsol feuoutiadxy 7 ajqey

pringer

Y) S
Journal of Real-Time Image Processing (2020) 17:1757-1768

1765

 

Fig.8 Kitchen scenario

SDE = (6)

 

RPD, VPD (real and virtual plane displacements) are the
measures of the displacement from the origin along the real
world plane and the virtual world plane respectively. The
RPD has been calculated starting from ground truth annota-
tions, while the VPD can be calculated once again with an
Euclidean distance from the origin, discarding the z com-
ponent. The plane displacement error (PDE) is computed as
| RPD — VPD |, once again in meters. The MRE is the mean
reprojection error calculated after applying the fundamental
matrix F to the set of points o,.

Our results for most of the scenarios are also better than
the checkerboard results obtained by [5] using the method
described in [34].

5.2 Reprojection error

After finding the fundamental matrices F for each scene and
the corresponding epipolar lines, we assess the precision of
our method by calculating the reprojection error in term of
point-line-distance, as follows:

| ax + bxy + |
Va? +b?

where a, b and c are the epipolar lines coefficients and [Xo, vo]
are the coordinates of the projected points. In Table 1 the
reprojection errors in pixels for each scenario are listed,
showing that the proposed method is robust in all the four
test environments considered.

5.3 Qualitative results

In Figs. 8 and 9 we provide some qualitative results
through the Autodesk Maya® 3D animation, modeling,
simulation, and rendering software. In each image a recon-
struction of the 3D scene is shown, including the 3D skel-
eton used for the matching, the virtual plane and every
virtual camera with correct roll, pitch, yaw, translation,
focal length and frustum size. We decided to discard the
approximate differentiable render OpenDR [17] used by
[18] and [13] in favour of Maya because the latter lets us
configure in fine details many camera parameters includ-
ing the focal length, the film gate and frustum size in mil-
limeters. Moreover, our entire code can directly run into
the Maya environment, allowing us to easily extend the
scope of our work to weak monocular 3D human motion
capture from video footage, also from a single camera. Our
3D reconstruction module in Maya is standalone and can
receive camera and skeleton data from an external machine
via a command port socket in real-time. As an alternative,
we provide bindings for Open3D.

Concluding, despite the lack of proper datasets to
benchmark these kind of applications, we also provide, in
addition to the original experiments, some good qualita-
tive results from the Panoptic Dataset [12] and from fully
simulated scenarios. In Fig. 10 we show an example of
camera pose estimation from 8 different views caught from
8 virtual cameras inside the Unity 3D environment. Simi-
lar results can be obtained for all the 480 VGA cameras in
the Panoptic Dataset.

g) Springer
1766

Journal of Real-Time Image Processing (2020) 17:1757-1768

 

 

Fig.9 Gym scenario

Top view

Free view

 

 

 

 

Side view

Fig. 10 Simulated scenario: estimating the pose of 8 virtual cameras inside Unity

6 Conclusions

We presented a completely unsupervised and one-shot
camera network calibration framework capable of cali-
brating a single camera or a camera network only from
monocular human pose estimation cues. We employ a
3-stage approach which comprises (1) fast, single network
whole body pose estimation and matching among camera
views, (11) perspective corrected, optimised monocular

D) Springer

human mesh recovery from a single frame and (ili) joint
2D and 3D skeleton matching in camera-centric and skel-
eton-centric coordinates. As final output we provide the
extrinsic parameters for linking world space with cam-
era space for each camera in the network, as well as their
fundamental matrices, to link camera views. Compared to
the other related works in literature and with our previous
approach, the presented framework enables the possibil-
ity for real-time, one-shot network calibration, which is
Journal of Real-Time Image Processing (2020) 17:1757-1768

camera-independent and which requires only one frame
as input. It is robust to occlusions and noise in the scene
thanks to the 3D skeleton matching approach, and it is able
to perform real-time re-calibration thanks to its stream-
lined parallel architecture.

6.1 Future work

As future work, the adoption of a capsule network model for
estimating the body pose could solve many issues, particu-
larly with respect to (1) pose flickering, (11) extreme camera
viewpoints and (111) non-existent viewpoint-equivariance.
Additional improvements could be made by reinforcing the
matching algorithm with SIFT/SURF features and alike.
Adding the possibility to estimate the intrinsic parameters
in a robust way could greatly improve the overall deploy-
ability and accuracy of our framework.

Acknowledgements This research was developed within the frame-
work of the project AUSILIA (2015-2020), funded by the Autonomous
Province of Trento (Italy).

Funding Open access funding provided by UniversitA degli Studi di
Trento within the CRUI-CARE Agreement.

Open Access This article is licensed under a Creative Commons Attri-
bution 4.0 International License, which permits use, sharing, adapta-
tion, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source,
provide a link to the Creative Commons licence, and indicate if changes
were made. The images or other third party material in this article are
included in the article’s Creative Commons licence, unless indicated
otherwise in a credit line to the material. If material is not included in
the article’s Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will
need to obtain permission directly from the copyright holder. To view a
copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.

References

1. Andriluka, M., Roth, S., Schiele, B.: Pictorial structures revisited:
People detection and articulated pose estimation. In: 2009 IEEE
Conference on Computer Vision and Pattern Recognition, pp.
1014-1021 (2009)

2. Cao, Z., Simon, T., Wei, S., Sheikh, Y.: Realtime multi-person 2d
pose estimation using part affinity fields. In: 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR), pp.
1302-1310 (2017)

3. Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., Sheikh, Y.: Openpose:
realtime multi-person 2d pose estimation using part affinity fields
(2018). arXiv preprint arXiv: 1812.08008

4. Coughlan, J.M., Yuille, A.L.: The manhattan world assumption:
Regularities in scene statistics which enable bayesian inference.
In: Leen, T.K., Dietterich, T.G., Tresp, V. (eds.) Advances in Neu-
ral Information Processing Systems, vol. 13, pp. 845-851. MIT
Press, Cambridge (2001)

10.

11.

12.

13.

14.

15.

16.

17.

18.

19,

20.

21.

1767

Desai, K., Prabhakaran, B., Raghuraman, S.: Skeleton-based con-
tinuous extrinsic calibration of multiple rgb-d kinect cameras. In:
Proceedings of the 9th ACM Multimedia Systems Conference,
MMsSys’ 18, pp. 250-257, New York, NY, USA (2018) (Associa-
tion for Computing Machinery)

Durrant-Whyte, H., Bailey, T.: Simultaneous localization and
mapping: part 1. IEEE Robot. Autom. Mag. 13(2), 99-110 (2006)
Garau, N., Conci, N.: Unsupervised continuous camera network
pose estimation through human mesh recovery. In: Proceedings
of the 13th International Conference on Distributed Smart Cam-
eras, ICDSC 2019, New York, NY, USA (2019) (Association for
Computing Machinery)

Geiger, A., Moosmann, F., Car, O., Schuster, B.: Automaticcamera
and range sensor calibration using a single shot. In: 2012IEEE
International Conference on Robotics and Automation, pp. 3936—
3943 (2012)

Hidalgo, G., Raaj, Y., Idrees, H., Xiang, D., Joo, H., Simon, T.,
Sheikh, Y.: Single-network whole-body pose estimation (2019).
arXiv preprint arXiv: 1909.13423

Hold-Geoffroy, Y., Sunkavalli, K., Eisenmann, J., Fisher, M.,
Gambaretto, E., Hadap, S., Lalonde, J.-F.: A perceptual measure
for deep single image camera calibration. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2354-2363 (2018)

Inomata, R., Terabayashi, K., Umeda, K., Godin, G.: Registra-
tion of 3d geometric model and color images using sift and range
intensity images. In: Bebis, G., Boyle, R., Parvin, B., Koracin,
D., Wang, S., Kyungnam, K., Benes, B., Moreland, K., Borst, C.,
DiVerdi, S., Yi-Jen, Ming J. (Eds.), Advances in Visual Comput-
ing, Berlin, Heidelberg. Springer Berlin Heidelberg, pp. 325—336
(2011)

Joo, H., Liu, H., Tan, L., Gui, L., Nabbe, B., Matthews, I., Kanade,
T., Nobuhara, S., Sheikh, Y.: Panoptic studio: A massively mul-
tiview system for social motion capture. In: 2015 IEEE Interna-
tional Conference on Computer Vision (ICCV), pp. 3334-3342
(2015)

Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end
recovery of human shape and pose. In: 2018 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 7122-7131
(2018)

Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3d
human dynamics from video. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 5614-5623
(2019)

Kim, H., Hong, K.S.: Practical self-calibration of pan-tilt cameras.
IEE Proc. Vis. Image Signal Process. 148(5), 349-355 (2001)
Kolotouros, N., Pavlakos, G., Black, M.J., Dantilidis, K.: Learning
to reconstruct 3d human pose and shape via model-fitting in the
loop (2019). arXiv preprint arXiv:1909.12828

Loper, M.M., Black, M.J.: Opendr: an approximate differenti-
able renderer. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars,
T. (eds.) Computer Vision—ECCV 2014, pp. 154-169. Springer,
Cham (2014)

Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black,
M.J.: Smpl: a skinned multi-person linear model. ACM Trans.
Gr. (TOG) 34(6), 248 (2015)

Lucas, B.D., Kanade, T.: An iterative image registration technique
with an application to stereo vision. pp. 674-679 (1981)

Miyata, S., Saito, H., Takahashi, K., Mikami, D., Isogawa, M.,
Kojima, A.: Extrinsic camera calibration without visible cor-
responding points using omnidirectional cameras. IEEE Trans.
Circuits Syst. Video Technol. 28(9), 2210-2219 (2018)

Nistér, D., Naroditsky, O., Bergen, J.: Visual odometry. In: Pro-
ceedings of the 2004 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, 2004. CVPR 2004,
vol. 1. IEEE, pp. I-I (2004)

Q) Springer
1768

22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

Peng, X.B., Kanazawa, A., Malik, J., Abbeel, P., Levine, S.: Sfv:
Reinforcement learning of physical skills from videos. In: SIG-
GRAPH Asia 2018 Technical Papers. ACM, p. 178 (2018)
Presti, L.L., Cascia, M.L.: 3d skeleton-based human action clas-
sification: a survey. Pattern Recogn. 53, 130—147 (2016)
Ramakrishna, V., Munoz, D., Hebert, M., Andrew Bagnell, J.,
Sheikh, Y.: Pose machines: articulated pose estimation via infer-
ence machines. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars,
T. (eds.) Computer Vision—ECCV 2014, pp. 33-47. Springer,
Cham (2014)

Seo, Y., Hong, K.S.: Theory and practice on the self-calibration of
a rotating and zooming camera from two views. IEE Proceedings
- Vision, Image and Signal Processing 148(3), 166-172 (2001)
Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M.,
Moore, R., Kipman, A., Blake, A.: Real-time human pose recog-
nition in parts from single depth images. In: CVPR 2011. Pro-
ceedings of the IEEE Computer Vision and Pattern Recognition
(CVPR) 2011, pp. 1297-1304 (2011)

Shotton, J., Sharp, T., Kipman, A., Fitzgibbon, A., Finocchio, M.,
Blake, A., Cook, M., Moore, R.: Real-time human pose recogni-
tion in parts from single depth images. Commun. ACM 56(1),
116-124 (2013)

Simek, K.: Pinhole camera diagram, dissecting the camera
matrix. http://ksimek.github.io/pinhole_camera_diagram/, 2013.
Accessed 26 Apr 2019

Tang, Z., Hwang, J., Lin, Y., Chuang, J.: Multiple-kernel adaptive
segmentation and tracking (mast) for robust object tracking. In:
2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 1115-1119 (2016)

Tang, Z., Lin, Y., Lee, K., Hwang, J., Chuang, J.: Esther: Joint
camera self-calibration and automatic radial distortion correction
from tracking of walking humans. IEEE Access 7, 10754-10766
(2019)

Tome, D., Russell, C., Agapito, L.: Lifting from the deep: Convo-
lutional 3d pose estimation from a single image. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 2500-2509 (2017)

Vasconcelos, F., Barreto, J.P., Boyer, E.: Automatic camera cali-
bration using multiple sets of pairwise correspondences. IEEE
Trans. Pattern Anal. Mach. Intell. 40(4), 791-803 (2018)

Wei, S., Ramakrishna, V., Kanade, T., Sheikh, Y.: Convolutional
pose machines. In: 2016 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 4724-4732 (2016)

Zhang, Z.: A flexible new technique for camera calibration. IEEE
Trans. Pattern Anal. Mach. Intell. 22(11), 1330-1334 (2000)
Zhang, G., Zhao, H., Hong, Y., Ma, Y., Li, J., Guo, H.: On-orbit
space camera self-calibration based on the orthogonal vanishing
points obtained from solar panels. Meas. Sci. Technol. 29(6),
065013 (2018)

Zhao, F., Tamaki, T., Kurita, T., Raytchev, B., Kaneda, K.:
Marker-based non-overlapping camera calibration methods with

Q) Springer

Journal of Real-Time Image Processing (2020) 17:1757-1768

additional support camera views. Image Vis. Comput. 70, 46-54
(2018)

Publisher's Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.

Nicola Garau received the BSc and MSc degrees in data science from
the University of Cagliari, Cagliari, Italy and from the University of
Trento, Trento, Italy, in 2016 and 2018 respectively. He is currently a
Ph.D student at the University of Trento. He was a visiting internship
student at the National Centre for Computer Animation, Bournemouth
University, Poole, U.K. His current research focuses on human pose
estimation and capsule networks, with particular application to ambient
assisted living and human behaviour analysis.

Francesco G.B. De Natale received the MSc degree in 1990, and the
PhD degree in 1994. He is a professor of Telecommunications with the
University of Trento, Italy, where he leads the Multimedia Lab (mmlab.
disi.unitn.it). His research interests include focused on multimedia sig-
nal processing, analysis, and retrieval. He was program co-chair of the
IEEE International Conf. on Image Processing in 2005 and General
chair of the ACM International Conf. on Multimedia Retrieval in 2011.
He has been associate editor of the IEEE Trans. on Multimedia and of
the IEEE Trans. on Circuits and Systems for Video Technologies, as
well as a member of the IEEE Signal Proc. Society Technical Commit-
tee on Multimedia Signal Processing (MMSP), chairing the Technical
Directions Committee. Currently, he is member of the Board of direc-
tors of the Italian Consortium for Telecommunications (CNIT), and
member of the management of the Italian Group of Telecommunica-
tions and Information Technologies (GTTI). He published more than
150 papers in international journals and conferences, mostly in the
area of multimedia signal processing and communications, and has
been scientific coordinator of large-scale research and development
projects, both at the national and international level. He was appointed
evaluator for several international bodies, including EU Commission,
NSF-US and NSF-Ireland.

Nicola Conci Ph.D, is Associate Professor at the University of Trento.
In 2007 he was a Visiting Student with the Image Processing Labora-
tory, University of California at Santa Barbara, Santa Barbara, CA,
USA, and a Postdoctoral Researcher with the Multimedia and Vision
Research Group, Queen Mary University of London, London, U.K.
He has authored and co-authored more than 110 scientific papers in
international journals and conferences. His current research focuses
on automatic video understanding for human behavior analysis with
particular application to environmental monitoring, surveillance, and
assisted living.
