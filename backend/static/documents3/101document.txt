 

publications (mpPt|

Article
Assessment of Factors Causing Bias in Marketing-
Related Publications

Mangirdas Morkunas '*, Elzé Rudiené 7, Lukas Giritinas ° and Laura Dautiiniené °

1 Division of Farms’ and Enterprises’ Economics, Lithuanian Institute of Agrarian Economics, Vivulskio str.

4A, 03220 Vilnius, Lithuania

2 Business School, Vilnius University, Sauletekio ave. 21, 10222 Vilnius, Lithuania; elze.rudiene@vm.vu.lt
3 Faculty of Public Governance and Business, Mykolas Romeris university, Ateities str. 20,
08303 Vilnius, Lithuania; lukas.giriunas@mruni.eu (L.G.); laura@dauciuniene.It (L.D.)
* — Correspondence: mangirdas.morkunas@evaf.vu.lt;
G check for
Received: 13 September 2020; Accepted: 15 October 2020; Published: 24 October 2020 updates

Abstract: The present paper aims at revealing and ranking the factors that most frequently cause bias
in marketing-related publications. In order to rank the factors causing bias, the authors employed
the Analytic Hierarchy Process method with three different scales representing all scale groups.
The data for the study were obtained through expert survey, which involved nine experts both from
the academia and scientific publishing community. The findings of the study confirm that factors that
most frequently cause bias in marketing related publications are sampling and sample frame errors,
failure to specify the inclusion and exclusion criteria for researched subjects and non-responsiveness.

Keywords: bias; research; marketing; publication

1. Introduction

Bias can be defined as any systematic error in the design, conduct or analysis of a study. In research,
bias occurs when “systematic error introduced into sampling or testing by selecting or encouraging
one outcome or answer over others”. Bias can attend at any phase of research, including study design
or data collection, as well as in the process of data analysis and publication. Bias is not a dichotomous
variable [1,2].

Most fields of science, including social sciences, are currently facing a deep ‘reproducibility
crisis’ [3-5]. Research is relevant in all fields of science, whether conducted in the form of experiments,
quantitative, qualitative studies or multidimensional research, so bias can manifest itself at every
stage of the research process. Bias patterns and risk factors can thus be assessed across multiple
topics within a discipline, across disciplines or larger scientific domains (social, biological and physical
sciences), and across all of science [6-9]. Many of the biases described can be accepted in literature
though their extent still remains unspecified. It is likely that different biases pose different threats to
different disciplines. The interests of the authors of the present paper relate precisely to the biases of
marketing research in interpreting and researching biases characteristics. Different scholars [6,10,11]
have confirmed that bias is uneven. The bias in research could be manifested in a number of ways,
as found in literature [5,12—14]. Publication or literature biases can be considered among the most
frequently mentioned and discussed. Bias is also relevant in life sciences [15-20], psychology [21,22]
education [23,24] and economics [25,26].

An important bias of studies is manifested in its lower precision [27]. This could be related to
the fact that minor studies might report effect of larger magnitude. This problem could stem from
the genuine heterogeneity in study design [6] and relevant in different areas of research and related to
measurement bias that occurs during the process and reflects a discrepancy between the information

Publications 2020, 8, 45; doi:10.3390/publications8040045 www.mdpi.com/journal/publications
Publications 2020, 8, 45 2 of 16

collected and the information the researcher seeks to obtain. As confirmed by [28], “publications from
authors working in the United States might overestimate the effect of sizes, a difference that could be
due to multiple sociological factors”. Bias in measurement methods is very important for research
processes [29] especially in medicine [30-33] and in social sciences [34,35]. Low precision biases in
marketing related publications could be important and must be also properly taken into account.

Another type of bias related to previous studies reporting an effect might overestimate its
magnitude relative to later studies. This is due to a decreasing field-specific publication bias over
time or to differences in study design between the earlier and the later studies [6,36]. On the other
hand, the decline effect might see earlier studies reporting extreme effects in any direction, because
controversial findings have better opportunities to be published [36,37].

All the biases in research mentioned earlier could be considered to be technical, and representing
only one side of biases. Usually, psychological and sociological factors that may lead to the bias patterns
as described above [6,38], such as pressures to publish, are forgotten: when scientists subjected to direct
or indirect pressures to publish are more likely to exaggerate the magnitude and importance of their
results to secure many high-impact publications and new grants [39,40]. Furthermore, there is peer
control, when researchers working in close collaboration are able to mutually control each other’s work
and might therefore be less likely to engage in questionable research practices (QRP) [6,41]. If so, risk
of bias might be lower in collaborative research but, adjusting for this factor, higher in long-distance
collaborations [6].

One more psychological factor—career stage—when early-career researchers might be more likely
to engage in ORP, because they are less experienced and have more to gain from taking risks [42].
However unbelievable it seems, a certain role upon the research bias is played by the gender of
the researcher: males are more likely to take risks to achieve higher status and might therefore be
more likely to engage in ORP. This hypothesis was supported by statistics of the US Office of Research
Integrity [43], which, however, may have multiple alternative explanations [44]. In case of psychological
factors, it is necessary to bear in mind individual integrity: narcissism or other psychopathologies
underlie misbehavior and unethical decision making and therefore might also affect individual research
practices [45,46].

In the area of marketing related publications all of the types of biases as covered above are
relevant, the issue is, however, to identify which of them are more important, and which are less
relevant in in marketing related publications. So, the novelty of the study covered by the present
paper lies in identifying and ranking the factors that most frequently cause bias in marketing literature.
The findings of this study also have practical implications, as they serve as a guide to young researchers
in marketing area helping to produce more robust and relevant results by avoiding most common
mistakes. The study reported in the present paper complements a number of studies by [47-49] by
providing an importance rankings of bias inducing factors.

2. Materials and Methods

2.1. The Researched Factors

An overview of the literature sources in the area showed a diversity of biases prevailing in different
fields of science and the related research. This is understandable because different research addresses
different issues, uses different methods; nevertheless, it is still possible to group and combine most
relevant biases. The material in the relevant research papers shows that biases are examined in medical
and natural sciences, somewhat less frequently in the social sciences, and very little in the marketing
area. It should be noted that many of the biases discussed are also relevant in marketing research;
however, there is a lack of more detailed research in this aspect. Therefore, our study must show
the relevance of biases in marketing research and their ranking in terms of importance (with the help
of experts and their position).
Publications 2020, 8, 45 3 of 16

Based on our observations from the literature review, the authors of the present paper distinguished
10 factors that are responsible for the biggest part of research bias in marketing related publications.

(1) Failure to examine and critically assess prior literature:

Most studies start with an idea, question or a topic, sometimes rather numerous—are not new, and
had already been previously studied. Often such bias emerges due to a failure to evaluate the issue in
previous studies or literature. Researchers and statisticians [14,50] have documented publication bias
across a variety of academic disciplines including behavioral sciences [51], education [52,53], special
education [54,55], ecology [56], medicine [28,57-61], psychology [13,50,51,62-66] and theatre and
performance [5]. As [67] mentioned, using databases, such as Emerald, EBSCO, Jstor and ScienceDirect,
it is necessary to read all more detailed articles on the chosen topic. However, it should be noted
that unpublished scientific results may differ systematically from published results [68-70]. Such bias
is referred to as publication bias that can affect the literature analysis. According to [54], there are
two possible approaches for correct literature analysis: search and inclusion procedures and a formal
and statistical approach. It involves, “conducting searches (such as electronic database search, hand
searching of journals, contacting experts) to identify all relevant studies, including gray literature (i.e.,
unpublished studies)”. Greenhalgh and Peacock [71] have confirmed that using data from electronic
databases is the result of a failure to carry out studies. An important aspect of bias in literature
and publications is gray literature bias [6,10,71,72]. Polanin, Tanner-Smith and Hennessey defined
gray literature as, “literature can be broadly thought of as anything not published in a professional
journal, including dissertations, policy reports, conference proceedings, book chapters, or otherwise
unpublished studies” [72].

(2) Failure to specify the inclusion and exclusion criteria for study subjects:

Researchers often omit the specification of the research subject, by engaging inclusion and
exclusion criteria. If the criteria were named, other researchers would understand why the current
results may differ from other published studies. To be eligible for publication, research papers needed
to be based on and refer to empirical research, rather than on commentaries, letters, editorials or
reviews. This process involved identification, screening and inclusion and exclusion criteria [8].

(3) Failure to determine and report errors in measurement methods:

Measurement bias occurs during the process and reflects a discrepancy between the information
collected and the information the researcher seeks to obtain. Reference [73] confirmed that response
rates to surveys have fallen, in particular, in developed countries, which highlights the actual problem
regarding measurements methods in researches. Some authors such as [74—76] state that non-response
rate is caused by a practice of bringing to the study a sample of reluctant people, who may provide
data filled with measurement errors. Questions arise when this hypothesized relationship between
low propensity to respond and measurement error arises. The first has to do with the quality of
the statistics (e.g., means, correlation coefficients) computed on the basis of a survey. That is, does
the mean square error of a statistic increase when sample persons who are less likely to be contacted or
cooperate are incorporated into the response? The level of effort analyses the change in statistics over
increased levels of effort, taking change in the statistics to indicate the risk of nonresponse bias, and
no change to indicate the absence of risk. However, if measurement error is correlated with level of
effort (or response propensity), then an observed change or lack of change in the statistic may be due
to measurement error and not to nonresponse bias [29].

(4) Failure to specify the exact statistical assumptions made in the analysis and failure to perform
sample size analysis before starting the study:
Publications 2020, 8, 45 4 of 16

When an omitted variable (i.e., an unmeasured variable not included in a model) creates
a correlation between the error terms in these two stages, traditional techniques, such as ordinary
least squares (OLS) regression, may report biased coefficient estimates [77]. Since most studies will
include statistical analysis of the data, specifying the level of significance (called the alpha level) that
is acceptable and the exact statistical tests methods used is commonplace. Most trials that claim
two methods are equivalent (or non-superior) or underpowered, which means they have too few
subjects. The sample size must be reasonable in order to obtain statistically reliable results. As stated
in [78], prior studies as an estimation can be used, but “although this strategy is intuitively appealing,
effect-size estimates, taken at face value, are typically not accurate estimates of the population effect
size because of publication bias and uncertainty. It is shown that the use of this approach often results
in underpowered studies, sometimes to an alarming degree” (p.1547).

(5) Improper Specification of the Population:

It is a biased study of population that loses validity in relation to the degree of the bias [7,79] in
their research, compared the precision and bias of projections of total population with the precision and
bias of projections by different dimensions and country level. Population specification biases could
occur when a researcher does not understand what the object of the study was. Many researchers have
used time series models to construct population forecasts and prediction intervals at the national level,
but few have evaluated the accuracy of their forecasts or the out-of-sample validity of their prediction
intervals [79]. Researchers studying bias in population specification have focused on patterns of overall
population growth [80,81], while others have examined individual components, such as mortality and
fertility, migration [41,82], but they were all linked by key issues, such as uncertainty in population
forecasts, the development of models that provide specific measures of uncertainty.

(6) Sampling and Sample Frame Errors:

Survey sampling and sample frame errors occur when the wrong subpopulation is used to select
a sample, or because of variation in the number or representativeness of the sample that responds, but
the resulting sample is not representative of the population concerned. In some cases, sample selection
bias can lead researchers to find significant relationships that do not exist, or in other cases it can lead
researchers to fail to find significant relationships that do exist [77] (Bias in sampling and sample frame
can occur when including inappropriate objects with or without certain characteristics [83].

(7) Selection Errors:

Determining whether or not an observation in an overall population appears in its final
representative sample is the first stage, and modeling the relation between the hypothesized dependent
and independent variables in the final [77]. This bias is related to the sampling error, sample selected
by anon-probability method. It could happen when respondents choose to self-participate in a study
and only those interested respond; you can end up with selection error because there may already be
an inherent bias. This can also occur when respondents who are not relevant to the study participate,
or when there is a bias in the way participants are put into groups.

(8) Non-Responsiveness:

Nonresponse error can exist when an obtained sample differs from the original selected sample.
(2002) [73] confirm, that best practices argue that researchers should attempt to maximize response rates
and to minimize risk of nonresponse errors [84]. However, research [85-87] has called the traditional
view into question by showing no strong relationship between nonresponse rates and nonresponse
bias [29]. This may occur because either the potential respondent was not contacted or the respondent
refused to respond. The key factor is the absence of data, rather than inaccurate data. An increase in
mean square error could occur because (a) incorporating the difficult to contact or reluctant respondents
Publications 2020, 8, 45 5 of 16

results in no nonresponse bias in the final estimate, but measurement error does exist, or (b) nonresponse
bias exists, but the measurement error in these reluctant or difficult to contact respondents’ reports
exceeds the nonresponse bias [73]. The second question has to do with methodological inquiries
for detecting nonresponse bias. Although many types of analyses of nonresponse bias can be
conducted, four predominant approaches have been used: (1) comparing characteristics of the achieved
sample, usually the demographic characteristics, with a benchmark survey [88], (2) comparing frame
information for respondents and nonrespondents [89], (3) simulating statistics based on a restricted
version of the observed protocol [85], often called a “level of effort” analysis, and (4) mounting
experiments that attempt to produce variation in response rates across groups known to vary on
a survey outcome of interest [90] Findings from these studies show that nonresponse bias varies across
individual statistics within a survey and is relatively larger on project needs to be able to answer
the question.

(9) Missing data, dropped subjects and use of an intention to treat analysis:

It must be acknowledged that incomplete or missing data and the publication of such data
can materially vary. This may in particular be related to the presentation of specific results when
the analysis was performed but not properly described. This may be due to not only to the lack of
understanding on the investigator’s part, but also to coincidence, on the other hand. There is a lack of
understanding in reporting and information, as it can be assumed that the data or part of the results
are not relevant.

More ad hoc research methods can be used to supplement missing data [10,91]. It can always be
assumed that the lack of data is accidental, but the reason for that is bias. Negative research findings
that are likely to outweigh the number of positive findings continue to be sidelined, not published
in unused files [9,12,50,51,92]. Fanelli [6] examined the situation of negative and insignificant results
in a study. The empirical studies in [6] have shown that negative or insignificant results are fairly
common, and in social sciences in particular.

The loss of negative data is due to the fact that results that do not meet expectations and/or
contradict the hypothesis are necessary for scientific progress. Negative findings are important to
consider because they encourage researchers to think critically, reassess or from a different angle,
correct, and perhaps confirm, their current beliefs and move forward [60]. It is therefore essential that
all findings—positive, negative and non-existent—are made available to researchers in order to ensure
a fair and comprehensive summary of research to inform policy, practice or research.

(10) Problems in pointing out weaknesses of own study:

The authors of [24] empirically identified the following weaknesses in studies: (1) lack of
an underlying theory of action, (2) disproportionate reliance on descriptive data, (3) conflation of
correlation with causation, (4) problems in measurement and statistical analyses, (5) absence of study
replication, (6) weak designs without comparability between library and non-library groups and (7)
evidence of publication bias focusing on positive results. Outcome reporting biases could be the most
problematic as every researcher believes that his/her study is good without any weaknesses. Research
by psychologists has shown that at least 63% of researchers have not published full research results [93],
thus not acknowledging any weaknesses in their work. This can be treated as ignoring the identification
of study weaknesses and biasing the results. Withholding negative, inconclusive or nonsignificant
findings distorts the understanding of research within a domain and causes the potential benefits of an
intervention to be overestimated [57].

2.2. Analytic Hierarchy Process Method

Analytic Hierarchy Process (AHP) method found application in marketing science in the first year
of its invention [92]. It is used in strategic marketing planning [94], analyzing marketing mix [95],
revealing consumer intentions [96], assessing determinants of purchase decisions [97] evaluating
Publications 2020, 8, 45 6 of 16

marketing personnel [98] and in comprehensive market evaluation [99]. AHP application is also
common in publishing research [100-102]. It is also a common tool for ranking independent factors
having impact on a complex phenomenon [103-105]. In order to ensure robustness of the results, we
chose an AHP method with three different scales, representing three main scale groups, i.e., from first
category, we chose inverse linear scale [106], logarithmic [107] and a power scale [108] from the second
and the third categories. Once the Eigenvectors using all the three scales were computed, the next step
was the normalization of the obtained results. We chose an AHP as a tool for our research, because it
is a suitable technique for evaluating phenomena that cannot be assessed using purely quantitative
method [109]. The number of factors potentially causing bias in marketing related publications was
limited to 10 positions (the maximum number of alternatives that AHP method is capable to process
adequately).

The factors used in the research were the following: failure to examine and critically assess
the background research literature; failure to specify the inclusion and exclusion criteria for researched
subjects; failure to determine and report the error of measurement methods; failure to specify the exact
statistical assumptions made in the analysis and failure to perform sample size analysis before
the study begins; improper population specification; sampling and sample frame errors; selection
errors; non-responsiveness; missing data, dropped subjects and use of an intention to treat analysis;
and problems to point out the weaknesses of own study. The inquiry method used for the purpose of
the study was an interview, involving nine experts. 5ix of the experts were professors at the Business
schools and/or Marketing/Management departments at the Universities in Lithuania, Poland and
the Czech Republic. Three experts work as editors-in-chief or the managing editors of Scopus
indexed business and economics related journals. The number of experts exceeds the required validity
threshold [110].

Research papers in the area recognize 11 different AHP measurement scales organized into three
different categories that are suitable for research [111]. It is considered that there are no significant
differences in research outcomes because of the different measurement scales, although in order to
ensure robustness of the results, it is recommended to use the combination of different scales. We chose
three different measurement scales, representing all three categories. The mathematical expression of
selected scales is presented below:

Inverselinearscale : c = a ; Logarithmicscale : c = log, (x +a-—1); andPowerscale : c = x°;
where: x—value on the integer judging scale for pairwise comparisons from 1 to 9, c—a ratio used as
entry into the decision matrix [112] (p. 3).

A typical data processing process in AHP appears to be as follows:

At first, experts are presented with pair-wise comparison matrices. After all of the experts
evaluated the factors causing biasness in marketing related publications by using an ex ante prepared
pair-wise questionnaire form, each completed questionnaire is checked for consistency. The matrix is
considered consistent if pix = pip jkr V i, j, k,and a priority vector w exists; then, w = (a1, ..., @n),
where pj; = ay? Yi, j. For the calculation of Consistency Index of experts, Ajax is calculated for every

matrix:

 

, (Pv),
Amax = y oi, (1)

j=l NO;
here:

Amax—largest eigenvector of each standardized matrix;
n—number of independent rows in the matrix;

“l—eigenvector of matrix.

A filled expert pair-wise comparison matrix A is considered consistent when, Ajay = 1, although
in a real-life situation, it happens quite infrequently. In case a marginal p; changes, matrix A satisfies
Publications 2020, 8, 45 7 of 16

the preselected compatibility threshold (0.2 was selected) and Ainay becomes close to n. After calculating
the eigenvalue Ajinay, the Consistency Index CI is being calculated:

Amax — 1

CI =
n-1

; (2)
here:

CI—Consistency Index;
n—number of possible alternatives.

Consistency Index is being used for calculation the overall Consistency Ratio:

CI

CR= =;
RI

(3)

where:

CR—Consistency Ratio;
RI—random Index.

If matrices show CI < 0.2, the aggregated expert evaluation indices are calculated using
a geometrical mean formula:

(4)

 

where:

p;;—aggregated evaluation of element, belonging to i row and j column;

n—number of matrices of the pair-wise comparison of each expert.

When new aggregated matrixes are being calculated, consistency check procedure has to be
performed again. If a matrix is found consistent, then preferred ranks of alternatives are being

V Tis Pi

Wi = Si feo (5)
j=l \ [Tia Pi
where: w;—weight of 7 alternative.

In case the matrices are consistent, but expert evaluations are significantly dispersed, index of

calculated using formula:

expert mutual agreement (S*) is being calculated [113]:

S*
1 — exp(Hamin) / exp(Hymax)

(6)

where:

H,—Shannon alpha diversity;
Hg—Shannon beta diversity;
H,—Shannon gamma diversity.
3. Results and Discussion

The results of calculations are presented in Tables 1-3:
Publications 2020, 8, 45 8 of 16

Table 1. Research reliability indicators.

Reliability Indicators

 

Scale Inverse Logarithmic Power
Lambda, A 8.432 8.256 8.331
Consistency Ratio, CR 0.019 0.013 0.017
Consensus Index, Cl, % 68.2 83.5 74.7

Table 2. Values of obtained eigenvectors of bias influencing factors.

 

Scale Inverse Logarithmic Power
Failure to examine and critically assess the prior literature 0.127 0.111 0.118
Failure to specify the inclusion and exclusion criteria for 0.142 0.151 0.147
researched subjects
Failure to determine and report the error of measurement 0.071 0.076 0.08
methods
Failure to specify the exact statistical assumptions and 0.065 0.069 0.057
failure to perform sample size analysis
Improper population specification 0.097 0.108 0.106
Sampling and sample frame errors 0.17 0.159 0.162
Selection errors 0.102 0.109 0.113
Non-responsiveness 0.124 0.113 0.119
Missing data, dropped subjects and use of an intention to 0.062 0.057 0.052
treat analysis
Problems to point out the weaknesses of your own study 0.04 0.047 0.046

Table 3. Ranks of researched factors.

 

Factors Causing Bias in Ob eank b Rank Obtained by Rank Obtained by _ Final

Marketing Related Publications I y Logarithmic Scale Power Scale Rank
nverse Scale

Sampling and sample frame errors 1 1 1 1
Failure to specify the inclusion
and exclusion criteria for 2 2 2 2
researched subjects
Non-responsiveness 4 3 3 3
Failure to examine and critically 3 4 4 4
assess the prior literature
Selection errors 5 5 5 5
Improper population specification 6 6 6 6
Failure to determine and report 7 7 7 7

the error of measurement methods
Failure to specify the exact
statistical assumptions and failure 8 8 8 8
to perform sample size analysis

Missing data, dropped subjects

and use of an intention to treat 9 9 9 9
analysis

Problems to point out

the weaknesses of your own study 10 10 10 10

Although differences in reliability indicators obtained using different scales are truly marginal,
the highest level of consistency index (83.5%) was derived using Logarithmic scale. It is rather an
accidental result, not a rule, and should be attributed to the characteristics of data researched as there
is no undisputable proof about superiority of one scale above others in terms of consensus index and
employment of a combination of scales is preferred in order to achieve the robustness of results [112].

The values of computed eigenvectors are presented in Table 2.
Publications 2020, 8, 45 9 of 16

Analyzing the eigenvectors of research bias inducing factors, we grouped them into two distinct
groups: important factors—items whose eigenvectors are above 0.1 (sampling and sample frame errors,
failure to specify the inclusion and exclusion criteria for researched subjects, non-responsiveness,
failure to examine and critically assess the prior literature and Selection errors). The rest are less
important. However, in this group differences still exist, as last ranked factor—problems to point out
weaknesses of your own study has twice lower eigenvector value compared to improper population
specification, which indicates its lesser influence on occurrence of bias in marketing related publication.

The results obtained confirm the necessity of using a combination of different measurement
scales, as results obtained with inverse and logarithmic/power scales differ not only in eigenvector
values, but also in ranks of studied factors. Although differences in results derived by logarithmic and
power scales are not significant, and vary only in eigenvector values, results of the inverse scale show
differences also in ranks. In order to offset these differences, the results were normalized. The process
was followed by the computation of the final rank of researched factors that induces bias in marketing
related publications (Table 3).

In different studies publication biases are indicated and analyzed by one or few most important
or dominated factors, especially in marketing publications.

An analysis of the most frequent cause for biases in marketing related publications led the authors
of the paper to the conclusion that the most important factors causing bias are sampling and sample
frame errors and failures to specify the inclusion and exclusion criteria for the study subjects (see
Table 3).

As evidenced by the data in Table 1, these results are confirmed by normalized eigenvector of
power scale as the biggest one, logarithmic scale as well. This ranking shows the importance of
sampling and sample frame errors in marketing publications. In different sciences this type of biases
bear different importance. For example, study in medicine science made by Lin [114] shows that
“sampling error did not cause noticeable bias but the standardized mean difference, odds ratio, risk
ratio, and risk difference suffered from this bias to extents” [114] research results shows importance of
sampling and sample frame errors in a different approach: how to decrease the sample size to a stratified
sample design to achieve an equivalent precision. The second in ranking importance by ranking in
most frequently caused biases in marketing related publications is failure to specify the inclusion and
exclusion criteria for study subjects. Researchers use inclusion and exclusion criteria to determine
characteristics of the subjects or elements in a study. Typical inclusion criteria might be demographic,
geographic and occupational groups [115]. Exclusion criteria are not the opposite of inclusion criteria:
they identify attributes that prevent a person from being included in the study [116]. The fundamental
problem still arises when the researchers do not define inclusion and exclusion criteria clearly. Simply
indicating subjects in the study met inclusion criteria is insufficient and does not allow readers to judge
the validity of the decision. Selecting inclusion criteria that are not related to the research object and do
not describe the variables in sufficient detail is another potential research pitfall.

Ranking third among the most frequent cause of bias in marketing related publications is
the non-responsiveness bias. Reference [117] confirmed general decline in survey response rates in
their research. The new wave of online polls could be as alternative; nevertheless, even that does not
ensure that the non-responsiveness biases can be avoided. Non-responsiveness bias in marketing
related publications are most common for different types of surveys. Probability based surveys still
display less bias than non-probability surveys [118]. A way to address the non-responsiveness bias is
to choose correct type of survey.

One more frequent reason for bias in marketing is related publications—literature bias (see Table 2).
As was mentioned in the literature review as part of the present paper, this type of bias is relevant
in many fields of science; however, it is less common in social sciences [119]. In marketing-related
publications, publication and literature bias are directly related. The findings of the studies conducted
by [119] show that “there is a strong relationship between the results of a study and whether it was
published, a pattern indicative of publication bias”.
Publications 2020, 8, 45 10 of 16

Selection bias in was ranked fifth in the scale of the most frequent cause of bias in marketing-
related publications (see Table 2). This bias is related to the sampling error. Bias like selection is
present and relevant in different fields of science, and in medicine in particular. Apparently, selection
bias is also important in marketing publications. Selection bias could be addressed before starting
the study [120] as it is hidden problem [121]. One of the possible ways to avoid selection biases is to
contact someone who is knowledgeable about causal inference methods [121].

The further two factors that most frequently cause biases in marketing related publications, i.e.,
improper population specification and failure to determine and report the error of measurement
methods, show more than 0.1 in logarithmic scale. Those two factors are important, although to a lesser
extent than the first five covered earlier.

The last three factors show less importance in most frequently caused biases in marketing related
publications: failure to specify the exact statistical assumptions and failure to perform sample size
analysis, missing data, dropped subjects and use of an intention to treat analysis and problems to point
out the weaknesses of own study. This can be seen as ignoring the identification of study weaknesses
and biasing the results. This is related for marketing biases in publications as well, but is not as
important as first seven factors, as was concluded in the present study.

4. Conclusions

Research literature in the area suggests a fairly broad range of factors that in one way or another
cause bias in marketing related publications. For the purpose of the study covered by the present
paper, the authors ranked the factors determining the ones, which most frequently cause bias in
marketing-related publications. The study concluded that sampling and sample frame errors are
the factors most frequently causing bias in marketing related publications. It may be attributed to
the fact that marketing is about revealing people’s preferences and improper selection of the sampling
frame may discredit the main research question, not only trigger some doubts about robustness of
the results. It should be noted that this is specifically related to the marketing area, and other disciplines
in social sciences may show different rankings of the factors. Failure to specify the inclusion and
exclusion criteria for researched subjects, which was ranked second, is very important to much broader
context [122]. The third factor—non-responsiveness—once again is related to possible improper
mirroring of researched population, which is very important in marketing research [123].

The least important factors were named missing data, dropped subjects and use of an intention to
treat analysis and problems to point out the weaknesses of your own study. These bias-creating factors
should be attributed not to the problems in research design, some methodologic weakness, but rather
associated to ethics of the researcher. In general, research ethics is improving as novel instruments for
assuring it is being implemented [124,125], so this research problem is of diminishing importance.

The findings of this study should be considered preliminary and treated as a trigger to start
wider scientific discussions on bias inducing factors in research publications. It would be scientifically
sound to conduct similar researches in other fields of social sciences in order to reveal common factors
causing bias in research publications. The findings of the study are instrumental in creating universal
recommendations helping to eradicate/mitigate effect of at least some of the factors creating bias in
scientific literature.

Author Contributions: Conceptualization, M.M.; literature review, E.R.; methodology, M.M.; data curation, E.R.;
validation L.G.; formal analysis L.D.; writing—original draft preparation, M.M. and E.R.; writing—review and
editing, M.M., L.G. and L.D.; funding acquisition L.G. and L.D. All authors have read and agreed to the published
version of the manuscript.

Funding: This research received external funding.

Conflicts of Interest: The authors declare no conflict of interest.
Publications 2020, 8, 45 11 of 16

References

1. Pannucci, C.J.; Wilkins, E.G. Identifying and Avoiding Bias in Research. Plast. Reconstr. Surg. 2011, 126,
619-625. [CrossRef] [PubMed]

2.  Althubaiti, A. Information bias in health research: Definition, pitfalls, and adjustment methods. J. Multidiscip.
Healthc. 2016, 9, 211-217. [CrossRef] [PubMed]

3. Thiem, A.; Mkrtchyan, L.; Haesebrouck, T.; Sanchez, D. Algorithmic bias in social research: A meta-analysis.
PLoS ONE 2020, 15, e0233625. [CrossRef] [PubMed]

4. | Munafo, M.R.; Nosek, B.A.; Bishop, D.V.M.; Button, K.S.; Chambers, C.D.; Percie du Sert, N. A Manifesto for
Reproducible Science. Nat. Hum. Behav. 2017, 1, 21. [CrossRef]

5, Bial, H. Guest editor’s introduction: Failing better. Theatre Top. 2018, 28, 61-62. [CrossRef]

6. Fanelli, D. When East meets West.does bias increase? A preliminary study on South Korea, United States
and other countries. In 8th International Conference on Webometrics, Informetrics and Scientometrics and 13th
COLLNET Meeting; Ho-Nam, C., Hye-Sun, K., Kyung-Ran, N., Seon-Hee, L., Hye-Jin, K., Kretschmer, H.,
Eds.; KISTI: Seoul, Korea, 2012; pp. 47-48.

7. Jamieson, L. Random and Systematic Bias in Population Oral Health Research: An introduction. Community
Dent. Health 2020, 37, 83. [CrossRef]

8. Russell, G.; Mandy, W.; Elliott, D.; White, R.; Pittwood, T.; Ford, T. Selection bias on intellectual ability in
autism research: A cross-sectional review and meta-analysis. Mol. Autism 2019, 10,9. [CrossRef]

9.  Stefl-Mabry, J.; Radlick, M.; Mersand, S.; Gulatee, Y. School Library Research: Publication Bias and the File
Drawer Effect. J. Thought 2019, 53, 19-34.

10. Song, F; Parekh, S.; Hooper, L.; Loke, Y.K.; Ryder, J.; Sutton, A.J.; Hing, C.; Kwok, C.S.; Pang, C.; Harvey, I.
Dissemination and publication of research findings: An updated review of related biases. Health Technol.
Assess. 2010, 14, 1-12. [CrossRef]

11. Chavalarias, D.; loannidis, J.P.A. Science mapping analysis characterizes 235 biases in biomedical research.
Clin. Epidemiol. 2010, 63, 1205-1215. [CrossRef]

12. Cook, G.B.; Therrien, J.W. Null Effects and Publication Bias in Special Education Research. Behav. Disord.
2017, 42, 149-158. [CrossRef]

13. Button, S.K.; Bal, L.; Clark, A.; Shipley, T. Preventing the ends from justifying the means: Withholding results
to address publication bias in peer-review. BMC Psychol. 2016, 4, 59. [CrossRef] [PubMed]

14. Vella, F Estimating models with sample selection bias: A survey. J. Hum. Resour. 1998, 33, 127-169. [CrossRef]

15. Ayorinde, A.A.; Williams, I.; Mannion, R.; Song, F.; Skrybant, M.; Lilford, J.R.; Chen, FY. Assessment of
publication bias and outcome reporting bias in systematic reviews of health services and delivery research:
A meta-epidemiological study. PLoS ONE 2020, 15, e0227580. [CrossRef]

16. Reio, T.G., Jr. Survey Nonresponse Bias in Social Science Research. New Horiz. Adult Educ. Hum. Resour. Dev.
2007, 21, 48-51. [CrossRef]

17. Mulimani, P. Publication bias towards Western populations harms humanity. Nat. Hum. Behav. 2019, 3,
1026-1027. [CrossRef]

18. Heidweiller-Schreurs, V. Publication bias may exist among prognostic accuracy studies of middle cerebral
artery Doppler ultrasound. J. Clin. Epidemiol. 2019, 116, 1-8. [CrossRef]

19. Shi, L.; Lin, L. The trim-and-fill method for publication bias: Practical guidelines and recommendations
based on a large database of meta-analyses. Medicine 2019, 98, 23. [CrossRef]

20. DeVito, N.J.; Goldacre, B. Catalogue of bias: Publication bias. BMJ Evid. Based Med. 2019, 24, 53-54.
[CrossRef]

21. Danks, D.; London, A.J. Algorithmic Bias in Autonomous Systems. In Proceedings of the Twenty-Sixth
International Joint Conference on Artificial Intelligence, Melbourne, Australia, 19-25 August 2017;
pp. 4691-4697.

22. van Aert, R.C.M.; Wicherts, I.M.; van Assen, M.A.L.M. Publication bias examined in meta-analyses from
psychology and medicine: A meta-meta-analysis. PLoS ONE 2019, 14, e0215052. [CrossRef]

23. Lozano-Blasco, R.; Cortés-Pascual, A.; Latorre-Martinez, P.M. Being a cybervictim and a cyberbully—The

duality of cyberbullying: A meta-analysis. Comput. Hum. Behav. 2020, 111. [CrossRef]
Publications 2020, 8, 45 12 of 16

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.
37.

38.

39.

40.

41.

42.

43.

44.

45.

46.

47.

Stefl-Mabry, J.; Radlick, M.S. School library research in the real world—What does it really take? In
Proceedings of the International Association of School Librarians Conference Proceedings, Long Beach, CA,
USA, 8 August 2017.

Iwasaki, I.; Ma, X.; Mizobata, S. Corporate ownership and managerial turnover in China and Eastern Europe:
A comparative meta-analysis. J. Econ. Bus. 2020. [CrossRef]

Nelson, A.J. The power of stereotyping and confirmation bias to overwhelm accurate assessment: The case
of economics, gender, and risk aversion. J. Econ. Methodol. 2014, 21, 211-231. [CrossRef]

Linm, L. Bias caused by sampling error in meta-analysis with small sample sizes. PLoS ONE 2018, 13,
e0204056. [CrossRef]

Fanelli, D.; Ioannidis, J.P.A. US studies may overestimate effect sizes in softer research. Proc. Natl. Acad. Sci.
USA 2013, 110, 15031-15036. [CrossRef] [PubMed]

Groves, I.; Robert, M. Nonresponse Rates and Nonresponse Error in Household Surveys. Public Opin. Q.
2006, 70, 646-675. [CrossRef]

Dehkordi, A. Effect of Bias in Contrast Agent Concentration Measurement on Estimated Pharmacokinetic
Parameters in Brain Dynamic Contrast-Enhanced Magnetic Resonance Imaging Studies. Iran. J. Med Phys.
2020, 17, 142-152.

Shu, D.; Yi, G.Y. Causal inference with measurement error in outcomes: Bias analysis and estimation methods.
Stat. Methods Int. Med. Res. 2019, 28, 2049-2068. [CrossRef]

Frenkel, R.; Farrance, I.; Badrick, T. Bias in analytical chemistry: A review of selected procedures for
incorporating uncorrected bias into the expanded uncertainty of analytical measurements and a graphical
method for evaluating the concordance of reference and test procedures. Clin. Chim. Acta 2019, 495, 129-138.
[CrossRef]

Handelsman, D.J.; Ly, L.P. An Accurate Substitution Method to Minimize Left ensoring Bias in Serum Steroid
Measurements. Endocrinology 2019, 160, 2395-2400. [CrossRef]

Bishara, A.J.; Hittner, J.B. Reducing Bias and Error in the Correlation Coefficient Due to Nonnormality. Educ.
Psychol. Meas. 2015, 75, 785-804. [CrossRef]

Charles, L.K.; Dattalo, V.P. Minimizing Social Desirability Bias in Measuring Sensitive Topics: The Use
of Forgiving Effect of Bias in Contrast Agent Concentration Measurement on Estimated Pharmacokinetic
Parameters in Brain Dynamic Contrast-Enhanced Magnetic Resonance Imaging Studies. J. Soc. Serv. Res.
2018, 44, 587-599.

Schooler, J. Unpublished results hide the decline effect. Nature 2011, 470, 437. [CrossRef] [PubMed]
Ioannidis, J.P.A. Why Most Published Research Findings Are False. PLoS Med. 2005, 2, e124. [CrossRef]
[PubMed]

Pang, D.; Yang, L. psychological acceptance mechanism and influencing factors of scientific research educatio.
Rev. Argent. Clin. Psicol. 2020, 2, 731-736.

Martinson, B.C.; Crain, A.L.; Anderson, M.S.; De Vries, R. Institutions’ expectations for researchers’
self-funding, federal grant holding and private industry involvement: Manifold drivers of self-interest and
researcher behavior. Acad. Med. 2009, 84, 1491-1499. [CrossRef]

Qiu, J. Publish or perish in China. Nature 2010, 463, 142-143. [CrossRef]

Lee, C.; Schrank, A. Incubating innovation or cultivating corruption? The developmental state and the life
sciences in Asia. Soc. Forces 2010, 88, 1231-1255. [CrossRef]

Lacetera, N.; Zirulia, L. The economics of scientific misconduct. J. Law Econ. Organ. 2011, 27, 568-603.
[CrossRef]

Fang, F.C.; Bennett, J.W.; Casadevall, A. Males are overrepresented among life science researchers committing
scientific misconduct. mBio 2013, 4, e00640—e006412. [CrossRef]

Kaatz, A.; Vogelman, P.N.; Carnes, M. Are men more likely than women to commit scientific misconduct?
Maybe, maybe not. mBio 2013, 4, 2. [CrossRef] [PubMed]

Bailey, C.D. Psychopathy, Academic accountants’ attitudes toward unethical research practices, and
publication success. Account. Rev. 2015, 90, 1307-1332. [CrossRef]

Antes, A.L.; Brown, R.P.; Murphy, S.T. Personality and ethical decision-making in research: The role of
perceptions of self and others. Empir. Res. Hum. Res. Ethics 2007, 2, 15-34. [CrossRef] [PubMed]
MacKenzie, 5.B.; Podsakoff, P.M. Common method bias in marketing: Causes, mechanisms, and procedural
remedies. J. Retail. 2012, 88, 542-555. [CrossRef]
Publications 2020, 8, 45 13 of 16

48.

49.

50.

51.

52.

53.
54.

55.

56.

57.

58.

59.

60.

61.

62.

63.

64.

65.

66.

67.

68.

69.

Eisend, M.; Tarrahi, F. Meta-analysis selection bias in marketing research. Int. J. Res. Mark. 2014, 31, 317-326.
[CrossRef]

Zaefarian, G.; Kadile, V.; Henneberg, S.C.; Leischnig, A. Endogeneity bias in marketing research: Problem,
causes and remedies. Ind. Mark. Manag. 2017, 65, 39-46. [CrossRef]

Kakoschke, N.; Kemps, E.; Tiggemann, M. Approach bias modification training and consumption: A review
of the literature. Addict. Behav. 2017, 64, 21-28. [CrossRef]

Rosenthal, M.; Symoens, J.; De Brabander, M.; Goldstein, G. Immunoregulation with levamisole. Springer
Semin. Immunopathol. 1979, 2, 49-68.

Piotrowskj, C. Scholarly research on educational adaption of social media: Is there evidence of publication
bias? Coll. Stud. J. 2015, 49, 447-451.

Welner, G.K.; Molnar, A. Truthiness in Education. Educ. Week 2007, 26, 32-44.

Gage, N.A.; Cook, G.B.; Reichow, B. Publication Bias in Special Education Meta-Analyses. Except. Child.
2017, 83, 428-445. [CrossRef]

Makel, C.M.; Steenbergen-Hu, S.; Olszewski-Kubilius, P. What One Hundred Years of Research Says About
the Effects of Ability Grouping and Acceleration on K-12 Students’ Academic Achievement: Findings of
Two Second-Order Meta-Analyses. Rev. Educ. Res. 2016, 86, 849-899.

Statzner, B.; Resh, H.V. Negative changes in the scientific publication process in ecology: Potential causes
and consequences. Freshw. Biol. 2010. [CrossRef]

Ekmekci, E. The Flipped Writing Classroom in Turkish EFL Context: A Comparative Study on a New Model.
Turk. Online J. Distance Educ. 2017, 18, 151-167. [CrossRef]

Ioannidis, J.P.A.; Trikalinos, T.A. Early extreme contradictory estimates may appear in published research:
The Proteus phenomenon in molecular genetics research and randomized trials. J. Clin. Epidemiol. 2005, 58,
543-549. [CrossRef]

Young, 5.N. Bias in the research literature and conflict of interest: An issue for publishers, editors, reviewers
and authors, and it is not just about the money. J. Psychiatry Neurosci. Jpn. 2009, 34, 412-417. [PubMed]
Dwan, K.; Altman, D.G.; Arnaiz, J.A.; Bloom, J.; Chan, A.-W.; Cronin, E.; Decullier, E.; Easterbrook, P.J.; Von
Elm, E.; Gamble, G.; et al. Systematic review of the empirical evidence of study publication bias and outcome
reporting bias. PLoS ONE 2008, 3, e3081. [CrossRef]

Mlinaric, A.; Horvat, M.; Smolcic, S.V. Dealing with the positive publication bias: Why you should really
publish your negative results. Biochem. Medica 2017, 27, 3. [CrossRef] [PubMed]

Francis, R. Report of the Mid Staffordshire NHS Foundation Trust Public Inquiry; Stationary Office: London, UK,
2013.

Jha, M.K.; Arnold, K.; Moriasi, N.D.; Gassman, P.W.; Abbaspour, K.C.; White, M.J.; Srinivasan, R.; Santhi, C.;
Harmel, R.D.; van Griensven, A.; et al. SWAT: Model Use, Calibration and Validation. Trans. ASABE 2012,
55, 1491-1508.

Owuamalam, K.C.; Rubin, M.; Spears, R. Addressing Evidential and Theoretical Inconsistencies in
System-Justification Theory with a Social Identity Model of System Attitudes. Curr. Dir. Psychol. Sci. 2018.
[CrossRef]

Sterling, T.; Savarese, D.; Becker, D.J.; Dorband, J.; Ranawake, U.; Packer, V.C. BEOWULF: A Parallel
Workstation for Scientific Computation. In Proceedings of the 1995 International Conference on Parallel
Processing, Urbana-Champain, IL, USA, 14-18 August 1995; CRC Press: Urbana-Champain, IL, USA, 1995;
Volume I: Archit, pp. 11-14.

Davis, M.S.; Wester, K.L.; King, B. Narcissism, entitlement, and questionable research practices in counseling:
A pilot study. Couns. Dev. 2008, 86, 200-210. [CrossRef]

Laroche, P.; Soulez, S. La Meéthodologie de la Méta-Analyse en Marketing Recherche et Applications en Marketing;
Sage Publications, Ltd.: Thousand Oaks, CA, USA, 2012; Volume 27, pp. 79-105.

Dickersin, K.; Min, C.M. Factors influencing publication results: Follow-up on applications submitted to two
institutional review boards. JAMA 1991, 267, 374-378. [CrossRef]

Song, Z.; Guan, B.; Bergman, A.; Nicholson, D.W.; Thornberry, N.A.; Peterson, E.P.; Steller, H. Biochemical
and genetic interactions between Drosophila caspases and the proapoptotic genes rpr, hid, and grim. Mol.
Cell. Biol. 2000, 20, 2907-2914. [CrossRef] [PubMed]
Publications 2020, 8, 45 14 of 16

70.

71.

72.

73.

7A.

79.

76.

77.

78.

79.

80.

81.

82.

83.

84.

85.

86.

87.

88.

89.

90.

91.

92.

93.

94.

Dickersin, K. Publication bias: Recognizing the problem, understanding its origins and scope, and preventing
harm. In Publication Bias in Meta-Analysis—Prevention, Assessment and Adjustments; Rothstein, H.R., Ed.; John
Wiley & Sons: New York, NY, USA, 2005; pp. 11-33.

Greenhalgh, T.; Peacock, R. Effectiveness and efficiency of search methods in systematic reviews of complex
evidence: Audit of primary sources. Br. Med. J. 2005, 331, 1064-1065. [CrossRef] [PubMed]

Polanin, J.R.; Tanner-Smith, E.E.; Hennessy, E.A. Estimating the difference between published and
unpublished effect sizes a metareview. Rev. Educ. Res. 2016, 86, 207-236. [CrossRef]

De Leeuw, E.; de Heer, W. Trends in Household Survey Nonresponse: A Longitudinal and International Comparison;
Survey Nonresponse; Wiley: New York, NY, USA, 2002; pp. 41-54.

Biemer, P.P. Nonresponse Bias and Measurement Bias in a Comparision of Face to Face and Telephone
Interviewing. J. Off. Stat. 2001, 17, 295-320.

Cannell, F.C.; Fowler, FJ. Comparision of a self-enumerative procedure and personal interview: A validity
study. Public Opin. Q. 1963, 27, 250-264. [CrossRef]

Muller, J.-L. Pour une revue quantitative de la littérature: Les méta-analyses. Psychol. Franc. 1988, 33,
295-303.

Certo, S.T.; Busenbark, J.R.; Woo, H.S.; Semadeni, M. Sample selection bias and Heckman models in strategic
management research. Strateg. Mag. 2016, 37, 2639-2657. [CrossRef]

Anderson, 5.F.; Ken, K.; Scott, EM. Sample-Size Planning for More ccurate Statistical Power: A Method
Adjusting Sample Effect Sizes for Publication Bias and Uncertainty. Psychol. Sci. 2017, 28, 1547-1562.
[CrossRef]

Rayer, S.; Smith, K.S. Population Projections by Age for Florida and its Counties: Assessing Accuracy and
the Impact of Adjustments. Popul. Res. Policy Rev. 2014, 33, 747-770. [CrossRef]

Tayman, J.; Smith, K.S.; Lin, J. Precision, bias, and uncertainty for state population forecasts: An exploratory
analysis of time series models. Popul. Res. Policy Rev. 2007, 26, 347-369. [CrossRef]

Alho, M.J.; Spencer, D.B. The Practical Specification of the Expected Error of Population Forecats. J. Off. Stat.
1997, 13, 203-225.

Pflaumer, P. Forecasting US population totals with the Box-Jenkins Approach. Int. J. Forecast. 1992, 8, 329-338.
[CrossRef]

Keilman, N.; Pham, Q.D.; Hetland, A. Why population forecasts should be probabilistic—Illustrated by
the case of Norway. Demogr. Res. 2002, 6, 409-454. [CrossRef]

Sartori, E.A. An Estimator for Some Binary-Outcome Selection Models without Exclusion Restrictions.
Political Analysis 2003, 11, 111-138. [CrossRef]

Japec, L.; Lundquist, P. Bortfallet-—Pdverkas det av Intervjuarnas Attityder och Strategier? Rapport inédit; Statistics
Sweden: Stockholm, Sweden, 2000.

Curtin, C.; Presser, S.; Singer, E. The Effects of Response Rate Changes on the Index of Consumer Sentiment.
Public Opin. Q. 2000, 64, 413-428. [CrossRef]

Keeter, S.; Miller, C.; Kohut, A.; Groves, R.; Presser, S. Consequences of Reducing Nonresponse in a Large
National Telephone Survey. Public Opin. Q. 2000, 64, 125-148. [CrossRef]

Young, N.S.; Ioannidis, J.P.A.; Al-Ubaydli, O. Why current publication practices may distort science. PLoS
Med. 2008, 5, e201. [CrossRef]

Groves, M.R.; Presser, S.; Dipko, S. The Role of Topic Interest in Survey Participation Decisions. Public Opin.
Q. 2004, 8&6, 2-31. [CrossRef]

Taraday, M. Lack of Publication Bias in Intelligence and Working Memory Research: Reanalysis of Ackerman,
Beier, Boyle, 2005. Stud. Psychol. 2019, 61, 203-212. [CrossRef]

Wallach, J.D.; Boyack, K.W.; Ioannidis, J.P.A. Reproducible Research Practices, Transparency, and Open
Access Data in the Biomedical Literature, 2015-2017. PLOS Biol. 2018, 16, e2006930. [CrossRef]

Wind, Y.; Saaty, T.L. Marketing applications of the analytic hierarchy process. Manag. Sci. 1980, 26, 641-658.
[CrossRef]

John, K.L.; Loewenstein, G.; Prelec, D. Measuring the Prevalence of Questionable Research Practices With
Incentives for Truth Telling. Psychol. Sci. 2012, 23, 524-532. [CrossRef]

Wickramasinghe, V.S.K.; Takano, S.E. Application of Combined SWOT and Analytic Hierarchy Process (AHP)
for Tourism Revival Strategic Marketing Planning; Eastern Asia Society for Transportation Studies: Surabaya,
Indonesia, 2009; Volume 7.
Publications 2020, 8, 45 15 of 16

95.

96.

97.

98.

99.

100.

101.

102.

103.

104.

105.

106.

107.

108.

109.

110.

111.

112.

113.

114.

115.

116.

117.

118.

Abedi, G.; Abedini, E. Prioritizing of marketing mix elements effects on patients’ tendency to the hospital
using analytic hierarchy process. Int. J. Healthc. Manag. 2017, 10, 34-41. [CrossRef]

Najmi, A.; Kanapathy, K.; Aziz, A.A. Prioritising factors influencing consumers’ reversing intention of
e-waste using analytic hierarchy process. Int. J. Electron. Cust. Relatsh. Manag. 2019, 12, 58-74. [CrossRef]
Wu, Y.; Chen, S.C.; Lin, LC. Elucidating the impact of critical determinants on purchase decision in virtual
reality products by Analytic Hierarchy Process approach. Virtual Real. 2019, 23, 187-195. [CrossRef]
Gupta, S.; Dawar, V.; Goyal, A. Enhancing the placement value of professionally qualified students in
marketing: An application of the analytic hierarchy process. Acad. Mark. Stud. J. 2018, 22, 1-10.

Jing, Z.X.; Shi, J.H.; Luo, Z.Y.; Chen, D.P.; Chen, Z.Y. Comprehensive Evaluation of Electricity Market Based
on Analytic Hierarchy Process and Evidential Reasoning Methods. In IOP Conference Series: Earth and
Environmental Science; IOP Publishing: Bristol, UK, 2019; Volume 354, p. 012117.

Shaverdi, M.; Heshmati, M.R.; Eskandaripour, E.; Tabar, A.A.A. Developing sustainable SCM evaluation
model using fuzzy AHP in publishing industry. Procedia Comput. Sci. 2013, 17, 340-349. [CrossRef]
Rostamy, A.A.A.; Shaverdi, M.; Ramezani, I. Green supply chain management evaluation in publishing
industry based on fuzzy AHP approach. J. Logist. Manag. 2013, 2, 9-14.

Diouf, M.; Kwak, C. Fuzzy AHP, DEA, and Managerial analysis for supplier selection and development;
From the perspective of open innovation. Sustainability 2018, 10, 3779. [CrossRef]

Owusu-Agyeman, Y.; Larbi-Siaw, O.; Brenya, B.; Anyidoho, A. An embedded fuzzy analytic hierarchy
process for evaluating lecturers’ conceptions of teaching and learning. Stud. Educ. Eval. 2017, 55, 46-57.
[CrossRef]

Myeong, S.; Jung, Y.; Lee, E. A study on determinant factors in smart city development: An analytic hierarchy
process analysis. Sustainability 2018, 10, 2606. [CrossRef]

Mayo, F.L.; Taboada, E.B. Ranking factors affecting public transport mode choice of commuters in an urban
city of a developing country using analytic hierarchy process: The case of Metro Cebu, Philippines. Transp.
Res. Interdiscip. Perspect. 2020, 4, 100078. [CrossRef]

Ma, D.; Zheng, X. 9/9-9/1 Scale Method of AHP. In Proceedings of the 2nd International Symposium on AHP,
Pittsburgh, PA, USA, 11-14 August 1991; Volume 1, pp. 197-202.

Ishizaka, A.; Balkenborg, D.; Kaplan, T. Influence of aggregation and measurement scale on ranking
a compromise alternative in AHP. J. Oper. Res. Soc. 2010, 62, 700-710. [CrossRef]

Harker, P.; Vargas, L. The Theory of Ratio Scale Estimation: Saaty’s Analytic Hierarchy Process. Manag. Sci.
1987, 33, 1383-1403. [CrossRef]

Saaty, T.L.; Vargas, L.G. Models, Methods, Concepts Applications of the Analytic Hierarchy Process; Springer
Science Business Media: New York, NY, USA, 2012; Volume 175.

Libby, R.; Blashfield, R.K. Performance of a composite as a function of the number of judges. Organ. Behav.
Hum. Perform. 1978, 21, 121-129. [CrossRef]

Goepel, K.D. Comparison of judgment scales of the analytical hierarchy process—A new approach. Int. J. Inf.
Technol. Decis. Mak. 2019, 18, 445-463. [CrossRef]

Dong, Y.; Zhang, G.; Hong, W.C.; Xu, Y. Consensus models for AHP group decision making under row
geometric mean prioritization method. Decis. Support Syst. 2010, 49, 281-289. [CrossRef]

Saaty, T.L. Fundamentals of the analytic hierarchy process. In The Analytic Hierarchy Process in natural Resource
and Environmental Decision Making; Springer: Dordrecht, Germany, 2001; pp. 15-35.

Benedetti, R.; Andreano, A.S.; Piersimoni, F. Sample selection when a multivariate set of size measures is
available. Stat. Methods Appl. 2019, 28, 1-25. [CrossRef]

Patino, C.M.; Ferreira, J.C. Inclusion and exclusion criteria in research studies: Definitions and why they
matter. J. Bras. De Pneumol. 2018, 44, 84. [CrossRef]

Gray, J.R.; Grove, 5.K.; Sutherland, 5. Burns and Grove’s the Practice of Nursing Research: Appraisal, Synthesis,
and Generation of Evidence, 8th ed.; Elsevier: Amsterdam, The Netherlands, 2017.

Beullens, K.; Loosveldt, G.; Vandenplas, C.; Stoop, I. Response Rates in the European Social Survey: Increasing,
Decreasing, or a Matter of Fieldwork Efforts? Survey Methods: Insights from the Field. 2018. Available
online: https://surveyinsights.org/?p=9673 (accessed on 16 July 2020).

Langer, G. Probability versus non-probability methods. In The Palgrave Handbook of Survey Research;
Vannette, D.L., Krosnick, J.A., Eds.; Springer: Cham, Switzerland, 2018; pp. 393-403.
Publications 2020, 8, 45 16 of 16

119. Franco, A.; Malhotra, N.; Simonovits, G. Publication bias in the social sciences: Unlocking the file drawer.
Science 2014, 345, 1502-1505. [CrossRef] [PubMed]

120. Peck, R.L.; D’Attoma, I.; Camillo, F; Guo, G. A New Strategy for Reducing Selection Bias in Nonexperimental
Evaluations, and the Case of How Public Assistance Receipt Affects Charitable Giving. Policy Stud. J. 2012,
40, 601-625. [CrossRef]

121. Showalter, A.D.; Mullet, B.L. Sniffing Out the Secret Poison: Selection Bias in Educational Research. Mid-West.
Educ. Res. 2017, 29, 207-234.

122. Clark, G.T.; Mulligan, R. Fifteen common mistakes encountered in clinical research. J. Prosthodont. Res. 2011,
55, 1-6. [CrossRef]

123. Churchill, G.A.; Iacobucci, D. Marketing Research: Methodological Foundations; Dryden Press: New York, NY,
USA, 2006.

124. Greenwood, M. Approving or improving research ethics in management journals. J. Bus. Ethics 2016, 137,
507-520. [CrossRef]

125. Plemmons, D.K.; Baranski, E.N.; Harp, K.; Lo, D.D.; Soderberg, C.K.; Errington, T.M.; Esterling, K.M. A
randomized trial of a lab-embedded discourse intervention to improve research ethics. Proc. Natl. Acad. Sci.
USA 2020, 117, 1389-1394. [CrossRef]

Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional
affiliations.

@) © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
(cc) article distributed under the terms and conditions of the Creative Commons Attribution
BY

(CC BY) license (http://creativecommons.org/licenses/by/4.0/).
