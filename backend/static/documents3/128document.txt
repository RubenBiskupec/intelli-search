Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 FEURASIP J ourna | on Adv ances
https://doi.org/10.1186/s13634-020-00695-2 . Sj | Process . ng
in Signa |

RESEARCH Open Access

High-dimensional neural feature design ®
for layer-wise reduction of training cost

updates
Alireza M. Javid’ ®, Arun Venkitaraman, Mikael Skoglund and Saikat Chatterjee

 

 

*Correspondence: almj@kth.se
School of Electrical Engineering and Abstract

Computer Science, KTH Royal _ , ; , ;
Institute of Technology, We design a rectified linear unit-based multilayer neural network by mapping the

Brinellvagen 8, Stockholm, Sweden feature vectors to a higher dimensional space in every layer. We design the weight
matrices in every layer to ensure a reduction of the training cost as the number of
layers increases. Linear projection to the target in the higher dimensional space leads
to a lower training cost if a convex cost is minimized. An £2-norm convex constraint is
used in the minimization to reduce the generalization error and avoid overfitting. The

regularization hyperparameters of the network are derived analytically to guarantee a
monotonic decrement of the training cost, and therefore, it eliminates the need for
cross-validation to find the regularization hyperparameter in each layer. We show that
the proposed architecture is norm-preserving and provides an invertible feature vector
and, therefore, can be used to reduce the training cost of any other learning method
which employs linear projection to estimate the target.

Keywords: Rectified linear unit, Feature design, Neural network, Convex cost function

 

1 Introduction

Nonlinear mapping of low-dimensional signal to high-dimensional space is a traditional
method for constructing useful feature vectors, specifically for classification problems.
The intuition is that, by extending to a high dimension, the feature vectors of differ-
ent classes become easily separable by a linear classifier. The drawback of performing
classification in a higher dimensional space is the increased computational complexity.
This issue can be handled by a well-known method called “kernel trick” in which the
complexity depends only on the inner products in the high-dimensional space. Support
vector machine (SVM) [1] and kernel PCA (KPCA) [2] are examples of creating high-
dimensional features by employing the kernel trick. The choice of the kernel function is
a critical aspect that can affect the classification performance in the higher dimensional
space. A popular kernel is the radial basis function (RBF) kernel or Gaussian kernel, and its
good performance is justified by its ability to map the feature vector to a very high, infinite,
dimensional space [3]. In this manuscript, we design a high-dimensional feature using
an artificial neural network (ANN) architecture to achieve a better classification perfor-

mance by increasing the number of layers. The architecture uses the rectified linear unit

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
GQ) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
— credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 2 of 19

(ReLU) activation, predetermined orthonormal matrices, and a fixed structured matrix.
We refer to this as high-dimensional neural feature (HNF) throughout the manuscript.

Neural networks and deep learning architectures have received overwhelming atten-
tion over the last decade [4]. Appropriately trained neural networks have been shown
to outperform the traditional methods in different applications, for example, in clas-
sification and regression tasks [5, 6]. By the continually increasing computational
power, the field of machine learning is being enriched with active research pushing
classification performance to higher levels for several challenging datasets [7-9]. How-
ever, very little is known regarding how many numbers of neurons and layers are
required in a network to achieve better performance. Usually, some rule-of-thumb
methods are used for determining the number of neurons and layers in an ANN, or
an exhaustive search is employed which is extremely time-consuming [10]. In partic-
ular, the technical issue—guaranteeing performance improvement with increasing the
number of layers—is not straightforward in traditional neural network architectures,
e.g., deep neural network (DNN) [11], convolutional neural network (CNN) [12], and
recurrent neural network (RNN) [13]. We endeavor to address this technical issue by
mapping the feature vectors to a higher dimensional space using predefined weight
matrices.

There exist several works employing predefined weight matrices that do not need to
be learned. Scattering convolution network [14] is a famous example of these approaches
which employs wavelet-based scattering transform to design the weight matrices. Ran-
dom matrices have also been widely used as a mean for reducing the computational
complexity of neural networks while achieving comparable performance as with fully
learned networks [15-18]. In the case of the simple, yet effective, extreme learning
machine (ELM), the first layer of the network is assigned randomly chosen weights and
the learning takes place only at the end layer [19-22]. It has also been shown recently
that a similar performance to fully learned networks may be achieved by training a net-
work with most of the weights assigned randomly and only a small fraction of them being
updated throughout the layers [23]. It has been shown that networks with Gaussian ran-
dom weights provide a distance-preserving embedding of the input data [16]. The recent
work [18] designs a deep neural network architecture called progressive learning network
(PLN) which guarantees the reduction of the training cost with increasing the number of
layers. In PLN, every layer is comprised of a predefined random part and a projection part
which is trained individually using a convex cost function. These approaches indicate that
randomness has much potential in terms of high performance at low computational com-
plexity. We design a multilayer neural network using predefined orthonormal matrices,
e.g., random orthonormal matrix and DCT matrix, to ensure reducing the training cost

as the number of layers increases.

1.1. Our contributions

Motivated by the prior use of fixed matrices, we design the HNF architecture using an
appropriate combination of ReLU, random matrices, and fixed matrices. We use pre-
defined weight matrices in every layer of the network, and therefore, the architecture
does not suffer from the infamous vanishing gradient problem. We theoretically show
that the output of each layer provides a richer representation compared to the previous

layers if a convex cost is minimized to estimate the target. We use an 2-norm convex
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 3 of 19

constraint to reduce the generalization error and avoid overfitting to the training data.
We analytically derive the regularization hyperparameter to ensure the decrement of the
training cost in each layer. Therefore, there is no need for cross-validation to find the
optimum regularization hyperparameters of the network. We show that the proposed
HNF is norm-preserving and invertible and, therefore, can be used to improve the perfor-
mance of other learning methods that use linear projection to estimate the target. Finally,
we show the classification performance of the proposed HNF against ELM and state-of-
the-art results. Note that a preliminary version of this manuscript has been submitted to
ICASSP 2020 recently.

1.2 Notations

We use the following notations unless otherwise noted: We use bold capital letters, e.g.,
W, to denote matrices and bold lowercase letters, e.g., x, to denote vectors. We use cal-
ligraphic letter / to denote a set and M° to denote compliment set. The cardinality of
a set M is denoted by |M|. For a scalar x € R, let us denote its sign and magnitude as
s(x) € {—1,+1} and |x|, respectively, and write x = s(x)|x|. For a vector x, we define the
sign vector s(x) and magnitude vector |x| by the element-wise operation. We define g(-)
as a nonlinear function comprised of a stack of element-wise ReLU activation functions.
A vector x has non-negative part xt and non-positive part x such that x = xt +x and
g(x) = x’. We use || - || and || - ||- to denote £2-norm and Frobenius norm, respectively.
For example, it can be seen that ||x||? = ||x* ||? + ||x7 ||?.

2 Proposed method
In this section, we illustrate the motivation to design a high-dimensional feature vector by
using ReLU activation function. We analyze the behavior of a single layer ReLU network
to the input perturbation noise and show that by mapping the feature vectors to a higher
dimension, we can increase the discrimination power of the ReLU network.

For an ANN, we wish to have noise robustness and discriminative power. We charac-
terize this in the following definition.

Definition 1 (Noise Robustness and Point Discrimination) Let x; and x2 be two input
vectors such that x; # X2, and we have outputs of ANN t; = f(x,) and tz = f(x2). We can
characterize a perturbation scenario with the perturbation noise A as x7 = x; + A. We
wish that the proposed ANN holds the property

c1||x1 — x2||? < |]£(x1) — £(x2)||* < cal|xi — x2(I?, (1)

whereO0 < cy < landc, < c9.

Note that the lower bound provides point discrimination power and the upper bound
provides noise robustness to the input.

2.1 Layer construction

We first concentrate on one block of ANN—this is called a layer in the neural network
literature. The layer has an input vector q € R™*! and the output vector y = g(Wa).
The dimension of y is the number of neurons in the layer. If we can guarantee that the
layer of ANN provides noise robustness and point discrimination property, then the full
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 4 of 19

ANN comprising of multiple layers connected sequentially can be guaranteed to hold
robustness and discriminative properties. We need to construct W in such a manner that
the layer has noise robustness and discriminative power according to Definition 1.

2.2 ReLU activation and a limitation

We first show three essential properties of ReLU function, required to develop our main
results. We then discuss one possible limitation of the ReLU function and propose a
remedy to circumvent the problem.

Property 1 (Scaling) ReLU function has a scaling property. If y = g(Wq), then ay =
g(W(aq)) for a scalar a > 0.

Property 2 (Sparsity) ReLU function provides sparse output vector y such that |ly|lo
dim(y).

lA

Property 3 (Noise Robustness) Let us consider z = Wgq. For two vectors q, and q2, we
define corresponding vectors Z; = Waq, and z2 = Wap, and output vectors y; = g(z1) =
g(Wq1) and y2 = g(Z2) = g(Wqz). Now, we have the following relation

0 < llyi — yoll? = Ilg(zi) — g(z2) |? < Iz. — 22”. (2)

The proof of Property 3 is shown in Appendix 1. The upper bound relation holds Lips-
chitz continuity that provides noise robustness. On the other hand, the lower bound being
zero cannot maintain a minimum distance between two points y; and y2. An example of
extreme effect is that when z; and zz are non-positive vectors, we get ||y; — yal? = 0.
This may limit the capacity of the ReLU function for achieving a good discriminative
power. A reason for the limitation “lower bound being zero” is due to the structure of the
input matrix W. We build an appropriate structure for the input matrix to circumvent the
limitation.

We now engineer a remedy for this limitation. Let us consider y = g(Vz) = g(VWgq),
where z = Wq € R” and V is a linear transform matrix. For two vectors q; and qo, we
have corresponding vectors z} = Wq, and z2 = Wgqp, and output vectors yy = g(Vz1)
and y2 = g(Vz2). Our interest is to show that there exists a predefined matrix V for which

we have both noise robustness and discriminative power properties.

Proposition 1 Let us construct a V matrix as follows

In |a
V= ca 4 V,,. (3)

For the output vectors 91 = g(Vnz1) € R™ and ¥2 = g(Vnz2) € R™”, we have |lyi||° =
Izu ||? and ||¥2||* = ||z2||* and

1] _ _
0 < Sila — zal” < Ilvi — y2ll” < lla — zal’. (4)

The proof of the above proposition can be found in Appendix 2. Based on the above
proposition, we can interpret the effect of noise passing through such layer. Let z2 =
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 5 of 19

Zz; + Az, where Az is a small perturbation noise. Note that Az = z; —z2 = W[ qi — q2] =
W Aq. To investigate effect of perturbation noise, we now state our main assumption.

Assumption 1 Given a small || Az||*, the sign patterns of 2, and zp do not differ signifi-
cantly. On the other hand, for a large perturbation noise \| Az||?, the sign patterns of z, and
Zo vary significantly.

The above assumption means that for a small || Az||?, the set M(z1,z2) = {ils(z,(i)) =
s(Z2(i)) 4 O} is close to a full set and M°(z,z2) is close to an empty set. On the other
hand, for a large ||Az||?, the set M(z,,z2) = {ils(z,(i)) = s(zo(i)) € 0} is close to an
empty set and M°(z),Z2) is close to a full set. Considering Assumption 1, we can present
the following remark regarding the effect of noise in the layer.

Remark 1 (Effect of perturbation noise) For a small perturbation noise || Az||?, we have

l¥1 — yall? © |lz1 — zal|?. On the other hand, a large perturbation noise is attenuated.

This follows from the proof of Proposition 1, specifically Eqs. (28) and (29a). In fact, if
MS = G, then ||y1 — yall? = ||z1 — za||?. We interpret that a small perturbation noise
passes through the single layer g(Vz) almost not attenuated. Let us construct an illustra-
tive example. Assume that M‘(zj, Zz) is a full set and Vi € M‘, |z1(i)| = |Z2(d)|. In that
case, |ly¥7 — yall? = 0.5||z1 — z2||? and we can comment that the perturbation noise is
attenuated.

3 High-dimensional neural feature

In this section, we employ the proposed weight matrix in (3) to construct a multilayer
ANN. We show that by designing the weight matrices in every layer, it is possible to con-
struct a network that provides noise robustness and point discrimination according to
Definition 1.

Let us establish the relation between the input vector q € R” and output vector y €
R2”. For two vectors qi and qo, we have corresponding vectors z} = Wq, and z2 =
Wqp, and output vectors yy = g(Vy,z1) = g(V,Wq)) and yo = g(VnzZ2) = g(V,Waz).
Our interest is to show that it is possible to construct a W € R”*”™ matrix for which
we have both noise robustness and discriminative power properties. We can construct
W € R”*” as orthonormal matrix, such that 1 > mand W'W = I,,. In that case, we
have ||qi — q2l|? = ||z1 — za||? for any pair of (qi, q2). By combining the this relation with
Eq. (4), we conclude the following proposition.

Proposition 2 Consider the single layer network y = g(Vy,z) = g(VnWq) where W €
R"*" is an orthonormal matrix, such that n > m and W'W = Ip. Then, |ly||? = |lq|l?,
and for every two vectors q, and qo, the following inequality holds

1] _ _
5 lian - qall? < llyi — yall? < lar — all. (5)

The above proposition shows that by designing the weight matrix in a single layer net-
work, it is possible to provide point discrimination and noise robustness according to

Definition 1. Note that the weight matrix W can be any orthonormal matrix such as
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 6 of 19

instances of random orthonormal matrix and DCT matrix. By considering the relation
Az = W Aq, we can present a similar argument as in Remark 1. We interpret that a small
perturbation noise || Aq||* passes through the single layer g}(VWgq) almost not attenuated.
This is stated in the following remark.

Remark 2 (Effect of perturbation noise) For a small ||Aq||?, we have \|y1 — yo\||?_ ~
llq1 — q2||?. On the other hand, a large perturbation noise is attenuated.

By directly using Proposition 1, we can present a similar bound in regard to the
perturbation of the weight matrix W in a single layer construction. We can show that
the perturbation norm in the output due to the perturbation to the weight matrix has an
upper bound that is a scaled version of the input norm. The scaling parameter || AW|I2. is
small for a small perturbation. The following remark illustrates this point in detail.

Remark 3 (Sensitivity to the weight matrix) Let the weight matrix W be perturbed by
AW. The effective weight matrix is W + AW. For an input q and the respective outputs
y = g(V,, Wq) and ya = g(Vi| W + AW] q), we have

ly — yall? < JAW|2 lal. (6)

The proof can be found in Appendix 3.

3.1 Multilayer construction

A feedforward ANN is comprised of similar operational layers in a chain. Let us consider
two layers in feedforward connection, e.g., /-th and (/ + 1)-th layers of an ANN. For the
/-th layer, we use a superscript (/) to denote appropriate variables and parameters. Let the
I-th layer has m™ nodes. The input to the /th layer is q = y“—)). The output of /-th layer
y =g (V,02) = g (V,0Ww® q”) is next used as the input to the (/ + 1)-th layer, that
means y = q"+)), Thus, the output of (J + 1)-th layer is

yd —~g (Vice z+)
—g (V,0en we? q*?)
= 8 (V,enW Ps (V,0w?g”)) (7)

Now, for the two vectors qi? and q’, we have the following relations in /-layer based on

 

Proposition 2

1 ] l _(1 -(1 l
slay — a2)? <li? — 927 IP lay’ — 45’. (8)
We present the above results as the following theorem to provide noise robustness
and discrimination power properties of the proposed ANN and call it high-dimensional
neural feature (HNF) afterwards.

Theorem 1 The proposed HNF uses ReLU activation function and is constructed as
follows:

(a) The HNF is comprised of L-layers where the I-th layer has the corresponding
structure y = g(V,0WOyD). The L-layers are in a chain. The input to the
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 7 of 19

first layer is q) = x. The output of HNF is
¥? =gV,0W gl... g(V,0 WX).

(b) Inthe HNF, WY © R" xm matrices are orthonormal matrices with appropriate
sizes, that isn > m® andm = 2n",

Then, ||y“ ||? = ||x||?, and the construted HNF provides noise robustness and discrimina-
tive power properties that are characterized by the following relation
—(L)

1 “(L
x1 — xall? < yO — 7

5h \!?

< |Ix1 — x2I’, (9)
where x, € R”"” and x2 € R”” are two input vectors to the HNF and their corresponding
out, yy and yy” ‘vel

puts are yy, and yp ’, respectively.

Note that a similar argument as in Remark 2 holds here as well. We interpret that a small
perturbation noise passes through the multilayer structure almost not attenuated. On
the other hand, a large perturbation noise is attenuated in every layer. Using Theorem 1,
we follow similar arguments as in Remark 3 in regard to the perturbation of the weight
matrices W in every layer of the HNF.

Remark 4 (Sensitivity to the weight matrix) Consider a scenario where the weight
matrix W is perturbed by AW. The effective weight matrix is W + AW, We can
show that

L
- -(L
WW — 901? < | [aw® Pix’. (10)
l=1

4 Reduction of training cost
In this section, we analyze the effectiveness of the weight matrix V in the sense of reducing
the training cost. We show that the proposed HNF provides lower training costs as the
number of layers increases. We also present how the proposed structure can be used to
reduce the training cost of other learning methods which employ linear projection to the target.
Consider a dataset containing N samples of pairwise P-dimensional input data x € R”
and Q-dimensional target vector t € RY as D = {(x,t)}. Let us construct two single
layer neural networks and compare effectiveness of their feature vectors. In one network,
we construct the feature vector as y = g(Wx), and in the other network, we build the
feature vector y = g(V, Wx). We use the same input vector x, predetermined weight
matrix W € R”*?, and ReLU activation function g(-) for both networks. However, in the

I
second network, the effective weight matrix is V,W where V, = "| © R27” is
nN

fully predetermined. To predict the target, we use a linear projection of feature vector. Let
the predicted target for the first network be Oy, and the predicted target for the second
network Oy. Note that O « R&*” and O € R&**”. By using £2-norm regularization, we
find optimal solutions for the following convex optimization problems.

lA

O* = argmin E[||t — Oy||7] s.t. |OllZ <«, (11a)

lA

O* = argminE[||t — Oy||7] s.t. |OllZ <«, (11b)
O
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 8 of 19

where the expectation operation is done by sample averaging over all N data points in
the training dataset. The regularization parameter € is the same for the two networks. By

HL} "

The above relation is due to the special structure of V, and the use of ReLU activation

oe A
defining z = Wx, we have

<<
|

g(-). Note that the solution O* =[O* 0] exists in the feasible set of the minimization
(11b), ie, |][ O* 0] \12. < €, where 0 is a zero matrix of size Q x n. Therefore, we can show
the optimal costs of the two networks have the following relation

E[\jt — O*y||?] < E[|lt — O*y|I7], (13)

where the equality happens when O* =[ O* 0]. Any other optimal solution of O will lead
to inequality relation due to the convexity of the cost. Therefore, we can conclude that the
feature vector y of the second network is richer than the feature vector y of the first net-
work in the sense of reduced training cost. The proposed structure provides an additional
property for the feature vector y which we state in the following proposition. The proof
idea of the proposition will be used in the next section to construct a multilayer structure,
and therefore, we present the proof here.

Proposition 3 For the feature vector y = g(V;,Wx), there exists an invertible mapping
h(y) = x when the weight matrix W is full-column rank.

Proof We now state lossless flow property (LFP), as used in [17, 24]. A nonlinear func-
tion g(-) holds the LFP if there exist two linear transformations V and U such that
Ug(Vz) = z,Vz © R’. It is shown in [17] that ReLU holds LEP. In other words, if V =

I
Vi= "| € R72?! and UU, = [L, —I)] € R”**”, then U,g(V,,z) = z holds for

nN
every z when g(-) is ReLU. Letting z = Wx, we can easily find x = W'z = W'U,y, where
+ denotes pseudo-inverse when W is a full-column rank matrix. Therefore, the resulting
inverse mapping h would be linear. O

4.1 Reduction of training cost with depth

In this section, we show that the proposed HNF provides lower training costs as the
number of layers increases. Consider an L-layer feedforward network according to our
proposed structure on the weight matrices as follows

y = 8V,0W gl... 8(V,0 Wx). (14)

Note that 27) is the number of neurons in the L-th layer of the network. The input-
output relation in each layer is characterized by

yY = gV,aW»), (15a)

0 =gV, ow), 2<1<L, (15b)

where W ¢ RIO xP. WO ¢e Rt xm and mY = 2n"-) for 2 < I < L. Let the
predicted target using the /-th layer feature vector y be O;y. We find optimal solutions
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 9 of 19

for the following convex optimization problems
O7_, =argminE | jt OF) ?| st. JO se. (16a)
Oj = arg min E iit - 09 |? | s.t. ||Ol|2 <ey. (16b)

Let us define 29 = Wy, Assuming that weight matrices W” are full-column
rank, we can similarly derive y/—) =[W]' z. By using Proposition 3, we have z” =

U, wy, and then, we can write the following relations
yd —[_w}t ZW) —[w}t Uy, (17)

where U,« =[I,0 — 1,w]. If we choose OF = OF [WO] U0, by using (17), we
can easily see that ory” = ory. Therefore, by including o7 [Ww |" U,@ in
the feasible set of the minimization (16b), we can guarantee that the optimal cost of /-
th layer would be lower or equal than that of layer (J — 1). In particular, by choosing
€; = ||O7_,[ wt U,0 2. we can see that the optimal costs follow the relation

Eliit— O79 P|] <E[e- OF 9 PP], (18)

where the equality happens when we have OF = or [Woy U,«. Any other optimal
solution of O; will lead to inequality relation due to the convexity of the cost. Therefore,
we can conclude that the feature vector y of an /-layer network is richer than the feature
vector y“— of an (J — 1)-layer network in the sense of reduced training cost. Note that if
we choose the weight matrix W to be orthonormal, then

€ = 107 [WO]! U,o lz
= trace [21,,0W? [o%_,]' of ,Lw]" |
= 2 trace lw [O7_,]' oF ,[w?]" |
= 2 trace Lrwoyt wor]! of, |
= 2 trace[ [O7,]' OF 4J= 2\|O7_, Iz, (19)

where we have used the fact that U,o[U,o]' = 21,. As we have Or 1112. < €j,a
sufficient condition to guarantee the cost relation (18) is to use the relation between reg-
ularization parameters as €; > 2€;_;. We can choose €; = 2€;_, = 2!~!e;. Note that the
regularization parameter €, in the first layer can also be determined analytically. Consider
Oj, to be the solution of the following least-square optimization

O;, = arg min E [it — Ox]. (20)

Note that the above minimization has a closed-form solution. Similar to the argument
in (18), by choosing €; = Ox, Ww]! U,@ 2. it can be easily seen that

E| it OF P| < E [le — OFxI?7], (21)

where the equality happens only when we have O} = O;[ wyr U,,a). Similar to Propo-
sition 3, we can prove the following proposition regarding the invertibility of the feature
vector at the /-th layer of the proposed structure.
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 10 of 19

Proposition 4 For the feature vector y“ in (14), there exists an invertible mapping
function h(y™) = x when the set of weight matrices {W Woy are full-column rank.

Proof It can be proved by repeatedly using the lossless flow property (LFP) similar to
Proposition 3. LO

4.2 Reduction of training cost of ELM

Note that the feature vector y) in (15a) can be any feature vector that is used for linear
projection to the target in any other learning method. In Section 4.1, we assume y) to be
the feature vector constructed from x using the matrix V, and therefore, the regularization
parameter € is derived to guarantee performance improvement compared to least-square
method as shown in (21). A potential extension would be to build the proposed HNF using
the feature vector y from other methods that employ linear projection to estimate the
target. For example, the extreme learning machine (ELM) uses a linear projection of the
nonlinear feature vector to predict the target [19]. In the following, we build the proposed
HNF by employing the feature vector used in ELM to improve the performance.

Similar to Eq. (18), we can show that it is possible to improve the feature vector of ELM
in the sense of training cost by using the proposed HNF. Consider y = g(W x), to
be feature vector used in ELM for linear projection to the target. In the ELM framework,
Ww ® ¢€ R”?*? is an instance of normal distribution, not necessarily full-column rank,
and g(-) can be any activation function, not necessarily ReLU. The optimal mapping to
the target in ELM is found by solving the following minimization problem.

Of = argmin E| jt — OF |? |. (22)

Note that this minimization problem has a closed-form solution. We construct the
feature vector in the second layer of the HNF as
jy? =¢ (V0 W 9) (23)
where W2) ¢ RM xm
feature vector can be found by solving

and m2) =n, The optimal mapping to the target by using this

O} = argminE it ~ Oy) I°| s.t. ||Oll2 <e9, (24)

where €2 is the regularization parameter. By choosing €2 = ||/O%,,[ ww)" U,@) 2, we can
see that the optimal costs follow the relation

E|\1t— O39 |? | = E[ IIe - 04,9 P|, (25)

where the equality happens when we have OF = O%,,1 WI] U,,2). Otherwise, the
inequality has to follow. Similarly, we can continue to add more layer to improve the
performance. Specifically, for /-th layer of the HNF, we have y = g (V,0WOFEDY),
and we can show that Eq. (18) holds here as well when the set of matrices {W Veo are

full-column rank.

4.3 Practical considerations

The dimension of feature vector y” increases as the number of layers increases. For a
multilayer feedforward network, if we use orthonormal matrix W” for /-th layer, then
each layer produces a feature vector that has at least twice the dimension of the input
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 11 of 19

feature vector. At the L-th layer, we get the dimension 2” times of the input data dimen-
sion. Note that y = g(V,z) is norm-preserving by Proposition 1, that means ||y||? = ||z|l’.
Using this principle successively, the full network is also norm-preserving, that means
iy ||? = ||x||?. Therefore, as the layer number increases, the amplitudes of scalars of the
feature vector y“) diminish at the rate of 2’. We show that the proposed HNF does not
require a large number of layers to improve the performance. This also answers the natu-
ral question that whether many layers are practically required for an ANN. Note that since
the dimension of the feature vector y” is growing exponentially as 2’, the proposed HNF
is not suitable for cases where the input dimension is too large. One way to circumvent
this issue is to employ the kernel trick [3] by using the feature vector y”’. We will address

this solution in future works.

5 Results and discussion

In this section, we carry out experiments to validate the performance improvement
and observe the effect of using the matrix V in the architecture of an HNF. We report
our results for three popular datasets in the literature as in Table 1. Note that we only
choose the datasets where the input dimension is not very large due to the compu-
tational complexities. Letter dataset [27] contains a 16-dimensional feature vector for
each of the 26 English alphabets from A to Z. Shuttle dataset [28] belongs to the STAT-
LOG project and contains a 9-dimensional feature vector that deals with the positioning
of radiators in the space shuttles. MNIST dataset [29] contains gray-scale 28 x 28-
pixel images of hand-written digits. Note that in all three datasets, the target vector t
is one-hot vector of dimension Q (the number of classes). The optimization method
used for solving the minimization problem (16b) is the alternating direction method of
multipliers (ADMM) [30]. The number of iterations of ADMM is set to 100 in all the
simulations.

We carry out two sets of experiments. First, we implement the proposed HNF with a
fixed number of layers by using instances of random matrices for designing the weight
matrix in every layer. In this setup, the weight matrix WY ¢€ R™ xm is an instance
of Gaussian distribution with appropriate size n > m and entries drawn indepen-
dently from NV (0, 1) to ensure being full-column rank. Second, we construct the proposed
HNF by using discrete cosine transform (DCT), as an example of full-column rank weight
matrix, instead of random matrices. In this scenario, we may need to apply zero-padding
before DCT to build the weight matrix W with appropriate dimension. The step size in
the ADMM algorithm is set accordingly in each of these experiments. Finally, we com-
pare the performance and computational complextiy of HNF and backpropagation over

the same-size network.

5.1 HNF using random matrix

In this subsection, we construct the proposed HNF by using instances of Gaussian distri-
bution to design the weight matrix W™. In particular, the entries of the weight matrix are
drawn independently from N (0, 1). For simplicity, the number of nodes is chosen accord-
ing to n© = m© for] > 2 in all the experiment. The number of nodes in the first layer
2n' is chosen for each dataset individually such that it satisfies ”) > P for every dataset
with input dimension P, as reported in Table 1. The step size in the ADMM algorithm is
set to 107’ in all the simulations in this subsection.
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 12 of 19

Table 1 Test classification accuracy of the proposed HIF for different datasets using random matrices

 

 

 

Dataset Size of Size of on Sener Proposed HNF_ —-ELM Proposed HNF State-of-the-art
training testing dimension of classes
data 9 data 9 (P) (Q) Accuracy n™ L Accuracy Accuracy n@ 1 lreferencel
Letter 13,333 6667 16 26 933 250 5883 946 1000 3 95.8[25]
Shuttle 43,500 14,500 9 7 993 250 5990 996 1000 3 999[19]
MNIST 60,000 10,000 784 10 97.1 10005969 977 4000 3 99.7 [26]

 

We implement two different scenarios. First, we implement the proposed HNF with
a fixed number of layers and show performance improvement throughout the lay-
ers. In this setup, the only hyperparameter that needs to be chosen is the number
of nodes in the first layer 21“). Note that the regularization parameter €, is chosen
such that it guarantees (21), and therefore eliminates the need for cross-validation in
the first layer. Second, we build the proposed HNF by using the ELM feature vector
in the first layer as in (23) and show the performance improvement throughout the
layers. In this setup, the only hyperparameter that needs to be chosen is the number
of nodes in the first layer 1“) which is the number of nodes of ELM to be exact. It
has been shown that ELM performs better as the number of hidden neuron increases
[24]; therefore, we choose a sufficiently large hidden neuron to make sure that ELM is
performing at its best. Note that the regularization parameter €; is chosen such that
it guarantees (25), and therefore eliminates the need for cross-validation. Finally, we
present the classification performance of the corresponding state-of-the-art results in
Table 1.

The performance results of the proposed HNF with L = 5 layers are reported in Table 1.
We report test classification accuracy as a measure to evaluate the performance. Note
that the number of neurons 2") in the first layer of HNF is chosen appropriately for
each dataset such that it satisfies 1) > P. For example, for MNIST dataset, we set ny) =
1000 > P = 784. The performance improvement in each layer of HNF is given in Fig. 1,
where train and test classification accuracy is shown versus total number of nodes in the
network ey 2n. Note that the total number of nodes being zero corresponds to direct
mapping of the input x to the target using least-squares according to (20). It can be seen
that the proposed HNF provides a substantial improvement in performance with a small

number of layers.

 

 

 

         

 

 

       

 

 

 

(a) Letter (b) Shuttle (c) MNIST
0.95
0.98 0.98 A untnstttutitianane
O.9 fp Se Daan
io 0.96 °
0.85 ° 0.96
ma ma a
2 08 8 g 0M
5 5 0.94 5
3 3 3 0.92
0.75 3 3
< < <
0.7 0.92 0.9
0.65 0.88
Training Accuracy 0.9 Training Accuracy | 4 Training Accuracy
0.6 Testing Accuracy Testing Accuracy 0.86 EE Testing Accuracy | |
0 5000 10000 15000 0 5000 10000 15000 0 1 2 3 4 5 6
Total number of neurons Total number of neurons Total number of neurons x 104

Fig. 1 Training and testing accuracy against size of one instance of HNF. Size of an L-layer HNF is represented
by the number of random matrix-based nodes, counted as }_; 2. Here, L = 5 for all three datasets. The
number of nodes in the first layer (2 n“) is set according to Table 1 for each dataset

 

 

 
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 13 of 19

The corresponding performance for the case of using the ELM feature vector in the
first layer of HNF is reported in Table 1. It can be seen that HNF provides a tangi-
ble improvement in performance compared to ELM. Note that the number of neurons
in the first layer n™ is, in fact, the same as the number of neurons used in ELM. We
choose 1) to get the best performance for ELM in every dataset individually. The
number of layers in the network is set to L = 3 to avoid the increasing computa-
tional complexity. The performance improvement in each layer of HNF in this case is
given in Fig. 2, where train and test classification accuracy is shown versus total num-
ber of nodes in the network mn“ + Vy 2n., Note that the initial point corresponding
to n™ is in fact equal to the ELM performance reported in Table 1, which is derived
according to (22).

Finally, we compare the performance of the proposed HNF with the state-of-
the-art performance for these three datasets. We can see that the proposed HNF
provides competitive performance compared to state-of-the-art results in the lit-
erature. It is worth mentioning that we have not used any pre-processing tech-
nique to improve the performance as in the state-of-the-art, but it can be done in

future works.

5.2. HNF using DCT

In this subsection, we repeat the same experiments as in Section 5.1 by using DCT instead
of the Gaussian weight matrix. The number of nodes in each layer of the network is
chosen as in Section 5.1. We apply zero-padding before DCT in the first layer to build
the weight matrix W” € RO” x? with appropriate dimension for each dataset. Note that
n = m for ] > 2 in all the experiments, and therefore, there is no need to apply zero-
padding in the next layers. The step size in the ADMM algorithm is set to 10? in all the
simulations in this subsection.

We implement the same two scenarios. First, we implement the proposed HNF by using
DCT and show performance improvement throughout the layers. Second, we build the
proposed HNF by using the ELM feature vector in the first layer and DCT matrices in the
next layers. Note that the regularization parameters €; for / > 2 are chosen according to
(19). The choice of €; is such that it guarantees (21) and (25) according to each scenario.

The performance results of the proposed HNF by using DCT matrices are reported in
Table 2. Note that the number of neurons 7 in the first layer and the number of layers

 

   

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a) Letter (b) Shuttle (c) MNIST
0.998 0.995 F
0.99 F
0.996 7
e e a
s 3 = 0.985
A A ql
3 3 0.994 5
< < < 9.98
oot wt | 0.992 F : 0.975
fa ——— Training Accuracy Training Accuracy ——— Training Accuracy
a | | en testing Accuracy 0.99 i So Accuracy | 0.97 be vee Testing scones |
1000 2000 3000 4000 5000 6000 7000 1000 2000 3000 4000 5000 6000 7000 0.5 1 1.5 2 2.5
Total number of neurons Total number of neurons Total number of neurons = x 104

Fig. 2 Training and testing accuracy against size of one instance of HNF using ELM feature vector in the first
layer. Size of an L-layer HNF is represented by the number of random matrix-based nodes, counted as

n) 4+ 5%, 2n. Here, L = 3 for all three datasets. The number of nodes in the first layer (n“) is set
according to Table 1 for each dataset

 

 

 
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40
(a) Letter 1 (b) Shuttle (c) MNIST
0.95 ¢
0.9 funnel enn —————~
0.85 } .
Borst & 5 5
< : < <
0.7 +F
0.65
0.6 Training Accuracy | 1 ———— Training Accuracy | 4 Training Accuracy
055 Testing Accuracy | Testing Accuracy |} neeh  Jeeeee Testing Accuracy | |
0 5000 10000 15000 0 5000 10000 15000 0 1 2 3 4 5 6
Total number of neurons Total number of neurons Total number of neurons = x 104
Fig. 3 Training and testing accuracy against size of HNF using DCT in every layer. Size of an L-layer HNF is
represented by the number of DCT-based nodes, counted as ey 2n. Here, L = 5 for all three datasets.
The number of nodes in the first layer (2. n“) is set according to Table 2 for each dataset

 

are the same as Table 1. The performance improvement in each layer of HNF is given in
Figs. 3 and 4. It can be seen that by using DCT in the proposed HNEFF, it is also possible to
improve the performance with a few layers.

Finally, we compare the performance of the DCT-based HNF and that of the random
matrix-based HNF as shown in Tables 1 and 2. We can see that using DCT as the weight
matrix is as powerful as using random weights in these three datasets.

5.3 Computational complexity

Finally, we compare test classification accuracy and computational complexity of HNF
with the backpropagation over the same learned HNF. We report training time of each
method in seconds. We run our experiments on a server with multiprocessors and 256 GB
RAM. The optimization method used for backpropagation is ADAM [31] from Ten-
sorFlow. The learning rate of ADAM is chosen via cross-validation, and the number of
epochs is fixed to 1000 in all the experiments.

We construct HNF by using random weights and use the same number of layers and
nodes as in Table 1. Note that we do not use ELM feature vector in the first layer for this
experiments, although it is possible to use it in order to improve the performance. The
results are shown in Table 3. As expected, backpropagation can improve the performance,
except for Shuttle, at the cost of a significantly higher computational complexity. HNF, on
the other hand, does not require cross-validation and only performs training at the last

 

 

 

 

    
 

     

 

 

 

   

 

 

 

 

 

 

 

  

 

 

 

 

 

 

(a) Letter (b) Shuttle (c) MNIST
0.98 5 0.999
0.995 7
0.96 F 0.99 |
RP | fant > 3
S0.94- fo rannnnt s S 0.985 >
Bf ff ane 5 5
QO fF fae 8 o
g weet 3 3
0.98
0.92 5
ef 993 nee 0.975 F
0.9+ rs ——— Training Accuracy ——— Training Accuracy Training Accuracy
ee oo Testing Accuracy || Qogol ot eens Testing Accuracy yee te Testing Accuracy
Ga 1 1 1 1 1 , 1 1 1 1 1 0.97 tet” 1 i i i 4
1000 2000 3000 4000 5000 6000 7000 1000 2000 3000 4000 5000 6000 7000 0.5 1 1.5 2 2.5

Total number of neurons Total number of neurons Total number of neurons =x 10+

Fig. 4 Training and testing accuracy against size of HNF using ELM feature vector in the first layer and DCT in
the next layers. Size of an L-layer HNF is represented by the number of nodes, counted as n“ + S77_5 2n".
Here, L = 3 forall three datasets. The number of nodes in the first layer (n‘) is set according to Table 2 for
each dataset

 

 

 

Page 14 of 19
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 15 of 19

Table 2 Test classification accuracy of the proposed HNMF for different datasets using DCT

 

 

 

Dataset Proposed HNF ELM Proposed HNF

Accuracy nO) L Accuracy Accuracy ni L
Letter 93.2 250 5 88.3 94,7 1000 3
Shuttle 99.8 250 5 99.0 99,3 1000 3
MNIST 97.7 1000 5 96.9 97.8 4000 3

 

layer of the network, leading to a much faster training. Note that training time reported
for backpropapation in Table 3 does not include cross-validation for the learning rate so
that we can have a fair comparison with HNF.

At this point, we also provide the reported classification performance of scattering net-
work on MNIST dataset for the sake of completeness. Scattering network with principal
component analysis (PCA) [14] over a modulus of windowed Fourier transforms yields
98.2% test classification accuracy for a spatial support equal to 8. This result shows that
scattering network can outperform HNF at the cost of a higher complexity of using several
scattering integrals in each layer. Note that HNF only uses a random choice of a Gaus-
sian distribution as the weight matrix in each layer. Besides, scattering network requires
accurate choice of several hyperparameters such as the spatial support, number of filter
banks, and type of the transforms, which can be crucial for the performance. For example,
in our experiments, a scattering network with PCA over a modulus of 2D Morlet wavelets
provides 94% accuracy, at best, for a spatial support of 28. The training on the our server
lasted 1158 s to yield such an accuracy, which highlights the learning speed of HNF in
Table 3. The same network with a spatial support of 14 gives a performance of 56.03%,
showing the importance of a precise cross-validation.

6 Conclusion

We show that by using a combination of orthonormal matrices and ReLU activation func-
tions, it is possible to guarantee a monotonically decreasing training cost as the number of
layers increases. The proposed method can be used by employing any other loss function,
such as cross-entropy loss, as long as a linear projection is used after the ReLU activa-
tion function. Note that the same principle applies if instead of random matrices, we use
any other real orthonormal matrices. Discrete cosine transform (DCT), Haar transform,
and Walsh-Hadamard transform are examples of this kind. The proposed HNF is a uni-
versal architecture in the sense that it can be applied to improve the performance of any
other learning method which employs linear projection to predict the target. The norm-
preserving and invertibility of the architecture make the proposed HNF suitable for other
applications such as auto-encoder design.

Table 3 Training time and test classification accuracy of the proposed HNF versus backpropagation

 

 

 

Dataset Proposed HNF Backpropagation

Accuracy Training time Accuracy Training time
Letter 93.42 475 95.03 4566 S$
Shuttle 99.21 24 5s 99.21 15,283 s

MNIST 97.14 108 s 98.30 20,433 s

 
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 16 of 19

Appendix 1. Proof of Property 3
Proof For scalars x, and x2, we have yj = g(x1) and yo = g(x2). We have following

relation

(x1 —x9)* if x; > 0.x. > 0
2 .
x if x17 > 0,x2 < 0
Qn — 92) = 45 ; (26)
x5 if x; < 0,x. > 0
0 if x1 < 0,x2 < 0.

Therefore, we find that ReLU function holds 0 < (y; — yn) < (x1 — x2)”. Considering
the vectors y; = g(z1) = g(Wq)) and y2 = g(z2) = g(Wqz), we have

0 < lly: — yall? = Yn @ — 2)

< @@ -2@) = lz — zl, (27)

 

where y (i) is the i-th scalar element of y; and z, (i) is the i-th scalar element of z). [

Appendix 2. Proof of Proposition 1

I
Proof We have z = Wq «€ R” andy = g(Vyz) € R*” where V, = "|. For
“Ay
two vectors q; and qz, we have corresponding vectors z} = Wq, and z2 = Wgqp, and
+
output vectors y; = g(V,Z1) and y2 = g(V,Z2). Note that yy = 71 } and therefore,

yall? = llzp ll? +llz7 I? = [Iz /2, by definition. Similarly, ||¥2||? = ||z2||?. Let us define a set
M (zi, Z2) = {i|s(z1@)) = s(z2@)) FO} € {1,2,...,n}.

Then, we have

Iz1 — zal? = ) (ad -— ald)’

i=1

= Vo@@MlaW| - s@W)le2@1)?

= YS da@l-la@)

ie M (21,22)

+ So (a@|+lea@)?. (28)

teM£(Z1,22)
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 17 of 19

We write z} = zp +z, =s (zy) zz | +s (z; ) |z, |. Then, after ReLU operation, we have

_ iz _ \z3 |
Yi =8(Vizi) =] — | andy2=g(VnZ2) =] * |.
|Z; | |Z. |
l¥1 — yall? = Wlzql — Izy il? + Wey | — lz DIP
. 1 \2 . ~ \2
= Yd (g@l-leg@l + Yo (lf Ol+iZ el)
ieM(|zq |,1Z5 |) ieME(|z7 |51z5 |)
_). — an \2 _). — pn \2
+ Yo (ag @l-le@)+ Yo (le @l+le_@l)
ieM (lz; [125 1) ieME((z; |,1Z5 |)
. 1 \2 _., — nx \2
= > (eg @l-lz@O)+ Yo (lg @l-le@l)
ieM (|z7 |1z5 |) ieM (2; |,1Z5 |)
. ~1\2 _,. — pn ,\2
+ Yo (g@l+lz@O) + Yo (AQ @l+lzz Al)
ieME(\z7 |,1Z5 I) ie ME (|Z [125 |)
= YY (da@l-la@)?+ Yo la@P+la@l.
iE M (z1,Z2) iE MC¢ (z1,Z2)
= lal? + lll? -2 Yo u@mi

i€ M (z},Z2)
N
= |lzi\I? + lz? -2 u@m@+2 Yo mma)
i=1 iE M¢(z1,Z2)
=|jz—nl?+2 Yo udm (29a)

te M°(Z1,Z2)

1 2,1 2 2

—||z] —Z —(l|z Z —2 Z1 (i)Z9(i

5 | 1 — Za +5 1|l° + ||zal| | ) 1 (4)Z2 (i)
iE M (z1,Z2)

+2 Yo m22(i)

ie M&(z1,Z2)

1 2,1 2 2

—||z] —Z — Z Z —2 Z1(i)||Zo(i

5 | 1 — Za +5 |z1 || + ||z2|| | ) [Z1 (Z) ||zZ2 (a)
iE M (z1,Z2)

-2 So |u@|lz2@|

ie ME (Z1,Z2)

1 2 1] 2 2 ” ° °
—||z1 —Z — Z Z —?2 Z1(1)||Zo(
liza — zal? + 5 ( ill? + [lzall 2 1(i)||z2(i)|

1] 1]
= 5 lla - Z|" + 5 lilzal — Iz2| |" (29b)

With similar calculations as in (28), we can derive the relationships in Eqs. (29a) and (29b).
Since the summation 0 <j, jc Z,(i)Z2(i) is always non-positive, from (29a), we can

see that

(Z1,Z2)

v1 — yall? < lz — zal, (30)

where equality holds when M° = @, that means when sign patterns of z; and zz match

exactly. From (29b), it can also be seen that

1] _ _
5 liza — Za\/? < llyi — yall’, (31)

where equality holds when |z;| = |z2]. OO
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 18 of 19

Appendix 3. Proof of Remark 3
Proof Consider z = Wq andz, =[W + AW]q = Wq4+[ AW] q = z+ Az. Based on
Proposition 1, we can simply write
ly — yall? < Azll? = [AW] all?
JAWIF lal’, (32)

lA

where we have used Eq. (4). a

Abbreviations

ReLU: Rectified linear unit; SVM: Support vector machine; KPCA: Kernel principal component analysis; RBF: Radial basis
function; ANN: Artificial neural network; HNF: High-dimensional neural feature; DNN: Deep neural network; CNN:
Convolutional neural network; RNN: Recurrent neural network; ELM: Extreme leaning machine; PLN: Progressive learning
network; DCT: Discrete cosine transform; ADMM: Alternating direction method of multipliers; PCA: Principal component
analysis; ADAM: Adaptive moment estimation

Acknowledgements
We acknowledge the support of our KTH colleagues Amirreza Zamani and Hamid Ghourchian for proofreading and
critical remarks.

Authors’ contributions
AJ, AV, MS, and SC conceived and designed the study. AJ performed the experiments. AJ and SC wrote the paper. AJ, AV,
and MS reviewed and edited the manuscript. All authors read and approved the manuscript.

Funding
Open access funding provided by Royal Institute of Technology.

Availability of data and materials
All datasets used in the experiments are publicly available online. Please contact the corresponding author for simulation
results.

Consent for publication
Not applicable.

Competing interests
The authors declare that they have no competing interests.

Received: 1 November 2019 Accepted: 3 August 2020
Published online: 10 September 2020

References

1. C.Cortes, V. Vapnik, Support-vector networks. Mach. Learn. 20(3), 273-297 (1995)

2. B. Schdlkopf, A. Smola, K.-R. Muller, Nonlinear component analysis as a kernel eigenvalue problem. Neural Comput.
10(5), 1299-1319 (1998)

3. CM. Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics). (Springer, Berlin,
Heidelberg, 2006)

4. D.Yu,L. Deng, Deep learning and its applications to signal and information processing [exploratory dsp]. IEEE Signal
Process. Mag. 28(1), 145-154 (2011)

5. O.Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L.
Fei-Fei, Imagenet large scale visual recognition challenge. Intl. J. Computer Vision. 115(3), 211-252 (2015)

6. S.F. Dodge, L. J. Karam, A study and comparison of human and deep learning recognition performance under visual
distortions. ArXiv e-prints (2017). 1705.02498. Accessed 1 Oct 2019

7. L.Wan, M. Zeiler, S. Zhang, Y. L. Cun, R. Fergus, in Proceedings of the 30th International Conference on Machine
Learning. Proceedings of Machine Learning Research, vol. 28. ed. by S. Dasgupta, D. McAllester, Regularization of neural
networks using dropconnect (PMLR, Atlanta, 2013), pp. 1058-1066

8. D.Mishkin, J. Matas, in 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings. ed. by Y. Bengio, Y. LeCun, All you need is a good init, (2016). http://arxiv.org/
abs/1511.06422

9. C.-Y. Lee, P. W. Gallagher, Z. Tu, in Proceedings of the 19th International Conference on Artificial Intelligence and
Statistics. Proceedings of Machine Learning Research, vol. 51. ed. by A. Gretton, C. C. Robert, Generalizing pooling
functions in convolutional neural networks: mixed, gated, and tree (PMLR, Cadiz, 2016), pp. 464-472

10. A.J. Thomas, M. Petridis, S. D. Walters, S. M. Gheytassi, R. E. Morgan, in 2075 International Conference on Computational
Science and Computational Intelligence (CSCI), On predicting the optimal number of hidden nodes, (2015),
pp. 565-570

11. C.Szegedy, A. Toshev, D. Erhan, in Advances in Neural Information Processing Systems 26. ed. by C. J.C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger, Deep neural networks for object detection, (2013), pp. 2553-2561

12. A. Krizhevsky, |. Sutskever, G. E. Hinton, in Advances in Neural Information Processing Systems 25, Imagenet
classification with deep convolutional neural networks (Curran Associates Inc., Red Hook, 2012), pp. 1097-1105
Javid et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:40 Page 19 of 19

13. |. Sutskever, Training Recurrent Neural Networks. PhD thesis. (University of Toronto, Toronto, 2013). AAINS22066

14. J. Bruna, S. Mallat, Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach. Intell. 35(8), 1872-1886
(2013)

15. R. Vidal, J. Bruna, R. Giryes, S. Soatto, Mathematics of deep learning. ArXiv e-prints (2017). 1712.04741. Accessed 1 Oct
2019

16. R. Giryes, G. Sapiro, A. M. Bronstein, Deep neural networks with random Gaussian weights: a universal classification
strategy? IEEE Trans. Signal Process. 64(13), 3444-3457 (2016)

17. S. Chatterjee, A. M. Javid, S. K. Mostafa Sadeghi, P. P. Mitra, M. Skoglund, SSFN: self size-estimating feed-forward
network and low complexity design. ArXiv e-prints (2019). 1905.07111. Accessed 1 Oct 2019

18. S. Chatterjee, A. M. Javid, M. Sadeghi, P. P. Mitra, M. Skoglund, Progressive learning for systematic design of large
neural networks. ArXiv e-prints (2017). 1710.08177. Accessed 1 Oct 2019

19. G-B. Huang, H. Zhou, X. Ding, R. Zhang, Extreme learning machine for regression and multiclass classification.

J. Trans. Sys. Man Cyber. B. 42(2), 513-529 (2012)

20. G. Huang, G.-B. Huang, S. Song, K. You, Trends in extreme learning machines: a review. Neural Netw. 61(Supplement
C), 32-48 (2015)

21. T. Hussain, S. M. Siniscalchi, C. C. Lee, S. S. Wang, Y. Tsao, W. H. Liao, Experimental study on extreme learning machine
applications for soeech enhancement. IEEE Access. PP(99), 1 (2017)

22. W. Zhu, J. Miao, L. Qing, G. Huang, in 2015 International Joint Conference on Neural Networks (UCNN), Hierarchical
extreme learning machine for unsupervised representation learning, (2015), pp. 1-8

23. A.Rosenfeld, J. K. Tsotsos, Intriguing properties of randomly weighted networks: generalizing while learning next to
nothing. ArXiv e-prints (2018). 1802.00844. Accessed 1 Oct 2019

24. A.M. Javid, S. Chatterjee, M. Skoglund, in 2078 15th International Symposium on Wireless Communication Systems
(ISWCS), Mutual information preserving analysis of a single layer feedforward network, (2018), pp. 1-5

25. J. Tang, C. Deng, G. Huang, Extreme learning machine for multilayer perceptron. IEEE Trans. Neural Netw. Learn. Syst.
27(4), 809-821 (2016)

26. L.Wan, M. Zeiler, S. Zhang, Y. L. Cun, R. Fergus, in Proceedings of the 30th International Conference on Machine
Learning. Proceedings of Machine Learning Research, vol. 28. ed. by S. Dasgupta, D. McAllester, Regularization of neural
networks using dropconnect (PMLR, Atlanta, 2013), pp. 1058-1066

27. P.W. Frey, D. J. Slate, Letter recognition using Holland-style adaptive classifiers. Mach. Learn. 6(2), 161-182 (1991).
https://archive.ics.uci.edu/ml/datasets/letter+recognition

28. C.L. Blake, C. J. Merz, UCI Repository of Machine Learning Databases. (Dept. Inf. Comput. Sci., Univ., Irvine, 1998). http://
www.ics.uciiedu/~mlearn/MLRepository.htm!

29. Y.Lecun, L. Bottou, Y. Bengio, P. Haffner, in Proceedings of the IEEE, Gradient-based learning applied to document
recognition, (1998), pp. 2278-2324. Available: http://yann.lecun.com/exdb/mnist/

30. S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, Distributed optimization and statistical learning via the alternating
direction method of multipliers. Found. Trends Mach. Learn. 3(1), 1-122 (2011)

31. D.P.Kingma, J. Ba, in Proceedings of the 3rd International Conference on Learning Representations (ICLR), ADAM: a
method for stochastic optimization, (2015)

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen®

journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
