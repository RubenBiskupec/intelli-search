 

Hashemi and Hall J Big Data (2020) 7:2 °
https://doi.org/10.1186/s40537-019-0282-4 oO Jou ral of Big Data
RESEARCH Oy oT-Ta waa -55 4

®

Check for
updates

   
  
 
 
 
 
 
 
 
  
 
 
  

Criminal tendency detection from facial
images and the gender bias effect

Mahdi Hashemi’ ® and Margeret Hall?

 

*Correspondence:
mhashem2@gmu.edu Abstract

] ; :
Department of Information Explosive performance and memory space grow
Sciences and Technology,

George Mason University,
4400 University Dr, Fairfax, VA
22030, USA

Full list of author information ; ; ; ;
‘s available at the end of the personality traits. This work is the first mile

article traits from facial images. With this ul#
image understanding, inferring crimit 2
ing. In particular, two deep learning meéels

ages. Confusion matrix and training and test accura-
using tenfold cross-validation on a set of 10,000

pagher than the SNN's test accuracy. Next, to explore the
due to gender, we controlled for gender by applying only

fac@ eyebrows, top of the eye, pupils, nostrils, and lips are taken advantage of by CNN
r to classify the two sets of images.

: Image classification, Facial images, Convolutional neural network, Deep
gj, Machine learning, Personality traits

   

Face is the primary means of recognizing a person, transmitting information, commu-
nicating with others, and inferring people’s feelings, among others. Our faces might dis-
close more than what we expect. A facial image can be informative of personal traits [1],
such as race, gender, age, health, emotion, psychology, and profession.

This study is triggered by Lombroso’s research [2], which showed that criminals could
be identified by their facial structure and emotions. While Lombroso’s study looked at
this issue from a physiology and psychiatry perspective, our study investigates whether
or not machine learning algorithms would be able to learn and distinguish between
criminal and non-criminal facial images. More specifically, we will look for gender biases

in machine predictions. This is important because criminal facial images used to train

. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
GO) Springer O pen adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
— the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material
in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco

mmons.org/licenses/by/4.0/.
Hashemi and Hall J Big Data (2020) 7:2 Page 2 of 16

the machine are mostly male. This is the result of the large gap between the number of
mugshots for arrested males and females, available to the public and used to train the
machine.

It is noteworthy that this study’s scope is limited to the technical and analytical aspects
of this topic, while its social implications require more scrutiny and its practical appli
cations demand even higher levels of caution and suspicion. With that in mind, this
paper explores the deep learning’s capability in distinguishing between crimig and
non-criminal facial images. To this effect, two deep learning models, a standaréafeec.
ward neural network (SNN) and a convolutional neural network (CNN), arg trained witn
10,000 neutral-emotion, mixed-gender, mixed-race facial images. A neutral\ \blankyrace
expression is characterized by neutral positioning of the facial feature. WA neutral face
expression could be caused by a lack of emotion, boredom, depr@jsion, or~ ght confu-
sion. A neutral face expression is also referred to as a poker“aces sis meant to con-
ceal one’s emotions when playing the card game poker [346% hile bot): neural network
models are controlled for facial emotions by applying ef. na yyemotion images, no
control has been imposed on race, due to our small dataset ‘= Wythe difficulty and occa-
sionally subjectivity of identifying the race from low < Wigyfacial images. Both models
are trained with and without controlling for gender. ‘We results indicated that control-
ling for gender does not have much effect_on accuraly or learning and both models
reach high classification accuracies rega¢dless. \NN achieves a tenfold cross-validation
accuracy of 97%.

The strength of this study lies i its applic. aon of neural networks to investigate if a
stack of non-linear functions gviti, “ousaiids of parameters can find useful facial fea-
tures to distinguish betwee )riminal% -d non-criminal face shots. Its weakness however
lies in its reliance on machine t ‘garn these features and on a limited number of images.

“Related work” segtion providés a review of related works. “Methodology” section
elaborates on this si \dy’s meyhodology. “Data preparation” section describes the image
dataset sources and ti jpgroach taken to prepare the dataset. “Neural network archi-
tecture” sectioi ~ ygibes the SNN’s and CNN’s architecture, proposed in this study, for
criminal.tendenwy recognition through facial images. “Visual criminal tendency detec-
tion pOsult! and discussion” section presents the results for both mixed gender and male
opi. cla ification scenarios. “Conclusion and future directions” section concludes the

ser by discussing the results and future directions.

Related work

NM achine learning has shown to be more effective than humans in discovering personal-
ity traits through facial images [4]. Geng et al. [5] trained a machine to estimate the age
through facial images. Reece and Danforth [6] applied an ensemble of machine learning
models and image processing to detect depression and psychiatric disorder in Instagram
facial images.

The goal in facial emotion detection is to train a machine to distinguish among six
emotional facial expressions: happiness, surprise, sadness, disgust, anger, and fear [7].
Fuzzy inference system [8], hidden Markov model based on real-time tracking of the
mouth shape [9], and Bayesian network [10] are among the approaches used for classify-

ing facial emotions.
Hashemi and Hall J Big Data (2020) 7:2 Page 3 of 16

Criminal tendency is another personality trait. Lombroso [2] was the first in 1871
to point out that criminals could be identified by their facial structure and emotions.
Recently, Wu and Zhang [11] revisited this theory and quantitatively demonstrated the
correlation between criminality and facial features. They trained four classifiers: logistic
regression, k nearest neighbors (KNN), support vector machines (SVM), and convolv
tional neural network (CNN) and claimed that their machine can identify a criminal face
with a 90% accuracy. Their model was controlled for gender, race, and facial exp@ \ssion
of emotions.

Neural networks have resurged and drawn much attention in the last decade !12]
with the new brand of deep learning, mainly due to the significant perforn ce gagn in
visual recognition tasks [13]. Deep learning has been applied to a wid \ange ot appli-
cations, such as tree disease recognition [14]. Among the most # ‘evant ap, cations of
deep learning to our work, we can point to the application 6{ CN for face recogni-
tion [15, 16]. Cristani [17] and Segalin et al. [18, 19] applied hine learning to predict
the self-assessed personality traits (openness to experie. yy. Wentiousness, extra-
version, agreeableness, and neuroticism) of a person from \ images he/she uploads
or likes on social media, such as Flicker, and what 135, ygigns in terms of personality
traits those images trigger in unacquainted people. Tey performed their experiments
with 60,000 images from 300 Flickr users. Cristani et al’ [17] and Segalin et al. [18] used
a hybrid approach where generative mé@dels,~ »d as latent representations of features
(color, composition, textural properties, © )\ ex racted from images, are built and then
passed to a discriminative classifér to predic each user’s personality traits. Simplifying
the problem into five distinctfaina. “classtication problems, one for each trait, Segalin
et al. [19] applied AlexNety, QJ, which ss an eight-layer version of CNN, pre-trained on
ImageNet 2012 competition au set. The problem they pose is to detect the personal-
ity traits based on thé images that one uploads or likes on social media, such as Flicker.
Their results showec shat the personality trait that others attribute to a person (based on
the images that that 1— ittual uploads or likes on social media) can be predicted 10%
more accuratel,_ ysbe personality traits that that individual attributes to him/her-self.
Wang angsKosinyki [4] trained a deep neural network to classify facial images based on

sexuaoorie tation.

n_ wthodology
Figu. )¥ shows the general workflow of this study. The first step is data collection and
preparation which is performed as follows.

Data preparation

A total of 8401 gray-scale mugshot images of arrested individuals are obtained from
National Institute of Standards and Technology (NIST) Special Database [20]. Images
are all in png format. Images are of mixed race, mixed gender, and neutral face expres-
sion and contain both front and side (profile) views. Since our focus is on frontal face
shots, we need to eliminate profile views. Haar feature-based cascade classifier [21]
detects images containing frontal face views and also detects the rectangular area con-
taining the face. Images are passed to the pre-trained version of this classifier, available
in the OPenCV library in Python, to keep only the images that contain frontal face views
Hashemi and Hall J Big Data (2020) 7:2 Page 4 of 16

 

   
  
   
  

 

  
  
 
 
   

39,7131 face images with mixed emotions obtained from
http: //cbcl.mit.edu/software-datas ets/heisele/facerecognition-database.html
http: //fei.edu.br/~cet/facedatabase.htm]
http: //anefian.com/research/face_reco.htm
http: //wiki.cnbc.cmu.edu/Face_Place
http: //vis-www.cs.umass.edu/fddb/

Pre-trained Harr feature-
based cascade classifier

Images with frontal face views are detected and Images with frontal face views are detected and /
the rectangular area containing the face is cropped the rectangular area containing the face is croppeu

‘ \

Manually eliminate falsely Manually eliminate falsely
detected frontal face images detected frontal face imagis

seeceshonpii

Manually eliminate face igiages with
neutral emotions, eld¢ and children

   
  

8401 gray-scale mugshots of arrested individuals with
neutral emotion obtained from
https: //www.nist.gov/srd/nist-special-database-18

  
   
 
    
  
  
 
 

Pre-trained Harr feature-
based cascade classifier

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

Convert togmmmy-scale ton.
withgrim. face images
Resize th ages to 128x128

A” umninal face images

Resize the images to 128x128

5000 criminal face images

Deep neural network for >| tendency detection

A crit bor not? Y

   
 
    

   
 
  
 

 

 

 

 

 

 

 

Fig. 1 Constructing criminal vs. non-crimmMal facial ima, datasets
L J

and then crop the rectafigula. \ea containing the face. Cropping the facial rectangle
from the rest of the@mage prevents the classifier from being affected by peripheral or
background effects| \urrounding the face. The false positive rate (none-frontal face
images misclassified <_ jgeftal face images) of the Haar feature-based cascade classifier
was 1.9% whici mmanually deleted. The result contains 5000 front view face images
of 4796 male anW 204 female individuals and of variable sizes, ranging from 238 x 238
up to@13 > 813 pikels. Since neural networks receive inputs of the same size, all images
argeresi. 4 to 128 x 128 using bilinear interpolation. This size is chosen considering the

pacity ox our platform (64-bit 3.00 GHz Xeon E3-1505 M v6 processor, 2400 MHz
16% \DDR4 SODIMM RAM, NVIDIA Quadro M2200 4 GB GDDR5 GPU) to process
the images collectively.

A total of 39,713 RGB facial images are obtained from five sources (Face Recognition
Database [22], FEI Face Database [23], Georgia Tech face database [24], Face Place [25],
Face Detection Data Set and Benchmark Home [26]). We consider these images as non-
criminal face shots. Images are all in jpg format. Images are of mixed race, mixed gen-
der, and mixed facial expressions. The database contains both front and side (profile)
views. Since our focus is on frontal face shots, we need to eliminate profile views, using
the Haar feature-based cascade classifier [21]. The false positive rate (none-frontal face
images misclassified as frontal face images) of the Haar feature-based cascade classi-
fier was 1.3% which were manually deleted. Facial images with any emotion expression

but neutral are manually deleted, in order for compatibility with criminal facial images
Hashemi and Hall J Big Data (2020) 7:2 Page 5 of 16

which are all neutral. Also, to keep the age, approximately, in the same range with the
criminal dataset, images of elderly and children are manually deleted from this dataset.
The images are then converted to gray-scale, again to be compatible with mugshots in
the criminal dataset. The result contains 5000 front view face images of 3727 male and
1273 female individuals and of variable size, ranging from 87 x 87 up to 799 x 799 pixels
Images are resized to 128 x 128.

Neural network architecture

As shown in Fig. 1, the data are passed to an artificial neural network for ‘urther dlas-
sification. Artificial neural networks do not rely on hand-engineer@ »fea. Wegwhich
are hard to select and design. The neural network in our applicatiéfarec. es, as input,
128 x 128 pixel gray-scale images. Without extra preprocessigg, Nye image pixels are
only divided by 255 so that they are in the range 0 to 1. Before.descriv. \yhe neural net-
work architecture, we justify our choice of activation funétion loss fufction, and train-
ing algorithm.

While saturated activation functions, e.g. sigmoidygxtanh, c dld trigger the vanishing
gradients problem and prevent the exploding gradientsy>. 5lem because of their near-
zero gradient at large values, non-saturated activation iunctions, e.g. rectified linear unit
(ReLU), could trigger the exploding gradisti)}roblem and prevent the vanishing gradi-
ents problem because of their non-zer€ \gadiet. jat large values. Both problems happen
for synaptic weights at lower layerg@nd wo )nyvevent the network from being properly
trained. The exploding gradient4 )objam \s easier to detect because the vanishing gra-
dients could also happen dug t the. Mainfing convergence. Besides, non-saturated acti-
vation functions make thé \ Ming several times faster [13]. Therefore, we chose the
non-saturated activatioyMfunctic WReLU [27, 28]. ReLU is a piecewise linear function,
defined as the positj’e part of its argument: ReLU(z)= max(0,z). By projecting negative
inputs to zero, ReLU jreateg/a sparseness in the activation of neural units, a desirable
effect similar Wadropout. The softmax function, softmax(z,) = exp(z,)/2; exp(z;), used in
the final layer, tkapsic ms the values (z,) to normalized exponential probabilities whose
summer his ond (ie. X, p= 1). This provision (2, p,=1) is a prerequisite for the appli-
cation, Wha ntropy loss function, which is calculated as: — X, y, log(p,), where c rep-
nésents a~ Wyron (or class) in the output layer, y, represents the desired value (0 or 1) at
ti, neuron, and p, is the predicted probability at that neuron. The cross-entropy loss
funct. f simplifies to — (y log(p)+ (1 —¥y) log(1 — p)) for the binary classification in our
case. The network is trained using the Adam optimization algorithm [29], which is an
gxtension to the stochastic gradient descent (SGD) approach, with a batch size of 100.
Unlike SGD which maintains a single and fixed learning rate for all synaptic weight
updates, Adam continually adjusts individual adaptive learning rates for each synap-
tic weight based on estimates of first and second moments of the gradients. The learn-
ing rate is initialized at 0.0001 and the exponential decay rate for the first and second
moment estimates are set to 0.9 and 0.999 respectively, suggested by Kingma and Ba
[29].

Two neural network architectures are applied for classifying facial images into
criminal and non-criminal categories, an SNN and a CNN. The SNN composes of

four fully-connected layers, in addition to the input layer which has 16,384 neurons,
Hashemi and Hall J Big Data

(2020) 7:2

equal to the total number of pixels in an image. The first three fully-connected layers
have 512 neurons and each are followed by an ReLU layer. The fourth fully-connected
layer has a size equal to the number of target categories, labeled as criminal and non-
criminal, and is followed by a softmax function. The overall architecture is shown in
Fig. 2.

CNN has recently outperformed other neural network architectures and other
machine learning and image processing approaches in image classification [4 \30-
36] and object detection [37] due to its independence from hand-crafted #®ual~
tures and excellent abstract and semantic abilities [34, 38]. CNN make) strong anu
mostly correct assumptions about the nature of images, namely, doca wy of pixel
dependencies and stationarity of statistics. Therefore, in comparison. \ith SNN with
similarly-sized layers, CNN has much fewer connections a@@ param: €rs which
makes it easier to train. The applied CNN in this work compéses G §wo convolutional
layers followed by two fully-connected layers. Convolutiofia. ayers hive the following
settings: f/=3x3x1,s,=1,1,=8, f,=3 x3 x 8, 5,=45 There fo S,» and n,,
denote the size, stride, and number of filters of the m-th layu )gespectively. Every con-
volutional layer is followed by a max pooling and Rv jer’ Pooling summarizes the
outputs of neighboring groups of neurons in the sanyé kernel map. We use 2 x 2 max
pooling with a stride of 2, which means the pooling regions do not overlap. Smaller
pooling regions cause over-fitting (hig), vari \ce) and larger regions are too generic
and lose the details (high bias [39}). Ty Mrst fully-connected layer has 64 neurons

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

( >\
E
os
~= 3m"
hy g nN! I
© S uu...
EEO
a yes
so me
ao AD mo
ype 7. N
= D> 4
o > mA w=
BES lA
i | i
LO 11
2 ov
& !
>
N
~~ gE 3 — x
y g lad ! wn
oO \ \
N
z Oo ee i
oOo | in
ped \
ny oO | |
& 5 ! !
| |
wo — iN
ur Y E I tn
o S oo
Oo | \
N
z o ee |
ee ' 11a
su
NY oO \ i
2 |
|
| |
! I
!
! 2
©
oO
| %
| >
0
\ MS
O
ce
©
N
—_
Fig. 2 The standard neural network architecture

 

 

Ne S

Page 6 of 16
Hashemi and Hall J Big Data (2020) 7:2 Page 7 of 16

and is followed by an ReLU layer. The second fully connected layer has a size equal to
the number of target categories, labeled as criminal and non-criminal, and is followed
by a softmax function. The overall architecture is shown in Fig. 3.

The convolution filter and the pooling filter (elaborated in the next section) would slip

       
 
     
 

outside the input image into the void, when they attempt to center themselves at borde
ing pixels. There are two strategies to solve this issue: (a) stopping the filter before it slips

outside the image and (b) padding the input image with zero pixels. The first a ach

 

Standard

Standard
feedforward

with stri

   
 

Convolution
with stride
of 1

  

Gray-scale

 

 

Fig. 3 The convolutional neural network architecture
\
Hashemi and Hall J Big Data (2020) 7:2 Page 8 of 16

comes at the cost of under-scanning the bordering pixels because the filter will not get a
chance to center itself at the bordering pixels. The second approach is referred to as pad-
ding and is the one applied in our model.

Visual criminal tendency detection results and discussion

Experiments are conducted on a 64-bit 3.00 GHz Xeon E3-1505M v6 pre@fessor,
2400 MHz 16 GB DDR4 SODIMM RAM, NVIDIA Quadro M2200 4 GB GDDR5 © OW.
Artificial neural network models are implemented in Python using the TensorFlow
library [40].

Splitting a small dataset into training and testing sets would ledve gj wii even a
smaller training set. In cross-validation, all the samples could be@ised for\. eh training
and testing, while the model is evaluated on previously unse€n sa Mples. Additionally,
in k-fold cross-validation, we train and test k models. This®. \ws us ts be more confi-
dent in the performance results. Consequently, we can “ )on! srt a more solid test
accuracy, but also the standard deviation for this test accura Finally, cross-validation
allows us to tune the number of layers in our neural i jpsk, which will be further elab-
orated at the end of this section. With these advantageé in/mind, the tenfold cross-vali-
dation approach is applied here. The tenfold_is preferreW over its fivefold counterpart to
produce a more accurate standard devias#ion.

The neural networks are trained un to~ (0 epi chs, after which the change in training
accuracy becomes imperceptible ¢the charts 1 Fig. 4 represent the average and stand-
ard deviation of training and test ‘« Maraci¢$ at each epoch. The tenfold cross-validation
has been performed at each@ynoch. Th ¥, the training and test accuracies at each epoch,
reported in Fig. 4, are the‘avera, over the ten folds. The standard deviation of accuracies
is also calculated oveythe ten folds at each epoch and depicted using the line’s thickness.
The CNN achieves | \s highest test accuracy (97% with a standard deviation of 0.91%)
at epoch 306. While. )ypafining accuracy keeps rising after this epoch, the test accu-
racy starts dro}, The test accuracy of 97%, achieved by CNN (Fig. 4a), exceeds our
expectations ands aclear indicator of the possibility to differentiate between criminals
and pn@n-ci minal)’using their facial images. It is noteworthy that the criminal mugshots
argecon. 2 trom a different source than non-criminal face shots. That means the condi-

ns unde, which the criminal images are taken are different than those of non-crimi-
nal ages. These different conditions refer to the camera, illumination, angle, distance,
background, resolution, etc. Such disparities which are not related to facial structure,
thSugh negligible in majority of cases, might have slightly contributed in training the
classifier and helping the classifier to distinguish between the two categories. Therefore,
it would be too ambitious to claim that this accuracy is easily generalizable.

Interestingly but not surprisingly, the CNN (Fig. 4a) achieves a higher test accuracy
than the SNN (Fig. 4b), also in a more consistent way. The CNN’s best test accuracy
(97%) is 8% higher than the SNN’s best test accuracy (89% with a standard deviation of
1.18%). This goes back to the SNN being general purpose but the CNN being specifi-
cally designed for image classification. On the other hand, the training accuracy is only
0.37% different for CNN and SNN, pointing to their equal capacity in learning from the
training data. The CNN is more consistent in learning because the variance around its
Hashemi and Hall J Big Data (2020) 7:2

Page 9 of 16

 

°
Training Accuracy + std

     

     

100

     
 

Test Accuracy + std

 

90

I
I
I
I
I
I

Epoch=306

Test Accuracy=96.90% (std=0.91%)
Training Accuracy=99.46% (std=0.36%)

80

  
 

70

 

60
50
40
0 Epo
Cc
@ Training Accuracy +
100
Test A + std
I est Accuracy +s
90
I
I
80
Epoch=169
70 Test Accuracy=96.28% (std=1.25%)
Training Accuracy=98.50% (std=0.24%)
60
I
I
50 !
I
I
40
0 Epoch 0

Fig. 4 Training and test accuracy wit@&one
SNN, ¢ CNN when applied to only@ gale image
(NS

 
  

training and test ac
higher consistency a

onfusion matrix for CNN

500

   
    
  
   
 

d its fewer parameters.
atpixes for the CNN and SNN are shown in Tables 1 and 2, respec-
e between the false positive and false negative rates is 1% for the

  
 
        
   
  

   

     
      
   
 

 

   
 
 
 
 
   

b @
Training Accuracy + std
100
90
I
80 ! Test Accuracy + std
Epoch=271
70 Test Accuracy=89.11% (std=1.18%)
Training Accuracy=99.83% (std=0.06%)
60
I
I
50 !
I
I
40
0 Epoc
Training Accuracy + std
100 6 La
| rr pe
90. mn .
80 "A ! Test Accuracy + std
\/ Epoch=326
70

Test Accuracy=89.23% (std=1.88%)
Training Accuracy=99.72% (std=0.24%)

J

 

0 Epoch 500

adarddeviation of uncertainty at different epochs for: a CNN, b
dd SNN when applied to only male images

racy cuyves (Fig. 4a) is tighter than that of the SNN (Fig. 4b). The
acy of the CNN are because of its assumption of locality of

 

 

 

 

 

 

 

 

Predicted
Criminal Non-criminal
Criminal 4881 142
Non-criminal 192 4785
Table 2 Confusion matrix for SNN
Predicted
Criminal Non-criminal
Truth Criminal 4515 508
Non-criminal 604 4373

 
Hashemi and Hall J Big Data (2020) 7:2 Page 10 of 16

Table 3 Confusion matrix for CNN when applied to only male images

 

 

 

Predicted
Criminal Non-criminal
Truth Criminal 4694 116
Non-criminal 261 3452

 

Table 4 Confusion matrix for SNN when applied to only male images

 

 

 

Predicted
Criminal n-criminal
Truth Criminal 4423 3287
Non-criminal 555 3158

 

CNN and 2% for the SNN. In other words, the false posit, Jane ise negative rates are
almost the same for both CNN and SNN, ie. the ¢lassifier sno meaningful bias in
making either type of mistake more than the other. We _jpbserved that there are mis-
classified men, women, white, and colored people fronXboth categories. Among the false
negatives (criminal images which were misglessified as non-criminal) by CNN, 81% were
male and 19% were female. This is promyrtione jo the 75% male vs. 25% female images
among non-criminals. Among the fgise pt ives (non-criminal images which were mis-
classified as criminal) by CNN, 89% Were mali and 12% were female. This is proportional
to the 95% male vs. 5% femal@ima_ % anzong criminals. Among the false negatives by
CNN, 79% were white pegf Nand 21% were colored people. This is proportional to the
69% white vs. 31% coloyed pec, )\2mong non-criminals. Among the false positives by
CNN, 79% were whife and 21% were colored. This is proportional to the 72% white vs.
28% colored people\ (nong griminals. This indicates that the classifier is not biased to
put people of a specific jaider or race in a specific category while ignoring their crimi-
nal tendency.

Theree@ mort females among non-criminal images than criminal ones. While 25%
of na Wcriy smalunages are female, only 4% of criminal images are female. The machine
magnt bu Wfairly taking advantage of this distinction to boost its classification accuracy.
» observe and control the gender bias effect, we separate male and female images in
each )tegory. Since the number of female images is too small, we only train and cross-
validate the models using male images. There are 4796 male images in the criminal and
3727 in the non-criminal category. Figure 4c, d show the average and standard devia-
tion of training and test accuracies over different training epochs for the CNN and SNN,
respectively. These charts very closely imitate their mixed gender counterparts in Fig. 4a,
b, a sign that gender has no effect on biasing the classifier one way or the other. The cor-
responding confusion matrixes for CNN and SNN when applied to only male images,
shown in Tables 3 and 4, endorse the same conclusion.

Choosing the CNN to have two convolutional layers was the result of an experimen-
tal model complexity vs. generalization accuracy analysis. Figure 5 shows how chang-
ing the number of convolutional layers affects the tenfold cross-validation accuracy
and its standard deviation. According to this graph, the CNN with five convolutional
Hashemi and Hall J Big Data (2020) 7:2 Page 11 of 16

 

 

cr >
100

> 1
oO
< '
5 i
3 95 a
3 1
fo 1
2
8 Selected
"Ss 90 .
D
oS 1
o
0 1

85 J.

0 1 2 3 4 5

Number of convolutional layers

 

 

Fig. 5 Number of convolutional layers in CNN vs. tenfold cross-validation accuggicy— Wits stanaard deviation
\ )

layers achieves the highest accuracy. However, the accurac \@ru.. CNN with four con-
volutional layers falls within one standard deviationymargin 0. fe accuracy of the CNN
with five convolutional layers. This is true for CNN§w. _Weree and two convolutional
layers as well. Thus, the CNN with two convolution] layers is considered optimum,
in this case. The architecture of the CNN sith two convolutional layers is explained in
Sect. 3. The CNNs with less than two g@evolut. nal layers are obtained by dropping the
last convolutional layers. For CNNs@with™ pre Xhan two convolutional layers, we have:
fp=3X3 X16, Ss=1, 13=32, [693 %3 x32)'s,=1, ng=64 and f-=3 x3 x 64, s-=1,
N-=128, where f,,, S,,» and n gence Wthessize, stride, and number of filters of the m-th

convolutional layer.

Facial features and cr minal tendency

Convolutional layers )CNWNare essentially feature generation layers. If a CNN achieves
a high accuracijit means that the generated features by convolutional layers are effective
in distinguishing bet. en classes. Therefore, to understand what facial features are used
by CN ylassiiv the images, we need to look at the facial features that are emphasized
or pi, pin preach convolutional layer. A convolutional layer usually has multiple fil-
térs. Eaci. "iter separately contributes in feature generation, though it is their cumulative
Kx, Wwledge that helps CNN to classify the images. Our CNN contains 2 convolutional
layers the first one has 8 filters and the second one has 16.

In Fig. 6, the output of one of the filters from the first convolutional layer and one of
the filters from the second convolutional layer are visualized. They highlight the facial
characteristics that are learned and used by CNN to distinguish between the two classes.
Additionally, Fig. 6 compares these facial features between a criminal and non-criminal
face shot. It is noteworthy that neither these facial features nor their differences are hard
coded into the machine. They are learned by the machine as most helpful in classify-
ing the two sets of images in the training dataset. Both convolutional layers detect and

underscore the shape of the face, eyebrows, top of the eye, pupils, nostrils, and lips.
Hashemi and Hall J Big Data (2020) 7:2 Page 12 of 16

 

   
   

      

 

Fig.6 Facial features detected by the first (a, c) and second (b, d) convolutional layers in CNN, for adsrimis

(a, b) vs. non-criminal (¢, d) face shot 5
\

 

 

  
 
    

 
   

  

io

WN

\\ \ \ \ NX XX AA AAALAC A

N

\
AUN

  
 
 

\\

Zs

—S

y

 

  
  
   

\

  
  

ANN
Yu

AA
\\
\

CRUE
YW QOL
AAR

\\

 

 

\ \ \ A \ XX AA \ AA XA

=

\ \ \ A XK KKK XA A AWA A AN

 

\\ \ \O\ XK KX XX AAA A

    

WY
Y
Y yj
U YY fp
/

SA

   
    
     
    

SSX

SS

 

 

SS

SS

 

 

SS

a

 

SSS

SS

SS

 

 

 

 

 

QR QAQG A QQ MMA AAG ALY

Fig.4»A no ein thenrst hidden layer is connected to only a small number of image pixels in CNN (left) while
itis COW ee teucccall image pixels in SNN (right)
S

 

 

J

Why CAN achieves higher accuracy than SNN?
T)yo architectural features of CNNs making them more convincing than SNNs for image
¢lassification are as follows:

a. Partial connectivity rather than full connectivity
A node in a CNN is connected only to a small number of nodes in the previous layer,
while the same node in an SNN is connected to all nodes in the previous layer. This
means that the number of synaptic weights that need to be calculated is mush fewer
in CNN than SNN. Assume we use a 3 x 3 convolution window in the CNN, shown
on the left side of Fig. 7. This means a node in the first hidden layer, for instance, is

only connected to 9 pixels in the image. The same node in the SNN, shown on the
Hashemi and Hall J Big Data (2020) 7:2 Page 13 of 16

right side of Fig. 7, is connected to all the 270 pixels of the image. In other words,
the number of synaptic weights is 30 times fewer in the CNN than SNN. Of course,
this number depends on the size of both the image and convolution window. If the
image is m x m and the convolution window is z x z, the number of synaptic weights
in CNN is 1 x m/z’ times fewer than SNN. We showed this only for the first hiddex
layer, but the same is true for all convolutional hidden layers. This has two advan-
tages. First, a much fewer unknown parameters (synaptic weights) can be“ }rned
more quickly (less computational complexity) and accurately by the machite, wi he
significantly reduced chance of overfitting. Second, deriving the value of zach node in
the next layer from only a small number of neighboring pixels, rath¢r th, \sthe eyitire
image, is based on the assumption that the relationship between.twe ‘stant pixels is
probably less significant than two close neighbors. This assung tion is ir Yred by the
visual cortex system in humans and other animals.
b. Shared weights

We mentioned that 1 x m synaptic weights need to“. le for one node in the
first hidden layer of SNN. With k nodes in the first hidd& Wgyer, a total of x mxk
synaptic weights must be calculated, because eats. hgairi the first hidden layer has

its own synaptic weights which are different thariythose of other nodes. In a CNN,
however, the number of synaptic weights that need to be learned remains z’, because
nodes in the first hidden layer do pnOt hav. Vifferent synaptic weights, but share the
same weights. Therefore, regardless 6. Yaw 1 iany nodes exist in the first hidden layer,
the number of synaptic weights\that nevd to be learned remains z*. Consequently,
the number of synaptic weigh Win CNN is 1 x m x k/z’ times fewer than SNN for

 

 
  

  

SS

\\A\A \ AWA XA NN XN AACA AC

   

SOOO

SS

y \ \ AA

\_\ AI AC AX AC ACY AYA XXX

SS

\\\\ XX AAA AAA XN AIMYAC NN

\

\\ \ AWW N A AAA AAA

A

SASSI SH

SS

    
  
    
      
  

 

S

AX \ S\N N\A AACN

Q
I\

KAA A \ AWS

WAAAY

\

   
 

\

\\

Synaptic weights are different for these two nodes
Synaptic weights are the same for these two nodes

\\ \ \ NAG RGA N\A ACN

|
A
7]
7]
HY)
ry
CZ
V7}
VW
V7
A

RQ NAY

 

  

\AA\ \ XX XXX A AN AANA A

 

\K\\ XXX NAN XXX XX YAWN
K\ \ AA AAA AN AN TNS
\\ \ AO XX NX XN AKA AA AAA
KAA A AAA AA AKA AA A AUN

CWE
\N

 

 

Fig. 8 Each node in the first hidden layer has its own synaptic weights in SNN (left) while nodes share the
same synaptic weights in CNN (right)
Ne

 

 
Hashemi and Hall J Big Data (2020) 7:2 Page 14 of 16

the first hidden layer. This is referred to as weight sharing property and is depicted
in Fig. 8. Despite this explanation concerned the first hidden layer, it is true for all
convolutional hidden layers. This property gives CNNs two advantages over SNN.
The first advantage is even less parameters for the machine to learn and the second is
enabling the CNN to look for certain objects in the image, regardless of where in th¢
image they are.

Conclusions and future directions
Classifying people in any manner requires care but predicting whethe/ a person Js
a criminal demands even more caution and scrutiny and must be lgoké pon with
suspicion. The danger of this technology lies in its imperfection, si. \ misclassify-
ing individuals can have grave repercussions. It would be too g@simistic \ €laim that
the 97% test accuracy, achieved by the CNN in this work, 14 eas (generalizable to
face shots from any other source. This is not only becatts. pf the ymall size of our
dataset, but also the fact that criminal and non-crimiftc }m*_ ome from different
sources. Thus, the conditions under which the images are~ ¥en are not exactly the
same, which raises the question, whether this disp ji,peripheral conditions was
captured by the deep classifier to unfairly distinguis’ between the two classes. In an
ideal dataset, all face shots, criminal and non-crimina, would be taken with the same
camera and under the same conditiong@i.e. i, Mination, angle, distance, background,
resolution, makeup, beard, hat, andgglas:

Facial emotions and age, maj@r sources \i bias in classifying facial images based
on criminal tendency, were gantt. ed in our work by eliminating non-neutral facial
images and images of eld@# & and ch dren. The bias due to background effects was

J

mitigated by cropping)tne fac \\area out of images. The gender bias was not only
eliminated by ignorghg female images, but also measured and shown to be of little
impact. Race, anotl )r sourc?: of bias, was not accounted for in this study because of
our small dataset anc wedifficulty and occasionally subjectivity of identifying the
race from low’ q- )jpfacial images. However, both categories contain images of all
races with rougmy similar proportions. Enlarging our dataset, measuring the impact

of ragal bi sand detecting other personality traits form our future research venues.

7 weviations
SNINW Wodard feedforward neural network; CNN: convolutional neural network; KNN: k nearest neighbors; SVM: support
vectors shines; ReLU: rectified linear unit; SGD: stochastic gradient descent.

Aknowledgements
Net applicable.

Authors’ contributions
Both authors have contributed in developing the idea, documenting it, conducting the experiments, writing and revis-
ing. Both authors read and approved the final manuscript.

Funding
Not applicable.

Availability of data and materials
Not applicable.

Competing interests
The authors declare that they have no competing interests.
Hashemi and Hall J Big Data (2020) 7:2 Page 15 of 16

Author details

' Department of Information Sciences and Technology, George Mason University, 4400 University Dr, Fairfax, VA 22030,
USA. * College of Information Science and Technology, University of Nebraska at Omaha, 1110S 67th St, Omaha, NE
68182, USA.

Received: 31 August 2019 Accepted: 30 December 2019
Published online: 07 January 2020

  
  
   
  
  
   
 

References

1. Zebrowitz LA, Montepare JM. Social psychological face perception: why appearance matters. Soc Pe
Compass. 2008;2(3):1497-517.

2. _Lombroso C. Criminal man. 5th ed. Durham: Duke Univ. Press; 2006.

Hargrave J. Poker face: the art of analyzing poker tells. Dubuque: Kendall Hunt Pub Co; 2010.

4. Wang Y, Kosinski M. Deep neural networks are more accurate than humans at detecting s
facial images. J Personal Soc Psychol. 2018;114(2):246-57.

5. Geng xX, Yin C, Zhou ZH. Facial age estimation by learning from label distributions. |
Intell. 2013;35(10):2401-12.

Ww

9. Oliver N, Pentland A, Bérard F. LAFTER: a real-time face and lips tra
Recogn. 2000;33(8):1369-82.

10. Zhang Y, JiQ. Active and dynamic information fusion for facial expre rstanding from image sequences.
IEEE Trans Pattern Anal Mach Intell. 2005;27(5):699-7 14.

11. Wu xX, Zhang X. Automated inference on criminality using face images./016. arXiv preprint. arXiv:1611.04135.

12. Taneja A, Arora A. Modeling user preferences using ‘works and tensor factorization model. Int J Inf Manag.
2019;45:132-48.

13. Krizhevsky A, Sutskever |, Hinton GE. Imagen

   
    
  
  
  
    
   
      
  
 
  
  
 
 
 
 
 
  

h deep convolutional neural networks. In: Advances

260-7.
15. Ouyang W, Wang X, Zeng X, Qi

convolutional neural networ detection. In: IEEE conference on computer vision and pattern recogni-
tion; 2015. p. 2403-12.

16. Sun, Liang D, Wang X ce recognition with very deep neural networks. 2015. arXiv preprint.
arXiv:1502.00873.

17. Cristani M, Vinciarelli A alin C, Périna A. Unveiling the multimedia unconscious: Implicit cognitive processes and
multimedia content an
18. Segalin C, Perit WypCristani M, Vinciarelli A. The pictures we like are our image: continuous mapping of favorite

d attributed personality traits. IEEE Trans Affect Comput. 2017;8(2):268-85.

19, §, Crigtani M. Social profiling through image understanding: personality inference using convolu-
s. Comput Vis Image Underst. 201 7;156(1):34-50.

20. Se 18. 2010. https://www.nist.gov/srd/nist-special-database-18. Accessed 12 Apr 2019.

21, enapid object detection using a boosted cascade of simple features. In: IEEE computer society

OA computer vision and pattern recognition; 2001. p. 1-9.

e Database. http://fei.edu.br/~cet/facedatabase.html. Accessed 02 June 2018.

rgia Tech face database. http://www.anefian.com/research/face_reco.htm. Accessed 02 June 2018.

Face Place. http://wiki.cnbc.cmu.edu/Face_Place. Accessed 02 June 2018.

. Face Detection Data Set and Benchmark Home. http://vis-www.cs.umass.edu/fddb/. Accessed 02 June 2018.

. Hinton GE, Srivastava N, Krizhevsky A, Sutskever |, Salakhutdinov RR. Improving neural networks by preventing co-

adaptation of feature detectors. 2012. arXiv preprint. arXiv:1207.0580.

. Nair V, Hinton GE. Rectified linear units improve restricted boltzmann machines. In: The 27th international confer-

ence on machine learning; 2010. p. 807-14.

. Kingma DP, Ba J. Adam: a method for stochastic optimization. 2014. arXiv preprint. arXiv:1412.6980.

. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. 2014. arXiv preprint.

arXiv:1409.1556.

31. Wang M, Liu X, Wu X. Visual classification by I1-hypergraph modeling. IEEE Trans Knowl Data Eng.
2015;27(9):2564-74.

32. Yu J, Tao D, Wang M. Adaptive hypergraph learning and its application in image classification. IEEE Trans Image
Process. 2012;21(7):3262-72.

33. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with
convolutions. In: IEEE conference on computer vision and pattern recognition; 2015. p. 1-9.
Hashemi and Hall J Big Data (2020) 7:2 Page 16 of 16

34.

Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In: European conference on computer
vision; 2014. p. 818-33.

 
  
 
  
  
      
 
 
  
 

35. Hashemi M, Hall M. Detecting and classifying online dark visual propaganda. Image Vis Comput. 2019;89(1):95-105.

36. Hashemi M. Enlarging smaller images before inputting into convolutional neural network: zero-padding vs. interpo-
lation. J Big Data. 2019;6(1):98.

37. Farfade SS, Saberian MJ, Li LJ. Multi-view face detection using deep convolutional neural networks. In: 5th Interna-
tional conference on multimedia retrieval; 2015. p. 643-50.

38. Hashemi M. Web page classification: a survey of perspectives, gaps, and future directions. Multimedia Tools Appl.
2020. https://doi.org/10.1007/s1 1042-01 9-08373-8.

39. Zeiler MD, Fergus R. Stochastic pooling for regularization of deep convolutional neural networks. 2013. arXiyglore-
print. arXiv:1301.3557.

40. TensorFlow. TensorFlow Tutorials. 2019. httos://www.tensorflow.org/tutorials. Accessed 01 Jan 2019.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and insti

    
    
  

ional

ON

Submit your manuscript to a SpringerOpen”®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

 

Submit your next manuscript at > springeropen.com

 

 

 
