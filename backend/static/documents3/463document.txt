Nassar et al. J Big Data (2020) 7:34 . :
https://doi.org/10.1186/s40537-020-00309-6 oO Journal of Big Data

RESEARCH Oy oT-Ta waa -55 4

- ; ; ®
Multi-criteria collaborative filtering a

recommender by fusing deep neural network
and matrix factorization

Nour Nassar ©, Assef Jafar and Yasser Rahhal

 

*Correspondence:
nournassar123@gmail.com; Abstract

nournassar@hiast.edu.sy Recommender systems have been an efficient strategy to deal with information

Higher Institute for Applied ; us ;

Sciences and Technology, overload by producing personalized predictions. Recommendation systems based on

Damascus, Syria deep learning have accomplished magnificent results, but most of these systems are
traditional recommender systems that use a single rating. In this work, we introduce a
multi-criteria collaborative filtering recommender by combining deep neural network
and matrix factorization. Our model consists of two parts: the first part uses a fused
model of deep neural network and matrix factorization to predict the criteria ratings
and the second one employs a deep neural network to predict the overall rating. The
experimental results on two datasets, including a real-world dataset, show that the pro-
posed model outperformed several state-of-the-art methods across different datasets
and performance evaluation metrics.

Keywords: Collaborative filtering, Deep learning, Deep neural network, Matrix
factorization, Multi-criteria, Recommender system

 

Introduction

In the era of big data, while the World Wide Web keeps growing exponentially, the size
and complexity of many websites (Google, YouTube, Netflix, Amazon, and others) grow
along with it, making it increasingly difficult and time-consuming for the users of these
websites to find the item (movie, music, restaurant, book, and product in general) they
are looking for. Recommender systems (RSs) provide personalized suggestion for items
that the user might like [1]. Exploiting the information from users’ ratings can be useful
to solve one of the problems recommender systems suffer from, predicting users’ prefer-
ences about an item using a single rating. This is a clear limitation, since the user who
makes a choice might take into consideration more than one aspect of the item. For an
example, in a movie recommender system, some users may like a movie based on its
plot, direction, or conflict, while others may like the same movie but for its acting, char-
acters, or any other attribute of that movie. So, Multi-Criteria Recommender System
(MCRS) that utilizes multi-criteria ratings to evaluate different attributes of the item can
improve the accuracy of the recommendations [2]. RSs use many techniques, Collabora-

tive Filtering (CF) is the most commonly used, and it makes recommendations based

. © The Author(s) 2020, corrected publication 2020. This article is licensed under a Creative Commons Attribution 4.0 International License,
GO) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to
— the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or
other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to
the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regula-
tion or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit

http://creativecommons.org/licenses/by/4.0/.
Nassar et al. J Big Data (2020) 7:34 Page 2 of 13

only on interactions between users and items such as ratings [3]. Matrix Factorization
(MF), is the most popular CF techniques, which maps users and items into a joint latent
space, using a vector of latent features to represent a user or an item [4]. Then the inter-
action of a user on an item is obtained from the inner product of their latent vectors.
Although MF is effective, the simple choice of the interaction function is insufficient to
model the complex relation between the users and items.

Deep Learning (DL) has achieved immense results in many research fields, such as
speech recognition, natural language processing, computer vision, and recently in rec-
ommender systems, where Deep Neural Networks (DNNs) have proved their capability
of modeling nonlinear users and items relationships and approximating any continuous
function [5].

Therefore, it is convenient to fuse the DNN with MF to formulate a more gen-
eral model that makes use of both the nonlinearity of DNN and the linearity of MF to
enhance recommendation accuracy.

The objective of this paper is just to propose a multi-criteria collaborative filtering rec-
ommender by fusing DNN and MF, to improve collaborative filtering performance. Our
model consists of two parts: in the first part, we get the users and items’ features and
feed them as an input to a fused model of a DNN and MF to predict the criteria ratings,
and we use a deep neural network to predict the overall rating in the second part.

By doing experiments on two datasets including a real-world dataset, it can be
observed that our proposed model achieves significant improvement compared to the
other state-of-the-art methods.

The main contributions of this work are as follows:

1. We present a multi-criteria collaborative filtering recommender by fusing DNN and
MF, to combine the non-linearity of DNN and linearity of MF in a multi-criteria RS
for modeling user-item latent structures.

2. We do comprehensive experiments on two datasets including a real-world dataset to
show the efficiency of our model and the importance of using multi-criteria and deep
learning in collaborative filtering.

The rest of this paper is organized as the following: In Sect. 2, we survey the related
work. We will give a detailed overview of the system in Sect. 3. Section 4 presents the
experimental evaluations and discussion and in Sect. 5, we provide a brief conclusion

and potential future work.

Related work

Multi-criteria recommendation techniques can be divided into two general classes:
memory-based and model-based methods. In memory-based methods, the similarity
can be computed in two ways: the first approach calculates the similarity on each criteria
rating separately using the traditional way and then aggregates the values into a single
similarity using an aggregation method such as average [6], worst-case [6], and weighted
sum of individual similarities [7]. The second approach uses the multidimensional dis-
tance metrics (Euclidean, Manhattan, and Chebyshev distance metrics) to calculate the
distance between multi-criteria ratings [6]. Model-based methods use the user-item
Nassar et al. J Big Data (2020) 7:34 Page 3 of 13

ratings to learn a predictive model and later use this model to predict unknown ratings,
like Probabilistic Modeling [8, 9], Multilinear Singular Value Decomposition (MSVD)
[10], Support Vector Regression (SVR) [11], and aggregation-function-based [6], this
approach assumes that there is a relation between the item’s overall rating and the multi-
criteria ratings and it is not independent of other ratings.

Many efforts have been made to improve MF, Koren [12] merged MF and neighbor-
hood models. Wang et al. [13] combined MF with topic models of item content. Rendle
[14] introduced Factorization Machines that combine Support Vector Machines with
factorization models. He et al. [15] proposed Neural Matrix Factorization (NeuMF)
model that changed the linearity nature of MF by combining it with Multi-Layer Percep-
tron (MLP).

There are very few researches on applying deep learning to Collaborative Filtering
model. Salakhutdinov et al. [16] proposed restricted boltzmann machines model to learn
the item ratings correlation, thereafter Georgiev et al. [17] expanded the former work by
adding both user—user and item—item correlations. Ouyang et al. [18] used autoencoder
in autoencoder-based collaborative filtering to model users ratings on items. He et al.
[15] introduced neural collaborative filtering model that uses MLP to learn the inter-
action function. Nassar et al. [19] presented deep multi-criteria collaborative filtering
(DMCCF) model which is the only attempt in applying deep learning and multi-criteria
to collaborative filtering. The model follows the aggregation-function-based approach,
where they used a deep neural network to predict the criteria ratings and another DNN
to learn the relationship between the criteria ratings and the overall rating in order to

predict the overall rating.

Method
The proposed model is based on the model that Nassar et al. [19] presented. It contains
three steps:

a. Predict criteria ratings r1,7r2,..., 7, using a DNN.

b. Learn aggregation function f, which represents the relationship between the criteria
ratings and the overall rating using a DNN.

c. Predict overall ratings using the predicted criteria ratings and the aggregation func-
tion f.

In our model, we used a fused model of DNN and MF to predict the criteria ratings
r1,12,...,1x in the first step, while in the second step, we kept using a DNN to learn
aggregation function f, as illustrated in Fig. 1.

Criteria ratings model

This model is used to predict the criteria ratings for a user on an item. In this model, we
will fuse MF and DNN as He et al. [15] proposed in their NeuMF framework. MF that
applies a linear kernel to model the latent feature interactions, and DNN that uses a non-

linear kernel to learn the interaction function from data.

1. MF
Nassar et al. J Big Data

(2020) 7:34

 

 

X

 

Training

Train dataset

   
   
 
 
   

(User ID, Item ID)

 
  
    

Train Deep Neural Network to
learn overall rating
ro = f(11, 12, ++ Tk)

  
    

Train DNN and MF to learn
criteria ratings 71,12, ..., rk

  

 

 

 

Recommendation

CU (User ID, Item ID) Using Criteria Ratings

Using Overall Rating DNN Provide recommendation

/
Model to to compute r/, based on rp

Test dataset compute 7, 14, ..., 7

 

Fig. 1 The framework of the proposed model, including two steps: the Training and the Recommendation
step

We can use the item ID or user ID as a feature since it is unique, but the ID is a cat-
egorical feature.

Therefore, we converted the IDs into embedding vectors initialized with random val-

ues that adjusted to minimize the loss function during the model training.

In the input layer, the input vector ~ is given by:
X=Vy, OV; (1)

where v,, and v; are user and item embedding vectors and © is the element-wise prod-
uct of vectors. The formula of the output layer is as follows:

Yui = Gout (wl vy © vi)) (2)

where do,; and w! are the activation function and weights of the output layer.

. DNN

The input vector x is given by:

x = Concatenate(vy, vj) = ve (3)

where v,, and v; are user and item embedding vectors.
Then followed by a number of dense Rectified Linear Units (ReLU) layers, we choose
ReLU activation function because it is the most efficient [20]. The formulation of

which is given as follows:
ReLU (z) = max(z, 0) (4)
The output of a hidden layer / is formulated as:

hy = ReLU (Why, + by) (5)

 

Page 4 of 13
Nassar et al. J Big Data (2020) 7:34 Page 5 of 13

 

 

Train dataset

User ID Item ID User ID Item ID
(71,12, -++5 Tk)

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

User Embedding Layer Item Embedding Layer User Embedding Layer Item Embedding Layer
Element-wise Product Concatenate
1 OOOO OOOO

OQ

OOO=O0O0 g

: By

| 2

2 000000 Seer

@ | OO=OO  g

2 OOO ™= OOO 0 2
DD
2

> ro

©
a
<=
°o
Qa
Oo

 

Concatenate

 

 

 

 

 

 

 

 

Fig. 2 The architecture of the two models: Criteria Ratings Model and Overall Rating DNN
\

where Wj, and b; are the weight matrix and bias vector for the layer / and hy = x.

In the output layer, we have:
Yui = ReLU(Wyhy_-1 + br) (6)

where L is the number of layers.

3. Fusion of MF and DNN
For more flexibility to the fused model, we let DNN and MF learn separate embed-
dings, as shown in Fig. 2, and then we combine them by concatenating the last hid-
den layer. The outputs of the DNN and MF denoted as 0”, o™ respectively, are given

as follows:
oP = ReLu( Wi (Reus (Ww-1 ( | Ret Ws "5 | + bn) .) + b1-.) + b)
| (7)
“=v oy" (8)

where v? and v! are user embedding vectors for DNN and MF part, and v? and vj"
are item embedding vectors.
In the model output layer, we predict the user criteria ratings r1,r2,...,7% using

Eq. (9) and (10):

yuu = ReLu( Ww] & | +8) (9)

1, TQ). 265 rele = Vui (10)
Nassar et al. J Big Data (2020) 7:34 Page 6 of 13

Overall rating deep neural network
The overall rating DNN is used to learn the aggregation function f, which represents the
relationship between the overall rating ro and the criteria ratings rj, r2,...,7r,, in order to

predict the overall rating:

ro =f (11, 12)---5Tk) (11)
In the input layer, the input vector is the criteria ratings r1,72,...,7% for user u and
item i, as shown in Fig. 2. We normalize the continuous features 71, r2,...,7% because the

DNNs are sensitive to the inputs scaling and distribution [21]. The normalization of a

sample r; is calculated as:

Vi — Mm;

 

Zi= (12)

Sj
with m; the mean of the training samples for rating r; and s; the standard deviation of the

training samples for rating r;. The input vector becomes:
_ T
x = [Z1,2Z2,...,Z| (13)

Then followed by a number of hidden layers, the output of a hidden layer is given again

by Eq. (5).
In the output layer, we predict the overall rating ro in Eq. (6), where:

YO = Yui
We used Adam optimizer [22] and MAE loss function in both parts.

Recommendation
After the finishing of the model training, where each part was trained individually with-
out knowing each other. We can use the model to predict the user’s overall rating on the
new items.

The recommendation process happens as shown in Fig. 1, for each user u and item i

pair we:

a) Get the user ID and the item ID, and use them as an input for the Criteria Ratings
model, to compute the criteria ratings r},15,...,1,.

b) Normalize the previously computed criteria ratings r},75,...,7;, and use them as an
input for the Overall Rating DNN, to compute the overall rating ro.

c) Recommend the user the items as in traditional recommender systems using the

overall rating 6.
Nassar et al. J Big Data (2020) 7:34

Table 1 TripAdvisor dataset statistics

 

 

 

 

 

 

 

 

Field description Value
Number of users 72,119
Number of hotels 1850
Number of ratings 81,085
Data sparsity 99.94%
Table 2 TripAdvisor dataset ratings distribution

Ratings 1 2 3 4 5
Value 4268 5853 10,374 24,696 35,894
Rooms 4002 5517 10,493 25,019 36,054
Location 1464 314] 8503 22,177 45,800
Cleanliness 2591 3585 8232 19,951 46,726
Check in/front desk 3775 4468 11,083 19,201 42,558
Service 4086 4462 10,036 20,114 42,387
Business service 4525 5849 19,624 21,661 29,426
Overall 4246 6052 8078 24,180 38,529
The number of examples of the values for each criteria rating and for the overall rating in the dataset

Table 3 Movies dataset statistics

Field description Value
Number of users 6078
Number of movies 976
Number of ratings 62,156
Data sparsity 98.95%

 

The number of examples of the values for each criteria rating and for the overall rating in the dataset

Results and discussion
Dataset

We evaluated our model on two multi-criteria rating datasets, a real-world TripAdvisor’

dataset and a Movies” dataset.

1. TripAdvisor dataset

Multi-criteria rating dataset for hotels. It includes an overall rating and seven criteria
ratings Value, Rooms, Location, Cleanliness, Check in/front desk, Service, and Busi-
ness service, the rating range between 1 and 5. Table 1 demonstrates the statistics of
the dataset and Table 2 the distribution of values for the different criteria ratings and

the overall rating.
2. Movies dataset

 

' The dataset is crawled from the TripAdvisor website: https://www.tripadvisor.com/.

2 https://github.com/an888ha/multi_criteria_recommender_system/blob/master/data_movies.txt.

Page 7 of 13
Nassar et al. J Big Data (2020) 7:34 Page 8 of 13

Table 4 Movies dataset ratings distribution

 

Ratings 1 2 3 4 5 6 7 8 9 10 11 12 13

 

Criteria1 3325 1046 1581 1151 1754 3018 2297 3462 6544 6582 5711 12,187 13,498
Criteria2 2567 796 1133 861 1375 2729 2038 3475 7360 6743 6163 12,330 14,586
Criteria3 3300 1067 1619 1113 1672 3073 2181 3477 7413 6328 5781 12,345 12,787
Criteria4 2533 655 1063 758 1163 2654 1802 3135 7156 6365 5794 12,004 17,074
Overall 3395, 1340 1522 1329) 2051 2428 «2489 «3251 5586 7006 6702 12,153 12,904

 

Multi-criteria rating dataset for movies, available on GitHub. It contains four criteria
ratings and an overall rating, the rating range between 1 and 13. Tables 3 and 4 dem-
onstrate the dataset statistics and the distribution of the different criteria ratings and
the overall rating.

Evaluation

To evaluate the performance of our model we used the same metrics used in [19].

a. Mean Absolute Error (MAE) [23]

1 ~
MAE = Mo Tui — Tui (15)
u,i

where r,,; the true rating of user u for item i, 7,; the predicted rating, and M is the
size of the test set.
b. F-measure (F, and F>) [23]

r, 2PR 16

‘PER (6)
5PR

Fy = ————

> 4P +R (17)
where P the precision and R the recall.
c. Fraction of Concordant Pairs (FCP) [24]
ne = 1 GA) Pui) Fyjandr yi > ruj}| (18)

where n¢ the number of concordant pairs for user u, and we calculate 1 the number

of discordant pairs for user u in a similar way.

ne = > ME (19)

na = > M4 (20)
Nassar et al. J Big Data (2020) 7:34 Page 9 of 13

 

 

 

 

 

 

 

 

 

 

 

( >)
—®- TripAdvisor 2.032 + - Movies
0.78 9 2.030
2.028
0.77
2.026 +
w Ww
$ S
0.76 4 2.024 +
2.022
0.75 4
2.020
0 10 20 30 40 50 60 70 80 90 1 0 0 11 0 1 20 1 30 0 10 20 30 40 50 6 0 70 80 90 100 1 10 1 2 0 1 30
Embedding Size Embedding Size
Fig. 3 The impact of embedding vector size in MF model for both datasets
Ne

 

Nc
FCP = ———
Nc + Ng (21)
d. Mean Average Precision (MAP) [23] is the average precision for all users.
e. Mean Reciprocal Rank (MRR) [25] is the average of the first relevant item rank for all

users.

Settings
We used Keras® with TensorFlow* as a backend to implement our model.

1. Criteria Ratings Model Settings
We conducted several experiments to find the optimal parameters for DNN and MF.

« For DNN, we randomly initialized the DNN parameters using a normal distribution
with mean of 0 and standard deviation of 0.05. We used Adam optimizer, with 0.001
learning rate and parameters values as provided in [22].

For TripAdvisor dataset, we used batch size of 512 and set epochs to 2. We
set each of user and item embedding vector size to 64, and we selected the
[128 > 64 32> 16— 8] hidden layers. For Movies dataset, we used batch size of
64 and set epochs to 4. We set each of user and item embedding vector size to 32,
and we selected the [64 > 32 > 16 — 8] hidden layers.

« For MF, we tried to find the optimal embedding vector size. As shown in Fig. 3, for
TripAdvisor dataset, we set user and item embedding vector size to 16, and for Mov-
ies dataset, we set them to 8.

¢ In the model output layer, for TripAdvisor dataset, there are 7 neurons, equal to the
number of the criteria ratings, and 4 neurons for Movies dataset.

¢ Overall Rating DNN Settings

 

> https://github.com/keras-team/keras.

* https://github.com/tensorflow/tensorflow.
Nassar et al. J Big Data (2020) 7:34 Page 10 of 13

Table 5 Single DNN settings

 

 

Dataset Parameter Value
TripAdvisor User and item embedding vector size 256
Hidden layers [512 > 256 128 64 32>
16— 8]
Output layer 1
Batch size 512
Movies dataset User and item embedding vector size 128
Hidden layers [256 > 128 > 64> 32> 16> 8]
Output layer 1
Batch size 64

 

We initialized randomly the DNN parameters like the previous DNN, using a normal
distribution with mean of 0 and standard deviation of 0.05. We also used Adam opti-
mizer with 0.001 learning rate and the same parameters values. For TripAdvisor dataset,
we Set the epochs to 50, and for Movies dataset, we set the epochs to 100. While for both
datasets, we set the batch size to 512. We used [64 — 32 — 16 — 8] hidden layers. Finally,
in the output layer, there is 1 neuron, for the overall rating.

Results
We used fivefold cross-validation method, to split the data randomly into 20% test set
and 80% training set including a 1% validation set. We repeated this process 5 times and
calculated the mean value of each metric. We compared the performance of our model
to the DMCCF model [19], then to a single DNN that predicts the overall rating directly
where the finest results of this DNN were acquired from settings shown in Table 5. In
addition, we compared our model to a number of famous single rating recommendation
methods such as SVD [26], SVD+-+ [12], Baseline Estimates [27], and SlopeOne [28].
This comparison was done on the overall rating. The results are illustrated in Table 6.
We can see that our model achieves the best performance on both datasets, significantly
outperforming each of DMCCF model, single DNN, and the other state-of-the-art meth-
ods on all the evaluation metrics. This indicates the high expressiveness of our model by
fusing the non-linear DNN and linear MF models to capture the methods. F1 and F2 of
our model are better than the other methods. Our model surpasses the other models in
FCP and it exceeds user-item interactions. According to the results, in MAE, our model
excelled the other compared methods at MAP. In MRR, our model is the best.

Conclusion and future work

In this paper, we proposed multi-criteria collaborative filtering recommender by fusing
DNN and MF. The model consists of two parts: in the first part, we get the users and
items’ features and feed them as an input to a fused model of a DNN and MF to pre-
dict the criteria ratings, and in the second part, we use a DNN to predict the overall
rating. Then we demonstrated the efficiency of our proposed model, where the experi-
mental results showed that our model significantly outperformed the other methods on
both datasets for all the evaluation metrics, which proved that the application of deep
Page 11 of 13

134

7.

(2020)

Nassar et al. J Big Data

J3}}9q BY} JAMO] AY} JPW al!yM Ja}yaq ayy JayHiy ayy YYW pue ‘dyW ddd “4 “4 ‘sWYWIOHje 49430 ay} JsuIeHe JapoW JNO JO} s}jNsai UOeNyeAz

 

 

6000 F 6020 8/000 19020 Sv L00F72769'0 75000 LZZ9°0 8r00'0F 58090 8r00'0F Z0S90 16000 15990 YY

0600°0F 8Sb70 60L00FSPET0 EvL00F9ELTO /7000¥#Z1910 /900'0F 68020 SE000F rrLZ0 9500001610 dvW

67200068790 £8000 8S7Z9'0 55000 #97290 /700°0F £E790 8600°0F 1$Z79°0 6Z000#S86S°0 LOLOO#7Z090 dd

95000 F Crr8'0 6h00'0 F E8E8'0 /870'0 9780 S000 66920 S000 SZEL'0 EZOOOFESSL'0 7900'0F 80920 cy

9€00'0 F £9580 Ly00'0 Fv Lr8'0 20L00F Pres L€00'0F ZO18'0 9700'0 £09820 01000# $8620 6000 ¥ 1908°0 ly

LSLOOF 88107 6LLOOF PLOT LyE00FEL907 69000 FEEELZ ZLLOOFS6LI'7 0S00'0FP6S1'Z 89L00FLOLZ FY Jaserep saiaoyy

9€000 F £6920 6£000F 79920 76000 PrZZ'0 87000 F rlEL0 /€00°0FZ0SZ'0 8LOO0FELSL0 S000 F7LZL0 YY

/v000F 9€8r0 ZLO00¥ £>9r'0 16000 ¥ L8rr'0 £7000 F L88€0 9700'0F 9000 6h00'0F E8rE0 /S000¥ SE6E0 dvW

8r000F 79150 09L00#971S0 ZOLOOF S670 LL L00#S7Z0S0 1990'0F 96050 9500026050 0LZ00# 2050 dd

r0700F61E60 y1000# 56260 /S00°0#92160 6700098620 | LO0'O# 8rZ6'0 /S000¥ 16620 LE000# 1560 cy

L€00'0 F L188 £4000 F6928'0 7000 F7ZS8°0 07Z00'0F 16€8°0 8100°0F9998'0 S000 06E8'0 S000 OLES'0 ly

89000 F PErZ0 0S00°0# S50 €5100FS582'0 61000 6080 €500°0F 09160 |L00'0F 27780 €000* LZ08'0 WW Josiapydi
ino [61] 4D9wa NN aj6uls sa}euysa auljaseg augado|s ++dAs GAS 22 yasejeq

 

s}[Nsai uOHeNjeAZ 9 aqQeL
Nassar et al. J Big Data (2020) 7:34 Page 12 of 13

learning and multi-criteria in collaborative filtering is a successful attempt and it can be
enhanced using different deep learning techniques or by building more complex models.

In future work, we will study the use of different deep learning techniques, such as
Autoencoder, Convolutional Neural Network (CNN), and Recurrent Neural Network
(RNN) in recommendation systems and attempt to further improve the performance of
our model, we will also try other feature representation methods precisely to solve the
cold start problem by using user and item content features.

Abbreviations

CF: Collaborative filtering; DL: Deep learning; DMCCF: Deep multi-criteria collaborative filtering; DNN: Deep neural net-
work; FCP: Fraction of Concordant Pairs; MAE: Mean Absolute Error; MAP: Mean Average Precision; MCRS: Multi-Criteria
Recommender System; MF: Matrix factorization; MLP: Multi-layer perceptron; MRR: Mean Reciprocal Rank; NeuMF: Neural
Matrix Factorization; ReLU: Rectified Linear Unit; RS: Recommender system.

Acknowledgements
The authors thank N. Mohanna for careful reading of the manuscript. Special thanks are due to S. Tarbouche for advice
and support.

Authors’ contributions
All the corresponding authors contributed equally to the conduct of the present study. All authors read and approved
the final manuscript.

Funding
Not applicable.

Availability of data and materials
The datasets used during the current study are available in the Datasets repository, https://github.com/nournassar/Datas
ets.

Competing interests
The authors declare that they have no competing interests.

Received: 18 October 2019 Accepted: 18 May 2020
Published online: 24 May 2020

References

1. Zhao J,Geng X, Zhou J, Sun Q, Xiao Y, Zhang Z, Fu Z. Attribute mapping and autoencoder neural network based
matrix factorization initialization for recommendation systems. Knowl Based Syst. 2019;166:1 32-9.

2. Turk AM, Bilge A. Robustness analysis of multi-criteria collaborative filtering algorithms against shilling attacks.
Expert Syst Appl. 2019;115:386-402.

3. Zhang S, Yao L, Sun A, Tay Y. Deep learning based recommender system: a survey and new perspectives. ACM Com-
put Surv. 2019;52:5.

4. FuM,QuH,YiZ,LUuL,LiuY.A novel deep learning-based collaborative filtering model for recommendation system.
IEEE Trans. Cybern. 2018. https://doi.org/10.1109/tcyb.2018.2795041,

5. Hornik K, Stinchcombe M, White H. Multilayer feedforward networks are universal approximators. Neural Networks.
1989;2:359-66.

6. Adomavicius G, Kwon Y. New recommendation techniques for multicriteria rating systems. IEEE Intell Syst.
2007;22:48-55. https://doi.org/10.1109/MIS.2007.58.

7. Tang TY, Mccalla G. The pedagogical value of papers: a collaborative-filtering based paper recommender. J. Digit. Inf.
2009. https://doi.org/10.1109/wkdd.2009.132.

8. L. Si, R. Jin, Flexible Mixture Model for Collaborative Filtering. In: Proc. Twent. Int. Conf. Int. Conf. Mach. Learn., AAAI
Press, 2003: pp. 704-711. http://dl.acm.org/citation.cfm?id=3041838.3041927.

9. Sahoo N, Krishnan R, Duncan G, Callan J. The halo effect in multicomponent ratings and its implications for recom-
mender systems: the case of yahoo! movies. Inf. Syst. Res. 2012;23:231-46. https://doi.org/10.1287/isre.1 100.0336.

10. Q.Li, C.Wang, G. Geng, Improving personalized services in mobile commerce by a novel multicriteria rating
approach. In: Proceeding 17th Int. Conf. World Wide Web - WWW/’'08. (2008) 1235. https://doi.org/10.1145/13674
97.1367743.

11. Drucker H, Burges CJC, Kaufman L, Smola A, Vapnik VN. Support vector regression machines. Adv Neural Inf Process
Syst. 1997;9(1):155-61.

12. Koren Y. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In: Proc. 14th ACM SIG-
KDD Int. Conf. Knowl. Discov. Data Min., ACM, New York, NY, USA, 2008: pp. 426-34. https://doi.org/10.1145/14018
90.1401944.

13. Wang H, Wang N, Yeung DY. Collaborative deep learning for recommender systems. In: Proc. 21th ACM SIGKDD Int.
Conf. Knowl. Discov. Data Min., 2015. pp. 1235-44.

14. Rendle S. Factorization machines. In: Data Min. (ICDM), 2010 IEEE 10th Int. Conf, 2010: pp. 995-1000.
Nassar et al. J Big Data (2020) 7:34 Page 13 of 13

15. He X, Liao L, Zhang H, Nie L, Hu X, Chua T-S. Neural Collab Filter. 2017. https://doi.org/10.1 145/303891 2.3052569,

16. Salakhutdinov R, Mnih A, Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. 24th Int. Conf.
Mach. Learn., ACM, New York, NY, USA, 2007: pp. 791-98. https://doi.org/10.1145/1273496.1 273596.

17. Georgiev K, Nakov P. A non-llD framework for collaborative filtering with restricted boltzmann machines. In: S. Das-
gupta, D. McAllester (Eds.), Proc. 30th Int. Conf. Mach. Learn., PMLR, Atlanta, Georgia, USA, 2013. pp. 1148-56. http://
proceedings.mlr.press/v28/georgiev1 3.htm.

18. Ouyang Y, Liu W, Rong W, Xiong Z. Autoencoder-based collaborative filtering. ln: Loo CK, Yap KS, Wong KW, Beng AT,
Huang K, editors. Neural Inf. Cham: Process. Springer International Publishing; 2014. p. 284-91.

19. Nassar N, Jafar A, Rahhal Y. A novel deep multi-criteria collaborative filtering model for recommendation system.
Knowledge-Based Syst. 2019. https://doi.org/10.1016/j.knosys.2019.06.019.

20. Fuentes O, Parra J, Anthony EY, Kreinovich V. Why rectified linear neurons are efficient: symmetry-based, complexity-
based, and fuzzy-based explanations. Dep. Tech. Reports (CS). (2017). https://digitalcommons.utep.edu/cs_techr
ep/1171.

21. loffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift.
arXiv. 2015. https://doi.org/10.1007/s13398-014-0173-7.2.

22. D.P.Kingma, J. Ba. Adam: A Method for Stochastic Optimization. In /CLR: International Conference on Learning Rep-
resentations. 2014...

23. Herlocker JL, Konstan JA, Terveen LG, Riedl JT. Evaluating collaborative filtering recommender systems. ACM Trans
Inf Syst. 2004;22:5-53.

24. Koreny, Sill J. Collaborative filtering on ordinal user feedback. In: JCAI Int. Jt. Conf. Artif. Intell., 2013: pp. 3022-26.
https://doi.org/10.1145/1985793.1986027.

25. Abel F, Araujo S, Gao Q, Houben GJ. Analyzing cross-system user modeling on the social web. In: Int. Conf. Web Eng,.,
2011: pp. 28-43.

26. Koren Y, Bell R, Volinsky C. Matrix factorization techniques for recommender systems. 2009, p. 42-49. https://doi.
org/10.1109/mc.2009.263.

27. Koren Y. Factor in the neighbors. ACM Trans Knowl Discov Data. 2010;4:1—24. https://doi.org/10.1145/1644873.16448
74.

28. Lemire D, Maclachlan A. Slope one predictors for online rating-based collaborative filtering. 2007. doi:
10.1137/1.978161197275743

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen”®

journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

 

Submit your next manuscript at > springeropen.com

 

 

 
