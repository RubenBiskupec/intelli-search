Pho and Manizaris J Big Data (2020) 7:91 ; :
https://doi.org/10.1186/s40537-020-00366-x oO Jou ral of Big Data

RESEARCH Oy oT-Ta waa -55 4

Regularized Simple Graph Convolution (SGC) oe

for improved interpretability of large datasets

Phuong Pho and Alexander V. Mantzaris ®

 

*Correspondence:
alexander.mantzaris@ucf.edu Abstract
Department of Statistics Classification of data points which correspond to complex entities such as people

and Data Science, University ; articles j ; h task. Notabl licati da-
of Central Florida (UCP), OF JOUFMNA! alliICl€S IS d ONGOING resealCn TaSk. NOTADIE appicatlons are recommMendad

4000 Central Florida Blvd, tion systems for customer behaviors based upon their features or past purchases and

Orlando 32816, USA in academia labeling relevant research papers in order to reduce the reading time
required. The features that can be extracted are many and result in large datasets which
are a challenge to process with complex machine learning methodologies. There is also
an issue on how this is presented and how to interpret the parameterizations beyond
the classification accuracies. This work shows how the network information contained
in an adjacency matrix allows improved classification of entities through their associa-
tions and how the framework of the SGC provide an expressive and fast approach. The
proposed regularized SGC incorporates shrinkage upon three different aspects of the
projection vectors to reduce the number of parameters, the size of the parameters and
the directions between the vectors to produce more meaningful interpretations.

Keywords: Graph neural networks, Big data, Dimensionality reduction, Simple Graph
Convolution, Interpretability, Graph Convolutional Network

 

Introduction

Research into network based approaches in machine learning has been ongoing with
the growing sizes of networks produced by users on commercial online social network-
ing platforms. The ways users engage with each other directly on online social network
platforms from independent activities have given rise to networks with billions of users
(nodes) [1]. The information provided by the users can be used for investigating different
phenomena. The interlinking information that produces edges which then can form a
network, or series of networks, has been studied in the growing field of network science
[2, 3]. Network science offers many tools and approaches to learn and to draw insight
about the community structures [4], the centrality distribution [5] of the nodes (users),
and provides models for how the networks can grow from an initial set of nodes [6, 7].
There are many applications for these insights such as in targeted advertising [8] where
brands seek to have highly central nodes spread advertising content, and another appli-
cation is in the effort to understand the ‘landscape’ of political polarization between

communities in Twitter [9].

. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
GO) Springer O pen adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
— the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material
in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco

mmons.org/licenses/by/4.0/.
Pho and Manizaris J Big Data (2020) 7:91 Page 2 of 17

As well as the node interlinking information which can produce a network (graph),
there are the attributes of the users which can provide important useful information
in improving upon predictive analytics. A notable technique used by online shop-
ping platforms is collaborative filtering [10] which works as a recommendation system
for improving the shopping experience of customers by optimizing the product view
to those items predicted to be of interest. These new link predictions are based upon
past purchases and the historical records of other customers. Here ‘clusters’ are formed
between groups of items in a multidimensional space of these choices [11, 12]. This relies
on the observation that the items are not selected independently of previous purchases
and that there is information gained from utilizing the data collected [13]. Another area
where predictive analytics has used information in order to make predictions is in stu-
dent performance [14]. Logistic regression has been used in situations where a model
is required in a decision framework in order to predict achievements [15]. A key differ-
ence between the network based approach and these approaches is that the information
contained in the network and how the links influence the node of concern are excluded
from the model which can play a key role in economic behaviors predicted [16]. In the
effort to fuse these sources together for models to incorporate [17] discusses how user
identity linkage across online social networks can be accomplished.

Online social networks have been at the forefront of the interest in networks and
approaches which use the interlinking information due to their size and the effect they
have on human behaviors [18]. The network topologies of the virtual networks can find
applications but they also carry over into the physical world. The techniques have been
used in other domains such as examining the centrality in streets of urban spaces [19]
which also can be seen as a continuation of the original network/topological graph theo-
retical formulation of Euler’s investigation of the ‘Seven Bridges of Konigsberg’ problem
[20]. As these urban networks fit within networks of urban spaces themselves, multi-
layer networks [21] are produced that span over the globe allowing an analysis of even
global migration patterns [22]. In a similar fashion, it is also possible to consider aca-
demic literature as a network with similar properties governing its construction, such
as homophily [23]. The nodes in such academic networks are publications and the links
are the citations between the articles that provide information of association. There
is active research in this field [24] which notes the key motivation is that researchers
can spend considerable amounts of time searching for the relevant research in order to
not allocate time on topics already explored with similar approaches. Being able to find
associative research is of importance since it is possible for research to be directed in
areas already investigated and waste time as well as materials in research such as studies
requiring expensive lab equipment. Navigating the network to extract relevant research
is therefore a key activity in preventing this. The work of [25] discusses how the investi-
gators can seek from these datasets insight for the dynamics of the growth and the inter-
connectivity of scientific thought. With the growth of the citation datasets (such as the
ones described in “Data” section) the concerns on the processing time, complexity of the
models and the ability to interpret the results are becoming a key issue.

This then poses key questions about how to process and then reason about the results
from large datasets with large variations. Questions about the results even require
effort in their interpretation. Work such as [26] look at the problem from a conceptual
Pho and Manizaris J Big Data (2020) 7:91 Page 3 of 17

perspective on the areas of focus for big data and how the user can interact with the data
that are results from a post analysis. The work of [27] provides a high level overview of
the tools and approaches available in visually investigating the data and the results of
different methods return. It is possible to include the full set of interpretable outcomes
and the full set of relevant data features, but that does produce a challenge for the prac-
titioner to determine which features are or prime interest. A dimensionality reduction
approach [28] provides a more effective experience for the practitioner.

Graph Neural Networks (GNNs), [29], provides a methodological framework for com-
bining node feature and the network data information in order to produce predictions
within a machine learning paradigm. There are many applications ranging from image
object positions in a non-euclidean space representation, molecular properties and cita-
tion networks [30]. Therefore this work deals with investigating an even more simple
GNN, the Simple Graph Convolution (SGC) [31], which has a simpler methodological
definition and a competitive predictive accuracy. It is in developing a modified SGC that
the task of reducing the dimensionality in large datasets with a GNN will be explored.
As will be shown in “Methodology” section, the simplicity of the model allows for it to
be a basis for extensions that can incorporate constraints such as shrinkage upon the
parameters. This is done in a manner similar to the regularization procedure of Lasso
[32]. This will allow the large complex datasets to be processed in such a manner as to
be interpretable and more accessible in terms of the computations resources required.
Models other than the SGC would incorporate more complexity upon procedures
already complex making large dataset investigations an increasingly large challenge to
apply. The work effectively takes the SGC and extends it so that the model can introduce
constraints upon the parameter vectors in such a manner as to allow the model to be
more easily interpreted. This allows the parameters for each class to be more sparse and
for each class to have less of an overlap between each other. Altogether this results in a
parameter matrix which can more easily be inspected.

In “Related work” section, a selection of previous work highlights the development
of the GNN which led to the SGC is acknowledged. The “Data” section describes the
datasets used in exploring the results of the proposed modification on the SGC with
regularization. The methodology is described in “Methodology” section, where the SGC
formalism is presented and the proposed modification where the regularization upon
the features and their parameters allows for a reduction of the redundant features and
hence ease in the interpretability. The results are displayed in “Results” section where the
ability for the SGC and the proposed SGC allow the model to fit data which would not
be linearly separable but is made separable by incorporating the graph information, and
then the application to a scientific citation dataset (Cora [33]) is shown.

Related work

Convolutional Neural Networks (CNNs) [34] has brought a methodological approach
for handling high dimensional problems more efficiently than other paradigms. As noted
in [35] in conjunction with deep learning, CNNs have greatly improved the ability to
classify sound and image data. The work of [36] introduces formally how graph based
methods can be used with CNNs. A key contribution of [36] is that the extension of
the model to generalize to graphs is founded upon localized graph filters instead of the
Pho and Manizaris J Big Data (2020) 7:91 Page 4 of 17

CNN’s localized convolution filter (or kernel). It presents a spectral graph formulation
and how filters can be defined in respect to individual nodes in the graph with a certain
number of ‘hops’ distance. These ‘hops’ are representative of the number of edges tra-
versed between nodes and is the result of the powers of the adjacency matrix where the
number of walks can be calculated [5] (walks are paths which allow node revisits).

An introduction to the motivation from basic principals can be found in [37], where
the fundamental analysis operations of signals from regular grids (lattice structures) to
more general graphs is developed. The authors in [38] utilize the theory of signals on
graphs in order to show how a shift-invariant convolution filter can be formulated as a
polynomial of adjacency matrices. The discussion of how low pass filters are an underly-
ing principal in the GNN is discussed in [39] which is also described in the work of the
SGC. [40] proposes a Graph Convolutional Network (GCN) by adapting Convolutional
Neural Networks (CNNs) for graph-structured data. The GCN learns a graph represen-
tation via layer-wise propagation rules that represents localized spectral filters.

The GNN can allow for the augmentation of a users social network and their features
to make a more accurate prediction and similarly for an academic paper that the features
(keywords or low dimensional representation) with the citation links can more accu-
rately place its relevance. The machine learning framework can introduce large over-
heads in the processing time especially for large datasets but fortunately research has
shown that simpler GNN models display peak performance [41]. The work of [40] which
introduces a semi-supervised approach to GNNs, shows in appendix B the performance
of the methodology with the number of ‘layers’ employed in the model and how there
is an actual degradation of the performance after a few layers. The SGC [31] provides
an efficient framework to provide a similar model of the data associations as the GCN
but avoid the necessity of the layers the GCN introduces. The methodology of the SGC,
as shown in “Methodology” section allows a single layer of matrix computations with
a non-linear activation function. This is similar to the processing steps taken for logis-
tic regression which can be computed for large datasets very efficiently. Building upon
this efficient model allows an investigator to explore further constraints which would be
much more computationally demanding with the incorporation of layers.

Data

Three different datasets are employed in order to explore the model proposed, with 2 of
them being synthetic and the last being a real dataset which is well explored [33]. The
first synthetic dataset has 2 dimensional features with data points placed in a circle and
labels applied on opposite sides of the identity line (x, = x2). The other synthetic dataset
also has 2 dimensions and placed in such a way which clustering or a non-network based
model, relying upon distance measures, would incorrectly classify the node labels. More
about these 2 datasets is described below.

Circular data

Figure 1 shows the synthetic data produced with points allocated along a circle based
at the origin. There are 100 points and 50 of them are allocated to each class placing
them on either side of the identity line. A key aspect of this data is that the model will

attempt to shrink the feature projections which can incur a penalty on the optimization
Pho and Manizaris J Big Data (2020) 7:91 Page 5 of 17

 

 

1.0 eeeeeOSees, o
@e** *Pe0, alt
O50

 

classes
e 0
Se e 1
- e
*Ceccccccceceete™
—1.0 —0.5 0.0 0.5 1.0
xl

Fig. 1 The application of the proposed methodology to the circular data. Data points are produced about
the origin at a fixed radius so that 100 points are equally spaced. The data points have class labels allocated so
that there are 50 in each of 2 classes. This is shown in the plot with a line which determines the separation

Ne

 

 

 

procedure. The compromise between the error function on the data and the regulari-
zation penalization term will require a balance as a single feature reduces the shrink-
age penalty but the direction for the optimal fit uses both dimensions. This compromise
is induced since the optimal projection will be with a vector containing non-negative
parameters for each dimension in equal value at a direction x; = —x2 therefore high-
lighting the shrinkage of one of the parameters. The network data (the adjacency matrix)

is a ring network connecting neighboring nodes.

Linearly inseparable data

Figure 2 shows 30 synthetically produced data points in 2 dimensions (x1, 2) which
form 4 distinct clusters. Each class has 15 data points randomly generated and it is sepa-
rated into 2 clusters across the axis. We produce a non-disjoint network (single compo-
nent) structure for the data points to be connected with a more dense connectivity set
between points of the same label. This production is inline with the concept of modular-
ity in networks [42] where the density of the edges between nodes of the same label is
proportionately greater than the density between nodes with different labels. Without
the network structure, distance metrics would produce erroneous results and the intro-
duction of this information increases the accuracy. This allows the linear operations to
produce a separation for the class labels.

Methodology

[40] develops Graph Convolutional Networks (GCNs) by adapting Convolutional Neural
Networks (CNNs) for graph-structured data and the work of [31] (proposing the Simple
Graph Convolution (SGC)) builds upon it. The SGC removes the non-linear transitions
between the layers in the model. This simplification speeds up processing time significantly
yet still performs on par with GCNs and other state-of-the-art graph neural network mod-
els across multiple benchmark graph datasets. The model modification will allow easier
interpretability of the parameters fitted by the optimization procedure with the applica-
tion of a set of constraints. The constraints introduced into the loss function will force the
stochastic gradient descent algorithm to find directions which have fewer non-zero values
and less overlap for the parameters between the classes. This addresses the problem of how
Pho and Manizaris J Big Data (2020) 7:91 Page 6 of 17

 

No WwW

x2
hb

classes
e O
| e« ]j

a)

 

Fig. 2 15 data points of 2 classes are randomly generated and form 2 distinct clusters residing on
the opposite sides of the axis. a Presents the scatter plots of these data points. b Shows the network
connections among those data points. In terms of a distance metric between the points, or from using a
linear projection, erroneous label assignments can arise from using only a data but the incorporation of the
network associations shown in b allows this to be bypassed. The results of the application to the proposed
methodology is shown in “Methodology” section

\

 

 

to inspect effectively the matrix of parameters and the vectors of the parameters for each
class. This takes inspiration from regularization methods.

We adopt the notations presented in [40] and [31] for the GCN and SGC respectively. A
graph G = (V; A) can be defined as a collection of nodes (vertexes) set V = (11, va, ..., VN)
containing N nodes and an adjacency matrix A ¢ RY*% where aj is the weighted edge
between node v; and v; (a = 0 if v; and v; are not connected). We define the degree matrix
D = diag(dj, do, ..., dx) as a diagonal matrix whose off-diagonal elements are zero and each
diagonal element d; capture the degree of node v; and dj = 5+ j Gif: There is a feature matrix
(also referred to as the design matrix) X ¢ R‘*? where each row «; is the feature vector
measured on each node of the graph. This can be thought of as each row is the feature data
belonging to a node, and the columns to a different dimension of the features. Each node i
has a class label from C classes and hence can be coded as one hot vector y; € {0, 1}.

The GCNs and SGC ad4d self-loops and normalize the adjacency matrix to get the matrix
S:

$= 9 3AD-} (1)

where A = A+I and D = diag(A). This normalization allows successive powers of the
matrix to not influence the overall size the projections. The SGC removes non-linear
transformation from the kth-layer of the GCN resulting in a linear model of the form:
Pho and Manizaris J Big Data (2020) 7:91 Page 7 of 17

Y = softmax(S... SSXxOO™ ...@%?), (2)

The SGC classifier is then achieved by collapsing the repetitive multiplication of matrix
S into the kth power matrix S* and reparameterizing the successive weight matrices as
0 =00 ...e%?.

Y = softmax(S‘ X@). (3)

The parameter k corresponds to the number of ‘hops’ which is the number of edge tra-
versals in the network adjacency matrix S. k can be thought of as accumulating informa-
tion from a certain number of hops away from a node (as described visually in [31)). If
k = 0 the methodology becomes equivalent to a logistic regression application which is
known to be scalable to large datasets. Since the SGC introduces the matrix S as linear
operation the same scalability applies. The weight matrix O is trained by minimizing the
cross entropy loss:

L=S YS Yieln Vic (4)

ley, cEC

where .); is a collection of labeled nodes.

As motivated in “Introduction’, the SGC shows how an efficient formulation of GNNs
can be derived, it does not provide as well the ability to reduce the feature set. To reduce the
number of parameter values, we introduce a flexible set of constraints as shrinkage opera-

tors in the loss for Eq. 4:

D (-1)
Le=Lt+hx > ps era" +L_x Y-Ox.c)ll2

cEC \d=1 cEC

+13 x | S> YO (Ok .e:)! lO xccayl #1 < ¢2)

cyEC c2EC

The first component of Lp is the loss from SGC being L. Next, Lj is the shrinkage term
for penalizing the number or parameters by reducing the penalization with a larger skew
in the number of elements in the columns of Op. The term |Or,.¢) \* denotes the normal-
ized vector for each class projection in the parameter matrix (which are columns) and
that each element is raised to the power of 4. The L2 term is the total magnitude of the
parameter vector so that the distribution of the terms are not influential but only the
norm result. The term L3 is the term which penalizes class label projection which have
large overlaps, so that vectors will be orthogonal or depending upon the value of L3 to
support opposing directions. The parameters for the regularized fit using the shrinkage
in the loss will be referred to as Og. To impose an orthogonality constraint between the
projection vectors the term for the L3 is modified:
Pho and Manizaris J Big Data (2020) 7:91 Page 8 of 17

D (-1)
Lr=LlLt+lxy ps ena" +12 x Y- Ox.c)ll2

cEC \d=1 cEC

2
+13 * | S2YO (Okey! |Oxceayl #1 < e2)

cyEC c2EC

This methodology therefore delivers a formulation which is based upon an approach
with layers as other ‘deep learning’ frameworks provide, but without the computational
burdens that come along with it. The simplified model implementation is therefore capa-

ble to be run on a personal computer with Pytorch [43].

Results

Here we present the results of applying the proposed methodology to the datasets
described in “Data” section. The synthetic circularly placed datapoints with labels allo-
cated on the sides of the identity line of 2 dimensions, described in “Circular data” sec-
tion of the “Data” section. The synthetic datapoints placed along 2 dimensions without
a linear separation of the labels based upon a distance metric but possible with the net-
work information is described in “Linearly inseparable data” section and the results for
it shown in the subsection of “Results” section, “Synthetic linearly inseparable data” The
results of the application to the real dataset of [33] (Cora citation dataset) is shown in the
subsection of “Results” section “Application to the Cora dataset”. The methods of logis-
tic regression, SGC and the regularized SGC are applied and the results are compared
revealing that the fitted parameter vectors for each class have less overlap between them-
selves so that their characteristics for the classes can be more effectively interpreted.

Synthetic circular data

The results of applying the SGC methodology with and without regularization on the
synthetic circular data, is shown in Fig. 3. The points in the dataset have 2 features x1
and x2. In Subfigure (a) the SGC model produces a perfect accuracy with 2 parameters
used the projection vectors pointing to the proper direction of each class. The regular-
ized SGC returns different solutions due to the regularization in the parameter matrix
O. Subfigure (b) shows the regularized parameter vectors under different initializations
of the learning algorithm which applies constraints. These constraints reduce the num-
ber of parameters used, the size of the vectors as a norm, and the direction between
the vectors to be more informative. It can be seen how different random initializations
produce different loss values and accuracy depending upon the local optima arrived at.
These different stable points do show that the shrinkage factors are affecting the vectors
for each class in O.

A separation based upon the identity line for the 2 dimensions, represents a situation
where there is equal weight upon all the features of the data and the inference scheme
must make a choice in the penalization. The choice results in a decrease in the loss of
accuracy in order to decrease the penalization from the regularization from the 3 com-
ponents calculated from the projections; Lj, Lz and L3 as discussed in “Methodology”

section. The variation shows that the model is able to explore a with range of vectors for
Pho and Manizaris J Big Data (2020) 7:91 Page 9 of 17

 

 

   

 

 

 

 

 

 

 

(- >)
a SGC with L1 =0, L2 = 0, L3 = 0, k = 2, Epoch = 150
1.0* ec _pineeee ree eta, ae
ee e* e
0.5 | is a
QY 0.0; »
- classes
-0.5 | a ° 0
fee ai ° 1
—1.01 a **Ceccccccccee®”
-1.0 -0.5 0.0 0.5 1.0
xl
SGC with L1 =2, L2 = 2, L3 = 15, k = 2, Epoch = 700
b seed = 40 seed = 1 seed = 31 seed = 96
loss = -7.63 loss = -4.80 loss = -1.69 loss = 10.71
_acc = 0.68 acc = 0.72 acc = 0.76 acc = 0.90
1.0 /
0.5
Y 0.0
classes
e O
—0.5 4
-1.0
-1 0 1 -1 0 1
xl xl xl xl

  

Fig.3 The above plots show the results of applying the SGC method with and without regularization

upon the synthetic circular data. In a the dashed line shows the separation line where each side defines the
point labels and the solid lines shows the SGC projections learned in ©, and perfect accuracy is achieved.

b Presents multiple subplots of separate independent runs of the proposed regularized SGC methodology
where there are different random initializations using gradient descent. Various comparable fittings are
found and it can be seen how all the aspects of the regularization upon Op are respected in terms of the
magnitude, relative directions between class vectors and the number of components (features) used

 

 

the matrix columns of Or. From the range the choice with the largest accuracy (lowest

loss) can be chosen.

Synthetic linearly inseparable data

In this subsection, we apply the SGC method with and without regularization on the
linearly inseparable data which contains feature coordinates and a network of associa-
tions. The dataset used here is described in “Data” section’s subsection “Linearly insepa-
rable data” where the coordinate space of the datapoints and the network are displayed.
The key aspect which this dataset emphasizes is that the features alone without the
network information cannot produce a linear separation, but with the incorporation of
the network information (with linear operators) this classification then becomes possi-
ble. Figure 4 shows the results of applying the SGC to the dataset without the network
information being used k = 0, and is effectively an application of logistic regression. The
methodology cannot separate the data correctly with a pair of linear projections but that
can be alleviated as seen in the next figures by incorporating the network information as
well (k > 0).

Using the SGC (by setting the shrinkage parameters to 0), in Fig. 5, Subfigure (a)
shows that although the vectors for the class projections, as columns in O, do not
enable a separation between the groups the network information enables a perfect
accuracy to be produced. This is because although the support for an erroneous
class can be accumulated for a point, the feature space ‘communicated’ to it from
the edge connections of features overrides the nodes’ own features in these cases.
Pho and Mantzaris J Big Data

 

 

 

 

 

 

 

 

 

 

(2020) 7:91
( >)
SGC with L1 =0, L2 = 0, L3 = 0, k = 0, Epoch = 700
seed = 40 seed = 1 seed = 31 seed = 96
loss = 0.60 loss = 0.60 loss = 0.60 loss = 0.60
acc = 0.67 acc = 0.67 acc = 0.67 acc = 0.67
31 ** 4 31 ** 4 3{«. 4 3]** 4
. * . a - a ° 4
2 * . 2 * . 24 - . 2 - .
2.1 ‘ 2 1 A 2 1 . QZ 1
ty 4, ty 4 t. 4 %. 4 classes
| * A * \/ A | * 4 | * A e 0
“i 7 | = ; oo A : "2 * /N A : wl Bs ey : = I
* Aa + Aa * Aa * Aa
a | * a —] 4 * A -1{ * A i i* A
-2 0 2 -2 0 2 -2 0 2 -2 0 2
x1 xl xl xl
Fig. 4 Applying logistic regression to a set of datapoints where there features are not linearly separable. This
is the SGC methodology where k = 0 and the network information is not incorporated

 

Each plot is an independent run with slight changes in O. Subfigure (b) shows a set
of plots but where the axes x1* and x2* for each data point represents the projection
of the features with the ‘neighborhoods’ of the points. With k = 2, S$”, aggregates the
weights from ‘2 hops’ distance in the network, so that the multiplication of $?X is
shown on these new axes. It can then be understood why the data is then ‘linearly’
separable after this transformation. This emphasizes how the network information
can be used to improve the accuracy and maintain model simplicity.

In Fig. 6 the regularized SGC is applied to the dataset (with L2 = 0) and the
parameter vectors for each class from Op are plotted in both Subfigures (a) and (b).
The constraints (shrinkages) are placed on the sum of the elements within 0.;, and
the direction of the vectors which reduces the total value summation for feature
extraction. In Subfigure (a) the projection vectors of Or are shown and as with Fig. 5
the results produce a perfect accuracy. What can be seen is that the model explores
alternative parameterizations which are not found previously without regulariza-
tion. The projections all display a drop of a features dimension. Subfigure (b) shows
the S?X projection and that perfect accuracy can still be achieved. In each of the
plots it can be seen how various equivalent (in terms of the accuracy) projections
can be searched which reduce effectively the number of features used for each class
being predicted. The ability for the non-linearly separable data to be correctly classi-
fied without introduction of new parameters or ‘layers’ in the CNN enables explora-
tions to be done more efficiently on large datasets in terms of time and processing
capabilities.

The change of the L3 constraint is utilized so that the projection vectors for each
class are fit to be orthogonal to each other (shown in Eq. 6. This allows the possibil-
ity for a smaller number of classes to be fit, if the class number is not known and dif-
fers from the previous application in that the support for each class would be seen as
a separate linear function’s projected value. Figure 7 shows the results in Subfigure
(a) and Subfigure (b) where the vector fit in the space of the data points is seen and
how the data is transformed into different axes using the network data respectively.
It can bee seen how the constraint for the orthogonality is preserved and the accu-
racy for the fits is still achieved for this problem.

Page 10 of 17
Pho and Manitzaris J Big Data

(2020) 7:91

 

 

 

 

 

 

 

 

 

 

 

 

 

 

SGC with L1 =0, L2 = 0, L3 = 0, k = 2, Epoch = 700
a seed = 40 seed = 1 seed = 31 seed = 96
loss = 0.00 loss = 0.00 loss = 0.00 loss = 0.00
acc = 1.00 acc = 1.00 acc = 1.00 acc = 1.00
37424 ° 3744 * 3744 +”
. . " * ‘ *
2; A / 2 A - 21 A .
2 1) * 2 1 * 2 1
%, 4 *. 4 *% 4 classes
Oj « 4 Oj « a Oj « el ° °
+ 4 A * * 4 a a 4 A = 1
Aa a Aa |" AA
—] L * 4 —] | 7 A —] 1 + 4
-2 0 2 -2 0 2 -2 0 2
xl xl x1
SGC with L1 =0, L2 = 0, L3 = 0, k = 2, Epoch = 700
b seed = 40 seed = 1 seed = 31 seed = 96
loss = 0.00 loss = 0.00 loss = 0.00 loss = 0.00
10 acc = 1.00 10 acc = 1.00 10 acc = 1.00 10 acc = 1.00
& . e s e . e .
0.5 0.5 0.5 0.5
© 0.0 ym % 0.0 we % 0.0 —| & 00 |
classes
e 0
-0.5 -0.5 -0.5 -0.5 2
-1.0+—— -1.0 +—— -1.0+ . -1.0+ -——
-1 oO 1 -1 0 1 -1 0 1 -1 0 1
xi? “i> xi? x1*
Fig.5 The results of applying the SGC method on the linearly inseparable data is presented here. a Plots the
data points and the 2 learned parameter vectors by SGC methodology under different initializations. It can
be seen how although the displayed classification vectors within @ with the network information provide
the ability for a lossless prediction. Similarly, for b it can be seen how a new set of axes for the projection $?X
(network and features) the linear separation becomes visible

Application to the Cora dataset

Here is presented the application of the SGC methodology and the proposed regu-
larized SGC to the dataset of Cora [33]. The purpose is to examine the capability
of both the SGC and the regularized SGC to a dataset with a large number of fea-
tures. There are many situations in big data applications where the datasets have
large numbers of features due to larger data gathering schemes and a requirement
to select key features without supervision. The SGC has been applied to the Cora
dataset [44], and here the performance with a regularized version is mainly directed
at the interpretability in highlighting the key variables in the feature set while also
applying other constraints. Figure 8 present the results with 2 Subfigures with heat-
maps displaying the parameter values fitted for each class in © and Op. The dataset
classifies each document as belonging to one of 7 different classes where the SGC
then produces a parameter matrix © with 7 columns and d rows for the feature num-
ber. The constraint upon L3 is set so that the projection vectors between classes are
in opposing direction so that class feature loadings are differentiated by their place-
ment in a histogram of the values. With the SGC applied, Subfigure (a) shows the
weights of the parameters for each class (a single column in ©) as a separate heat-
map with a legend for the values indicated. Analogously the same set of results but
produced with the regularized SGC proposed here is shown in Subfigure (b). The
approach produces a new parameter matrix Op introduces the regularizations in the

 

Page 11 of 17
 

 

 

 

 

  

 

 

 

 

 

 

Pho and Manitzaris J Big Data (2020) 7:91
(— >)
a SGC with L1 =2, L2 = 0, L3 = 10, k = 2, Epoch = 700
seed = 40 seed = 1 seed = 31 seed = 96
loss = -6.00 loss = -5.94 loss = -6.00 loss = -5.96
acc = 1.00 acc = 1.00 acc = 1.00
3744 * 3444 *
i 8 . B
2; A 2 A
* -
2% 1 * 2% 1 *
m 2 w. 4 classes
* A on A e 0
° * A ° A A x 1
Aa 2 Aa
—] * A =] { * A
-2 0 2 -2 0 2
xl xl
SGC with L1 =2, L2 = 0, L3 = 10, k = 2, Epoch = 700
b seed = 40 seed = 1 seed = 31 seed = 96
loss = -6.00 loss = -5.94 loss = -6.00 loss = -5.96
10 acc = 1.00 ~ acc = 1.00 T acc = 1.00 16 acc = 1.00
Q z t bi @ + ‘ +
0.5 0.5; 0.54 0.5;
y 0.0} oo y 0.0; y 0.0 —— y 0.0;
classes
e 0
—0.54 -0.5 -0.5 —0.54 i
|
—1.0 —1.0 +. —1.0 +. —1.0
-—1 oO 1 -1 0O 1 -1 oO 1 -—1 O 1
x1* x1l* x1l* x1*

Fig.6 The application of the regularized SGC to the dataset where linear projections are incapable of class
separation. a Shows the parameter vectors produced by introducing constraints into SGC method on the
feature space with original data points and how the classification can then produce perfect accuracy. It can
be seen the proposed SGC reduces the effective number of features used in the columns of the matrix Op.
b Shows the plots for the same set of weight vectors displayed on the projection axes of S7X where each
datapoint (node) accumulates feature information from neighbors 2-hops away (k = 2)

 

 

 

inference scheme for the parameters by penalizing their total sum and directions
to be as informative about the features in terms of accuracy prediction and lack of
overlap (removing redundancy within large feature spaces typical of large datasets).
It can be seen there are fewer variables highlighted for the practitioner to examine,
which looks to investigate and highlight which variables are important for the class
membership determination. The 7 cells highlighted in the bottom right are padding.

Using the data in the heatmaps shown in Fig. 8 a histogram of the values for the
parameter values for each class and the features is created for the SGC and the val-
ues from the regularized SGC held in the matrices © (Subfigure a) and Og (Subfig-
ure b). In Fig. 9, Subfigure (a) shows how there is a smaller group of features which
provide positive contribution to the class identification and that an apparent 2 mode
distribution can be made out. Each plot belongs to a different class in the dataset
and are a different column in ©. Subfigure (b) shows the parameter value distribu-
tion within Or. The effect of the regularization can be seen in comparison with Sub-
figure (a) where the number of feature values at value 0 are the majority. This makes

the exploration and backtracking process the features easier.

Page 12 of 17
Pho and Manizaris J Big Data (2020) 7:91 Page 13 of 17

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

( >)
a SGC with L1 =2, L2 = 0, L3 = 10, k = 2, Epoch = 700
seed = 40 seed = 1 seed = 31 seed = 96
loss = 4.00 loss = 4.25 loss = 4.06 loss = 4.04
_ acc = 1.00 acc = 1.00 acc = 1.00 acc = 1.00
3744 * 34 Aa *
A n A .
2 A 2 a
QY 1 * Yl
%, 4 t. 4 classes
© A a e 0
O71 « 4 0 a | A : 1
* AA oS AA
=] 4 A —]j* a
-2 0 2 -2 0 2
xl xl
SGC with L1 =2, L2 = 0, L3 = 10, k = 2, Epoch = 700
b seed = 40 seed = 1 seed = 31 seed = 96
loss = 4.00 loss = 4.25 loss = 4.06 loss = 4.04
acc = 1.00 19 aCe = 1.00 19 aCe = 1.00 19 acc = 1.00
é s e . z . € .
0.5 0.5 0.5 | 0.5
Yy 0.0] qy 0.0 y 0.0; y 0.0) —~
classes
e 0
-0.5 j -0.5 j -0.5 j -0.5 | aw 4
-1.0 -1.0 +. -1.0 +. -1.0 +
-1 oO 1 -1 o 1 -1 0 1 -1 0 1
xL* xL* xi* xa
Fig. 7 The results of applying the regularized SGC with an orthogonal constraint upon the projection
achieved by using a change in regularization term L3. In a the vector projections within the methodology can
be fit so that there is no loss and the orthogonality constraint is satisfied. In b the transformation upon the
data with the network information is presented and how within this space the linear projections can separate
the classes with the orthogonal vectors
NS J
Discussion

This work proposes a model extension of the Simple Graph Convolution (SGC) which
aims at producing a smaller and more meaningful set of projections in which clas-
sification labels are presented. It addresses a key issue with interpretability of model
applications in big data where many features may be used which are redundant and
remove the ability for a practitioner to examine the weights. A key reason for why the
SGC was chosen to be extended with this capability is that the operations are linear
in the methodology with the exception of the softmax function application can be run
relatively efficiently in comparison to methodologies relying on more parameteriza-
tions and more ‘layers’ in order to improve accuracy.

The SGC incorporating the network information can produce accurate classifica-
tion of points in a feature space which is not linearly separable by utilizing the net-
work information via linear operations. The results demonstrated this capability on a
small dataset where the network projection effectively linearizes the search by having
information from the node ‘neighborhood’ accumulated from ‘k-hops’ distance (rely-
ing upon the powers of the adjacency matrix). This allows for fast run times and the
application to services which rely upon small delays. The methodology was applied to
the Cora citation dataset which has a large number of features and the reduction is
 

Pho and Manizaris J Big Data (2020) 7:91 Page 14 of 17
a Heat map of SGC without regularization
class 0 class 1 class 2
oT as ~_ me o a Tan . onl =. w=
wie ose kse a 0 rn er ehe— a]
ve, = * > p . "= . - ao i -_ ‘ a a) . ‘.
S F my >. a Lf , a “ = t — ~y Bu 5 ant ” = oe i = : ‘ “ss!
= -- i? ee Be a a aa i . 3 % — > = ©
— bane oa i. - a. S = - . 3) i <n . "J
Ww > an re re wn i - 4 wf ae ee re ee
a ._-- -* -% - S . ; ' : a 3 —— —o _ *|
| Pe es a | yy) eg eS ee
™O 4 8 121620 2428 32 36 ™O 4 8 1216202428 32 36 "0 4 8 121620 2428 32 36 50
class 3 class 4 class 5
- a oy 7 SCT Tee eS, oso we 25
re ee oe Page 0 Es eT fm Re ee |
© tl oe “el 06 at oe Oo Pe eS ee ee
nn bh * " = - . in = , wa Se ot ni . . k ~ 5 ° -O
So: r. One . S o aha ="es . “ea © bagel ees. ee 4
wi. ne ee Ps a i noe eh wine Pee Se
Stee Seen SNe | 8 Ser a a tg
w * oo — norte. 5 — wn oo i — : — Wie 4 p. ase gt ze li -25
™ 0 4 8 121620 2428 32 36 ™O 4 8 1216202428 32 3% " O 4 8 121620 2428 32 36
—50
class 6
ofa 2S
br = . al -s- 2’
= Poi a al _ ==. -75
° =*, < . Sot -_
on toe ae ee
ot Pek aim, al. 4, tt “2
S he = pnd cities _.
MP ee Daly go eel
& Li = ™ D's — = ' —_ aie
™ 0 4 8 121620 2428 32 36
b Heat map of SGC with regularization
class 0 class 1 class 2
o o of
oO | o' -
oe 4 4
rc - ea
o o © '
™N ™ ~N
ws ~ N |
8 8 8
wy ~ ' ' ' ' ' t a wt * ' t ' ' ' r a Ww) “ry ' ' ' ' ' ne gue
™O 4 8 121620 2428 32 36 ™O 4 8 1216202428 32 36 "0 4 8 121620 2428 32 36
4
class 3 class 4 class 5
So o © |
n- Ww - . wie 0
o °o So}
co at re
4 a ~«
Ri R & |
A CS Q | .
m m R
w ’ ' 1 1 — ww A 1 ' 1 1 1 ‘ _- w ‘, ¥ 1 ‘ y —_——
™ 0 4 8 121620 2428 32 36 ™O 4 8 1216202428 32 3% "0 4 8 121620 24 28 32 36 -8
class 6 =
si 12
wy
eA
rc
w
os |
&
w
N |
4
w

™O 4 8 121620 2428 32 36

Fig. 8 The plots showing the results of applying the proposed method of SGC with and without
regularization on the features to the dataset of Cora. a Shows the heatmap of the class columns of matrix

O which holds the parameter values for the feature projections of the data X after the inference with SGC
without the regularization. b Analogously shows the parameter values but with the inference procedure
applying the constraints for the regularization as proposed which produces the shown values of Og. On the
bottom right of each plot there are 7 cells with padded values to produce the heatmaps. The columns of the
parameter vector correspond to different classes, each shown separately, and the weights applied to each
feature belonging to the nodes. It can be seen that the regularization reduces the amount of weighting over
the features highlighting key variables

 
Pho and Manizaris J Big Data (2020) 7:91 Page 15 of 17

 

 

a Distributon of parameter values SGC without regularization
class 0 class 1 class 2
150
150
100 100
50 50
oO" =50 0 °
class 3 class 4 class 5
150 200
150
100
100 100
30 50
-25 0 25 -50 0 50 : -50 0 SO
class 6

 

50
b Distributon of parameter values SGC with regularization
class 0 class 1 class 2
1000 1000 1000
500 500 500
oO 1 ° -2 0 0” 0 25 00

class 3 class 4 class 5

1000 1000 1000

500 500

a:

~
w

“2 ss ~& 0 “28 <5 0

class 6

500

.

-1 0
Fig.9 The distribution of the parameter values inferred for the Cora dataset with the application of the SGC
and the regularized SGC. In a the SGC is applied and the histograms of the parameter values for each class
in @ is shown in the plots. b Shows the equivalent plots but using the regularized SGC that penalizes the
number of features. The majority of the features are around value zero

 
Pho and Manizaris J Big Data (2020) 7:91 Page 16 of 17

significant in the number of features highlighted to the user. This provides a set small

enough to explore manually if required.

Conclusion and future work

The SGC model extension presented here allows for a more explainable set of results
to be presented to the user. The regularization terms reduces the number of non-zero
parameters and the overlap between parameterizations of the different classes. Future
work could entail a more in depth exploration of how the network can be ‘decomposed’
in such a way as to minimize the number of label alterations. Producing a network sep-
aration by eliminating edges can find applications in social networks where polarized

communities must be isolated as a means of inoculation.

Abbreviations
SGC: Simple Graph Convolution; GNN: Graph Neural Network; GCN: Graph Convolutional Network; CNN: Convolutional
Neural Networks.

Acknowledgements
Not applicable.

Authors’ contributions

Conceptualization, PP and AVM; Formal analysis, PP and AVM; Investigation, PP and AVM; Methodology, PP and AVM;
Software, PP; Supervision, AVM; Validation, PP; Visualization, PP; Writing original draft, PP and AVM; Writing—review and
editing, PP and AVM. Both authors read and approved the final manuscript.

Funding

This work was partially supported by grant FA8650-18-C-7823 from the Defense Advanced Research Projects Agency
(DARPA). The views and opinions contained in this article are the authors and should not be construed as official or as
reflecting the views of the University of Central Florida, DARPA, or the U.S. Department of Defense.

Availability of data and materials
The dataset, Cora, is presented in [33] and is available at https://docs.dgl.ai/en/0.4.x/api/python/data.html.

Competing interests
The authors declare that they have no competing interests.

 

Received: 2 July 2020 Accepted: 8 October 2020
Published online: 20 October 2020

References

1. Krallman A, Pelletier MJ, Adams FG.. @ size vs.# impact: Social media engagement differences amongst facebook,
twitter, and instagram. In: Celebrating America’s Pastimes: Baseball, Hot Dogs, Apple Pie and Marketing? Berlin:
Springer; 2016. p. 557-61.

2. Newman M. Networks. Oxford: Oxford University Press; 2018.

3. Barabasi A-L, et al. Network science. Cambridge: Cambridge University Press; 2016.

4. Girvan M, Newman ME. Community structure in social and biological networks. Proc Natl Acad Sci.
2002;99(12):7821-6.

5. Borgatti SP. Centrality and network flow. Soc Netw. 2005;27(1):55-71.

6. Jeong H, Néda Z, Barabasi A-L. Measuring preferential attachment in evolving networks. EPL (Europhys Lett).
2003;61(4):567.

7. Albert R, Barabasi A-L. Statistical mechanics of complex networks. Rev Mod Phys. 2002;74(1):47.

8. Laflin P Mantzaris AV, Ainley F, Otley A, Grindrod P, Higham DJ. Discovering and validating influence in a dynamic
online social network. Soc Netw Anal Min. 2013;3(4):1311-23.

9. Soares FB, Recuero R, Zago G. Influencers in polarized political networks on twitter. In: Proceedings of the 9th inter-
national conference on social media and society. 2018. p. 168-77.

10. Ricci F, Rokach L, Shapira B. Introduction to recommender systems handbook. In: Recommender systems handbook.
Berlin: Springer; 2011. p. 1-35.

11. Su X, Khoshgoftaar TM. A survey of collaborative filtering techniques. Advances in artificial intelligence. 2009;2009.

12. Unga, LH, Foster DP. Clustering methods for collaborative filtering. In: AAAl workshop on recommendation systems,
Menlo Park, CA, vol. 1. 1998. p. 114-29.

13. Zhang R, Tran T. An information gain-based approach for recommending useful product reviews. Knowl Inf Syst.
201 1;26(3):419-34.
Pho and Manizaris J Big Data (2020) 7:91 Page 17 of 17

 

14. Kabakchieva D. Student performance prediction by using data mining classification algorithms. Int J Comput Sci
Manag Res. 201 2;1(4):686-90.

15. Thai-Nghe N, Drumond L, Krohn-Grimberghe A, Schmidt-Thieme L. Recommender system for predicting student
performance. Procedia Comput Sci. 2010;1(2):2811-9.

16. Jackson MO. Networks in the understanding of economic behaviors. J Econ Perspect. 2014;28(4):3-22.

17. Shu K,Wang S, Tang J, Zafarani R, Liu H. User identity linkage across online social networks: a review. Acm Sigkdd
Explor Newsl. 2017;18(2):5-17.

18. AlthoffT, Jindal P Leskovec J. Online actions with offline impact: how online social networks influence online and
offline user behavior. In: Proceedings of the tenth ACM international conference on web search and data mining.
2017. p. 537-46.

19. Crucitti P Latora V, Porta S. Centrality measures in spatial networks of urban streets. Phys Rev E. 2006;73(3):036125.

20. Euler L. Solutio problematis ad geometriam situs pertinentis. Commentarii academiae scientiarum Petropolitanae.
1741: 128-40.

21. Kivela M, Arenas A, Barthelemy M, Gleeson JP, Moreno Y, Porter MA. Multilayer networks. J Complex Netw.
2014;2(3):203-71.

22. Belyi A, Bojic |, Sobolevsky S, Sitko |, Hawelka B, Rudikova L, Kurbatski A, Ratti C. Global multi-layer network of human
mobility. Int J Geogr Inf Sci. 201 7;31(7):1381-402.

23. McPherson M, Smith-Lovin L, Cook JM. Birds of a feather: homophily in social networks. Ann Rev Sociology.
2001;27(1):415-44.

24. Son J, Kim SB. Academic paper recommender system using multilevel simultaneous citation networks. Decis Sup-
port Syst. 2018;105:24-33.

25. Radicchi F, Fortunato S, Vespignani A. Citation networks. In: Models of science dynamics. Berlin: Springer; 2012. p.
233-57.

26. Suthaharan S. Big data classification: problems and challenges in network intrusion prediction with machine learn-
ing. ACM SIGMETRICS Perform Eval Rev. 2014;41(4):70-3.

27. Caldarola EG, Rinaldi AM. Big data visualization tools: a survey. Research Gate 2017.

28. Reddy GT, Reddy MPK, Lakshmanna K, Kaluri R, Rajput DS, Srivastava G, Baker T. Analysis of dimensionality reduction
techniques on big data. IEEE Access. 2020;8:54776-88.

29. Scarselli F, Gori M, Tsoi AC, Hagenbuchner M, Monfardini G. The graph neural network model. IEEE Trans Neural
Netw. 2008;20(1):61-80.

30. Zhou J, Cui G, Zhang Z, Yang C, Liu Z, Wang L, LiC, Sun M. Graph neural networks: a review of methods and applica-
tions. arXiv preprint arXiv:1812.08434 2018.

31. Wu F, Zhang T, Souza AHd, Fifty C, Yu T, Weinberger KQ. Simplifying graph convolutional networks. In: 36th interna-
tional conference on machine learning, ICML 2019, 2019-June, 2019. p. 11884—94,

32. Tibshirani R. Regression shrinkage and selection via the lasso. J R Stat Soc Ser B (Methodol). 1996;58(1):267-88.

33. Mccallum A. Cora research paper classification dataset. people. cs. umass. edu/mccallum/data. html. KDD 2001.

34. LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proc IEEE.
1998;86(1 1):2278-324.

35. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436-44.

36. Defferrard M, Bresson X, Vandergheynst P. Convolutional neural networks on graphs with fast localized spectral
filtering. In: Advances in neural information processing systems. 2016. p. 3844-52

37. Shuman DI, Narang SK, Frossard P Ortega A, Vandergheynst P. The emerging field of signal processing on graphs:
extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Process Mag.
2013;30(3):83-98.

38. Sandryhaila A, Moura JM. Discrete signal processing on graphs. IEEE Trans Signal Process. 2013;61(7):1644—56.

39. NTH, Maehara T. Revisiting graph neural networks: All we have is low-pass filters. arXiv preprint arXiv:1905.09550
2019.

40. Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. In: 5th international confer-
ence on learning representations, ICLR 2017—conference track proceedings 2016.

41. Shchur O,Mumme M, Bojchevski A, GUnnemann S. Pitfalls of graph neural network evaluation. arXiv preprint arXiv
:1811.05868 2018.

42. Newman ME. Modularity and community structure in networks. Proc Nat! Acad Sci. 2006;103(23):8577-82.

43. Ketkar N. Introduction to pytorch. In: Deep learning with Python. Berlin: Springer; 2017. p. 195-208.

44, Bidoki NH, Mantzaris AV, Sukthankar G. Exploiting weak ties in incomplete network datasets using simplified graph
convolutional neural networks. Mach Learn Knowl Extract. 2020;2(2):125-46.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
