Lerique et al. Applied Network Science (2020) 5:5 . .
https://doi.org/10.1007/s41 109-019-0237-x Ap p | | ed N etwo rk SC lENCce

RESEARCH Open Access

Joint embedding of structure and ®
features via graph convolutional networks

updates
Sébastien Lerique!” ©, Jacob Levy Abitbol! and Marton Karsai!?

 

 

*Correspondence:

sebastien.lerique@normalesup.org Abstract

‘Univ Lyon, ENS de Lyon, Inria, The creation of social ties is largely determined by the entangled effects of people's
eens Don one 7088 similarities in terms of individual characters and friends. However, feature and structural
Full list of author information is characters of people usually appear to be correlated, making it difficult to determine

available at the end of the article which has greater responsibility in the formation of the emergent network structure.
We propose AN2VEC, a node embedding method which ultimately aims at
disentangling the information shared by the structure of a network and the features of
its nodes. Building on the recent developments of Graph Convolutional Networks
(GCN), we develop a multitask GCN Variational Autoencoder where different
dimensions of the generated embeddings can be dedicated to encoding feature
information, network structure, and shared feature-network information. We explore
the interaction between these disentangled characters by comparing the embedding
reconstruction performance to a baseline case where no shared information is
extracted. We use synthetic datasets with different levels of interdependency between
feature and network characters and show (i) that shallow embeddings relying on
shared information perform better than the corresponding reference with unshared
information, (ii) that this performance gap increases with the correlation between
network and feature structure, and (iii) that our embedding is able to capture joint
information of structure and features. Our method can be relevant for the analysis and
prediction of any featured network structure ranging from online social systems to
network medicine.

Keywords: Network embedding, Attributed networks, Graph, Convolutional networks,
Autoencoders, Feature-network dependency

 

Although it is relatively easy to obtain the proxy social network and various individual
features for users of online social platforms, the combined characterisation of these types
of information is still challenging our methodology. While current approaches have been
able to approximate the observed marginal distributions of node and network features
separately, their combined consideration was usually done via summary network statistics
merged with otherwise independently built feature sets of nodes. However, the entangle-
ment between structural patterns and feature similarities appears to be fundamental to a
deeper understanding of network formation and dynamics. The value of this joint infor-
mation then calls for the development of statistical tools for the learning of combined
representation of network and feature information and their dependencies.

© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0

G) S rin er O en International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and
pringer Op ral p/creaty ) es/by/4.0/), which pe and
— reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the
Creative Commons license, and indicate if changes were made.
Lerique et al. Applied Network Science (2020) 5:5 Page 2 of 24

The formation of ties and mesoscopic structures in online social networks is arguably
determined by several competing factors. Considering only network information, neigh-
bour similarities between people are thought to explain network communities, where
triadic closure mechanisms (Kumpula et al. 2007; Kossinets and Watts 2006) induce ties
between peers with larger fractions of common friends (Granovetter 1977). Meanwhile,
random bridges (Kossinets and Watts 2006) are built via focal closure mechanisms,
optimising the structure for global connectedness and information dissemination. At the
same time, people in the network can be characterised by various individual features such
as their socio-demographic background (Leo et al. 2016; Levy Abitbol et al. 2018b), lin-
guistic characters (Gumperz 2009; Levy Abitbol et al. 2018a; Levy Abitbol et al. 2018b), or
the distributions of their topics of interests (Cataldi et al. 2010; Hours et al. 2016), to men-
tion just a few. Such features generate homophilic tie creation preferences (McPherson et
al. 2001; Kossinets and Watts 2009), which induce links with higher probability between
similar individuals, whom in turn form feature communities of shared interest, age, gen-
der, or socio-economic status, and so on (Leo et al. 2016; Shrum et al. 1988). Though
these mechanisms are not independent and lead to correlations between feature and net-
work communities, it is difficult to define the causal relationship between the two: first,
because simultaneously characterising similarities between multiple features and a com-
plex network structure is not an easy task; second, because it is difficult to determine,
which of the two types of information, features or structure, is driving network forma-
tion to a greater extent. Indeed, we do not know what fraction of similar people initially
get connected through homophilic tie creation, versus the fraction that first get con-
nected due to structural similarities before influencing each other to become more similar
(La Fond and Neville 2010; Aral et al. 2009).

Over the last decade popular methods have been developed to characterise structural
and feature similarities and to identify these two notions of communities. The detec-
tion of network communities has been a major challenge in network science with various
concepts proposed (Blondel et al. 2008; Rosvall et al. 2009; Peixoto 2014) to solve it as
an unsupervised learning task (Fortunato 2010; Fortunato and Hric 2016). Commonly,
these algorithms rely solely on network information, and their output is difficult to cross-
examine without additional meta-data, which is usually disregarded in their description.
On the other hand, methods grouping similar people into feature communities typically
ignore network information, and exclusively rely on individual features to solve the prob-
lem as a data clustering challenge (Jain 2010; Gan et al. 2007). Some semi-supervised
learning tasks, such as link prediction, may take feature and structural information simul-
taneously into account, but only by enriching individual feature vectors with node-level
network characteristics such as degree or local clustering (Liben-Nowell and Kleinberg
2007; Li and Zhou 2011; Hours et al. 2016). Methods that would take higher order net-
work correlations and multivariate feature information into account at the same time are
still to be defined. Their development would offer huge potential in understanding the
relation between individuals’ characters, their social relationships, the content they are
engaged with, and the larger communities they belong to. This would not only provide
us with deeper insight about social behaviour, it would give us predictive tools for the
emergence of network structure, individual interests and behavioural patterns.

In this paper we propose a contribution to solve this problem by developing a joint
feature-network embedding built on multitask Graph Convolutional Networks (Kipf
Lerique et al. Applied Network Science (2020) 5:5 Page 3 of 24

and Welling 2016a; Bruna et al. 2013; Hamilton et al. 2017; Ying et al. 2018) and
Variational Autoencoders (GCN-VAE) (Kingma and Welling 2013; Rezende et al. 2014;
Kipf and Welling 2016b; Wang et al. 2016; Zhu et al. 2018), which we call the Attributed
Node to Vector method (AN2VEC). In our model, different dimensions of the gener-
ated embeddings can be dedicated to encode feature information, network structure,
or shared feature-network information separately. Unlike previous embedding methods
dealing with features (Gao and Huang 2018; Tran 2018; Shen et al. 2018; Bojchevski and
Giinnemann 2017), this interaction model (Aiken et al. 1994) allows us to explore the
dependencies between the disentangled network and feature information by comparing
the embedding reconstruction performance to a baseline case where no shared infor-
mation is extracted. Using this method, we can identify an optimal reduced embedding,
which indicates whether combined information coming from the structure and features is
important, or whether their non-interacting combination is sufficient for reconstructing
the featured network.

In practice, as this method solves a reconstruction problem, it may give important
insights about the combination of feature- and structure-driven mechanisms which deter-
mine the formation of a given network. As an embedding, it is useful to identify people
sharing similar individual and structural characters. And finally, by measuring the optimal
overlap between feature— and network-associated dimensions, it can be used to verify net-
work community detection methods to see how well they identify communities explained
by feature similarities.

In what follows, after summarising the relevant literature, we introduce our method
and demonstrate its performance on synthetic featured networks, for which we con-
trol the structural and feature communities as well as the correlations between the two.
As a result, we will show that our embeddings, when relying on shared information,
outperform the corresponding reference without shared information, and that this per-
formance gap increases with the correlation between network and feature structure since
the method can capture the increased joint information. Next, we extensively explore the
behaviour of our model on link prediction and node classification on standard benchmark
datasets, comparing it to well-known embedding methods. Finally, we close our paper
with a short summary and a discussion about potential future directions for our method.

Related work

The advent of increasing computational power coupled with the continuous release and
ubiquity of large graph-structured datasets has triggered a surge of research in the field
of network embeddings. The main motivation behind this trend is to be able to convert
a graph into a low-dimensional space where its structural information and properties are
maximally preserved (Cai et al. 2018). The aim is to extract unseen or hard to obtain
properties of the network, either directly or by feeding the learned representations to a

downstream inference pipeline.

Graph embedding survey: from matrix factorisation to deep learning

In early work, low-dimensional node embeddings were learned for graphs constructed
from non-relational data by relying on matrix factorisation techniques. By assuming
that the input data lies on a low dimensional manifold, such methods sought to reduce

the dimensionality of the data while preserving its structure, and did so by factorising
Lerique et al. Applied Network Science (2020) 5:5 Page 4 of 24

graph Laplacian eigenmaps (Tenenbaum et al. 2000) or node proximity matrices
(Cao et al. 2015).

More recent work has attempted to develop embedding architectures that can use deep
learning techniques to compute node representations. DeepWalk (Perozzi et al. 2014), for
instance, computes node co-occurrence statistics by sampling the input graph via trun-
cated random walks, and adopts a SkipGram neural language model to maximise the
probability of observing the neighbourhood of a node given its embedding. By doing so
the learned embedding space preserves second order proximity in the original graph.
However, this technique and the ones that followed (Grover and Leskovec 2016; Li et
al. 2017) present generalisation caveats, as unobserved nodes during training cannot be
meaningfully embedded in the representation space, and the embedding space itself can-
not be generalised between graphs. Instead of relying on random walk-based sampling of
graphs to feed deep learning architectures, other approaches have used the whole network
as input to autoencoders in order to learn, at the bottleneck layer, an efficient representa-
tion able to recover proximity information (Wang et al. 2016; Cao et al. 2016; Tran 2018).
However, the techniques developed herein remained limited due to the fact that success-
ful deep learning models such as convolutional neural networks require an underlying

euclidean structure in order to be applicable.

Geometric deep learning survey: defining convolutional layers on non-euclidean domains
This restriction has been gradually overcome by the development of graph convolutions
or Graph Convolutional Networks (GCN). By relying on the definition of convolutions
in the spectral domain, Bruna et al. (2013) defined spectral convolution layers based on
the spectrum of the graph Laplacian. Several modifications and additions followed and
were progressively added to ensure the feasibility of learning on large networks, as well
as the spatial localisation of the learned filters (Bronstein et al. 2017; Ying et al. 2018). A
key step is made by (Defferrard et al. 2016) with the use of Chebychev polynomials of the
Laplacian, in order to avoid having to work in the spectral domain. These polynomials,
of order up to r, generate localised filters that behave as a diffusion operator limited to r
hops around each vertex. This construction is then further simplified by Kipf and Welling
by assuming among others that r © 2 (Kipf and Welling 2016a).

Recently, these approaches have been extended into more flexible and scalable
frameworks. For instance, Hamilton et al. (Hamilton et al. 2017) extended the original
GCN framework by enabling the inductive embedding of individual nodes, training a set
of functions that learn to aggregate feature information from a node’s local neighborhood.
In doing so, every node defines a computational graph whose parameters are shared for
all the graphs nodes.

More broadly, the combination of GCN with autoencoder architectures has proved
fertile for creating new embedding methods. The introduction of probabilistic node
embeddings, for instance, has appeared naturally from the application of variational
autoencoders to graph data (Rezende et al. 2014; Kingma and Welling 2013; Kipf and
Welling 2016b), and has since led to explorations of the uncertainty of embeddings
(Bojchevski and Giinnemann 2017; Zhu et al. 2018), of appropriate levels of disentangle-
ment and overlap (Mathieu et al. 2018), and of better representation spaces for measuring
pairwise embedding distances (see in particular recent applications of the Wasserstein
distance between probabilistic embeddings, Zhu et al. 2018;Muzellec and Cuturi 2018).
Lerique et al. Applied Network Science (2020) 5:5 Page 5 of 24

Such models consistently outperform earlier techniques on different benchmarks and
have opened several interesting lines of research in fields ranging from drug design
(Duvenaud et al. 2015) to particle physics (Kipf et al. 2018). Most of the more recent
approaches mentioned above can incorporate node features (either because they rely on
them centrally, or as an add-on). However, with the exception of DANE (Gao and Huang
2018), they mostly do so by assuming that node features are an additional source of
information, which is congruent with the network structure (e.g. multi-task learning with
shared weights, Tran 2018), or fusing both information types together (Shen et al. 2018).
That assumption may not hold in many complex datasets, and it seems important to
explore what type of embeddings can be constructed when we lift it, considering different
levels of congruence between a network and the features of its nodes.

We therefore set out to make a change to the initial GCN-VAE in order to: (i) create
embeddings that are explicitly trained to encode both node features and network struc-
ture; (ii) make it so that these embeddings can separate the information that is shared
between network and features, from the (possibly non-congruent) information that is spe-
cific to either network or features; and (iii) be able to tune the importance that is given to
each type of information in the embeddings.

Methods

In this section we present the architecture of the neural network model we use to generate
shared feature-structure node embeddings.! We take a featured network as input, with
structure represented as an adjacency matrix and node features represented as vectors
(see below for a formal definition). Our starting point is a GCN-VAE, and our first goal
is a multitask reconstruction of both node features and network adjacency matrix. Then,
as a second goal, we tune the architecture to be able to scale the number of embedding
dimensions dedicated to feature-only reconstruction, adjacency-only reconstruction, or
shared feature-adjacency information, while keeping the number of trainable parameters
in the model constant.

Multitask graph convolutional autoencoder

We begin with the graph-convolutional variational autoencoder developed by (Kipf and
Welling 2016b), which stacks graph-convolutional (GC) layers (Kipf and Welling 2016a)
in the encoder part of a variational autoencoder (Rezende et al. 2014; Kingma and Welling
2013) to obtain a lower dimensional embedding of the input structure. This embedding
is then used for the reconstruction of the original graph (and in our case, also of the
features) in the decoding part of the model. Similarly to Kipf and Welling (2016a), we
use two GC layers in our encoder and generate Gaussian-distributed node embeddings at
the bottleneck layer of the autoencoder. We now introduce each phase of our embedding

method in formal terms.

Encoder

We are given an undirected unweighted featured graph G = (V,€), with N = |V| nodes,
each node having a D-dimensional feature vector. Loosely following the notations of Kipf
and Welling (2016b), we note A the graph’s N x N adjacency matrix (diagonal elements
set to 0), X the N x D matrix of node features, and X; the D-dimensional feature vector
of a node i.

 

!The implementation of our model is available online at github.com/ixxi-dante/an2vec.
Lerique et al. Applied Network Science (2020) 5:5 Page 6 of 24

The encoder part of our model is where F-dimensional node embeddings are generated.
It computes “« and o, two N x F matrices, which parametrise a stochastic embedding of

each node:
fu = GCN, (X,A) and logo = GCNg(X, A).

Here we use two graph-convolutional layers for each parameter set, with shared weights

at the first layer and parameter-specific weights at the second layer:
GCN, (X, A) = AReLU(AXW6") Wa

In this equation, Wj" and Wy/r are the weight matrices for the linear transforma-
tions of each layer’s input; ReLU refers to a rectified linear unit (Nair and Hinton 2010);
and following the formalism introduced in Kipf and Welling (2016a), A is the standard

normalised adjacency matrix with added self-connections, defined as:

>>
|

2AD-3
In

>
|

D~
A+

j
where Iy is the N x N identity matrix.

Embedding
The parameters «& and o produced by the encoder define the distribution of an

F-dimensional stochastic embedding &; for each node i, defined as:
—|A,X~N (Mj, diag (o?))

Thus, for all the nodes we can write a probability density function over a given set of

embeddings &, in the form of an N x F matrix:

N
q(é|X, A) = | [ q(é,A,X).

i=1
Decoder
The decoder part of our model aims to reconstruct both the input node features and the
input adjacency matrix by producing parameters of a generative model for each of the
inputs. On one hand, the adjacency matrix A is modelled as a set of independent Bernoulli
random variables, whose parameters come from a bilinear form applied to the output of

a single dense layer:

Ay|§;, §; ~ Ber(MLB(é),)
MLB(é) = sigmoid(y Pw iY)

y =ReLU (ews) ‘

Similarly to above, wees is the weight matrix for the first adjacency matrix decoder
layer, and wie is the weight matrix for the bilinear form which follows.

On the other hand, features can be modelled in a variety of ways, depending on whether
they are binary or continuous, and if their norm is constrained or not. Features in our
experiments are one-hot encodings, so we model the reconstruction of the feature matrix

X by using N single-draw D-categories multinomial random variables. The parameters
Lerique et al. Applied Network Science (2020) 5:5

of those multinomial variables are computed from the embeddings with a two-layer

perceptron:”

X;|&; ~ Multinomial(1, MLP(&),)
MLP(&) = softmax (ReLU (§ ws) we")

 

In the above equations, sigmoid(z) = j,-= refers to the logistic function applied
element-wise on vectors or matrices, and softmax(z); = = Fi refers to the normalised
i

exponential function, also applied element-wise, with j running along the rows of matrices
(and along the indices of vectors).
Thus we can write the probability density for a given reconstruction as:

P(X, Al§) = p(Alé )p(Xl§)

N
piAl§) = I] MLB(é);/ (1 _ MLB(é)j)!~4¥
ij=1
N D .,
p(xlé) = [|] [ MLP@);,’
i=1 j=1

Learning
The variational autoencoder is trained by minimising an upper bound to the marginal
likelihood-based loss (Rezende et al. 2014) defined as:

— log p(A, X) < L(A, X)
= Dx (q(|A, X)|IN ©, Ir))
— Eqeja.x)[log(p(A, X1g, 0)p@))]
=Lert+Latlx + Lo
Here Lyxz is the Kullback-Leibler divergence between the distribution of the embed-
dings and a Gaussian prior, and @ is the vector of decoder parameters whose associated
loss Lp acts as a regulariser for the decoder layers.* Computing the adjacency and feature
reconstruction losses by using their exact formulas is computationally not tractable, and
the standard practice is instead to estimate those losses by using an empirical mean. We
generate K samples of the embeddings by using the distribution q(&|A, X) given by the

encoder, and average the losses of each of those samples* (Rezende et al. 2014; Kingma
and Welling 2013):

La = —Ege|a,x) [log p(Alé, 9)]
K N

-2 dX » i log (me (: *),)

~1ij=1

+ (1— Ay) log (1 — MLB () )]
ij

I?

 

Other types of node features are modelled according to their constraints and domain. Binary features are modelled as
independent Bernoulli random variables. Continuous-range features are modelled as Gaussian random variables in a
similar way to the embeddings themselves.

3Indeed, following (Rezende et al. 2014) we assume 0 ~ N(0, xg), such that

Lo = —logp() = 5 dim() log(2xK9) + 5+ |10|I3-

“In practice, K = 1 is often enough.

Page 7 of 24
Lerique et al. Applied Network Science (2020) 5:5 Page 8 of 24

Lx = —Egeja,x)[log p(XIé, 4)]
LAX 2
~ . (k)
~-2 OYE} rylog (me (§ ),)
k=1 i=1 j=1
Finally, for diagonal Gaussian embeddings such as the ones we use, Lx, can be
expressed directly (Kingma and Welling 2013):

N F
1
Le = 5 Soy Ku +o; — 2 log oi —]
i=1 j=1

Loss adjustments

In practice, to obtain useful results a few adjustments are necessary to this loss function.
First, given the high sparsity of real-world graphs, the Aj; and 1—Aj, terms in the adjacency
loss must be scaled respectively up and down in order to avoid globally near-zero link
reconstruction probabilities. Instead of penalising reconstruction proportionally to the
overall number of errors in edge prediction, we want false negatives (Aj terms) and false
positives (1 — Aj terms) to contribute equally to the reconstruction loss, independent of
dij

Aj ; re ;
a Cn denote the density of the graph’s adjacency matrix

(d= At x density(G)); then we replace La by the following re-scaled estimated loss (the

graph sparsity. Formally, let d =
so-called “balanced cross-entropy”):

x= 22 [ At t0g (ute (e
=e LYalF 08 ( (§ ),)

Y

1 Ay (k)
+ Fog (1 - MB (6 ),

Second, we correct each component loss for its change of scale when the shapes of

 

the inputs and the model parameters change: £xz is linear in N and F, La is quadratic
in N, and Lyx is linear in N (but not in F, remember that i Xj = 1 since each X; is a
single-draw multinomial).

Beyond dimension scaling, we also wish to keep the values of La and Lx comparable
and, doing so, maintain a certain balance between the difficulty of each task. As a first
approximation to the solution, and in order to avoid more elaborate schemes which would
increase the complexity of our architecture (such as Chen et al. 2018), we divide both loss
components by their values at maximum uncertainty,’ respectively log 2 and log D.

Finally, we make sure that the regulariser terms in the loss do not overpower the actual
learning terms (which are now down-scaled close to 1) by adjusting «g and an additional
factor, Kxz, which scales the Kullback-Leibler term.© These adjustments lead us to the
final total loss the model is trained for:

La Lx LEL HA 115
~ N2log2 | NlogD ' NE«x, | 2ke

where we have removed constant terms with respect to trainable model parameters.

 

 

 

Scaling shared information allocation
The model we just presented uses all dimensions of the embeddings indiscrimi-

nately to reconstruct the adjacency matrix and the node features. While this can

 

°That is, p(Aj|é,0) = 4 Vij, and p(Xyl&,0) = 4 Vij.
We use kx, = 2kKg = 10°.
Lerique et al. Applied Network Science (2020) 5:5 Page 9 of 24

be useful in some cases, it cannot adapt to different interdependencies between
graph structure and node features; in cases where the two are not strongly corre-
lated, the embeddings would lose information by conflating features and graph struc-
ture. Therefore our second aim is to adjust the dimensions of the embeddings used
exclusively for feature reconstruction, or for adjacency reconstruction, or used for
both.

In a first step, we restrict which part of a node’s embedding is used for each task. Let Fa
be the number of embedding dimensions we will allocate to adjacency matrix reconstruc-
tion only, Fx the number of dimensions allocated to feature reconstruction only, and Fax
the number of dimensions allocated to both. We have Fa + Fax + Fx = F. We further
introduce the following notation for the restriction of the embedding of node i to a set of
dedicated dimensions {a,..., b}:’

E iab = (Sif) je{a,...b}

This extends to the full matrix of embeddings similarly:

Ea.b = (Eij)ie{1,..,N}j€la,...b}

Using these notations we adapt the decoder to reconstruct adjacency and features as
follows:

Aijl§ it:F,-+Fax? §j,1:Fa+Fax ~ Ber(MLB(§ 1.7, +r, ij)
Xil§ pF. 41:F ~ Multinomial(1, MLP€& p, +1:F)i)

In other words, adjacency matrix reconstruction relies on Fa + Fax embedding
dimensions, feature reconstruction relies on Fx + Fax dimensions, and Fax overlap-
ping dimensions are shared between the two. Our reasoning is that for datasets where
the dependency between features and network structure is strong, shallow models with
higher overlap value will perform better than models with the same total embedding
dimensions F and less overlap, or will perform on par with models that have more total
embedding dimensions and less overlap. Indeed, the overlapping model should be able
to extract the information shared between features and network structure and store it
in the overlapping dimensions, while keeping the feature-specific and structure-specific
information in their respective embedding dimensions. This is to compare to the non-
overlapping case, where shared network-feature information is stored redundantly, both
in feature- and structure-specific embeddings, at the expense of a larger number of
distinct dimensions.

Therefore, to evaluate the performance gains of this architecture, one of our measures
is to compare the final loss for different hyperparameter sets, keeping Fa + Fax and
Fx + Fax fixed and varying the overlap size Fax. Now, to make sure the training losses
for different hyperparameter sets are comparable, we must maintain the overall number
of trainable parameters in the model fixed. The decoder already has a constant num-
ber of trainable parameters, since it only depends on the number of dimensions used for
decoding features (Fx + Fax) and adjacency matrix (Fa + Fax), which are themselves
fixed.

 

7Note that the order of the indices does not change the training results, as the model has no notion of ordering inside its
layers. What follows is valid for any permutation of the dimensions, and the actual indices only matter to downstream
interpretation of the embeddings after training.
Lerique et al. Applied Network Science (2020) 5:5 Page 10 of 24

On the other hand, the encoder requires an additional change. We maintain the dimen-
sions of the encoder-generated « ando parameters fixed at Fa +2Fax+Fyx (independently
from Fax, given the constraints above), and reduce those outputs to Fa +Fax+Fx dimen-
sions by averaging dimensions {Fa +1,...,Fa + Fax} and {Fa + Fax +1,...,Fa +2Fax}
together.® In turn, this model maintains a constant number of trainable parameters,
while allowing us to adjust the number of dimensions Fax shared by feature and adja-
cency reconstruction (keeping Fa + Fax and Fx + Fax constant). Figure 1 schematically
represents this architecture.

Results

We are interested in measuring two main effects: first, the variation in model performance
as we increase the overlap in the embeddings, and second, the capacity of the embeddings
with overlap (versus no overlap) to capture and benefit from dependencies between graph
structure and node features. To that end, we train overlapping and non-overlapping mod-
els on synthetic data with different degrees of correlation between network structure and
node features.

Synthetic featured networks

We use a Stochastic Block Model (Holland et al. 1983) to generate synthetic featured
networks, each with M communities of nm = 10 nodes, with intra-cluster connection
probabilities of 0.25, and with inter-cluster connection probabilities of 0.01. Each node is
initially assigned a colour which encodes its feature community; we shuffle the colours
of a fraction 1 — a of the nodes, randomly sampled. This procedure maintains constant
the overall count of each colour, and lets us control the correlation between the graph
structure and node features by moving a from 0 (no correlation) to 1 (full correlation).

Node features are represented by a one-hot encoding of their colour (therefore, in all
our scenarios, the node features have dimension M = N/n). However, since in this case
all the nodes inside a community have exactly the same feature value, the model can have
difficulties differentiating nodes from one another. We therefore add a small Gaussian
noise (0 = .1) to make sure that nodes in the same community can be distinguished from
one another.

Note that the feature matrix has less degrees of freedom than the adjacency matrix in
this setup, a fact that will be reflected in the plots below. However, opting for this minimal
generative model lets us avoid the parameter exploration of more complex schemes for
feature generation, while still demonstrating the effectiveness of our model.

Comparison setup
To evaluate the efficiency of our model in terms of capturing meaningful correlations
between network and features, we compare overlapping and non-overlapping models as

 

8Formally:

~ 1
h= iF, I 2 (Mey 41:F, +Fax + MES + Fay +1:Fq +2Eax )
Il ME, +2F 4x -+1:Fy +2F ax +Fx
~ 1
loga = log 01:64 I 2 (log O Fa+1:Fa+Fax + log OF, +Fax +1:Fa+2Fax )

|| log OF, +2Fqx-+1:Fa +2Fax +Ex

where || denotes concatenation along the columns of the matrices.
Lerique et al. Applied Network Science (2020) 5:5 Page 11 of 24

 

 

QD TF Q) “Weighted ZB |
oa Uap A I Dense Bilinear
_ Nu. 2°) ~ Dense Dense
x 4 .

QO Y-

 

 

 

node features

aa

wn
oO
oc
aS
yp
oO
oO
we
oO
To
oO
c

| | | | |
Encoder Sampler Embeddings Decoder

Fig. 1 Diagram of the overlapping embedding model we propose. Red and blue blocks with a layer name
(GC, Dense, Weighted Bilinear) indicate actual layers, with their activation function depicted to the right as a
curve in a green circle (either ReLU or sigmoid). Red blocks concern processing for the adjacency matrix, blue
blocks processing for the node features. The encoder is made of four parallel GC pipelines producing ja, Lx,
log og and log ox (the last two being grayed out in the background). Their output is then combined to create
the overlap, then used by the sampler to create the node embeddings. The decoder processes parts of the
node embeddings and separately reconstructs the adjacency matrix (top) and the node features (bottom)

 

 

 

 

 

follows. For a given maximum number of embedding dimensions Fy,4,, the overlapping
models keep constant the number of dimensions used for adjacency matrix reconstruc-
tion and the number of dimensions used for feature reconstruction, with the same amount
allocated to each task: FaY + Fay = Fy + Fay = 5 Emax: However they vary the over-
lap Fx from 0 to 5 Emax by steps of 2. Thus the total number of embedding dimensions
F varies from Fyyqx to 5 Emax» and as F decreases, Fy increases. We call one such model
Mer.

Now for a given overlapping model M®’, we define a reference model Mrs , which
has the same total number of embedding dimensions, but without overlap: Be = 0,
and Fo = Fe = 5E (explaining why we vary F with steps of 2). Note that while the
reference model has the same information bottleneck as the overlapping model, it has less
trainable parameters in the decoder, since Fo + Fa = Fe + Fa = SE will decrease
as F decreases. Nevertheless, this will not be a problem for our measures, since we will
be mainly looking at the behaviour of a given model for different values of a (i.e. the
feature-network correlation parameter).

For our calculations (if not noted otherwise) we use synthetic networks of N = 1000
nodes (i.e. 100 clusters), and set the maximum embedding dimensions F,,g, to 20. For all
models, we set the intermediate layer in the encoder and the two intermediate layers in
the decoder to an output dimension of 50, and the internal number of samples for loss
estimation at K = 5. We train our models for 1000 epochs using the Adam optimiser
(Kingma and Ba 2014) with a learning rate of 0.01 (following Kipf and Welling 2016b),
after initialising weights following Glorot and Bengio (2010). For each combination of
F and a, the training of the overlapping and reference models is repeated 20 times on
independent featured networks.

Since the size of our synthetic data is constant, and we average training results over
independently sampled data sets, we can meaningfully compare the averaged training
losses of models with different parameters. We therefore take the average best training
loss of a model to be our main measure, indicating the capacity to reconstruct an input

data set for a given information bottleneck and embedding overlap.
Lerique et al. Applied Network Science

(2020) 5:5

Advantages of overlap
Absolute loss values

Figure 2 shows the variation of the best training loss (total loss, adjacency reconstruction
loss, and feature reconstruction loss) for both overlapping and reference models, with a
ranging from 0 to 1 and F decreasing from 20 to 10 by steps of 2. One curve in these plots
represents the variation in losses of a model with fixed F for data sets with increasing cor-
relation between network and features; each point aggregates 20 independent trainings,
used to bootstrap 95% confidence intervals.

 

 

 

With overlap Without overlap
925 -
B40 ae (a) eae oe (b) F
. ~e
ee 900 - =e —20
“Ss Tre —18
R820 eee x B75 po “on iome —16
- ~N ~
S SN _ anne —14
—_ s s Sen
® g00- Sots B50 ~ eames = te
3 en \. ~ ene. ~ 10
E e SS ~~ 2 ~ . ae ~~
wv one os . Os “A ‘Sy 825 D Sees ~
5 780 ea ean, =
— se Se SO sy N __. Se .
D oe eS ~ “a, ~ 800- * SSSeEESg * =
Oo ~ ~ De Ze, \ Se ===, e~_
<x a Ss 5 a
760 i “Ne Ba SS SSS, <..
Tae SSE a
“eS, 750- —a
740 L L 1 L L 1 L L 1 L 1 L L 1 L L 1 L 1 1
880 -
. (0) en.
se a
800 - Omg 860 - STEEL
N @-——~ ~¢—_-~¢-_-__ ~
g - 840
oe ». we ie
> Teme SS rs
< 780- \ ‘ 820 / aes
VO a *, ——
e---*~_ 1, sy ‘SS <==.
aie “a ~ . x ~ L —— ee
Z - N > a ~ ‘sy 800 a ~ ==
760 | Sens Sar SS : oe
w SMe SF 780--, me
2 _ — ~el ~ Ne SC ~ WA n ng ne =
Oo “- SON YS a se =
a <> oS. 760- “era me
© SS — = Tee ~e
q /40- sa eet. le
TPs es 740- ee,
720 -
5.0-
= (e) 7-
eta Onno On
4.5 gee eee mS “
wn tte ~s \
O *s, *Se 6 [ %
ee ene Se .
— 4.0 o a ~ we x.
Oo women ng 2. we Ne 5 | Sana ‘S
2 3 5 ea ~~ Ne mae “Sy TTL ~,
© 0 Vly SSN SS a “AL \.
sAQ ~~ N b SS
v 3 0 SSSESS==5 Se Me TRS. ~ 4- @---e__ ‘~AY ‘ew
oO QV ye Se ‘~ =, ~Y aoa a Sy
wo ve _~ ~ ~ Sy Sy en ON SS Dr ~
~ x Nx S ~ ~ xn ——
3 2.5- “SSS te te Sy “ey : Sirre-- 3 S “ss x mA ~
“ ™~ ~ “Se ~~ ~ 3° OSS SS _— se, .
a 2 0 i ~ ~ mS ~ . “Sen ease SS ~»
. SC SK e, yy ae, TS *% >.
‘ey Sse, SSS 2- a a eg
AL » “322 et
1.5- Ss “SRE
‘e
0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8
a a
Fig. 2 Absolute training loss values of overlapping and reference models. The curve colours represent the
total embedding dimensions F, and the x axis corresponds to feature-network correlation. The top row is the
total loss, the middle row is the adjacency matrix reconstruction loss and the bottom row is the feature
reconstruction loss. The left column shows overlapping models, and the right column shows reference
non-overlapping models

 

 

Page 12 of 24
Lerique et al. Applied Network Science (2020) 5:5 Page 13 of 24

We first see that all losses, whether for overlapping model or reference, decrease as we
move from the uncorrelated scenario to the correlated scenario. This is true despite the
fact that the total loss is dominated by the adjacency reconstruction loss, as feature recon-
struction is an easier task overall. Second, recall that the decoder in a reference model
has less parameters than its corresponding overlapping model of the same F dimensions
(except for zero overlap), such that the reference is less powerful and produces higher
training losses. The absolute values of the losses for overlap and reference models are
therefore not directly comparable. However, the changes in slopes are meaningful. Indeed,
we note that the curve slopes are steeper for models with higher overlap (lower F) than
for lower overlap (higher F), whereas they seem relatively independent for the reference
models of different F. In other words, as we increase the overlap, our models seem to ben-
efit more from an increase in network-feature correlation than what a reference model

benefits.

Relative loss disadvantage
In order to assess this trend more reliably, we examine losses relative to the maximum

embedding models. Figure 3 plots the loss disadvantage that overlap and reference models
EMe EM emax

have compared to their corresponding model with F = Fyyj,q,, that is, lu . We
Fmax
call this the relative loss disadvantage of a model. In this plot, the height of a curve thus
ov|ref

represents the magnitude of the decrease in performance of a model M,.°* relative to
the model with maximum embedding size, Met . Note that for both the overlap model
and the reference model, moving along one of the curves does not change the number of
trainable parameters in the model.

As the correlation between network and features increases, we see that the relative
loss disadvantage decreases in overlap models, and that the effect is stronger for higher
overlaps. In other words, when the network and features are correlated, the overlap cap-
tures this joint information and compensates for the lower total number of dimensions
(compared to Me"): the model achieves a better performance than when network and
features are more independent. Strikingly, for the reference model these curves are flat,
thus indicating no variation in relative loss disadvantage with varying network-feature
correlations in these cases. This confirms that the new measure successfully controls
for the baseline decrease of absolute loss values when the network-features correlation
increases, as observed in Fig. 2. Our architecture is therefore capable of capturing and
taking advantage of some of the correlation by leveraging the overlap dimensions of the
embeddings.

Finally note that for high overlaps, the feature reconstruction loss value actually
increases a little when a grows. The behaviour is consistent with the fact that the total
loss is dominated by the adjacency matrix loss (the hardest task). In this case it seems that
the total loss is improved more by exploiting the gain of optimising for adjacency matrix
reconstruction, and paying the small cost of a lesser feature reconstruction, than decreas-
ing both adjacency matrix and feature losses together. If wanted, this strategy could be
controlled using a gradient normalisation scheme such as Chen et al. (2018).

Standard benchmarks
Finally we compare the performance of our architecture to other well-known embedding
methods, namely spectral clustering (SC) (Tang and Liu 2011), DeepWalk (DW) (Perozzi
 

 

Lerique et al. Applied Network Science (2020) 5:5
With overlap Without overlap
F
a oot --e~L b
ooT (@) oa75-- te) FE
tenn” ‘\ —18
NX
W 0.06 - Sr 0.150- —16
Ww SS = —
oO Ss ~~~. ---7 <> yawngy Prem 2 14
— 0.05> “Ser 0.125 - TORRE OS ge —12
6 oe Sa 10
© 0.04- a — 0.100 -
~A Ss -e— -®
- SC _. we e@---e----e-~ “7@—=-@ —--@ —---e~~¢ -~
~-- -— x
© W- oN man e---e a 2 2 S
—_— L » SA Ne -—— = -" "3. -—-0--— 0-—— »9—~_ »-~
@ 0.02- pore "= *0.050- = . °
a “ oe me
SA =o 2S “ee
ea Feet SELF 0.025 - Saas STrreccctnnngaentonsenn=4
0.00 - ae a - ------------+--- +--+. -_ ae
. 0.000 ~~ ean a te teen
0.07 - 0.175 -
(c) _(d)
Cm @-—-—@---e.~__,. __-e---e---¢
yy 0:06 Pern tana 0.150-
XN
o SY
— 0.05- ae ; ants
> ~”Y 0.125 ~~... ---©7 SA Une en
UO SS
Cc L en
G 0.04- we 0.100 -
x x -®
ry s s --@ ——~¢ —---e ~~~» —--e---e-~
— ~ ~w @-——~¢ —_-¢- o- o- or
g 9.03 Soe ~~ 0.075 -
©. n4| 9 »eene--- o---e_ - _-
> 0.02 - ooo e e oe os 0.0501 anne nn ne
© one ee seal Lr .
© 0.01- = a SS see
ac “le AT OE 0.025 - Site enna tenn tne geen trncencr®
7 ~~ 1-7 ~ SA ,
0.00 - 2. -_-..___.-__-.___.. ae
0.000 - tr re nt te tt 8
1.07 1.6-
_ ) (f)
. wo 1.4- reer
Ww 0.8- 7 Ta yZ ia X
3 J 1.2 psa Ss
_l La oe” Sel
) cere Heo L Drag eet
5 0.6- oo ee poceaees e 1.0 =
aoe) -e- o
© : L
WM po One 0H-- Ue o 0.8
04-0 Je centr
$ penn gen 0.6--°>~ STE tne enger NR
-@~—~_ 4 —---*~
_ eu _eoo- ere ° "=~ 0.4- Spee
a 0.2 - Ga m * a~ eo Tome nee
o se ~ —_---@
eo He - mee ~e---*--~., 0.2 - piensa se eee —-
on Cao e--~e--_
ee OT Te Hg *
0.0 - rn rt 0 8 8 0.0 - one 0 oo 0 eee
0 0.2 0.4 0.6 0.8 0 0.2 0.4 0.6 0.8
a a
Fig. 3 Relative loss disadvantage for overlapping and reference models. The curve colours represent the
total embedding dimensions F, and the x axis corresponds to feature-network correlation. The top row is the
total loss, the middle row is the adjacency matrix reconstruction loss and the bottom row is the feature
reconstruction loss. The left column shows overlapping models, and the right column shows reference
non-overlapping models. See main text for a discussion

 

 

 

et al. 2014), the vanilla non-variational and variational Graph Autoencoders (GAE and
VGAE) (Kipf and Welling 2016b), and GraphSAGE (Hamilton et al. 2017) which we look
at in more detail. We do so on two tasks: (i) the link prediction task introduced by Kipf
and Welling (2016b) and (ii) a node classification task, both on the Cora, CiteSeer and
PubMed datasets, which are regularly used as citation network benchmarks in the litera-
ture (Sen et al. 2008; Namata et al. 2012). Note that neither SC nor DW support feature

information as an input.

The Cora and CiteSeer datasets are citation networks made of respectively 2708 and

3312 machine learning articles, each assigned to a small number of document classes (7

Page 14 of 24
Lerique et al. Applied Network Science (2020) 5:5 Page 15 of 24

for Cora, 6 for CiteSeer), with a bag-of-words feature vector for each article (respectively
1433 and 3703 words). The PubMed network is made of 19717 diabetes-related articles
from the PubMed database, each assigned to one of three classes, with article feature
vectors containing term frequency-inverse document frequency (TF/IDF) scores for 500

words.

Link prediction

The link prediction task consists in training a model on a version of the datasets where
part of the edges has been removed, while node features are left intact. A test set is formed
by randomly sampling 15% of the edges combined with the same number of random dis-
connected pairs (non-edges). Subsequently the model is trained on the remaining dataset
where 15% of the real edges are missing.

We pick hyperparameters such that the restriction of our model to VGAE would match
the hyperparameters used by Kipf and Welling (2016b). That is a 32-dimensions inter-
mediate layer in the encoder and the two intermediate layers in the decoder, and 16
embedding dimensions for each reconstruction task (Fa + Fax = Fx +Fax = 16). We call
the zero-overlap and the full-overlap versions of this model AN2VEC-0 and AN2VEC-16
respectively. In addition, we test a variant of these models with a shallow adjacency matrix
decoder, consisting of a direct inner product between node embeddings, while keeping
the two dense layers for feature decoding. Formally: Aj|§;,§; ~ Ber(sigmoid(§ r §;)). This
modified overlapping architecture can be seen as simply adding the feature decoding and
embedding overlap mechanics to the vanilla VGAE. Consistently, we call the zero-overlap
and full-overlap versions AN2VEC-S-0 and AN2VEC-S-16.

We follow the test procedure laid out by Kipf and Welling (2016b): we train for 200
epochs using the Adam optimiser (Kingma and Ba 2014) with a learning rate of .01, ini-
tialise weights following Glorot and Bengio (2010), and repeat each condition 10 times.
The j parameter of each node’s embedding is then used for link prediction (i.e. the
parameter is put through the decoder directly without sampling), for which we report
area under the ROC curve and average precision scores in Table 1.’

We argue that AN2VEC-0 and AN2VEC-16 should have somewhat poorer performance
than VGAE. These models are required to reconstruct an additional output, which is not

Table 1 Link prediction task in citation networks

 

 

 

Method Cora CiteSeer PubMed
AUC AP AUC AP AUC AP

SC 84.6 £0.01 88.5 +0.00 80.5 £0.01 85.0 £0.01 84.2 +0.02 87.8 £0.01

DW 83.1 £0.01 85.0 +0.00 80.5 40.02 83.6 £0.01 84.4 +0.00 84.1 +£0.00
GAE 91.0 £0.02 92.0 0.03 89.5 +0.04 89.9 £0.05 96.4 +0.00 96.5 +0.00
VGAE 91.4 +0.01 92.6 £0.01 90.8 £0.02 92.0 £0.02 94.4 +0.02 94.7 £0.02

AN2VEC-0 89.5 £0.01 90.6 £0.01 91.2 +£0.01 91.5 £0.02 91.8 £0.01 93.2 £0.01

AN2VEC-16 89.4 £0.01 90.2 +0.01 91.1 +£0.01 91.3 £0.02 92.1 £0.01 92.8 £0.01

AN2VEC-S-0 92.9 £0.01 93.4 £0.01 94.3 +£0.01 94.8 +0.01 95.1 £0.01 95.4 +0.01

AN2VEC-S-16 93.0 +0.01 93.5 +0.00 94.9 +0.00 95.1 +0.00 93.1 £0.01 93.1 £0.01

 

 

SC, DW, GAE and VGAE values are from Kipf and Welling (2016b). Error values indicate the sample standard deviation

 

Note that in Kipf and Welling (2016b), the training set is also 85% of the full dataset, and test and validation sets are
formed with the remaining edges, respectively 10% and 5% (and the same amount of non-edges). Here, since we use the
same hyperparameters as Kipf and Welling (2016b) we do not need a validation set. We therefore chose to use the full
15% remaining edges (with added non-edges) as a test set, as explained above.
Lerique et al. Applied Network Science

(2020) 5:5

directly used for the link prediction task at hand. First results confirmed our intuition.
However, we found that the shallow decoder models AN2VEC-S-0 and AN2VEC-S-16
perform consistently better than the vanilla VGAE for Cora and CiteSeer while their deep
counterparts (AN2VEC-0 and AN2VEC-16) outperforms VGAE for all datasets. As nei-
ther AN2VEC-0 nor AN2VEC-16 exhibited over-fitting, this behaviour is surprising and
calls for further explorations which are beyond the scope of this paper (in particular,
this may be specific to the link prediction task). Nonetheless, the higher performance of
AN2VEC-S-0 and AN2VEC-S-16 over the vanilla VGAE on Cora and CiteSeer confirms
that including feature reconstruction in the constraints of node embeddings is capable of
increasing link prediction performance when feature and structure are not independent
(consistent with Gao and Huang 2018; Shen et al. 2018; Tran 2018). An illustration of the
embeddings produced by AN2VEC-S-16 on Cora is shown in Fig. 4.

On the other hand, performance of AN2VEC-S-0 on PubMed is comparable with GAE
and VGAE, while AN2VEC-S-16 has slightly lower performance. The fact that lower over-
lap models perform better on this dataset indicates that features and structure are less
congruent here than in Cora or CiteSeer (again consistent with the comparisons found in
Tran 2018). Despite this, an advantage of the embeddings produced by the AN2VEC-S-
16 model is that they encode both the network structure and the node features, and can
therefore be used for downstream tasks involving both types of information.

We further explore the behaviour of the model for different sizes of the training set,
ranging from 10% to 90% of the edges in each dataset (reducing the training set accord-
ingly), and compare the behaviour of AN2VEC to GraphSAGE. To make the comparison
meaningful we train two variants of the two-layer GraphSAGE model with mean aggre-
gators and no bias vectors: one with an intermediate layer of 32 dimensions and an
embedding layer of 16 dimensions (roughly equivalent in dimensions to the full over-
lap AN2VEC models), the second with an intermediate layer of 64 dimensions and an
embedding layer of 32 dimensions (roughly equivalent to no overlap in AN2VEC). Both

 

32's ee ea

te Soke 0 % bs f

oh e e
3 _
eae .

   

Fig. 4 Cora embeddings created by AN2VEC-S-16, downscaled to 2D using Multidimensional scaling. Node
colours correspond to document classes, and network links are in grey

 

 

 

Page 16 of 24
Lerique et al. Applied Network Science (2020) 5:5 Page 17 of 24

layers use neighbourhood sampling, 10 neighbours for the first layer and 5 for the second.
Similarly to the shallow AN2VEC decoder, each pair of node embeddings is reduced by
inner product and a sigmoid activation, yielding a scalar prediction between 0 and 1 for
each possible edge. The model is trained on minibatches of 50 edges and non-edges (edges
generated with random walks of length 5), learning rate 0.001, and 4 total epochs. Note
that on Cora, one epoch represents about 542 minibatches,!° such that 4 epochs represent
about 2166 gradient updates; thus with a learning rate of 0.001, we remain comparable to
the 200 full batches with learning rate 0.01 used to train AN2VEC.

Figure 5 plots the AUC produced by AN2VEC and GraphSAGE for different training
set sizes and different embedding sizes (and overlaps, for AN2VEC), for each dataset. As
expected, the performance of both models decreases as the size of the test set increases,
though less so for AN2VEC. For Cora and CiteSeer, similarly to Table 1, higher overlaps
and a shallow decoder in AN2VEC give better performance. Notably, the shallow decoder
version of AN2VEC with full overlap is still around .75 for a test size of 90%, whereas
both GraphSAGE variants are well below .65. For PubMed, as in Table 1, the behaviour is
different to the first two datasets, as overlaps 0 and 16 yield the best results. As for Cora
and CiteSeer, the approach taken by AN2VEC gives good results: with a test size of 90%,
all AN2VEC deep decoder variants are still above .75 (and shallow decoders above .70),
whereas both GraphSAGE variants are below .50.

Node classification

Since the embeddings produced also encode feature information, we then evaluate the
model’s performance on a node classification task. Here the models are trained on a ver-
sion of the dataset where a portion of the nodes (randomly selected) have been removed;
next, a logistic classifier! is trained on the embeddings to classify training nodes into
their classes; finally, embeddings are produced for the removed nodes, for which we show
the F1 scores of the classifier.

Figure 6 shows the results for AN2VEC and GraphSAGE on all datasets. The scale of
the reduction in performance as the test size increases is similar for both models (and
similar to the behaviour for link prediction), though overlap and shallow versus deep
decoding seem to have less effect. Still, the deep decoder is less affected by the change in
test size than the shallow decoder; and contrary to the link prediction case, the 0 over-
lap models perform best (on all datasets). Overall, the performance levels of GraphSAGE
and AN2VEC on this task are quite similar, with slightly better results of AN2VEC on
Cora, slightly stronger performance for GraphSAGE on CiteSeer, and mixed behaviour
on PubMed (AN2VEC is better for small test sizes and worse for large test sizes).

Variable embedding size

We also explore the behaviour of AN2VEC for different embedding sizes. We train models
with Fa = Fx € {8, 16,24, 32} and overlaps 0, 8, 16, 24, 32 (whenever there are enough
dimensions to do so), with variable test size. Figure 7 shows the AUC scores for link
prediction, and Fig. 8 shows the Fl-micro scores for node classification, both on Cite-
Seer (the behaviour is similar on Cora, though less salient). For link prediction, beyond

 

10One epoch is 2708 nodes x 5 edges per node x 2 (for non-edges) = 27080 training edges or non-edges; divided by 50,
this makes 541.6 minibatches per epoch.
‘1 Using Scikit-learn’s (Pedregosa et al. 2011) interface to the liblinear library, with one-vs-rest classes.
Lerique et al. Applied Network Science

 

(2020) 5:5

 

 

 

CiteSeer PubMed
WN
>
o
oO
Y =
< o
a
oO
lok
@
Fyx (overlap)
0
---- 8
ceeeeeeeees 16
UO
oO
S
S °
<x a
[e)
lok
@
0.5- F -
0.4 20% 40% 60% 20% 40% 60% 20% 40% 60%
test size test size test size
(a)
Cora CiteSeer PubMed
Q dim(E 4)
<x wuveeennuee 16
— 32

 

0.4

60%

40%
test size

40% 60% 20%

test size

(b)

Fig. 5 AUC for link prediction using AN2VEC and GraphSAGE over all datasets. AN2VEC top row is the shallow
decoder variant, and the bottom row is the deep decoder variant; colour and line styles indicate different
levels of overlap. GraphSAGE colours and line styles indicate embedding size as described in the main text
(colour and style correspond to the comparable variant of AN2VEC). Each point on a curve aggregates 10
independent training runs. a AN2VEC. b GraphSAGE

40% 60% 20%

test size

20%

 

 

confirming trends already observed previously, we see that models with less total embed-
ding dimensions perform slightly better than models with more total dimensions. More
interestingly, all models seem to reach a plateau at overlap 8, and then exhibit a slightly
fluctuating behaviour as overlap continues to increase (in models that have enough
dimensions to do so). This is valid for all test sizes, and suggests (i) that at most 8
dimensions are necessary to capture the commonalities between network and features in
CiteSeer, and (ii) that having more dimensions to capture either shared or non-shared
information is not necessarily useful. In other words, 8 overlapping dimensions seem to
capture most of what can be captured by AN2VEC on the CiteSeer dataset, and further
increase in dimensions (either overlapping or not) would capture redundant information.

Node classification, on the other hand, does not exhibit any consistent behaviour
beyond the reduction in performance as the test size increases. Models with less total
dimensions seems to perform slightly better at 0 overlap (though this behaviour is
reversed on Cora), but neither the ordering of models by total dimensions nor the effect of

increasing overlap are consistent across all conditions. This suggests, similarly to Fig. 6a,

Page 18 of 24
Lerique et al. Applied Network Science

 

 

 

 

(2020) 5:5
Cora CiteSeer PubMed
WN
>
ov
° °
Y =
F 2
u
Qa
@
F4x (overlap)
0
---- 8
eeeeeeeteee 16
0
. Oo
° O
Oo To
E 2
rs g
@
20% 40% 60% 20% 40% 60% 20% 40% 60%
test size test size test size
(a)
Cora CiteSeer PubMed
2
Y dim(€,)
€ ceeeeeeeeee 16
ce
re — 32
20% 40% 60% 20% 40% 60% 20% 40% 60%
test size test size test size
Fig. 6 F1-micro score for node classification using AN2VEC and GraphSAGE over all datasets. AN2VEC top
row is the shallow decoder variant, and the bottom row is the deep decoder variant; colour and line styles
indicate different levels of overlap. GraphSAGE colours and line styles indicate embedding size as described
in the main text (colour and style correspond to the comparable variant of AN2VEC). Each point on a curve
aggregates 10 independent training runs. a AN2VEC. b GraphSAGE

 

 

 

that overlap is less relevant to this particular node classification scheme than it is to link

prediction.

Memory usage and time complexity
Finally, we evaluate the resources used by our implementation of the method in terms
of training time and memory usage. We use AN2VEC with 100-dimensions intermediate
layers in the encoder and the (deep) decoder, with 16 embedding dimensions for each
reconstruction task (Fa + Fax = Fx + Fax = 16), and overlap Fax € {0,8,16}. We
train that model on synthetic networks generated as in the “Synthetic featured networks”
section (setting wa = 0.8, and without adding any other noise on the features), with M €
{50, 100, 200, 500, 1000, 2000, 5000} communities of size 1 = 10 nodes.

Only CPUs were used for the computations, running on a 4 x Intel Xeon CPU E7-
8890 v4 server with 1.5 TB of memory. Using 8 parallel threads for training,'* we record

 

12 There are actually two levels of threading: the number of threads used in our code for computing losses, and the
number of threads used by the BLAS routines for matrix multiplication. We set both to 8, and since both computations
alternate this leads to an effective 8 compute threads, with some fluctuations at times.

Page 19 of 24
Lerique et al. Applied Network Science (2020) 5:5 Page 20 of 24

 

test size = 10% test size = 50% test size = 90%
L Wn
>
ov
i oO
EE... =
Sa ‘$2
4 an
1 °
/ a
/ O
Fy, Fx
I I I I I @ 8
+ 16
- a 24
OD *« 32
oO
- oO
To
a
Je 8
? ait rridp semen ¥ oO
(7% ao

 

0 8 16 24 32 0 8 16 24 32 0 8 16 24 32
Fyx (overlap) Fyx (overlap) Fyx (overlap)

Fig. 7 AUC for link prediction using AN2VEC on CiteSeer, as a function of overlap, with variable total
embedding dimensions. Columns correspond to different test set sizes. Top row is with shallow decoder,
bottom row with deep decoder. Colours, as well as marker and line styles, indicate the number of
embedding dimensions available for adjacency and features

 

 

 

the peak memory usage,'* training time, and full job time!* for each network size, aver-
aged over the three overlap levels. Results are shown in Fig. 9. Note that in a production
setting, multiplying the number of threads by n will divide compute times by nearly n,
since the process is aggressively parallelised. A further reduced memory footprint can also

be achieved by using sparse encoding for all matrices.

Conclusions

In this work, we proposed an attributed network embedding method based on the com-
bination of Graph Convolutional Networks and Variational Autoencoders. Beyond the
novelty of this architecture, it is able to consider jointly network information and node
attributes for the embedding of nodes. We further introduced a control parameter able
to regulate the amount of information allocated to the reconstruction of the network, the
features, or both. In doing so, we showed how shallow versions of the proposed model out-
perform the corresponding non-interacting reference embeddings on given benchmarks,
and demonstrated how this overlap parameter consistently captures joint network-feature
information when they are correlated.

Our method opens several new lines of research and applications in fields where
attributed networks are relevant. As an example one can take a social network with the
task of predicting future social ties, or reconstructing existing but invisible social ties.
Solutions to this problem can rely on network similarities in terms of overlapping sets
of common friends, or on feature similarities in terms of common interest, professional
or cultural background, and so on. While considering these types of information sepa-
rately would provide us with a clear performance gain in the prediction, these similarities

are not independent. For example, common friends may belong to the same community.

 

13 Using the top utility program.
14.\s reported by our scripts and by GNU Parallel.
Lerique et al. Applied Network Science (2020) 5:5 Page 21 of 24

 

 

 

test size = 10% test size = 50% test size = 90%
0.7 - r r
WN
0.6 -- fi. | - s
o a =H oot” >
UO
= 0.5- r =
5 2
it 0.4- - Q
Qa
O
0.3 ie a 7 F4, Fy
I I I I I e@ 8
0.7- L L + 16
ay — UE- ae ¥ a 24
0.6-7" oo: L = ak wees } L U « 32
© 27" 8
UO
205- - L
E 8
i 0.4- - S
©
0.3- r
0 8 16 24 32 0 8 16 24 32 0 8 16 24 32
Fyx (overlap) Fyx (overlap) Fyx (overlap)
Fig. 8 F1-micro score for node classification using AN2VEC on CiteSeer, as a function of overlap, with variable
total embedding dimensions. Columns correspond to different test set sizes. Top row is with shallow
decoder, bottom row with deep decoder. Colours, as well as marker and line styles, indicate the number of
embedding dimensions available for adjacency and features

 

 

 

By exploiting these dependencies our method can provide us with an edge in terms of
predictive performance and could indicate which similarities, structural, feature-related,
or both, better explain why a social tie exists at all. Another setting where we believe
our framework might yield noteworthy insights is when applied to the prediction of side
effects of drug pairs (polypharmacy). This problem has recently been approached by
Zitnik et al. (2018) by extending GraphSAGE for multirelational link prediction in multi-
modal networks. In doing so, the authors were able to generate multiple novel candidates
of drug pairs susceptible to induce side effects when taken together. Beyond using drug
feature vectors to generate polypharmacy edge probabilities, our overlapping encoder
units would enable a detailed view on how these side effects occur due to confounding
effects of particular drug attributes. It would pinpoint the feature pairs that interacting
drugs might share (or not), further informing the drug design process. Furthermore, we

 

   
   
  

 
 
 
   

 

> 1075 {9 Full script time
° — 104 E - :
EOD 4 = —*e— Training time
oy c r
£2 ° iF
#2 101. @ 107:
w@ Oo “2 F
= oO L
@ =) r E 102 F
x 3 1= =
2 >— i 1 1 bao i 1 \ rool 1 1 roto 1
ore Sy So" so so so sv sw sv so so so
ty © at ot” © at” ot ot at” ot at” ot
#nodes #nodes
(a) Memory usage (b) Time complexity
Fig. 9 Memory usage and time complexity of AN2VEC on graphs generated by the Stochastic Block Model
with colour features (see main text for details). a Peak resident memory usage in Gibibytes (1024? bytes). b
Full script time (including data loading, pre-compilation of Julia code, etc.) and training time (restricted to the
actual training computation) in seconds

 

 
Lerique et al. Applied Network Science (2020) 5:5 Page 22 of 24

expect that our method will help yield a deeper understanding between node features
and structure, to better predict network evolution and ongoing dynamical phenomena. In
particular, it should help to identify nodes with special roles in the network by clarifying
whether their importance has structural or feature origin.

In this paper our aim was to ground our method and demonstrate its usefulness
on small but controllable featured networks. Its evaluation on more complex synthetic
datasets, in particular with richer generation schemes, as well as its application to larger
real datasets, are therefore our immediate goals in the future.

Abbreviations

AN2VEC: Attributed node to vector model; AN2VEC-O: Zero overlap model; AN2VEC-16: 16-dimensions overlap model;
AN2VEC-S-0: zero overlap model with shallow adjacency decoder; AN2VEC-S-16: 16-dimensions overlap model with
shallow adjacency decoder; AP: Average precision; AUC: Area under the ROC curve; Ber: Bernoulli random variable; DANE:
Deep attributed network embedding; DW: DeepWalk embedding model; VAE: Variational autoencoder; GAE: Graph
autoencoder; GC: Graph convolutional layer; GCN: Graph convolutional network; GCN-VAE: Graph convolutional
variational autoencoder; KL: Kullback-Leibler divergence; MLP: Multi-layer perceptron; ReLU: Rectified linuar unit; ROC:
Receiver operating characteristic; SC: Spectral clustering; TF/IDF: Term-frequency-inverse-document-frequency; VGAE:
Variational graph autoencoder

Acknowledgements

We thank E. Fleury, J-Ph. Magué, D. Seddah, and E. De La Clergerie for constructive discussions and for their advice on
data management and analysis. Some computations for this work were made using the experimental GPU platform at
the Centre Blaise Pascal of ENS Lyon, relying on the SIDUS infrastructure provided by E. Quemener.

Authors’ contributions

Mk, JLA and SL participated equally in designing and developing the project, and in writing the paper. SL implemented
the model and experiments. SL and JLA developed and implemented the analysis of the results. All authors read and
approved the final manuscript.

Funding
This project was supported by the LIAISON Inria-PRE project, the SoSweet ANR project (ANR-15-CE38-0011), and the
ACADEMICS project financed by IDEX LYON.

Availability of data and materials

The synthetic datasets generated for this work are stochastically created by our implementation, available at
github.com/ixxi-dante/an2vec.

The datasets used for standard benchmarking (Cora, CiteSeer, and PubMed) are available at lings.soe.ucsc.edu/data.
Our implementation of AN2VEC is made using the Julia programming language, and particularly making heavy use of
Flux (Innes 2018). Parallel computations were run using GNU Parallel (Tange 2011). Finally, we used StellarGraph (Data61
2018) for the GraphSAGE implementation.

 

Competing interests
The authors declare that they have no competing interests.

Author details
'Univ Lyon, ENS de Lyon, Inria, CNRS, UCB Lyon 1, LIP UMR 5668, IXXI, 69007 Lyon, France. *Department of Network and
Data Science, Central European University, 1051 Budapest, Hungary.

Received: 1 March 2019 Accepted: 13 November 2019
Published online: 09 January 2020

References

Aiken LS, West SG, Reno RR (1994) Multiple Regression: Testing and Interpreting Interactions, vol. 45. Taylor & Francis,
New York

Aral S, Muchnik L, Sundararajan A (2009) Distinguishing influence-based contagion from homophily-driven diffusion in
dynamic networks. Proc Natl Acad Sci 106(51):21544-21549

Blondel VD, Guillaume JL, Lambiotte R, Lefebvre E (2008) Fast unfolding of communities in large networks. J Stat Mech
Theory Exp 2008(10)

Bojchevski A, GUnnemann S (2017) Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking.
arXiv:1707.03815. Accessed 21 Jan 2019

Bronstein M, Bruna J, LeCun Y, Szlam A, Vandergheynst P (2017) Geometric deep learning: Going beyond euclidean data.
IEEE Signal Proc Mag 34(4):18-42. https://doi.org/10.1109/MSP.2017.2693418

Bruna J, Zaremba W, Szlam A, LeCun Y (2013) Spectral Networks and Locally Connected Networks on Graphs. arXiv:
1312.6203

Cai H, Zheng VW, Chang K (2018) A comprehensive survey of graph embedding: Problems, techniques, and applications.
IEEE Trans Knowl Data Eng 30(9):1616-1637. https://doi.org/10.1109/tkde.2018.2807452
Lerique et al. Applied Network Science (2020) 5:5 Page 23 of 24

Cao S, Lu W, Xu Q (2015) Grarep: Learning graph representations with global structural information. In: Proceedings of
the 24th ACM International on Conference on Information and Knowledge Management. CIKM ‘15. ACM, New York.
pp 891-900. https://doi.org/10.1145/2806416.28065 12

Cao S, Lu W, Xu Q (2016) Deep neural networks for learning graph representations. In: Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence. AAAI'16. AAAI Press, Phoenix. pp 1145-1152

Cataldi M, Caro LD, Schifanella C (2010) Emerging topic detection on twitter based on temporal and social terms
evaluation. In: Proceedings of the Tenth International Workshop on Multimedia Data Mining. ACM. p 4. https://doi.
org/10.1145/1814245.1814249

Chen Z, Badrinarayanan V, Lee C-Y, Rabinovich A (2018) GradNorm: Gradient Normalization for Adaptive Loss Balancing in
Deep Multitask Networks. In: Dy J, Krause A (eds). Proceedings of the 35th International Conference on Machine
Learning. Proceedings of Machine Learning Research, vol. 80. PMLR, Stockholmsmassan. pp 794-803

Data61 C (2018) StellarGraph Machine Learning Library. GitHub. https://github.com/stellargraph/stellargraph

Defferrard M, Bresson X, Vandergheynst P (2016) Convolutional Neural Networks on Graphs with Fast Localized Spectral
Filtering. In: Lee DD, Sugiyama M, Luxburg UV, Guyon I, Garnett R (eds). Advances in Neural Information Processing
Systems 29. Curran Associates, Inc., Red Hook. pp 3844-3852

Duvenaud DK, Maclaurin D, lparraguirre J, Bombarell R, Hirzel T, Aspuru-Guzik A, Adams RP (2015) Convolutional
Networks on Graphs for Learning Molecular Fingerprints. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R
(eds). Advances in Neural Information Processing Systems 28. Curran Associates, Inc., Red Hook. pp 2224-2232

Fortunato S (2010) Community detection in graphs. Phys Rep 486(3-5):75-174

Fortunato S, Hric D (2016) Community detection in networks: A user guide. Phys Rep 659:1-44

Gan G, Ma C, Wu J (2007) Data Clustering: Theory, Algorithms, and Applications, vol. 20. Siam, New York

Gao H, Huang H (2018) Deep Attributed Network Embedding. In: Proceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence (IJCAI-18). UCAI-18. International Joint Conferences on Artificial Intelligence, CA,
USA. pp 3364-3370

Glorot X, Bengio Y (2010) Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of
the Thirteenth International Conference on Artificial Intelligence And Statistics. PMLR, Sardinia. pp 249-256

Granovetter MS (1977) The Strength of Weak Ties. In: Leinhardt S (ed). Social Networks. Academic Press, Cambridge.
pp 347-367. https://doi.org/10.1016/B978-0- 12-442450-0.50025-0

Grover A, Leskovec J (2016) Node2vec: Scalable feature learning for networks. In: Proceedings of the 22Nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. KDD '16. ACM, New York. pp 855-864. https://
doi.org/10.1145/2939672.2939754

Gumperz JJ (2009) The speech community. Linguist Anthropol A Read 1:66

Hamilton W, Ying Z, Leskovec J (2017) Inductive Representation Learning on Large Graphs. In: Guyon |, Luxburg UV,
Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds). Advances in Neural Information Processing Systems
30. Curran Associates, Inc., Red Hook. pp 1024-1034

Holland PW, Laskey kK, Leinhardt S (1983) Stochastic blockmodels: First steps. Soc Netw 5(2):109-137

Hours H, Fleury E, Karsai M (2016) Link prediction in the twitter mention network: impacts of local structure and similarity
of interest. In: Data Mining Workshops (ICDMW), 2016 IEEE 16th International Conference On. IEEE. pp 454-461.
https://doi.org/10.1109/icdmw.2016.0071

Innes M (2018) Flux: Elegant machine learning with julia. J Open Source Softw. https://doi.org/10.21105/joss.00602

Jain AK (2010) Data clustering: 50 years beyond k-means. Pattern Recog Lett 31(8):65 1-666

Kingma DP, Ba J (2014) Adam: A Method for Stochastic Optimization. arXiv:1412.6980

Kingma DP, Welling M (2013) Auto-Encoding Variational Bayes. arXiv:1312.6114

Kipf T, Fetaya E, Wang K-C, Welling M, Zemel R (2018) Neural Relational Inference for Interacting Systems.

arXiv:1802.04687, Accessed 20 May 2019

Kipf TN, Welling M (2016a) Semi-Supervised Classification with Graph Convolutional Networks. arXiv:1609.02907

Kipf TN, Welling M (2016b) Variational Graph Auto-Encoders. arXiv:1611.07308

Kossinets G, Watts DJ (2006) Empirical analysis of an evolving social network. Science 311(5757):88—90

Kossinets G, Watts DJ (2009) Origins of homophily in an evolving social network. Am J Sociol 115(2):405-450

Kumpula JM, Onnela JP, Saramaki J, Kaski K, Kertész J (2007) Emergence of communities in weighted networks. Phys Rev
Lett 99(22):228701

La Fond T, Neville J (2010) Randomization tests for distinguishing social influence and homophily effects. In: Proceedings
of the 19th International Conference on World Wide Web. ACM. pp 601-610. https://doi.org/10.1145/1772690.
1772752

Leo Y, Fleury E, Alvarez-Hamelin JI, Sarraute C, Karsai M (2016) Socioeconomic correlations and stratification in
social-communication networks. J R Soc Interface 13(125). https://doi.org/10.1098/rsif.2016.0598

Levy Abitbol J, Karsai M, Fleury E (2018a) Location, occupation, and semantics based socioeconomic status inference on
twitter. In: 2018 IEEE International Conference on Data Mining Workshops (ICDMW). pp 1192-1199. https://doi.org/
10.1 109/ICDMW.2018.00171

Levy Abitbol J, Karsai M, Magué J-P, Chevrot J-P, Fleury E (2018b) Socioeconomic Dependencies of Linguistic Patterns in
Twitter: A Multivariate Analysis. In: Proceedings of the 2018 World Wide Web Conference. WWW ‘18. International
World Wide Web Conferences Steering Committee, Republic and Canton of Geneva. pp 1125-1134. https://doi.org/
10.1145/3178876.3186011. Accessed 23 Jan 2019

Lic, Ma J, Guo X, Mei Q (2017) Deepcas: An end-to-end predictor of information cascades. In: Proceedings of the 26th
International Conference on World Wide Web. WWW '17. International World Wide Web Conferences Steering
Committee, Republic and Canton of Geneva. pp 577-586. https://doi.org/10.1145/3038912.3052643

Liben-Nowell D, Kleinberg J (2007) The link-prediction problem for social networks. J Am Soc Inf Sci Technol
58(7):1019-1031

LUL, Zhou T (2011) Link prediction in complex networks: A survey. Physica A Stat Mech Appl 390(6):1150-1170

Mathieu E, Rainforth T, Siddharth N, Teh YW (2018) Disentangling Disentanglement in Variational Auto-Encoders.
arXiv:1812.02833. Accessed 30 Jan 2019

 
Lerique et al. Applied Network Science (2020) 5:5 Page 24 of 24

McPherson M, Smith-Lovin L, Cook JM (2001) Birds of a feather: Homophily in social networks. Ann Rev Soc 27(1):415-444

Muzellec B, Cuturi M (2018) Generalizing Point Embeddings using the Wasserstein Space of Elliptical Distributions.
arXiv:1805.07594. Accessed 24 Jan 2019

Nair V, Hinton GE (2010) Rectified Linear Units Improve Restricted Boltzmann Machines. In: Proceedings of the 27th
International Conference on International Conference on Machine Learning. ICML'10. Omnipress, Madison.
pp 807-814. event-place: Haifa, Israel

Namata G, London B, Getoor L, Huang B (2012) Query-driven active surveying for collective classification. In: 10th
International Workshop on Mining and Learning With Graphs, Edinburgh

Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V,
Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot M, Duchesnay E (2011) Scikit-learn: Machine learning in
Python. J Mach Learn Res 12:2825-2830

Peixoto TP (2014) Hierarchical block structures and high-resolution model selection in large networks. Phys Rev X
4(1):011047

Perozzi B, Al-Rfou R, Skiena S (2014) Deepwalk: Online learning of social representations. In: Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD ‘14. ACM, New York. pp 701-710.
https://doi.org/10.1145/2623330.2623732

Rezende DJ, Mohamed S, Wierstra D (2014) Stochastic Backpropagation and Approximate Inference in Deep Generative
Models. arXiv:1401.4082

Rosvall M, Axelsson D, Bergstrom CT (2009) The map equation. Eur Phys J Spec Top 178(1):13-23

Sen P, Namata G, Bilgic M, Getoor L, Galligher B, Eliassi-Rad T (2008) Collective classification in network data. Al Mag
29(3):93-93. https://doi.org/10.1609/aimag.v29i3.2157

Shen E, Cao Z, Zou C, Wang J (2018) Flexible Attributed Network Embedding. arXiv:1811.10789, Accessed 10 Dec 2018

Shrum W, Cheek Jr NH, MacD S (1988) Friendship in school: Gender and racial homophily. Sociol Educ:227-239. https://
doi.org/10.2307/2112441

Tang L, Liu H (2011) Leveraging social media networks for classification. Data Min Knowl Discov 23(3):447-478. https://

doi.org/10.1007/s10618-010-0210-x. Accessed 28 Feb 2019

Tange O (2011) Gnu parallel - the command-line power tool. login: USENIX Mag 36(1):42-47

Tenenbaum JB, Silva V, Langford JC (2000) A global geometric framework for nonlinear dimensionality reduction. Science
290(5500):2319-2323. https://doi.org/10.1126/science.290.5500.2319.
http://science.sciencemag.org/content/290/5500/2319.full.odf

Tran PV (2018) Multi-Task Graph Autoencoders. arXiv:1811.02798. Accessed 9 Jan 2019

Wang D, Cui P, Zhu W (2016) Structural deep network embedding. In: Proceedings of the 22Nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. KDD '16. ACM, New York. pp 1225-1234. https://
doi.org/10.1145/2939672.2939753

Ying R, He R, Chen K, Eksombatchai P, Hamilton WL, Leskovec J (2018) Graph convolutional neural networks for web-scale
recommender systems. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining. KDD '18. ACM, New York. pp 974-983. https://doi.org/10.1145/3219819.3219890

Zhu D, Cui P, Wang D, Zhu W (2018) Deep variational network embedding in wasserstein space. In: Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. KDD ‘18. ACM, New York.
pp 2827-2836. https://doi.org/10.1145/3219819.3220052

Zitnik M, Agrawal M, Leskovec J (2018) Modeling polypharmacy side effects with graph convolutional networks.
Bioinformatics 34(13):457-466

 

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
