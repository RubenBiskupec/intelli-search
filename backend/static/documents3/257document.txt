NP} | Digital Medicine

REVIEW ARTICLE OPEN

www.nature.com/npjdigitalmed

® Check for updates

Fusion of medical imaging and electronic health records
using deep learning: a systematic review and implementation

guidelines

Shih-Cheng Huang @'*°*“, Anuj Pareek@***, Saeed Seyyedi~’, Imon Banerjee @**” and Matthew P. Lungren'*”

Advancements in deep learning techniques carry the potential to make significant contributions to healthcare, particularly in fields
that utilize medical imaging for diagnosis, prognosis, and treatment decisions. The current state-of-the-art deep learning models for
radiology applications consider only pixel-value information without data informing clinical context. Yet in practice, pertinent and
accurate non-imaging data based on the clinical history and laboratory data enable physicians to interpret imaging findings in the
appropriate clinical context, leading to a higher diagnostic accuracy, informative clinical decision making, and improved patient
outcomes. To achieve a similar goal using deep learning, medical imaging pixel-based models must also achieve the capability to
process contextual data from electronic health records (EHR) in addition to pixel data. In this paper, we describe different data
fusion techniques that can be applied to combine medical imaging with EHR, and systematically review medical data fusion
literature published between 2012 and 2020. We conducted a systematic search on PubMed and Scopus for original research
articles leveraging deep learning for fusion of multimodality data. In total, we screened 985 studies and extracted data from 17
papers. By means of this systematic review, we present current knowledge, summarize important results and provide
implementation guidelines to serve as a reference for researchers interested in the application of multimodal fusion in medical

imaging.

npj Digital Medicine (2020)3:136; https://doi.org/10.1038/s41746-020-00341-z

INTRODUCTION

The practice of modern medicine relies heavily on synthesis of
information and data from multiple sources; this includes imaging
pixel data, structured laboratory data, unstructured narrative data,
and in some cases, audio or observational data. This is particularly
true in medical image interpretation where substantial clinical
context is often essential to provide diagnostic decisions. For
example, it has repeatedly been shown that a lack of access to
clinical and laboratory data during image interpretation results in
lower performance and decreased clinical utility for the referring
provider’. In a survey of radiologists, the majority (87%) stated
that clinical information had a significant impact on interpreta-
tion’. The importance of clinical context for accurate interpreta-
tion of imaging data is not limited to radiology; instead many
other imaging-based medical specialties such as pathology,
ophthalmology, and dermatology, also rely on clinical data to
guide image interpretation in practice*”°. Pertinent and accurate
information regarding the current symptoms and past medical
history enables physicians to interpret imaging findings in the
appropriate clinical context, leading to a more relevant differential
diagnosis, a more useful report for the physicians, and optimal
outcome for the patient.

In the current digital era, the volume of radiological imaging
exams is growing. To meet this increased workload demand, an
average radiologist may have to interpret an image every 3-4s
over an 8-h workday which contributes to fatigue, burnout, and
increased error-rate’. Deep learning in healthcare is proliferating
due to the potential for successful automated systems to either
augment or offload cognitive work from busy physicians®”'°. One

class of deep learning, namely convolutional neural networks
(CNN) has proven very effective for image recognition and
classification tasks, and are therefore often applied to medical
images. Early applications of CNNs for image analysis in medicine
include diabetic retinopathy, skin cancer, and chest X-rays''~'®.
Yet, these models consider only the pixel data as a single modality
for input and cannot contextualize other clinical information as
would be done in medical practice, therefore may ultimately limit
clinical translation.

As an example consider the “simple” task in radiology of
identifying pneumonia on a chest radiograph, something that has
been achieved by many investigators training deep learning
models for automated detection and classification of pathologies
on chest X-rays'”?°. Yet without clinical context such as patient
history, chief complaint, prior diagnoses, laboratory values, such
applications may ultimately have limited impact on clinical
practice. The imaging findings on chest X-rays consistent with
pneumonia, despite having imaging features that can generally
differentiate alternative diagnoses, are nonspecific and accurate
diagnosis requires the context of clinical and laboratory data. In
other words, the chest X-ray findings that suggest pneumonia
would be accurate in one person with fever and an elevated white
blood cell count but in another patient without those supporting
clinical characteristics and laboratory values, similar imaging
finding may instead represent other etiologies such as atelectasis,
pulmonary edema, or even lung cancer. There are countless
examples across different medical fields in which clinical context,
typically in the form of structured and unstructured clinical data
from the electronic health record (EHR), is critical for accurate and

‘Department of Biomedical Data Science, Stanford University, Stanford, USA. *Center for Artificial Intelligence in Medicine & Imaging, Stanford University, Stanford, USA.
’Department of Radiology, Stanford University, Stanford, USA. “Department of Biomedical Informatics, Emory University, Atlanta, USA. >Department of Radiology,
Emory University, Atlanta, USA. °These authors contributed equally: Shih-Cheng Huang, Anuj Pareek. “email: mschuang@stanford.edu

Seoul National University Bundang Hospital

np} nature partner

journals
S.-C. Huang et al.

 

clinically relevant medical imaging interpretation. As with human
physicians, automated detection and classification systems that
can successfully utilize both medical imaging data together with
clinical data from the EHR, such as patient demographics, previous
diagnoses and laboratory values, may lead to better performing
and more clinically relevant models.

Multimodal deep learning models that can ingest pixel data
along with other data types (fusion) have been successful in
applications outside of medicine, such as autonomous driving and
video classification. As an example, a multimodal fusion detection
system for autonomous vehicles, that combines visual features
from cameras along with data from Light Detection and Ranging
(LIDAR) sensors, is able to achieve significantly higher accuracy
(3.7% improvement) than a single-modal CNN detection model?'.
Similarly, a multimodal social media video classification pipeline
leveraging both visual and textual features increased the
classification accuracy to 88.0%, well above single modality neural
networks such as Google’s InceptionV3 which reached an accuracy
of 76.4% on the same task**. The improvements in performance
for these efforts not only echo the justification in medical
applications, leveraging fusion strategies for medical imaging is
also primarily motivated by the desire to integrate complementary
contextual information and overcome the limitation of image-only
models.

The recent medical imaging literature shows a similar trend
where both EHR and pixel data are leveraged in a “fusion-
paradigm” for solving complex tasks which cannot readily be
tackled by a single modality (Fig. 1). The new fusion paradigm
covers a wide range of methodologies and techniques with
varying terms and model architectures that have not been studied
systematically. The purpose of this review paper is to present a
comprehensive analysis of deep learning models that leverage
multiple modalities for medical imaging tasks, define and
consolidate relevant terminology, and summarize the results from
state-of-the-art models in relevant current literature. We hope this
review can help inform future modeling frameworks and serve as
a reference for researchers interested in the application of
multimodal fusion in medical imaging.

Terminology and strategies in fusion

Data fusion refers to the process of joining data from multiple
modalities with the aim of extracting complementary and more

5140

Article Types
5000 - :
Fusion
i All
4000 - 3827
3000 -
2421
2000 - 1825
1398 1466 1477 1542
1000 -
244 287
48 74 91 79 92 133
0-

2012 2013 2014 2015 2016 2017 2018 2019
Year

Number of publications

Fig. 1 Timeline of publications in deep learning for medical
imaging. Timeline showing growth in publications on deep learning
for medical imaging, found by using the same search criteria on
PubMed and Scopus. The figure shows that fusion has only
constituted a small, but growing, subset of medical deep learning
literature.

npj Digital Medicine (2020) 136

complete information for better performing machine learning
models as opposed to using a single data modality.

Figure 2 illustrates the three main different fusion strategies,
namely early, joint, and late fusion. Here we define and describe
each fusion strategy in detail:

Early fusion®’, commonly known as feature level fusion, refers
to the process of joining multiple input modalities into a single
feature vector before feeding into one single machine learning
model for training (Fig. 2 Early Fusion). Input modalities can be
joined in many different ways, including concatenation, pooling or
by applying a gated unit****. Fusing the original features
represents early fusion type |, while fusing extracted features,
either from manual extraction, imaging analysis software or
learned representation from another neural network represents
early fusion type Il. We consider predicted probabilities to be
extracted features, thus fusing features with predicted probabil-
ities from different modalities is also early fusion type Il.

Joint fusion (or intermediate fusion) is the process of joining
learned feature representations from intermediate layers of neural
networks with features from other modalities as input to a final
model. The key difference, compared to early fusion, is that the
loss is propagated back to the feature extracting neural networks
during training, thus creating better feature representations for
each training iteration (Fig. 2 Joint Fusion). Joint fusion is
implemented with neural networks due to their ability to
propagate loss from the prediction model to the feature extraction
model(s). When feature representations are extracted from all
modalities, we consider this joint fusion type I. However, not all
input features require the feature extraction step to be defined as
joint fusion (Fig. 2 Joint Fusion—Type Il).

Late fusion~® refers to the process of leveraging predictions
from multiple models to make a final decision, which is why it is
often known as decision-level fusion (Fig. 2 Late Fusion). Typically,
different modalities are used to train separate models and the
final decision is made using an aggregation function to combine
the predictions of multiple models. Some examples of aggrega-
tion functions include: averaging, majority voting, weighted
voting or a meta-classifier based on the predictions from each
model. The choice of the aggregation function is usually empirical,
and it varies depending on the application and input modalities.

RESULTS

A total of 985 studies were identified through our systematic
search. After removing duplicates and excluding studies based on
title and abstract using our study selection criteria (see Methods),
44 studies remained for full-text screening. A total of 17 studies
fulfilled our eligibility criteria and were included for systematic
review and data extraction. The studies were in English except for
a single paper in Chinese. Figure 3 presents a flowchart of the
study screening and selection process and Table 1 displays the
included studies and extracted data.

Early fusion

The majority of the studies that remained after our full-text
screening (11/17) used early fusion to join the multimodal input.
Thung et al.*? conducted image-image fusion of PET and MRI
images using a joint fusion approach, but since they concatenated
clinical and imaging features into one single feature vector before
feeding into their neural network, we categorized their approach
as early fusion. Six out of eleven early fusion studies extracted
features from medical imaging using a CNN (Table 1). Four out of
the six studies that applied neural networks for feature extraction
simply concatenated the extracted imaging features with clinical
features for their fusion strategy*°**. The remaining two studies
by Liu et al.°° and Nie et al.°' applied dimensionality reduction
techniques before concatenating the features. Five studies used

Seoul National University Bundang Hospital
     

(XX XY IWOee

Early Fusion — Type |

 

Early Fusion — Type II

S.-C. Huang et al.

 

 

Joint Fusion — Type Il

OC) Feature
1) Prediction
2 Extracted Feature

MY Modality 1
) Modality 2
WY Output

 

 
   

Late Fusion

Fig. 2 Fusion strategies using deep learning. Model architecture for different fusion strategies. Early fusion (left figure) concatenates original
or extracted features at the input level. Joint fusion (middle figure) also joins features at the input level, but the loss is propagated back to the
feature extracting model. Late fusion (right figure) aggregates predictions at the decision level.

software generated and/or manually extracted features from
medical imaging before fusing with clinical data. Software-based
feature extraction included radiomics features such as skewness
and kurtosis°* or volume and thickness quantification of the
regions of interest*?°°. Manually extracted features included
radiological assessments such as size, angle, and morphology of
anatomical structures’. Out of these five studies, two applied
feature selection strategies to reduce the feature dimension and
improve predictive performance. The employed feature selection
strategies included a rank-based method using Gini coefficients>”,
a filter-based method based on mutual information of the
features’, and a genetic-algorithm based method®’. Seven of
the early fusion studies compared the performance of their fusion
models against single modality models (Table 1). Six of these
studies showed an improvement in performance when using
fusion??*°7®793'3) and the remaining one achieved the same
performance but reduced standard deviation*’, alluding to a
model with better stability.

Joint fusion

Joint fusion was used in four out of the seventeen studies. Spasov
et al.°°, Yala et al.°’, and Yoo et al.*® implemented CNNs to learn
image features and fused these feature representations with
clinical features before feeding them into a feed-forward neural
network. Spasov et al. and Yala. et al. both used simple
concatenation to fuse the learned imaging and clinical features.
To cater to the differences between the dimensionality and
dynamic range between the imaging and clinical features, Yoo
et al. replicated and scaled their clinical features before fusion and
they observed improvements in performances. Kawahara et al.°”

Seoul National University Bundang Hospital

also used CNNs as feature extractors for imaging modalities but
experimented with a unique multimodal multi-task loss function
that considers multiple combinations of the input modalities. The
predicted probabilities of these multi-task outputs were aggre-
gated for prediction, but we do not consider this late fusion since
the probabilities were not from separate models. Kawahara et al.,
Yala et al. and Yoo et al. reported an improvement in performance
using fusion compared to image-only models (Table 1). Yoo et al.
further compared their joint fusion model to a late fusion model
and achieved a 0.02 increase in Area Under Receiver Operating
Characteristic Curve (AUROC).

Late fusion

Late fusion was used in three out of the seventeen included
studies (Table 1). Each of the three late fusion papers applied a
different type of aggregation strategy. Yoo et al.*® took the mean
of the predicted probabilities from two single modality models as
the final prediction. Reda et al.*° built another classifier using the
single modality models’ prediction probabilities as inputs. Qiu
et al.*' trained three independent imaging models that took as
input a single MRI slice, each from a specific anatomical location.
Max, mean and majority voting were applied to aggregate
predictions from the three imaging models. The results from the
three aggregation methods were combined again by majority
voting before another round of late fusion with the clinical
models. All late fusion models showed improvements in
performances when compared to models that used only single
modalities.

npj Digital Medicine (2020) 136

np)
np}

S.-C. Huang et al.

 

: Free-Text Search
i
I
I

Google Scholar

Systematic Search
|
|
|

 

PubMed | | Scopus

 

   

 

(ox
fo}
yom)
140]
1S)
=
vary
(a=
®o
=

 

 

 

 

 

 

> re B
985 ass 163
Studies identified ) :  Duplicatesremoved —
fo) ( es .
z eee 778
oO Studies screened based on title and | - - - p: :
® Irrelevant studies
= abstract
S) . 2
Y se tt eta .
27
r . Studies excluded
p> ‘ 7 Not multi-modality
a 44 | ‘ 5 Not diagnosis / prediction
2 Full-text studies assessed for --- DP: 4 Imaging only
D ahi: ‘ 3 Not deep learning
nn eligibility : 3 Nomedical imaging
XS ‘ 2 Duplicate
: 2 Segmentation
1 Poor quality
17

Studies included in systematic
review

Inclusion

 

 

Fig. 3 PRISMA flowchart of the study selection process. Two
authors independently screened all records for eligibility. Seventeen
studies were included in the systematic review.

DISCUSSION

The purpose of this review is to aggregate the collective
knowledge of prior work applying multimodal deep learning
fusion techniques that combine medical imaging with clinical
data. We propose consistent terminology for multimodal fusion
techniques and categorize prior work by fusion strategy. Overall,
we found that multimodality fusion models generally led to
increased accuracy (1.2-27.7%) and AUROC (0.02-0.16) over
traditional single modality models for the same task. However,
no single fusion strategy consistently led to optimal performance
across all domains. Since our literature review shows that
additional patient information and clinical context can result in
better model performance, and fusion methods better replicate
the human expert interpretation workflow, it is recommended to
always experiment with fusion strategies when multimodal data is
available.

The deep learning fusion models reviewed represent a
spectrum of medical applications ranging from radiology*' to
hematology*”. For example, fusion strategies were often applied
to the diagnosis and prediction of Alzheimer’s disease*?*°777"",
In clinical practice, neither imaging nor clinical data alone are
sufficient for the diagnosis of Alzheimer’s disease. Leveraging
deep learning fusion techniques consistently showed improve-
ments in performance for Alzheimer’s disease diagnosis, while
physicians struggle with accurate and reliable diagnostics even
when multimodality is present, as proven by histopathological
correlation*”. This highlights the importance and utility of
multimodal fusion techniques in clinical applications.

Fusion approaches in other less complex clinical applications
also improved performance over single modality models, even
those in which single modality models have been widely reported
to achieve high performance, such as pixel-based models for
automated skin cancer detection**. While the fusion approach
varied widely, the consistent improvement in reported perfor-
mance across a wide variety of clinical use cases suggests that
model performance based on single-modal data may not

npj Digital Medicine (2020) 136

represent state of the art for a given application when multimodal
data are not considered.

The complexity of the non-imaging data in multimodal fusion
work was limited, particularly in the context of available feature-
rich and time-series data in the EHR. Instead, most studies focused
primarily on basic demographic information such as age and
gender~”*”””, a limited range of categorical clinical history such as
hypertension or smoking status**** or disease-specific clinical
features known to be strongly associated with the disease of
interest such as APOE4 for Alzheimer’s*?****°° or PSA blood test
for prediction of prostate cancer*”. While selecting features known
to be associated with disease is meaningful, future work may
further benefit from utilizing large volumes of feature-rich data, as
seen in fields outside medicine such as autonomous driving**””.

Implementation guidelines for fusion models

In most applications early fusion was used as the first attempt for
multimodal learning, a straightforward approach that does not
necessarily require training multiple models. However, when the
input modalities are not in the same dimensions, which is typical
when combining clinical data represented in 1D with imaging
data in 2D or 3D, then high-level imaging features must be
extracted as a 1D vector before fusing with the 1D clinical data.
There were a variety of strategies used to accomplish this;
including using manually extracted imaging features or software-
generated features*”°* °°. It is worth noting, that unless there is a
compelling reason for using such an approach, outputs from linear
layers of a CNN are usually effective feature representations of the
original image*®??*"'. This is because learned features representa-
tions often result in much better task-specific performance than
can be obtained with manual or software extracted features”°.
Based on the reviewed papers, early fusion consistently improved
performance over single modality models, and is supported by
this review as an initial strategy to fuse multimodal data.

When using CNNs to extract features from imaging modalities,
the same CNNs can also be used in joint fusion. However, joint
fusion is implemented using neural networks which can be a
limitation especially with smaller datasets better suited for
traditional machine learning models. For example, if there are
disproportionately few samples relative to the number of features
in the dataset or if some of the input features are sparsely
represented, early or late fusion is preferred because they can be
implemented with traditional machine learning algorithms (e.g.,
Lasso and ElasticNet*’) that are better suited for this type of
data*®. Nevertheless, joint and early fusion neural networks are
both able to learn shared representations, making it easier for the
model to learn correlations across modalities, thereby resulting in
better performance*”. Studies have also shown that fusing highly
correlated features in earlier layers and less correlated features in
deeper layers improve model performance°®”'. In addition, we
suspect that joint fusion models have the potential to outperform
other fusion strategies, as the technique iteratively updates its
feature representations to better complement each modality
through simultaneous propagation of the loss to all feature
extracting models. Yet to date, there is insufficient evidence to
systematically assess this effect in fusion for medical imaging and
is an important area for future exploration.

When signals from different modalities do not complement
each other, that is to say input modalities separately inform the
final prediction and do not have inherent interdependency, then
trying a late fusion approach is preferred. This is chiefly because
when feature vectors from multiple modalities are concatenated,
such as in early and joint fusion, high-dimensional vectors are
generated which can be difficult for machine learning models to
learn without overfitting, unless a large number of input samples
are available. This is the so-called “curse of dimensionality” in
machine learning’”°. Late fusion mitigates this problem by

Seoul National University Bundang Hospital
Np}

S.-C. Huang et al.

 

JOUNV £6°0
:sainjeaj JUNOD poojg

(xapul Jaz} Ua)
‘Y ulqo;boway ‘zy ulqoj|bowey
Ly ulqo|boway ‘My ‘DHIW

 

DOUNY 88'0 :sebeuw| ‘HOW ‘ADW ‘LDH ‘WD0}eWseY soinjeay elwosysodAy 6z IF 2°
JOYNV O0'L :uOolsn4 0Z ‘JUNOD poojq ajajdWOd) 1s9} poojg s|ja> poojg pay jo sobew| pe}e11X9 NND DIADOIDIW JO UO!}D9}0q ABojoyewsay Jemind OZ0Z A\iey
AdeINIDV %96'7O
isainyea} JOWWNY
pue ssiudeibowaqg
AdeINIDY %v0'LS ‘IHW
Adeind3y/ (adj jed1H0j03s1y soinjeajy = suatzed sOUWN} Ulelq JO4 ABojoduO
%99'06 :uOIsN4 £6 ‘8ZIS JOLUN ‘abe) eyep JUaeg MW pe pes}xe NND AW} [eAIAINS JO UO!DIPaAd /ABojoipey icl2 38 2IN 6L0Z A\iey
(vaOdY ‘UOnednpe
‘apuab ‘abe) eyep juaied
(UOIZeUILUeXA 9321S JeJUaP-IUIY/\
pue ‘aJIEUUO!SaNH JUsLUSSESsSY
jeuolaun4 ‘jsey Huruseay
X@PuI-D jequa, Asoypny Aay ‘ajedsqns
968'0 :evep aaiubo> BAI}UHOD-a]e9S JUaLUSSASsSY sainjeaj aseasip
X9PUI-D 1L06'0 :uoIsn4 778 aseasiq SJBWIDUZ|\) S}JUBWSSASSY/ MW pepesxes NND = SJASWIBYZ|\V JO UO!DIPeAd ABojoinen ezl@ 211 6L0Z A\iey
(snje}s Burjouus sainjeay pa}de11x9 ABojoouO
JOUNV 6S8’0 :uoIsn4 96€ = ‘azIS JOUWUN} ‘xas ‘aHe) eJep JUAaIeg 1D/1ad  Ajenuew 410 aremyos Ja2ue> 6un7 /Abojoipey ,,"12 38 UNAH 6L0Z A\aeq
DIOYUNV OL8°0
reyepejaw jualjed
DOYUNV 8880
:sobew] DIdodIsOoWaG PWOURjaW JO UO!}Da}8p
+ dIdodsoseWy (uoI}eD0] sabeuwl dIdodsowag sainjeay pue uOlse}
JOYNV 888'0 :uoIsn4 L162 Apoq ‘xas ‘abe) eyep juaned ‘sobew! DIdodsoseyp pepeixe NND ulys JO UOIeDYISSe]> Abojoyeuag =e idea gL1oz Ayez
Joy ainjosqy soinjeaj
Ueda] SSP'O :uOIsn4 8S8'LL (xas ‘a6e) eyep juaied Kel-X pa}e11X9 NND juauussasse abe auo0g ABojoipey oe l2 NIT ~BLOzZ A\iey
suusAunaue
Asaye Buljyediunuiwod
(suqey Burjouus sainjeajy pa}de1}x9 Jouajue ul
JOYNV 8Z6'0 :uoIsn4 v6g ‘UoIsuUdayedKy ‘xas ‘abe) ejep juaieg 1D Ayjenuew so aiemyos = ysl aINJdN JO UOIPDIPAIg ABojoinen yel@ NI 8Loz A\iey
Adeindoy
%I'GL -9|YOld USN}
AdeINDDV %ZL'V8
‘sobew] DIdoDsoWeq
Adeind.V (9ZIS UOISa| ‘UOI}ED0] UOISA| saunjeaj U01}99}e8p oz le 8
%L'L6 :UOISN4 L6LL ‘uoleAaja ‘xas ‘abe) eyep uae, sabewl I1IdodsoWag pe}e11X9 NND eBWOUIDIeD |ja> jeseg ABojoyewaq IwzeJeUy SLOT A\iey
DOYNV 260
isainquyy jedul|D BSeISIP SJOWIBYZ|V (abes
JOYNV €8'0 ‘IMW (vaOdV) e}ep DeUayH sainjeajy pa}de1}x9 Ul Sa110}Defe12 WO YdWAS ec le 2 -1]/NUW)
YOUN 66'0 :uOlsn4 ZOEL (sasods jediulj> ‘a6e) eyep jUaleg IMI = Ayjenuew Jo a1emyos JEDIUI]D JO UOIDIPAd ABojounan yembeug gloz Ayez
Aydes6moy apydeds saseq
Adeind3y/ (quajeainba ‘AydesBowo} sainjeay pajde1xo
%8°Zg8 :uodIsSn4 €9L jesuuayuds ‘xas ‘abe) ejep juaied adUaJBYOD jeIIdO = Ajjenuew JO a1eMYOoS uoljedyIssej> eEWUUODNe|yH ABojowyeyjyydo cc l@ 2 UY 8LOZ A\iey
ADeINDDV %0'8S “IHW
Adeindoy
%LLO -Ldd + YW
AdeindoV (vaOdV) e}ep WaUayH sainjeaj pe}de1}x9 aseasip ‘ye jo
%9°E9 :uOIsSN4 S08 (uolzednp—a ‘xas ‘abe) eyep jUaled IMW ‘Ladd = Ajjenuew Jo aiemyos = S,Jawulayzjy Jo sisoubeig ABojoinen buny, Z1L0Z A\aeq
sajduwes ABaye13s
BdUeEWOJed japoyW) JO JequuNny eyep GHulbewi-uou :3nduy| BHuiBeuul jedipau :3ndu| sjlejyap uoisn4 3WOIINO uleWOp |eDdIUl|> Jouiny ea, uolsn4
“MAIADI DIVEWAYSAS DY} UL PAPNIIU! SIIPNIS JO MAIAIBAQ “*L aqe]

npj Digital Medicine (2020) 136

Seoul National University Bundang Hospital
S.-C. Huang et al.

‘Apnys yea wolJ paypesxa eyep pue ABaje1}s UOIsny Hulpnjdu! MalAas DeWAsAS BY} Ul PapNyjru! salpnys |je JO MAaIAJBAO Ue SAapIAOJd ajqe} aUL

ADeINDDVY %BL'ZZ ‘WSd
AdeINDDY %68'88 ‘IHW
Adeindoy

%V'VH -UOISNY

ADdeINDDVY %L'°68 ‘IW
Adeind.y

WE'VE -ASWW
ADeINDDV WL'EB “IMW
Adeind.y

%6'06 -UOISNY O8E

DOYNV 629°0
sounjea4 |ed1Ul|D
DOUNV 8LZLO0 ‘YW
DOYNV V7ZL'0
:uOIsn4 93e7]

IOYNV 9vZ0
:UOISN4 JUIOF

ABojoouO
/ABojoipey 4, "Je 2 epay LOZ ae]

sisoubelp J@2uUeD 3}e}SO1d

}S9} POOo|q WSd Jaylsse|> eyo

(3893 (WW)
Aloway [221607 ajeds Aioway|

Ja[syram ‘(ASWW) UOMeUIWeXy
3323S je}UaJ IUI|\/) SJUaLUSSaSsY

(IDW) JUaWIedwuI

Bunon Aywofley SAINUBOD PII AbojounaN =, 18 1A NID 8 L0z a1e7

(quana s[D [elu

JO uo!}ed0] ‘JaSUO Je (S|D) BWOIpUAS
payejos! Ajjediurj> jed0jnjNwW

‘SA -IUN ‘ajeds snyeys AzIGQesIp
papuayxe ‘xas) eyep juaied

(Ayisuap
yseaiq pue ‘nis ul eWOUIDIeD
Jeingqoy jo Ayoysiy ‘eisejduadAy

jeridAje Jo Asoysiy ‘snzeys
uoleinw WDYg “4aDuUeD UeLeAO
pue yseaiq jo Asoysiy Ajiwmey
payjieyap ‘snyeys jesnedouaw ‘abe
aydeusW

‘yUuBlay ‘yUuBIam ‘abe) eyep uae,

(93e7) Buibesany = sueadA OM} UIYUM SISOJB|DS
aunjeas 3]diyjnu 0} UOISJaAUOD a}e7]
pe peixe NND snjeys BulydIpad ABojONeN = g'JP 39 COA 6 LOZ yulor

DOYNV Z9°0
:SB1ODS SIY

DOYNV 89°0
:SuUeIBOWULUe|

DJOYNV O20 -YOIsny

UO!}DIPaId
ysl JaDuUeD JSeaIg

ABojoouO

swesbowwey| /ABojoipey "JB 38 LILA

(3S9} Buluseay jequan

Asoypne Aay ‘ajeds JUawssasse
aseasip SJOWIaYZI|V ‘[GSHYSD] Bunes
el]UaWag |ed1UI|D) s}JUaWSsassy
(vdOdV) ep 28uUeD

(spInyoiq ‘Uolednpe

‘gne1 ‘xas ‘abe) eyep juaied

aseasip
DOYNV OO'L -UOo!sn4 SABUWIDUZIY JO UOIDIPAd

Adeindoy

ABojounan

Seoul National University Bundang Hospital

Np)

 

®WL'yO :sebeuy jed1Ul|D

AdeINDDVY %S'TZ
‘sobew] DIdodsoueq

Adeindoy
%MLEL :UOISN4

aDURWOJJad japoy|

LLOL

sajdwes
Jo Jaquinn

(uo!}ed0] UOISa] ‘xaS) eJeEP JUAaIJeYg

eyep GHulbewi-uou :3nduy|

sabe] jed1ul|>
‘sobew| DIdoDsOUaEG

BHuiBeuul jedipau :3ndu|

yse}-13/NWW jePoWNyy

sjl€jop UOISnN4

PWOURJAW JO} 1SI[4IOUD
jUIOd-UaAaS ay} JO YDeA
Jo uolnedyissej> Aseuig
pue uoledyisse|>
ewouray

aW021NO

ABojoyewaq

UJEWOP |e>1UI|D

eyeyemey 8L0Z

6c IF 30
«UIP

ABaye13s

JOYuINY Jeo, uolsn

panuljUoD | aIqeL

 

npj Digital Medicine (2020) 136
Table 2. Properties and benefits of different fusion strategies.

Able to make predictions when not all modalities are present

Able to model interactions between features from different modalities
Able to learn more compatible features from each modality

Does not necessarily require a large amount of training data

Does not require training multiple models

Does not necessarily require meticulous designing efforts

Flexibility to join input at different levels of abstraction

Different properties and benefits for each fusion strategy.

S.-C. Huang et al.

np)

 

“Specialized joint fusion architecture such as Kawahara et al.'s multi-modal multi-task model is capable of handling missing data.
PEarly fusion requires training of multiple models when the imaging features are extracted using CNN.

utilizing multiple models that are each specialized on a single
modality, thus limiting the input feature vector size for each
model. For example, the quantitative result of a Mini Mental State
Examination and the pixel data obtained from a brain MRI (e.g.,
Qiu et al.*") are largely independent data, and would therefore be
suitable candidates for input into late fusion models.

Furthermore, in the common real-world scenario of missing or
incomplete data, i.e. some patients have only clinical data
available but no imaging data or vice-versa, late fusion retains
the ability to make predictions. This is because late fusion employs
separate models for separate modalities, and aggregation
functions such as majority voting and averaging can be applied
even when predictions from a modality is missing. When the
different input modalities have very different numbers of features,
predictions might be overly influenced by the most feature-rich
modality (e.g., Reda et al.*°). Late fusion is favorable in this
scenario as it considers each modality separately. Yoo et al.°° also
showed that repeating or scaling the modality that has fewer
features before fusion achieved a boost in the model’s perfor-
mance. Nonetheless, joint fusion can also be tuned to mitigate the
difference in number of features, by setting feature producing
linear layers of the feature extraction model to output a similar
number of features as the other modalities. Our recommendations
are summarized in Table 2.

Ideally, researchers want to first build and optimize single
modality models to dually serve as baseline models and provide
inputs to fusion models. Multiple fusion strategies can then be
implemented to compare model performance and guide sub-
sequent fusion experiments. Since better performance is consis-
tently achieved with multimodal fusion techniques, routine best
practice should include reporting of the systematic investigation
of various fusion strategies in addition to deep learning
architectures and hyperparameters.

Limitations

We devised our search string to only consider papers after 2012.
This constitutes a limitation as we excluded earlier papers that
applied fusion using traditional machine learning techniques or
simple feed-forward neural networks. Publication bias is an
important limitation since positive results can be disproportio-
nately reported in the published literature, which may have the
aggregate effect of overrepresenting the advantages of fusion
techniques. Furthermore, using our study selection criteria, we
only looked at fusion techniques applied to clinical prediction and
diagnosis, but we recognize that fusion can be applied to other
interesting medical tasks such as segmentation and registration.

As the included studies investigate different objectives, use
different input modalities, report different performance metrics,
and not all papers provide confidence bounds, we are not able to
aggregate or statistically compare the performance gains in a

Seoul National University Bundang Hospital

meta-analysis. In addition, the reported metrics cannot always be
considered valid, since some studies didn’t use an independent
test-set for an unbiased performance estimate*”*°. The limited
number of studies per medical field and the heterogeneity of each
study also makes it difficult to compare the studies qualitatively. A
few studies implemented fusion in unconventional ways, which
may introduce subjectivity when we classify each study into early,
late, and joint fusion.

Future research

This systematic review found that multimodal fusion in medicine
is a promising yet nascent field that complements the clinical
practice of medical imaging interpretation across all disciplines.
We have defined and summarized key terminology, techniques,
and evaluated the state of the art for multimodal fusion in medical
imaging, honing in on key insights and unexplored questions to
guide task and modality-specific strategies. The field of multi-
modal fusion for deep learning in medical imaging is expanding
and novel fusion methods are expected to be developed. Future
work should focus on shared terminology and metrics, including
direct evaluation of different multimodal fusion approaches when
applicable. We found that multimodal fusion for automated
medical imaging tasks broadly improves the performance over
single modality models, and further work may discover additional
insights to inform optimal approaches.

METHODS

This systematic review was conducted based on the PRISMA
guidelines”’.

Search strategy

A systematic literature search was implemented in PubMed and
Scopus under the supervision of a licensed librarian. The key
search terms included a combination of the three major themes:
‘deep learning’, ‘multimodality fusion’, and ‘medical imaging’.
Terms for segmentation, registration, and reconstruction were
used as exclusion criteria in the search. The search encompassed
papers published between 2012 and 2020. This range was
considered appropriate due to the rise in popularity in applying
CNN on medical images since the 2012 ImageNet challenge. The
complete search string for both databases is provided in
Supplementary Methods. For potentially eligible studies cited by
articles already included in this review, additional targeted free-
text searches were conducted on Google Scholar if they did not
appear in Scopus or PubMed.

We included all research articles in all languages that applied
deep learning models for clinical outcome prediction or diagnosis
using a combination of medical imaging modalities and EHR data.

npj Digital Medicine (2020) 136

 
np}

S.-C. Huang et al.

 

Studies specific to deep learning were included rather than the
broader field of machine learning because deep learning has
consistently shown superior performance in image-related tasks.
We selected only studies that fused medical imaging with EHR
data since, unlike image-image fusion, this is an exciting new
technique that effectively merges heterogeneous data types and
adds complementary rather than overlapping information to
inform prediction and diagnosis. We defined medical imaging
modalities as any type of medical images used in clinical care.
Studies that used deep learning only for feature extractions were
also included for our review. We excluded any study that
combined extracted imaging features with the original imaging
modality, as we still considered this a single modality. Articles that
fused multimodal data for segmentation, registration or recon-
struction were also excluded due to our criteria for outcome
prediction and diagnosis. Articles from electronic preprint archives
such as ArXiv were excluded in order to ensure only papers that
passed peer-review were included. Lastly, papers with poor quality
that hindered our ability to meaningfully extract data were also
excluded.

Study selection

The Covidence software (www.covidence.org) was used for
screening and study selection. After removal of duplicates, studies
were screened based on title and abstract, and then full-texts were
obtained and assessed for inclusion. Study selection was
performed by two independent researchers (S.-C.H. and A.P.),
and disagreements were resolved through discussion. In cases
where consensus could not be achieved a third researcher was
consulted (I.B.).

Data extraction

For benchmarking the existing approaches we extracted the
following data from each of the selected articles: (a) fusion
Strategy, (b) year of publication, (c) authors, (d) clinical domain,
(e) target outcome, (f) fusion details, (g) imaging modality, (h)
non-imaging modality, (i) number of samples, and (j) model
performance (Table 1). We classified the specific fusion strategy
based on the definitions in the section “Terminology and
strategies in fusion”. The number of samples reported is the full
data-size including training, validation and testing data. For
classification tasks we extracted AUROC whenever this metric
was reported, otherwise we extracted accuracy. When the article
contained several experiments, metrics from the experiment
with the best performing fusion model were extracted. These
items were extracted to enable researchers to find and compare
current fusion studies in their medical field or input modalities
of interest.

DATA AVAILABILITY

The authors declare that all data supporting the findings of this study are available
within the paper and its Supplementary information files.

Received: 22 April 2020; Accepted: 17 September 2020;
Published online: 16 October 2020

REFERENCES
1. Leslie, A., Jones, A. J. & Goddard, P. R. The influence of clinical information on the
reporting of CT by radiologists. Br. J. Radiol. 73, 1052-1055 (2000).
2. Cohen, M. D. Accuracy of information on imaging requisitions: does it matter? J.
Am. Coll. Radiol. 4, 617-621 (2007).
3. Boonn, W. W. & Langlotz, C. P. Radiologist use of and perceived need for patient
data access. J. Digit. Imaging 22, 357-362 (2009).

npj Digital Medicine (2020) 136

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.

28.

29.

. Comfere, N. I. et al. Provider-to-provider communication in dermatology and

implications of missing clinical information in skin biopsy requisition forms: a
systematic review. Int. J. Dermatol. 53, 549-557 (2014).

. Comfere, N. |. et al. Dermatopathologists’ concerns and challenges with clinical

information in the skin biopsy requisition form: a mixed-methods study: Clinical
information in dermatopathology. J. Cutan. Pathol. 42, 333-345 (2015).

. Jonas, J. B. et al. Glaucoma. The Lancet 390, 2183-2193 (2017).
. McDonald, R. J. et al. The effects of changes in utilization and technological

advancements of cross-sectional imaging on radiologist workload. Acad. Radiol.
22, 1191-1198 (2015).

. Dean, N. C. et al. Impact of an electronic clinical decision support tool for

emergency department patients with pneumonia. Ann. Emerg. Med. 66, 511-520
(2015).

. Banerjee, |. et al. Development and Performance of the Pulmonary Embolism

Result Forecast Model (PERFORM) for Computed Tomography Clinical Decision
Support. JAMA Netw. Open 2, e198719 (2019).

Sandeep Kumar, E. & Jayadev, P. S$. Deep Learning for Clinical Decision Support
Systems: A Review from the Panorama of Smart Healthcare. In Deep Learning
Techniques for Biomedical and Health Informatics. Studies in Big Data (eds. Dash, S.
et al.) vol. 68, 79-99 (Springer, Cham, 2020).

Hinton, G. Deep learning—a technology with the potential to transform health
care. JAMA 320, 1101 (2018).

Stead, W. W. Clinical implications and challenges of artificial intelligence and
deep learning. JAMA 320, 1107 (2018).

Dunnmon, J. A. et al. Assessment of convolutional neural networks for automated
classification of chest radiographs. Radiology 290, 537-544 (2019).

Irvin, J. et al. CheXpert: a large chest radiograph dataset with uncertainty labels
and expert comparison. Proc. AAAI Conf. Artif. Intell. 33, 590-597 (2019).

Jaeger, S. et al. Two public chest X-ray datasets for computer-aided screening of
pulmonary diseases. Quant. Imaging Med. Surg. 4, 475-477 (2014).

Johnson, A. E. W. et al. MIMIC-CXR, a de-identified publicly available database of
chest radiographs with free-text reports. Sci. Data 6, 317 (2019).

Kallianos, K. et al. How far have we come? Artificial intelligence for chest radio-
graph interpretation. Clin. Radiol. 74, 338-345 (2019).

Gulshan, V. et al. Development and validation of a deep learning algorithm for
detection of diabetic retinopathy in retinal fundus photographs. JAMA 316, 2402
(2016).

Rajpurkar, P. et al. Deep learning for chest radiograph diagnosis: A retrospective
comparison of the CheXNext algorithm to practicing radiologists. PLoS Med. 15,
e€1002686 (2018).

Majkowska, A. et al. Chest radiograph interpretation with deep learning models:
assessment with radiologist-adjudicated reference standards and population-
adjusted evaluation. Radiology 294, 421-431 (2020).

Person, M., Jensen, M., Smith, A. O. & Gutierrez, H. Multimodal fusion object
detection system for autonomous vehicles. J. Dyn. Syst. Meas. Control! 141,
071017 (2019).

Trzcinski, T. Multimodal social media video classification with deep neural net-
works. In Photonics Applications in Astronomy, Communications, Industry, and
High-Energy Physics Experiments 2018 (eds. Romaniuk, R. S. & Linczuk, M.) (SPIE,
2018).

Ramachandram, D. & Taylor, G. W. Deep Multimodal Learning: A Survey on
Recent Advances and Trends. /EEE Signal Process. Mag. 34, 96-108 (2017).

Kiela, D., Grave, E., Joulin, A. & Mikolov, T. Efficient large-scale multi-modal clas-
sification. In The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18),
(2018).

Thung, K.-H., Yap, P.-T. & Shen, D. Multi-stage Diagnosis of Alzheimer’s Disease
with Incomplete Multimodal Data via Multi-task Deep Learning. In Deep Learning
in Medical Image Analysis and Multimodal Learning for Clinical Decision Support
(eds. Cardoso, M. J. et al.) vol. 10553, 160-168 (Springer International Publishing,
2017).

Kharazmi, P., Kalia, S., Lui, H., Wang, Z. J. & Lee, T. K. A feature fusion system for
basal cell carcinoma detection through data-driven feature learning and patient
profile. Skin Res. Technol. 24, 256-264 (2018).

Yap, J., Yolland, W. & Tschandl, P. Multimodal skin lesion classification using deep
learning. Exp. Dermatol. 27, 1261-1267 (2018).

Li, H. & Fan, Y. Early Prediction Of Alzheimer’s Disease Dementia Based On
Baseline Hippocampal MRI and 1-Year Follow-Up Cognitive Measures Using Deep
Recurrent Neural Networks. In 2079 IEEE 16th International Symposium on Bio-
medical Imaging (ISBI 2019) 368-371 (IEEE, 2019).

Purwar, S., Tripathi, R. K., Ranjan, R. & Saxena, R. Detection of microcytic hypo-
chromia using cbc and blood film features extracted from convolution neural
network by different classifiers. Multimed. Tools Appl. 79, 4573-4595 (2020).

. Liu, Minggian, Lan, Jun, Chen, Xu, Yu, Guangjun & Yang, Xiujun Bone age

assessment model based on multi-dimensional feature fusion using deep
learning. Acad. J. Second Mil. Med. Univ. 39, 909-916 (2018).

Seoul National University Bundang Hospital
31.

32.

33.

34.

35.

36.

37.

38.

39.

40.

41.

42.

43.

AA,

45.

46.

47.

48.

49.

50.

51.

52.

Nie, D. et al. Multi-channel 3D deep feature learning for survival time prediction
of brain tumor patients using multi-modal neuroimages. Sci. Rep. 9, 1103 (2019).
Hyun, S. H., Ahn, M. S., Koh, Y. W. & Lee, S. J. A machine-learning approach using
PET-based radiomics to predict the histological subtypes of lung cancer. Clin.
Nucl. Med. 44, 956-960 (2019).

Bhagwat, N., Viviano, J. D., Voineskos, A. N., Chakravarty, M. M. & Alzheimer’s
Disease Neuroimaging Initiative. Modeling and prediction of clinical symptom
trajectories in Alzheimer’s disease using longitudinal data. PLOS Comput. Biol. 14,
e€1006376 (2018).

Liu, J. et al. Prediction of rupture risk in anterior communicating artery aneurysms
with a feed-forward artificial neural network. Eur. Radiol. 28, 3268-3275 (2018).
An, G. et al. Comparison of Machine-Learning Classification Models for Glaucoma
Management. J. Healthc. Eng. 2018, 1-8 (2018).

Spasov, S. E., Passamonti, L., Duggento, A., Lio, P. & Toschi, N. A. Multi-modal
Convolutional Neural Network Framework for the Prediction of Alzheimer’s Dis-
ease. In 2018 40th Annual International Conference of the IEEE Engineering in
Medicine and Biology Society (EMBC) 1271-1274 (IEEE, 2018).

Yala, A. Lehman, C., Schuster, T., Portnoi, T. & Barzilay, R. A deep learning
mammography-based model for improved breast cancer risk prediction. Radi-
ology 292, 60-66 (2019).

Yoo, Y. et al. Deep learning of brain lesion patterns and user-defined clinical and
MRI features for predicting conversion to multiple sclerosis from clinically iso-
lated syndrome. Comput. Methods Biomech. Biomed. Eng. Imaging Vis. 7, 250-259
(2019).

Kawahara, J., Daneshvar, S., Argenziano, G. & Hamarneh, G. Seven-point checklist
and skin lesion classification using multitask multimodal neural nets. /EEE J.
Biomed. Health Inform. 23, 538-546 (2019).

Reda, |. et al. Deep learning role in early diagnosis of prostate cancer. Technol.
Cancer Res. Treat. 17, 153303461877553 (2018).

Qiu, S. et al. Fusion of deep learning models of MRI scans, Mini-Mental State
Examination, and logical memory test enhances diagnosis of mild cognitive
impairment. Alzheimers Dement. Diagn. Assess. Dis. Monit. 10, 737-749 (2018).
Beach, T. G., Monsell, S. E., Phillips, L. E. & Kukull, W. Accuracy of the Clinical
Diagnosis of Alzheimer Disease at National Institute on Aging Alzheimer Disease
Centers, 2005-2010. J. Neuropathol. Exp. Neurol. 71, 266-273 (2012).

Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural
networks. Nature 542, 115-118 (2017).

Hecker, S., Dai, D. & Van Gool, L. End-to-End Learning of Driving Models with
Surround-View Cameras and Route Planners. In Computer Vision — ECCV 2018 (eds.
Ferrari, V. et al.) vol. 11211, 449-468 (Springer International Publishing, 2018).
Jain, A., Singh, A., Koppula, H. S., Soh, S. & Saxena, A. Recurrent Neural Networks
for driver activity anticipation via sensory-fusion architecture. In 2076 IEEE Inter-
national Conference on Robotics and Automation (ICRA) 3118-3125 (IEEE, 2016).
Goodfellow, |., Bengio, Y. & Courville, C. Deep Learning (MIT Press, 2017).

Zou, H. & Hastie, T. Regularization and variable selection via the elastic net. J. R.
Stat. Soc. Ser. B Stat. Methodol. 67, 301-320 (2005).

Subramanian, V., Do, M. N. & Syeda-Mahmood, T. Multimodal Fusion of Imaging
and Genomics for Lung Cancer Recurrence Prediction. In 2020 IEEE 17th Inter-
national Symposium on Biomedical Imaging (ISBI) 804-808 (IEEE, 2020).

Ngiam, J. et al. Multimodal Deep Learning. In Proceedings of the 28th International
Conference on Machine Learning (ICML) 689-696 (2011).

Karpathy, A. et al. Large-Scale Video Classification with Convolutional Neural
Networks. In 2074 IEEE Conference on Computer Vision and Pattern Recognition
1725-1732 (IEEE, 2014).

Neverova, N., Wolf, C., Taylor, G. & Nebout, F. ModDrop: adaptive multi-modal
gesture recognition. /EEE Trans. Pattern Anal. Mach. Intell. 38, 1692-1706 (2016).
Bach, F. Breaking the curse of dimensionality with convex neural networks. J.
Mach. Learn. Res. 18, 1-53 (2017).

Seoul National University Bundang Hospital

S.-C. Huang et al.

Np}

 

53. Mwangi, B., Tian, T. S. & Soares, J. C. A review of feature reduction techniques in
neuroimaging. Neuroinformatics 12, 229-244 (2014).

54. David, M., Alessandro, L., Jennifer, T. & Douglas, G. A. The PRISMA Group Preferred
reporting items for systematic reviews and meta-analyses: The PRISMA state-
ment. PLoS Med. 6, €1000097 (2009).

ACKNOWLEDGEMENTS

The authors wish to thank John Alexander Borghi from Stanford Lane Medical Library
for his help with creating the systematic search. The research reported in this
publication was supported by the National Library of Medicine of the National
Institutes of Health under Award Number RO1LM012966. The content is solely the
responsibility of the authors and does not necessarily represent the official views of
the National Institutes of Health.

AUTHOR CONTRIBUTIONS

S-C.H. and A.P. are co-first authors who contributed equally to this study. Concept
and design: S.-C.H., A.P., M.P.L., and I.B. Study selection: S.-C.H. and A.P. Data extraction:
S.-C.H., A.P., and S.S. Drafting of the manuscript: S.-C.H., A.P., |.B., and M.P.L. Critical
revision of the manuscript for important intellectual content: S.-C.H., A.P., |.B., and M.P.L.
Supervision: |.B. and M.P.L.

COMPETING INTERESTS

The authors declare no competing interests.

ADDITIONAL INFORMATION

Supplementary information is available for this paper at https://doi.org/10.1038/
541746-020-00341-z.

Correspondence and requests for materials should be addressed to S.-C.H.

Reprints and permission information is available at http://www.nature.com/
reprints

Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims
in published maps and institutional affiliations.

Open Access This article is licensed under a Creative Commons

BY Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this license, visit http://creativecommons.
org/licenses/by/4.0/.

© The Author(s) 2020

npj Digital Medicine (2020) 136
