Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 FEURASIP J ournal on Advances
https://doi.org/10.1186/s13634-020-00684-5 ins ign 2 |p rocessi ng
IN Sl

RESEARCH Open Access

DeConFuse: a deep convolutional ®
transform-based unsupervised fusion
framework

Pooja Gupta!” Jyoti Maggu!, Angshul Majumdar!#, Emilie Chouzenoux? and Giovanni Chierchia4

 

 

*Correspondence:

poojag@iiitd.ac.in

‘Indraprastha Institute of This work proposes an unsupervised fusion framework based on deep convolutional

Information Technology, Delhi, India ; ; y ; . .

Full list of author information is transform learning. The great learning ability of convolutional filters for data analysis is

available at the end of the article well acknowledged. The success of convolutive features owes to the convolutional
neural network (CNN). However, CNN cannot perform learning tasks in an unsupervised
fashion. In a recent work, we show that such shortcoming can be addressed by
adopting a convolutional transform learning (CTL) approach, where convolutional
filters are learnt in an unsupervised fashion. The present paper aims at (i) proposing a
deep version of CTL, (ii) proposing an unsupervised fusion formulation taking
advantage of the proposed deep CTL representation , and (iii) developing a
mathematically sounded optimization strategy for performing the learning task. We
apply the proposed technique, named DeConFuse, on the problem of stock
forecasting and trading. A comparison with state-of-the-art methods (based on CNN
and long short-term memory network) shows the superiority of our method for
performing a reliable feature extraction.

Abstract

Keywords: Information fusion, Deep learning, Convolution, Stock trading, Financial
forecasting

 

1 Introduction
In the last decade, convolutional neural network (CNN) has enjoyed tremendous success
in different types of data analysis. It was initially applied for images in computer vision
tasks. The operations within the CNN were believed to mimic the human visual sys-
tem. Although such a link between human vision and CNN may be present, it has been
observed that deep CNNs are not exact models for human vision [1]. For instance, biol-
ogists consider that the human visual system would consist of 6 layers [2, 3] and not 20+
layers used in GoogleNet [4].

Neural network models have also been used for analyzing time series data. Until
recently, long short-term memory (LSTM) networks were the almost exclusively used
neural network models for time series analysis as they were supposed to mimic memory

and hence were deemed suitable for such tasks. However, LSTM are not able to model

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
GQ) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
— credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 2 of 32

very long sequences, and their training is hardware intensive. Owing to these short-
comings, LSTMs are being replaced by CNNs. The reason for the great results of CNN
methods for time series analysis (1D data processing in general) is not well understood.
One possibility may lie in the universal function approximation capacity of deep neural
networks [5, 6] rather than its biological semblance. The research in this area is primarily
led by its success rather than its understanding.

An important point to mention is that the performance of CNN is largely driven by
the availability of very large labeled datasets. This probably explains their tremendous
success in facial recognition tasks. Google’s FaceNet [7] and Facebook’s DeepFace [8]
architectures are trained on 400 million facial images, a significant proportion of world’s
population. These companies are easily equipped with gigantic labeled facial images data
as these are “tagged” by their respective users. In the said problem, deep networks reach
almost 100% accuracy, even surpassing human capabilities. However, when it comes to
tasks that require expert labeling, such as facial recognition from sketches (requiring
forensic expertise) [8] or ischemic attack detection from EEG (requiring medical exper-
tise) [9], the accuracies become modest. Indeed, such tasks require expert labeling that is
difficult to acquire, thus limiting the size of available labeled dataset.

The same is believed by a number of machine learning researchers, including Hin-
ton himself, who are wary of supervised learning. In an interview with Axios!, Hinton
mentioned his “deep suspicion” on backpropagation, the workhorse behind all super-
vised deep neural networks. He even added that “I don’t think it’s how the brain works,’
and “We clearly don’t need all the labeled data” It seems that Hinton is hinting towards
unsupervised learning frameworks. Unsupervised learning technique does not require
targets/labels to learn from data. This approach typically takes benefit from the fact that
data is inherently very rich in its structure, unlike targets that are sparse in nature. Thus,
it does not take into account the task to be performed while learning about the data, sav-
ing from the need of human expertise that is required in supervised learning. More on the
topic of unsupervised versus supervised learning can be found in a blog by DeepMind?.

In this work, we would like to keep the best of both worlds, i.e., the success of convo-
lutive models from CNN and the promises of unsupervised learning formulations. With
this goal in mind, we developed convolutional transform learning (CTL) [10]. This is a
representation learning technique that learns a set of convolutional filters from the data
without label information. Instead of learning the filters (by backpropagating) from data
labels, CTL learns them by minimizing a data fidelity loss, thus making the technique
unsupervised. CTL has been shown to outperform several supervised and unsupervised
learning schemes in the context of image classification. In the present work, we propose
to extend the shallow CTL version to deeper layers, with the aim to generate a feature
extraction strategy that is well suited for 1D time series analysis. This is the first major
contribution of this work—deep convolutional transform learning.

In most applications, time series signals are multivariate, as they arise from multiple
sources/sensors. For example, biomedical signals like ECG and EEG come from multiple
leads; financial data from stocks are recorded with different inputs (open, close, low, high,
and net asset value) and demand forecasting problems in smartgrids come with multiple

 

‘https://www.axios.com/artificial-intelligence- pioneer-says-we-need-to-start- over- 1513305524-f61 9efbd-9db0-4947-
a9b2-7a4c310a28fe.html
*https://deepmind.com/blog/article/unsupervised-learning
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 3 of 32

types of data (power consumption, temperature, humidity, occupancy, etc.). In all such
cases, the final goal is to perform prediction/classification task from such multivariate
time series. We propose to address such problem as one of feature fusion. The information
from each of the sources will be processed by the proposed deep CTL pipeline, and the
generated deep features will be finally fused by an unsupervised fully connected layer.
This is the second major contribution of this work—an unsupervised fusion framework
with deep CTL.

The resulting features can be used for different applicative tasks. In this paper, we will
focus on the applicative problem of financial stock analysis. The ultimate goal may be
either to forecast the stock price (regression problem) or to decide whether to buy or sell
(classification problem). Depending on the considered task, we can pass the generated
features into suitable machine learning tool that may not be as data hungry as deep neural
networks. Therefore, by adopting such a processing architecture, we expect to yield better
results than traditional deep learning especially in cases where access to labeled data is
limited.

2 Literature review

2.1 CNN for time series analysis

Let us briefly review and discuss CNN-based methods for time series analysis. For a more
detailed review, the interested reader can peruse [11]. We mainly focus on studies on
stock forecasting as it will be our use case for experimental validation.

The traditional choice for processing time series with neural network is to adopt a recur-
rent neural network (RNN) architecture. Variants of RNN like long short-term memory
(LSTM) [12] and gated recurrent unit (GRU) [13] have been proposed. However, due to
the complexity of training such networks via backpropagation through time, they have
been progressively replaced with 1D CNN [14]. For example, in [15], a generic time
series analysis framework was built based on LSTM, with assessed performance on the
UCR time series classification datasets https://www.cs.ucr.edu/~eamonn/time_series_
data/. The later study from the same group [17], based on 1D CNN, showed considerable
improvement over the prior model on the same datasets.

There are also several studies that convert 1D time series data into a matrix form
so as to be able to use 2D CNNs [16, 18, 19]. Each column of the matrix corresponds
to a subset of the 1D series within a given time window, and the resulting matrix is
processed as an image. The 2D CNN model has been especially popular in stock fore-
casting. In [19], the said techniques have been used on stock prices for forecasting. A
slightly different input is used in [20]: instead of using the standard stock variables (open,
close, high, low, and NAV), it uses high frequency data for forecasting major points
of inflection in the financial market. In another work [21], a similar approach is used
for modeling exchange -traded fund (ETF). It has been seen that the 2D CNN model
performs the same as LSTM or the standard multi-layer perceptron [22, 23]. The appar-
ent lack of performance improvement in the aforementioned studies may be due to an
incorrect choice of CNN model, since an inherently 1D time series is modeled as an

image.

2.2 Deep learning and fusion

We now review existing works for processing multivariate data inputs, within the deep
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 4 of 32

learning framework. Since the present work aims at being applied to stock price forecast-
ing/trading, we will mostly focus our review on the multi-channel/multi-sensor fusion
framework. Multimodal data and fusion for image processing, less related to our work,
will be mentioned at the end of this subsection for the sake of completeness.

Deep learning has been widely used recently for analyzing multi-channel/multi-sensor
signals. In several of such studies, all the sensors are stacked one after the other to
form a matrix and 2D CNN is used for analyzing these signals. For example, [24] uses
this strategy for analyzing human activity recognition from multiple body sensors. It is
important to distinguish such an approach from the aforementioned studies [19-23].
Here, the images are not formed from stacking windowed signals from the same sig-
nal one after the other, but by stacking signals from different sensors. The said study
[24] does not account for any temporal modeling; this is rectified in [25]. In there, 2D
CNN is used on a time series window; but the different windows are finally processed
by GRU, thus explicitly incorporating time series modeling. There is however no explicit
fusion framework in [24, 25]. The information from raw multivariate signals is simply
fused to form matrices and treated by 2D convolutions. A true fusion framework was
proposed in [26]. Each signal channel is processed by a deep 1D CNN, and the output
from the different signal processing pipelines are then fused by a fully connected layer.
Thus, the fusion is happening at the feature level and not in the raw signal level as it was
in [24, 25].

Another area that routinely uses deep learning based fusion is multi-modal data pro-
cessing. This area is not as well defined as multi-channel data processing; nevertheless,
we will briefly discuss some studies on this topic. In [27], a fusion scheme is shown
for audio-visual analysis that uses a fusion scheme for deep belief network (DBN) and
stacked autoencoder (SAE) for fusing audio and video channels. Each channel is pro-
cessed separately and connected by a fully connected layer to produce fused features.
These fused features are further processed for inference. We can also mention the work
on video-based action recognition addressed in [28], which proposes a fusion scheme for
incorporating temporal information (processed by CNN) and spatial information (also
processed by CNN).

There are several other such works on image analysis [29-31]. In [29], a fusion scheme
is proposed for processing color and depth information (via 3D and 2D convolutions,
respectively) with the objective of action recognition. In [30], it was shown that by fusing
hyperspectral data (high spatial resolution) with Lidar (depth information), better classi-
fication results can be achieved. In [31], it was shown that fusing deeply learnt features
(from CNN) with handcrafted features via a fully connected layer can improve analysis
tasks. In this work, our interest lies in the first problem; that of inference from 1D/time-
series multi-channel signals. To the best of our knowledge, all prior deep learning-based
studies on this topic are supervised. In keeping with the vision of Hinton and others,
our goal is to develop an unsupervised fusion framework using deeply learnt convolutive

filters.

2.3. Convolutional transform learning
Convolutional transform learning (CTL) has been introduced in our seminal paper [10].
Since it is a recent work, we present it in detail in the current paper, to make it self-

content. CTL learns a set of filters (t))<,<jg operated on observed samples (s®), <k<K
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 5 of 32

(k)

to generate a set of features (in . Formally, the inherent learning model is

) 1<m<M,1<k<K
expressed through convolution operations defined as

(Wm € {1,...,M},Vke{1,...,K}) ty *s% = 2, (1)

Following the original study on transform learning [32], a sparsity penalty is imposed
on the features for improving representation ability and limit overfitting issues. Moreover,
in the same line as CNN models, the non-negativity constraint is imposed on the fea-
tures. Training then consists of learning the convolutional filters and the representation
coefficients from the data. This is expressed as the following optimization problem

minimize 1 y y le * SM) — xh) | + ve)
k) 2 a+ m mM IIo m
=1 M=

(tm)m(*m )mk

M
+H Y> Iltmll3 — Alogdet ([t1|...|twl), (2)
m=1
where y is a suitable penalization function. Note that the regularization term “vy ||- \12. —
i log det” ensures that the learnt filters are unique, something that is not guaranteed in
CNN. Let us introduce the matrix notation

ty xs) — x) . ty xs) — x
TxS—X= me (3)
ft) * sK) _ a) ... fy * sK) _ a)
where T' = | f Lae tu |, S = | .. s%) | and X = | xt" a xe | . The cost
1<k<kK
function in problem (2) can be compactly rewritten as?
F(T,X) = - |T *S — XZ + W(X) + wITII2 — A log det (7), (4)

where W applies the penalty term w column-wise on X.

A local minimizer to (4) can be reached efficiently using the alternating proximal algo-
rithm [33-35], which alternates between proximal updates on variables T and X. More
precisely, set a Hilbert space (H, || - ||) and define the proximity operator [23] at x € H of

a proper lower-semi-continuous convex function g : H —]—0o, +00] as
~ , 1 ~ 12
prox, (%) = arg min g(x) + = |x — x| . (5)
xEH 2

Then, the alternating proximal algorithm reads

Forn = 0,1,...
Tint] = P©Ox,, F(.,x14)) (7'"!) (6)
Xt = Prox, e(rt+4,.) (x'!)

with initializations T!!, X!! and 1,72 positive constants. For more details on the
derivations and the convergence guarantees, the readers can refer to [10].

3 Fusion based on deep convolutional transform learning

In this section, we discuss our proposed formulation. First, we extend the aforementioned
CTL formulation to a deeper version. Next, we develop the fusion framework based on
transform learning, leading to our DeConFuse’ strategy.

 

3Note that T is not necessarily a square matrix. By an abuse of notation, we define the “log-det” of a rectangular matrix
as the sum of logarithms of its singular values.
4Code available at: https://github.com/pooja290992/DeConFuse.git
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 6 of 32

Input i
Pp mi 7 f
Steps] a :
. a -—> @ >

Fig. 1 Deep CTL architecture. The illustration is given for L = 2 layers, with the first layer 7) composed of
M, = 4 filters of size 5 x 1, and the second layer composed of Mz = 8 filters of size 3 x 1

 

  

Convolutional Layer
Convolutional Layer
SELU + MAXPOOL

 

 

 

 

3.1 Deep convolutional transform learning

Deep CTL consists of stacking multiple convolutional layers on top of each other to gen-
erate the features, as shown in Fig. 1. To learn all the variables in an end-to-end fashion,
deep CTL relies on the key property that the solution X to the CTL problem, assuming
fixed filters T, can be reformulated as the simple application of an element-wise activation
function, that is

argmin F(T, X) = $(T *S), (7)
X

with ¢ the proximity operator of W [36]. For example, if V is the indicator function of the
positive orthant, then ¢ identifies with the famous rectified linear unit (ReLU) activation
function. Many other examples are provided in [36]. Consequently, deep features can be
computed by stacking many such layers

(Ve € {1, ...yL 1}) X¢ = de( Te * X~_1), (8)

where Xo = S and ¢¢ a given activation function for layer @.
Putting all together, deep CTL amounts to

minimize Feony(T1,..., Tz,X |S) (9)
T1,..1,,X
where

1
Feonv(1s. ++) 11X15) = FI Zz * P1r-1C1-1 * 6171 * 5) — Xl

L

+ W(X) + Y > (ull Tell? — A log det(Tv)) . (10)
t=1

This is a direct extension of the one-layer formulation in (4).

3.2 Multi-channel fusion framework
We now propose a fusion framework to learn in an unsupervised fashion a suitable rep-
resentation of multi-channel data that can then be utilized for a multitude of tasks. This
framework takes the channels of input data samples to separate branches of convolu-
tional layers, leading to multiple sets of channel-wise features. These decoupled features
are then concatenated and passed to a fully connected layer, which yields a unique set of
coupled features. The complete architecture, called DeConFuse, is shown in Fig. 2.

Since we have multi-channel data, for each channel c € {1,..., C}, we learn a different

(c)
L

set of convolutional filters TO yeeey and features X). At the same time, we learn the
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 7 of 32

 

   

   

 

   

   

  
  

      
 

 

   

 

 

   

 

 

> e
Input % %
npu . ‘ %, . : ‘ %
a> vs za 7
5 3, |
Time 3 3
S : 3
Z : Z
8 3 5
is)
wn
%
é ‘a E
= 8 = § Output - Z
Time 3 z 3
Steps a : 3 s 5 F
5 3 z 5
Oo o Oo + A
a | 20 T
; | & U
g R
a E
s
‘ % N
B 3 2
5 os] fa
Time q 2 z
Steps 2 =
+ :
$ 2 $
5 re 5
Oo a Oo
Fig. 2 DeConFuse architecture

XQ

(not convolutional) linear transform T = (Te)1<cec to fuse the channel-wise features
X = (XM) 1<c<c, along with the corresponding fused features Z, which constitute the
final output of the proposed DeConFuse model, as shown in Fig. 2. This leads to the joint

optimization problem

C
minimize Fyasion(T,Z,X) + )~ Feonv(T 19...) Ti, X |S) (11)

TX,TZ =]

 

J(T,X,T,Z)
where

2

C
+14 (Z)+)— (ull Tell? — Alog det(T.))
EF c=1

C
Z— > flat(X©)T,

c=1

~ 1
Frusion(T, Z, X) = 9

 

 

 

 

(12)

where the operator “flat” transforms X into a matrix where each row contains the
features of a sample flattened as a vector.

To summarize, our formulation aims to jointly train the channel-wise convolutional
filters T° and the fusion coefficients T’ in an end-to-end fashion. We explicitly learn the
features X and Z subject to non-negativity constraints so as to avoid trivial solutions and
make our approach completely unsupervised. Moreover, the “log-det” regularization on
both T° and T' breaks symmetry and forces diversity in the learnt transforms, whereas

the Frobenius regularization ensures that the transform coefficients are bounded.
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 8 of 32

3.3. Optimization algorithm

As for the solution of problem (11), we remark that all terms of the cost function are
differentiable, except the indicator function of the non-negativity constraint. We can,
therefore, find a local minimizer to (11) by employing the projected gradient descent,

whose iterations read

For n = 0,1,...
Tet — Tl — yyy, x4), TH, zi)
xi = py (xt — yVxJ(T), xl, TI, Zly) (13)

Tat) — Tin) — yVei(T™), xt, TW zn)
zit] — Py (zi — yVz( TH, xl, TI, Zly)

with initialization T®, x, Tl, ZO, y > 0,and P, = max{-,0}. In practice, we make
use of accelerated strategies [37] within each step of this algorithm to speed up learning.

There are two notable advantages with the proposed optimization approach. Firstly,
we rely on automatic differentiation [38] and stochastic gradient approximations to effi-
ciently solve problem (11). Secondly, we are not limited to ReLU activation in (8), but
rather we can use more advanced ones, such as SELU [39]. This is beneficial for the

performance, as shown by our numerical results.

3.4 Computational complexity of proposed framework—DeConFuse

Table 1 summarizes the computational complexity of DeconFuse architecture, both for
training and test phases. Specifically, it is reported the cost incurred for every input
sample at each iteration of gradient descent in the training phase and for the output
computation in testing phase. The computational complexity of DeConFuse architecture
is comparable to a regular CNN. The only addition is the log-det regularization, which
requires to compute the truncated singular value decomposition of T° and T. However,
as the size of these matrices is determined by the filter size, the number of filters, and the
number of output features per sample, the training complexity is not worse than that of a
CNN.

4 Experimental evaluation
We carry out experiments on the real-world problem of stock forecasting and trading.
The problem of stock forecasting is a regression problem aiming at estimating the price

of a stock at a future date (next day for our problem) given inputs till the current date.

Table 1 Time complexity in training and test phases (for one input sample)

 

 

Phase Steps Time complexity Dimension description
Training phase 1. Convolution layers O(PeDeMeC)

2. Fully-connected (f-c.) layer O(?C?) SO © RKxP

3. Frobenius norm on conv. layers O (PeMeC) 1? E RoexMe

4. Frobenius norm on f.c. layer O(?C?) flat(x©) € RK*!

5. log-det on conv. layers O(PEMeC) T e RO

6. log-det on f.-c. layer OPC’)
Testing phase Step 1.+ Step 2. Step 1.+ Step 2.

 

D= input sample size — K = num. of samples — C = num. of channels — L = num. of layers

Pe = filter size at layer € -My =num. of filters at layer £- De = output sample size at layer €

| = DM, isthe num. of output features per sample at last convolution layer

O=alC (witha €[0, 1]) is the num. of output features per sample at the fully connected layer
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26

Stock trading is a classification problem, where the decision whether to buy or sell a stock
has to be taken at each time. The two problems are related by the fact that simple logic
dictates that if the price of a stock at a later date is expected to increase, the stock must be
bought; and if the stock price is expected to go down, the stock must be sold.

We will use the five raw inputs for both the tasks, namely open price, close price,
high, low, and net asset value (NAV). One could compute technical indicators based
on the raw inputs [19], but in keeping with the essence of true representation learn-
ing, we chose to stay with those raw values. Each of the five inputs is processed by
a separate 1D processing pipeline. Each of the pipelines produces a flattened output
(Fig. 1). The flattened outputs are then concatenated and fed into the transform learn-
ing layer acting as the fully connected layer (Fig. 2) for fusion. While our processing
pipeline ends here (being unsupervised), the benchmark techniques are supervised and
have an output node. The node is binary (buy/sell) for classification and real valued
for regression. More precisely, we will compare with two state-of-the-art time series
analysis models, namely TimeNet [15] and ConvTimeNet [17]. In the former, the pro-
cessing individual processing pipelines are based on LSTM and in the later they use
1D CNN.

We make use of a real dataset from the National Stock Exchange (NSE) of India. The
dataset contains information of 150 symbols between 2014 and 2018; these stocks were
chosen after filtering out stocks that had less than 3 years of data. The companies avail-
able in the dataset are from various sectors such as IT (e.g., TCS, INFY), automobile
(e.g, HEROMOTOCO, TATAMOTORS), bank (e.g., HDFCBANK, ICICIBANK), coal

Table 2 Description of compared models

 

Method Architecture description Other parameters
layer1 : 1D Conv(1,4,5,1,2)'

Maxpool(2, 2)?
DeConFuse 5 x Learning rate = 0.001, A = 0.01, 4 = 0.0001,
SELU Optimizer used: Adam **with parameters**
layer2 : 1D Conv(5, 8, 3,1,1)' (B61, B2) = (0.9,0.999), weight_decay = 5e-5,
epsilon = 1e-8

 

layer3 : Fully connected
layer1 : 1D Convolution(1, 32,9, 1,4)!
Batch normalization + SELU
layer2 : 1D Convolution(32, 32, 3, 1,1)!
Batch normalization ++ SELU + SC°

ConvTimeNet 5x4 layer3 : 1D Convolution(32,64,9,1,4)' For forecasting: Learning rate = 0.001, For

+a: trading: Learning rate = 0.0001, Optimizer
Batch normalization + SELU
2 . ton + used: Adam **with parameters** (61, 82) =
layer4 : 1D Convolution(64,64,3,1,1)' (0.9, 0.999), weight_decay = le-4, epsilon =

Batch normalization + SELU+SC°> __1e-8
layer3 : Global Average Pooling

layer4 : Fully connected
For Trading, added layer5 : Softmax

. | layer1 : LSTM unit(1, 12, 2, True)4 .
TimeNet 5 Xx . For forecasting: Learning Rate = 0.001, For
layer2 : Global Average Pooling trading: Learning Rate = 0.0005, Optimizer
layer3 : Fully connected used: Adam **with parameters** (61, 82) =
For trading, added layer4 : Softmax OO. weight_decay = 5e-5, epsilon =
i

 

"(in_planes, out_planes, kernel_size, stride, padding)
2(kernel_size, stride)

SC - Skip-Connection
4(input_size,hidden_size,#layers, bidirectional)

Page 9 of 32
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 10 of 32

and petroleum (e.g., OIL, ONGC), steel (e.g., JOWSTEEL, TATASTEEL), construction
(e.g., ABIRLANUVO, ACC), and public sector units (e.g., POWERGRID, GAIL). The
detailed architectures for each tested techniques, namely DeConFuse, ConvlimeNet, and
TimeNet, are presented in Table 2. For DeConFuse, TimeNet, and ConvTimeNet, we have
tuned the architectures to yield the best performance and have randomly initialized the
weights for each stock’s training.

4.1 Stock forecasting—regression

Let us start with the stock forecasting problem. We feed the generated unsupervised fea-
tures from the proposed architecture into an external regressor, namely ridge regression.
Evaluation is carried out in terms of mean absolute error (MAE) between the predicted
and actual stock prices for all 150 stocks. The stock forecasting results are shown in
Table 5 in Appendix 1 section. The MAE for individual stocks are presented for each of
close price, open price, high price, low price, and net asset value.

From Table 5 in Appendix 1 section, it can be seen that the MAE values reached for
the proposed DeConFuse solution for the four first prices (open, close, high, low) are
exceptionally good for all of the 150 stocks. Regarding NAV prediction, the proposed
method performs extremely well for 128 stocks. For the remaining 22 stocks, there are 13
stocks, highlighted in red, for which DeConFuse does not give the lowest MAE but it is
still very close to the best results given by the TimeNet approach.

For a concise summary of the results, the average values over all stocks are shown in
Table 3.

From the summary Table 3, it can be observed that our error is more than an order of
magnitude better than the state of the arts. The plots for one of the regressed prices (close
price) for some examples of stocks in Fig. 3 show that the predicted close prices from
DeConFuse are closer to the true close prices than benchmark predictions.

4.2 Stock trading—classification

We now focus on the stock trading task. In this case, the generated unsupervised fea-
tures from DeConFuse are inputs to an external classifier based on random decision forest
(RDF) with 5 decision tree classifiers and depth 3. Even though we used this architec-
ture, we found that the results from RDF are robust to changes in architecture. This is a
well known phenomenon about RDFs [40]. We evaluate the results in terms of precision,
recall, Fl score, and area under the ROC curve (AUC). From the financial viewpoint, we
also calculate annualized returns (AR) using the predicted trading signals/labels as well
as using true trading signals/labels named as predicted AR and true AR, respectively. The
starting capital used for calculating AR values for every stock is Rs. 100,000 and the trans-
action charges are Rs 10. The stock trading results are shown in Table 6 in Appendix 2

section.

Table 3 Summary of forecasting results

 

 

Method Close Open High Low NAV
DeConFuse 0.016 0.007 0.012 0.013 0.410
ConvTimeNet 1.550 1.550 1.530 1.560 2.350

TimeNet 0.295 0.295 0.294 0.295 0.511

 
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 11 of 32

 

Comparative Analysis for ABIRLANUVO Comparative Analysis for ALBK

—— True Price
-——— DeConFuse
—— TimeNet
—— ConvTimeNet

—— True Price

25001 ~~ DeConFuse
—— TimeNet

22504 —— ConvTimeNet

Close Price
Close Price

wv e wv w N
Oo N uw ~ oO
o uw oO uw oO
oO oO oO oO oO

~
wu
o

  

0 50 100 150 200
Days Days

(a) ABIRLANUVO and ALBK

Comparative Analysis for JPASSOCIAT Comparative Analysis for JSWENERGY

line
d i en Ni W —— True Price

-——— DeConFuse
—— TimeNet

Close Price
Close Price

—— ConvTimeNet

—— True Price
——— DeConFuse
— TimeNet
—— ConvTimeNet

  

0 25 50 75 100 125 150 175 200

0 50 100 150 200
Days

Days

(b) JPASSOCIAT and JSWENERGY

 

 

Fig. 3 Stock forecasting performance

 

Certain results from Table 6 in Appendix 2 section are highlighted in bold or red. The
first set of results, marked in bold, are the ones where one of the techniques for each met-
ric gives the best performance for each stock. The proposed solution DeConFuse gives
the best results for 89 stocks for precision score, 85 stocks for recall score, 125 stocks for
F1 score, 91 stocks for AUC measure, and 56 stocks in case of the AR metric. The other
set marked in red highlights the cases where DeConfuse has not performed the best but
performs nearly equal (here, a difference of maximum 0.05 in the metric is considered) to
the best performance given by one of the benchmarks, i.e., DeConFuse gives the next best
performance. We noticed that there are 24 stocks for which DeConFuse gives the next
best precision metric value. Likewise, 18 stocks in case of recall, 22 stocks for F1 score, 26
stocks for AUC values, and 1 stock in case of AR. Overall, DeConfuse reaches a very sat-
isfying performance over the benchmark techniques. This is also corroborated from the

summary of trading results in Table 4.

Table 4 Summary of trading results

 

 

Method Precision Recall F1 score AUC MAE AR
DeConFuse 0.520 0.810 0.628 0.543 17.350
ConvTimeNet 0.510 0.457 0413 0.524 19.410

TimeNet 0.470 0.648 0.490 0.513 18.760

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

We also display empirical convergence plots for few stocks, namely RELIANCE,
ONGC, HINDUNILVR, and ICICIBANK, in Fig. 4. We can see that the training loss

decreases to a point of stability for each example.

5 Conclusion

In this work, we propose DeConFuse, a deep fusion end-to-end framework for the pro-
cessing of 1D multi-channel data. Unlike other deep learning models, our framework
is unsupervised. It is based on a novel deep version of our recently proposed con-
volutional transform learning model. We have applied the proposed model for stock
forecasting/trading leading to very good performance. The framework is generic enough
to handle other multi-channel fusion problems as well.

The advantage of our framework is its ability to learn in an unsupervised fashion. For
example, consider the problem we address. For traditional deep learning-based models,
we need to retrain to deep networks for regression and classification. But we can reuse our
features for both the tasks, without the requirement of re-training, for specific tasks. This
has advantages in other areas as well. For example, one can either do ischemia detection,
i.e., detect whether one is having a stroke at the current time instant (from EEG); or one
can do ischemia prediction, i.e., forecast if a stroke is going to happen. In standard deep
learning, two networks need to be retrained and tuned to tackle these two problems. With
our proposed method, there is no need for this double effort.

In the future, we would work on extending the framework for supervised/semi-
supervised formulations. We believe that the semi-supervised formulation will be of
immense practical importance. We would also like to extend it to 2D convolutions in

order to handle image data.

 

 

 

 

1e7 Loss plot for RELIANCE 1e7 Loss plot for ONGC
—— Train Loss —— Train Loss
6
4
5
3
4
g, g
2
2
1
1
0 0
0 100 200 300 400 500 600 0 100 200 300 400 500 600
Epochs Epochs
(a) RELIANCE and ONGC
1e8 Loss plot for HINDUNILVR 1e7 Loss plot for ICICIBANK
1.4
—— Train Loss 3.5 —— Train Loss
1:2
3.0
1.0
2.5
0.8
a g 2.0
5 06 S
, 1.5
0.4 1.0
0.2 0.5
0.0 0.0
0 100 200 300 400 500 600 0 100 200 300 400 500 600
Epochs Epochs
(b) HINDUNILVR and ICICIBANK
Fig. 4 Empirical convergence plots

 

 

Page 12 of 32
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Appendix 1: Detailed stock forecasting results

Table 5 Stock-wise forecasting results

Page 13 of 32

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
ABIRLANUVO DeConFuse 0.021 0.015 0.019 0.017 0.416
ConvTimeNet 0.204 0.212 0.219 0.195 1.804
TimeNet 0.112 0.111 0.111 0.112 0.467
ACC DeConFuse 0.012 0.016 0.014 0.017 0.580
ConvTimeNet 0.158 0.161 0.159 0.158 0.765
TimeNet 0.116 0.116 0.115 0.118 0.388
ADANIENT DeConFuse 0.041 0.015 0.024 0.038 0.359
ConvTimeNet 4.656 4.795 4.654 4.800 0.748
TimeNet 0.538 0.549 0.540 0.551 0.475
ADANIPORTS DeConFuse 0.012 0.005 0.009 0.010 0.391
ConvTimeNet 0.124 0.122 0.122 0.123 1.258
TimeNet 0.283 0.283 0.280 0.285 0.43
ADANIPOWER DeConFuse 0.026 0.010 0.019 0.020 0.405
ConvTimeNet 0.610 0.600 0.590 0.602 1.796
TimeNet 0.205 0.205 0.204 0.206 0.448
AJANTPHARM DeConFuse 0.016 0.007 0.012 0.012 0.418
ConvTimeNet 0.401 0.374 0.384 0.400 0.867
TimeNet 0.262 0.261 0.258 0.264 0.480
ALBK DeConFuse 0.020 0.009 0.015 0.015 0.362
ConvTimeNet 0.908 1.029 0.995 0.953 1.020
TimeNet 0.184 0.181 0.180 0.185 0.448
AMARAJABAT DeConFuse 0.015 0.007 0.011 0.012 0.435
ConvTimeNet 0.047 0.046 0.047 0.047 0.631
TimeNet 0.087 0.088 0.086 0.089 0.386
AMBUJACEM DeConFuse 0.012 0.005 0.008 0.009 0.355
ConvTimeNet 2.283 2.272 2.280 2.267 3.132
TimeNet 0.106 0.107 0.105 0.107 0.414
ANDHRABANK DeConFuse 0.022 0.009 0.016 0.016 0.373
ConvTimeNet 5.095 5.074 5.008 5.158 2.200
TimeNet 0.144 0.140 0.138 0.148 0.471
APOLLOHOSP DeConFuse 0.025 0.009 0.015 0.021 0.687
ConvTimeNet 0.268 0.240 0.258 0.254 0.719
TimeNet 0.153 0.155 0.151 0.156 0.536
APOLLOTYRE DeConFuse 0.014 0.006 0.010 0.011 0.391
ConvTimeNet 0.552 0.547 0.543 0.558 1.267
TimeNet 0.283 0.283 0.281 0.284 0.346
ARVIND DeConFuse 0.015 0.006 0.010 0.011 0.423
ConvTimeNet 0.302 0.278 0.294 0.290 1.251
TimeNet 0.268 0.268 0.267 0.269 0.465
ASHOKLEY DeConFuse 0.017 0.005 0.010 0.013 0.376
ConvTimeNet 1.042 1.018 0.987 1.096 0.586
TimeNet 0.343 0.343 0.344 0.342 0.451
ASIANPAINT DeConFuse 0.008 0.004 0.007 0.006 0.370
ConvTimeNet 0.816 0.801 0.804 0.816 1.272
TimeNet 0.290 0.289 0.288 0.290 0.465
AUROPHARMA DeConFuse 0.015 0.005 0.009 0.010 0.312
ConvTimeNet 1.802 1.847 1.801 1.829 1.034
TimeNet 0.075 0.076 0.075 0.076 0.393

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

Page 14 of 32

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
BAJAJ-AUTO DeConFuse 0.012 0.007 0.009 0.010 0.392
ConvTimeNet 0.329 0.326 0.328 0.327 0.580
TimeNet 0.175 0.176 0.175 0.176 0.466
BAJFINANCE DeConFuse 0.013 0.004 0.009 0.009 0.361
ConvTimeNet 2.519 2.518 2.534 2.506 2.575
TimeNet 0.509 0.509 0.508 0.510 0.693
BANKBARODA DeConFuse 0.021 0.007 0.015 0.014 0.299
ConvTimeNet 0.891 0.860 0.849 0.887 0.845
TimeNet 0.130 0.131 0.130 0.132 0.402
BANKINDIA DeConFuse 0.022 0.009 0.016 0.016 0.354
ConvTimeNet 245] 2.437 2.449 244] 1.35]
TimeNet 0.374 0.375 0.373 0.375 0.384
BATAINDIA DeConFuse 0.015 0.009 0.012 0.011 0.391
ConvTimeNet 0.143 0.111 0.129 0.125 1.095
TimeNet 0.301 0.299 0.299 0.301 0.477
BEL DeConFuse 0.019 0.007 0.013 0.014 0.366
ConvTimeNet 1.576 1.537 1.524 1.622 3.338
TimeNet 0.145 0.146 0.142 0.148 0.410
BHARATFORG DeConFuse 0.013 0.006 0.009 0.01 0.567
ConvTimeNet 3,207 3.178 3.162 3.219 7 468
TimeNet 0.345 0.345 0.343 0.347 0.555
BHARTIARTL DeConFuse 0.019 0.012 0.015 0.016 0.381
ConvTimeNet 1.849 1.809 1.817 1.841 1.042
TimeNet 0.167 0.167 0.168 0.166 0.500
BHEL DeConFuse 0.016 0.007 0.012 0.012 0.765
ConvTimeNet 2.664 2.613 2.660 2.617 8.514
TimeNet 0.389 0.389 0.391 0.386 0.928
BIOCON DeConFuse 0.016 0.007 0.013 0.012 0.450
ConvTimeNet 1.338 1.287 1.303 1.330 1.031
TimeNet 0.604 0.603 0.604 0.602 0.470
BOSCHLTD DeConFuse 0.012 0.005 0.009 0.007 0.516
ConvTimeNet 0.158 0.158 0.159 0.155 0.600
TimeNet 0.724 0.723 0.727 0.72] 0.551
BPCL DeConFuse 0.014 0.006 0.010 0.011 0.323
ConvTimeNet 0.243 0.267 0.267 0.244 1.614
TimeNet 0.276 0.277 0.276 0.276 0.374
BRITANNIA DeConFuse 0.009 0.004 0.006 0.006 0.367
ConvTimeNet 0.800 0.828 0.813 0.812 1.442
TimeNet 0.414 0.413 0.413 0.413 0.450
CAIRN DeConFuse 0.016 0.008 0.011 0.013 0.334
ConvTimeNet 3,945 3.988 3.939 4.025 0.969
TimeNet 0.159 0.159 0.159 0.158 0.345
CANBK DeConFuse 0.021 0.008 0.015 0.015 0.276
ConvTimeNet 2.140 2.023 2.065 2.100 0.806
TimeNet 0.151 0.153 0.151 0.154 0.444
CASTROLIND DeConFuse 0.014 0.005 0.010 0.011 0.523
ConvTimeNet 2.055 2.107 2.036 2.162 12.249
TimeNet 0.141 0.141 0.141 0.143 0.527
CEATLTD DeConFuse 0.015 0.006 0.010 0.011 0.319
ConvTimeNet 2.341 2.308 2.295 2.344 1.118
TimeNet 0.160 0.163 0.161 0.162 0.326

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

Page 15 of 32

 

 

Stock name method MAE close MAE open MAE high MAE low MAE NAV
CENTURYTEX DeConFuse 0.015 0.006 0.010 0.013 0.404
ConvTimeNet 1.11] 1.072 1.106 1.083 2.685
TimeNet 0.405 0.406 0.404 0.407 0.352
CESC DeConFuse 0.013 0.005 0.009 0.010 0.404
ConvTimeNet 0.390 0.377 0.374 0.395 0.602
TimeNet 0.364 0.363 0.362 0.364 0.477
CIPLA DeConFuse 0.012 0.004 0.009 0.008 0.408
ConvTimeNet 2.063 2.074 2.052 2.066 0.695
TimeNet 0.064 0.064 0.063 0.065 0.476
COALINDIA DeConFuse 0.012 0.005 0.009 0.009 0.393
ConvTimeNet 1.635 1.737 1.632 1.723 2./91
TimeNet 0.154 0.156 0.155 0.154 0.474
COLPAL DeConFuse 0.009 0.004 0.006 0.007 0.553
ConvTimeNet 0.115 0.119 0.114 0.117 1.158
TimeNet 0.164 0.164 0.164 0.165 0.566
DABUR DeConFuse 0.010 0.005 0.008 0.008 0.474
ConvTimeNet 1.369 1.398 1.360 1.409 1.530
TimeNet 0.271 0.269 0.270 0.271 0.539
DHFL DeConFuse 0.016 0.007 0.012 0.012 0.471
ConvTimeNet 0.302 0.285 0.291 0.289 1.118
TimeNet 0.456 0.456 0.457 0.457 0.657
DISHTV DeConFuse 0.016 0.006 0.013 0.012 0.478
ConvTimeNet 0.722 0.733 0.742 0.708 1.948
TimeNet 0.224 0.225 0.225 0.224 0.586
DIVISLAB DeConFuse 0.014 0.006 0.012 0.010 0.508
ConvTimeNet 0.183 0.195 0.190 0.190 0.871
TimeNet 0.160 0.159 0.161 0.159 0.422
DLF DeConFuse 0.021 0.012 0.015 0.018 0.318
ConvTimeNet 1.053 1.104 1.053 1.100 0.590
TimeNet 0.311 0.309 0.308 0.312 0.402
DRREDDY DeConFuse 0.013 0.006 0.010 0.010 0.393
ConvTimeNet 0.213 0.210 0.210 0.210 0.628
TimeNet 0.373 0.373 0.37 0.374 0.505
EICHERMOT DeConFuse 0.012 0.004 0.008 0.008 0.363
ConvTimeNet 0.295 0.296 0.295 0.297 0.452
TimeNet 0.816 0.816 0.818 0.814 0.393
ENGINERSIN DeConFuse 0.023 0.019 0.020 0.022 0.452
ConvTimeNet 0.265 0.260 0.258 0.260 2.059
TimeNet 0.128 0.128 0.128 0.128 0.500
EXIDEIND DeConFuse 0.012 0.005 0.009 0.009 0.418
ConvTimeNet 0.442 0.453 0.449 0.448 1.209
TimeNet 0.265 0.263 0.263 0.265 0.420
FEDERALBNK DeConFuse 0.015 0.006 0.010 0.012 0.407
ConvTimeNet 2.405 2.345 2.360 2.378 1.292
TimeNet 0.146 0.148 0.147 0.147 0.502
GAIL DeConFuse 0.014 0.009 0.011 0.012 0.369
ConvTimeNet 0.209 0.169 0.182 0.195 1.070
TimeNet 0.330 0.330 0.330 0.330 0.394
GLENMARK DeConFuse 0.013 0.005 0.010 0.009 0.374
ConvTimeNet 0.614 0.675 0.612 0.666 2.597
TimeNet 0.399 0.401 0.402 0.397 0.448

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

Page 16 of 32

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
GMRINFRA DeConFuse 0.029 0.017 0.023 0.024 0.616
ConvTimeNet 0.101 0.137 0.118 0.116 1.044
TimeNet 0.094 0.092 0.095 0.094 0.799
GODREJIND DeConFuse 0.012 0.008 0.010 0.011 0.376
ConvTimeNet 0.287 0.298 0.28 0.296 1.647
TimeNet 0.327 0.326 0.325 0.328 0.362
GRASIM DeConFuse 0.014 0.008 0.011 0.011 0.445
ConvTimeNet 0.307 0.318 0.309 0.312 1.289
TimeNet 0.259 0.259 0.259 0.257 0.386
HAVELLS DeConFuse 0.012 0.005 0.009 0.009 0.377
ConvTimeNet 0.426 0.410 0.422 0.421 1.182
TimeNet 0.403 0.402 0.402 0.404 0.400
HCLTECH DeConFuse 0.014 0.010 0.011 0.014 0.383
ConvTimeNet 1.854 1.839 1.818 1.853 1.457
TimeNet 0.113 0.113 0.113 0.114 0.442
HDFC DeConFuse 0.009 0.004 0.006 0.006 0.314
ConvTimeNet 0.747 0.713 0.734 0.746 1.239
TimeNet 0.318 0.319 0.317 0.321 0.383
HDFCBANK DeConFuse 0.007 0.003 0.005 0.005 0.330
ConvTimeNet 0.529 0.533 0.533 0.544 3.680
TimeNet 0.422 0.422 0.421 0.423 0.576
HDIL DeConFuse 0.027 0.014 0.021 0.022 0.624
ConvTimeNet 0.300 0.560 0.352 0.439 10.715
TimeNet 0.297 0.291 0.290 0.296 1.106
HEROMOTOCO DeConFuse 0.009 0.004 0.006 0.006 0.322
ConvTimeNet 0.129 0.134 0.129 0.134 0.810
TimeNet 0.191 0.192 0.190 0.193 0.416
HEXAWARE DeConFuse 0.017 0.007 0.012 0.013 0.496
ConvTimeNet 2./98 2.710 2.69] 2.769 1.050
TimeNet 0.425 0.422 0.424 0.423 0.473
HINDALCO DeConFuse 0.016 0.007 0.012 0.012 0.310
ConvTimeNet 0.984 0.995 0.979 1.002 1.159
TimeNet 0.403 0.403 0.402 0.404 0.388
HINDPETRO DeConFuse 0.016 0.007 0.012 0.012 0.37
ConvTimeNet 0.998 0.961 0.965 0.999 1.545
TimeNet 0.375 0.377 0.376 0.376 0.397
HINDUNILVR DeConFuse 0.008 0.003 0.006 0.006 0.413
ConvTimeNet 0.181 0.153 0.151 0.183 0.997
TimeNet 0.414 0.413 0.413 0.415 0.427
HINDZINC DeConFuse 0.012 0.006 0.009 0.010 0.333
ConvTimeNet 0.057 0.063 0.062 0.055 2.246
TimeNet 0.346 0.346 0.344 0.347 0.362
IBREALEST DeConFuse 0.033 0.026 0.031 0.028 0.694
ConvTimeNet 6.230 6.588 6.338 6.375 6.639
TimeNet 0.612 0.611 0.613 0.611 0.662
IBULHSGFIN DeConFuse 0.014 0.006 0.011 0.011 0.381
ConvTimeNet 0.352 0.341 0.343 0.354 0.556
TimeNet 0.357 0.358 0.356 0.358 0.584
ICICIBANK DeConFuse 0.016 0.012 0.014 0.014 0.314
ConvTimeNet 2./73 2.80] 2.766 2.784 1.126
TimeNet 0.156 0.155 0.156 0.155 0.609

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

Page 17 of 32

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
IDBI DeConFuse 0.026 0.010 0.019 0.018 0.408
ConvTimeNet 0.900 0.909 0.846 0.952 1.995
TimeNet 0.135 0.137 0.134 0.138 0.550
IDEA DeConFuse 0.022 0.008 0.015 0.015 0.396
ConvTimeNet 1.612 1.684 1.629 1.67] 1.676
TimeNet 0.576 0.575 0.569 0.580 0.454
IDFC DeConFuse 0.017 0.008 0.012 0.013 0.523
ConvTimeNet 0.693 0.674 0.629 0.748 5.201
TimeNet 0.134 0.135 0.134 0.134 0.680
IFCI DeConFuse 0.024 0.009 0.020 0.016 0.623
ConvTimeNet 0.807 0.744 0.837 0.755 10.316
TimeNet 0.244 0.243 0.246 0.245 0.967
IGL DeConFuse 0.015 0.005 0.011 0.011 0.490
ConvTimeNet 0.264 0.255 0.257 0.264 1,324
TimeNet 0.348 0.350 0.350 0.346 0.369
INDIACEM DeConFuse 0.023 0.014 0.018 0.020 0.360
ConvTimeNet 0.712 0.689 0.692 0.705 1.012
TimeNet 0.149 0.150 0.148 0.150 0.360
INDUSINDBK DeConFuse 0.009 0.004 0.006 0.006 0.315
ConvTimeNet 0.502 0.509 0.507 0.500 1.253
TimeNet 0.453 0.453 0.452 0.454 0.419
INFY DeConFuse 0.010 0.005 0.008 0.008 0.405
ConvTimeNet 2417 2.415 2.410 2.409 1.837
TimeNet 0.140 0.139 0.14 0.139 0.605
IOC DeConFuse 0.012 0.006 0.010 0.011 0.369
ConvTimeNet 0.334 0.285 0.309 0.327 1.359
TimeNet 0.205 0.206 0.204 0.206 0.392
IRB DeConFuse 0.019 0.007 0.014 0.014 0.475
ConvTimeNet 0.365 0.360 0.355 0.380 1.583
TimeNet 0.076 0.076 0.077 0.076 0.580
ITC DeConFuse 0.009 0.004 0.007 0.006 0.383
ConvTimeNet 0.539 0.545 0.549 0.540 1.089
TimeNet 0.106 0.106 0.105 0.108 0.457
JINDALSTEL DeConFuse 0.029 0.017 0.023 0.024 0.337
ConvTimeNet 6.234 6.467 6.223 6.34 5,342
TimeNet 0.394 0.392 0.392 0.394 0.565
JISLJALEQS DeConFuse 0.022 0.008 0.014 0.018 0.461
ConvTimeNet 0.965 0.969 0.953 0.976 1.066
TimeNet 0.238 0.237 0.238 0.236 0.474
JPASSOCIAT DeConFuse 0.046 0.028 0.035 0.040 0.675
ConvTimeNet 0.321 0.318 0.309 0.321 1.660
TimeNet 0.565 0.567 0.563 0.568 1.227
JSWENERGY DeConFuse 0.024 0.019 0.023 0.021 0.610
ConvTimeNet 0.453 0.462 0.438 0.469 1.320
TimeNet 0.045 0.044 0.044 0.044 0.621
JSWSTEEL DeConFuse 0.014 0.006 0.011 0.010 0.304
ConvTimeNet 1.093 1.206 1.093 1.135 1.895
TimeNet 0.535 0.534 0.535 0.535 0.365
JUBLFOOD DeConFuse 0.013 0.006 0.010 0.011 0.409
ConvTimeNet 7.716 7.395 7.52) 7.733 3.604
TimeNet 0.442 0.441 0.44 0.443 0.672

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

Page 18 of 32

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
JUSTDIAL DeConFuse 0.026 0.009 0.018 0.020 0.473
ConvTimeNet 7.726 7.839 7.787 7.856 3.06
TimeNet 0.738 0.745 0.735 0.750 0.505
KOTAKBANK DeConFuse 0.009 0.004 0.007 0.007 0.342
ConvTimeNet 0.278 0.294 0.258 0.307 5.970
TimeNet 0.401 0.400 0.399 0.402 0.349
KSCL DeConFuse 0.016 0.006 0.011 0.012 0.503
ConvTimeNet 0.240 0.252 0.235 0.242 1.831
TimeNet 0.089 0.086 0.085 0.090 0.739
KTKBANK DeConFuse 0.016 0.006 0.012 0.013 0.452
ConvTimeNet 2.447 2.452 2.430 2.464 1.224
TimeNet 0.124 0.125 0.123 0.126 0.504
L&TFH DeConFuse 0.017 0.006 0.011 0.012 0.366
ConvTimeNet 0.343 0.345 0.339 0.35] 0.741
TimeNet 0.466 0.467 0.468 0.465 0.416
LICHSGFIN DeConFuse 0.013 0.005 0.009 0.010 0.354
ConvTimeNet 1.587 1.604 1.59] 1.584 1.97]
TimeNet 0.126 0.127 0.126 0.128 0.400
LT DeConFuse 0.010 0.005 0.007 0.008 0.372
ConvTimeNet 0.877 0.858 0.851 0.877 0.732
TimeNet 0.222 0.222 0.221 0.224 0.338
LUPIN DeConFuse 0.014 0.004 0.009 0.010 0.406
ConvTimeNet 0.687 0.658 0.678 0.663 1.229
TimeNet 0.707 0.706 0.705 0.706 0.514
M&M DeConFuse 0.014 0.008 0.010 0.011 0.361
ConvTimeNet 2.729 2.723 2./13 2.684 1.088
TimeNet 0.207 0.207 0.206 0.208 0.413
M&MFIN DeConFuse 0.018 0.011 0.014 0.016 0.356
ConvTimeNet 1.800 1.789 1.795 1.807 1.489
TimeNet 0.371 0.370 0.371 0.372 0.358
MARUTI DeConFuse 0.009 0.003 0.006 0.006 0.356
ConvTimeNet 0.253 0.249 0.248 0.254 1.103
TimeNet 0.542 0.542 0.542 0.542 0.546
MINDTREE DeConFuse 0.019 0.010 0.015 0.013 0.491
ConvTimeNet 0.594 0.559 0.545 0.599 1.058
TimeNet 0.319 0.318 0.319 0.317 0.770
MOTHERSUMI DeConFuse 0.014 0.005 0.009 0.011 0.381
ConvTimeNet 0.954 0.995 0.962 0.964 0.955
TimeNet 0.388 0.389 0.388 0.39 0.413
MRF DeConFuse 0.010 0.004 0.007 0.008 0.597
ConvTimeNet 0.422 0.421 0.420 0.423 0.618
TimeNet 0.915 0.915 0.916 0.914 0.489
NHPC DeConFuse 0.012 0.006 0.010 0.010 0.608
ConvTimeNet 2.957 3.029 2.986 3.006 9.161
TimeNet 0.083 0.082 0.083 0.084 0.706
NMDC DeConFuse 0.018 0.012 0.015 0.016 0.385
ConvTimeNet 0.747 0.743 0.741 0.746 1.214
TimeNet 0.103 0.103 0.105 0.101 0.491
NTPC DeConFuse 0.009 0.007 0.008 0.008 0.370
ConvTimeNet 0.507 0.507 0.515 0.499 1.082
TimeNet 0.111 0.110 0.110 0.112 0.563

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
OFSS DeConFuse 0.012 0.007 0.009 0.009 0.590
ConvTimeNet 0.144 0.158 0.150 0.152 0.727
TimeNet 0.105 0.103 0.105 0.103 0.552
OIL DeConFuse 0.014 0.007 0.010 0.012 0.441
ConvTimeNet 0.294 0.271 0.283 0.279 1.602
TimeNet 0.068 0.070 0.070 0.069 0.455
ONGC DeConFuse 0.012 0.006 0.009 0.010 0.467
ConvTimeNet 5.154 5.167 5.140 5.171 1.673
TimeNet 0.074 0.075 0.076 0.073 0.548
ORIENTBANK DeConFuse 0.023 0.009 0.017 0.016 0.378
ConvTimeNet 1.706 1.565 1.632 1.63 6.662
TimeNet 0.673 0.675 0.674 0.674 0.725
PAGEIND DeConFuse 0.017 0.006 0.013 0.010 0.431
ConvTimeNet 0.377 0.376 0.377 0.377 0.862
TimeNet 0.824 0.823 0.826 0.821 0.655
PETRONET DeConFuse 0.014 0.006 0.011 0.011 0.494
ConvTimeNet 0.785 0.793 0.793 0.787 3.480
TimeNet 0.119 0.118 0.118 0.118 0.455
PFC DeConFuse 0.019 0.011 0.015 0.016 0.335
ConvTimeNet 6.082 6.160 6.127 6.107 5.328
TimeNet 0.208 0.208 0.206 0.209 0.377
PIDILITIND DeConFuse 0.011 0.006 0.008 0.008 0.339
ConvTimeNet 0.148 0.159 0.158 0.148 1.693
TimeNet 0.328 0.327 0.328 0.328 0.514
PNB DeConFuse 0.025 0.010 0.019 0.017 0.402
ConvTimeNet 9.020 9.009 8.898 9.059 4.502
TimeNet 0.358 0.357 0.357 0.358 0.593
POWERGRID DeConFuse 0.009 0.006 0.007 0.008 0.351
ConvTimeNet 1.329 1.354 1.32] 1.359 4.055
TimeNet 0.196 0.196 0.194 0.197 0.412
PTC DeConFuse 0.016 0.007 0.011 0.012 0.385
ConvTimeNet 1.190 1.146 1.140 1.190 1.877
TimeNet 0.187 0.188 0.187 0.187 0.353
RCOM DeConFuse 0.049 0.019 0.040 0.033 0.515
ConvTimeNet 11.473 11.273 11.142 11.890 2.267
TimeNet 0.363 0.360 0.344 0.377 0.581
RECLTD DeConFuse 0.017 0.005 0.012 0.012 0.401
ConvTimeNet 7.043 6.659 6.798 6.912 12.186
TimeNet 0.145 0.145 0.144 0.147 0.521
RELCAPITAL DeConFuse 0.025 0.014 0.018 0.021 0.302
ConvTimeNet 2.394 2.359 2.289 2.428 0.498
TimeNet 0.127 0.130 0.128 0.131 0.474
RELIANCE DeConFuse 0.011 0.004 0.008 0.008 0.305
ConvTimeNet 0.251 0.234 0.245 0.239 1.609
TimeNet 0.459 0.458 0.458 0.459 0.654
RELINFRA DeConFuse 0.019 0.007 0.013 0.014 0.270
ConvTimeNet 2.045 2.032 1.998 2.084 0.970
TimeNet 0.157 0.159 0.157 0.159 0.320
RPOWER DeConFuse 0.024 0.007 0.018 0.016 0.475
ConvTimeNet 2.229 2.178 2.159 2.268 2.384
TimeNet 0.296 0.297 0.295 0.302 0.757

 

Page 19 of 32
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 5 Stock-wise forecasting results (Continued)

Page 20 of 32

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
SAIL DeConFuse 0.023 0.008 0.014 0.016 0.284
ConvTimeNet 1.319 1.225 1.274 1.276 1,399
TimeNet 0.161 0.160 0.163 0.158 0.495
SBIN DeConFuse 0.015 0.006 0.011 0.01 0.339
ConvTimeNet 0.663 0.661 0.675 0.648 1.428
TimeNet 0.120 0.118 0.120 0.118 0.696
SIEMENS DeConFuse 0.011 0.006 0.008 0.009 0.452
ConvTimeNet 0.289 0.212 0.292 0.245 8.495
TimeNet 0.085 0.086 0.085 0.086 0.763
SOUTHBANK DeConFuse 0.018 0.008 0.013 0.013 0.539
ConvTimeNet 7.863 7712 7.795 7.788 10.684
TimeNet 0.162 0.161 0.161 0.163 0.534
SRF DeConFuse 0.016 0.006 0.011 0.012 0.396
ConvTimeNet 0.373 0.318 0.339 0.359 0.791
TimeNet 0.226 0.225 0.225 0.225 0.508
SRTRANSFIN DeConFuse 0.019 0.011 0.015 0.017 0.445
ConvTimeNet 2.900 2.892 2.838 2.946 0.667
TimeNet 0.297 0.295 0.296 0.297 0.482
STAR DeConFuse 0.027 0.015 0.022 0.025 0.464
ConvTimeNet 2.46] 2.586 2.307 2.629 6.115
TimeNet 0.827 0.820 0.821 0.825 0.642
SUNPHARMA DeConFuse 0.016 0.006 0.011 0.011 0.368
ConvTimeNet 0.203 0.202 0.203 0.203 0.655
TimeNet 0.388 0.390 0.385 0.391 0.645
SUNTV DeConFuse 0.015 0.005 0.010 0.011 0.356
ConvTimeNet 0.175 0.172 0.173 0.176 1.482
TimeNet 0.471 0.472 0.470 0.472 0.483
SYNDIBANK DeConFuse 0.024 0.009 0.017 0.017 0.361
ConvTimeNet 1.405 1.391 1.271 1.521 4.672
TimeNet 0.176 0.175 0.174 0.177 0.410
TATACHEM DeConFuse 0.011 0.005 0.008 0.009 0.392
ConvTimeNet 1.044 1.066 1.044 1.025 0.690
TimeNet 0.368 0.368 0.367 0.368 0.412
TATACOMM DeConFuse 0.013 0.006 0.009 0.010 0.443
ConvTimeNet 0.231 0.249 0.239 0.241 0.835
TimeNet 0.241 0.241 0.239 0.243 0.541
TATAGLOBAL DeConFuse 0.017 0.006 0.013 0.012 0.599
ConvTimeNet 1.724 1.807 1.737 1.813 4.354
TimeNet 0.418 0.417 0.418 0.416 0.477
TATAMOTORS DeConFuse 0.015 0.007 0.012 0.011 0.333
ConvTimeNet 0.644 0.688 0.660 0.650 1.844
TimeNet 0.279 0.278 0.278 0.277 0.659
TATAMTRDVR DeConFuse 0.015 0.006 0.013 0.011 0.380
ConvTimeNet 1.153 1.213 1.16] 1.153 1.219
TimeNet 0.444 0.443 0.445 0.440 0.455
TATAPOWER DeConFuse 0.012 0.005 0.009 0.010 0.413
ConvTimeNet 0.435 0.442 0.431 0.452 1.265
TimeNet 0.096 0.096 0.096 0.096 0.571
TATASTEEL DeConFuse 0.015 0.005 0.009 0.012 0.258
ConvTimeNet 1.363 1.390 1.369 1.365 0.862
TimeNet 0.381 0.381 0.380 0.381 0.662

 
Table 5 Stock-wise forecasting results (Continued)

Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

 

 

Stock name Method MAE close MAE open MAE high MAE low MAE NAV
TCS DeConFuse 0.012 0.005 0.009 0.008 0.445
ConvTimeNet 1.48] 1.337 1.409 1.323 6.096
TimeNet 0.231 0.229 0.230 0.231 0.525
TECHM DeConFuse 0.014 0.005 0.009 0.009 0.386
ConvTimeNet 1.857 1.634 1.753 1.746 6.126
TimeNet 0.175 0.174 0.176 0.173 0.416
TITAN DeConFuse 0.014 0.005 0.010 0.010 0.419
ConvTimeNet 2.649 2.676 2.698 2.633 3.126
TimeNet 0.548 0.548 0.547 0.548 0.630
TVSMOTOR DeConFuse 0.014 0.005 0.009 0.011 0.385
ConvTimeNet 1.120 1.120 1.108 1.129 0.848
TimeNet 0.441 0.441 0.441 0.441 0.403
UBL DeConFuse 0.018 0.008 0.014 0.013 0.418
ConvTimeNet 0.144 0.191 0.157 0.173 0.915
TimeNet 0.246 0.244 0.248 0.241 0.593
ULTRACEMCO DeConFuse 0.011 0.005 0.008 0.007 0.408
ConvTimeNet 0.088 0.086 0.086 0.088 0.712
TimeNet 0.237 0.236 0.236 0.237 0.483
UNIONBANK DeConFuse 0.023 0.009 0.017 0.016 0.307
ConvTimeNet 8.195 8.076 8.034 8.207 11.330
TimeNet 0.395 0.395 0.396 0.394 0.394
UPL DeConFuse 0.014 0.004 0.009 0.010 0.391
ConvTimeNet 1.182 1.034 1.122 1.101 2.592
TimeNet 0.275 0.276 0.274 0.277 0.410
VEDL DeConFuse 0.018 0.010 0.014 0.015 0.235
ConvTimeNet 2.904 3.024 2.967 2.959 0.605
TimeNet 0.295 0.295 0.295 0.295 0.720
VOLTAS DeConFuse 0.016 0.009 0.012 0.013 0.369
ConvTimeNet 1.244 1.272 1.268 1.254 4.493
TimeNet 0.475 0.475 0.474 0.476 0.354
WIPRO DeConFuse 0.009 0.005 0.007 0.007 0.456
ConvTimeNet 0.301 0.290 0.298 0.295 0.799
TimeNet 0.067 0.065 0.067 0.066 0.647
WOCKPHARMA DeConFuse 0.021 0.009 0.015 0.016 0.504
ConvTimeNet 2.407 2.486 2.335 2.582 5.903
TimeNet 0.394 0.395 0.394 0.393 0.492
YESBANK DeConFuse 0.014 0.006 0.01 0.011 0.335
ConvTimeNet 0.875 0.868 0.866 0.879 1.066
TimeNet 0.422 0.423 0.424 0.422 0.599
ZEEL DeConFuse 0.010 0.005 0.008 0.008 0.400
ConvTimeNet 1.132 1.135 1.136 1.123 1.449
TimeNet 0.265 0.265 0.264 0.267 0.513

 

Page 21 of 32
Appendix 2: Detailed stock trading results

Maggu et al. EURASIP Journal on Advances in Signal Processing

Table 6 Stock-wise trading results

(2020) 2020:26

Page 22 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC True AR Predicted AR
ABIRLANUVO DeConFuse 0.553 0.886 0.681 0.558 41.950 15.600
ConvTimeNet 0.515 0.966 0.672 0.541 3.090
TimeNet 0.512 0.989 0.674 0.478 8.340
ACC DeConFuse 0.449 0.761 0.565 0.600 -7.090 — 1.070
ConvlimeNet 0.449 0.337 0.385 0.529 — 4.100
TimeNet 0.389 0.152 0.219 0.506 — 9.020
ADANIENT DeConFuse 0.581 0.962 0.724 0.560 20.690 4.570
ConvlimeNet 0.594 0.145 0.233 0.504 69.400
TimeNet 0.565 0.962 0.712 0.571 — 3.610
ADANIPORTS DeConFuse 0.520 0.919 0.660 0.546 0.900 0.010
ConvTimeNet 0.503 0.694 0.583 0.570 2.560
TimeNet 0.534 0.568 0.550 0.559 17.750
ADANIPOWER DeConFuse 0.461 0.862 0.601 0.492 -34.600 10.840
ConvlimeNet 0.473 0.569 0.517 0.460 - 28.930
TimeNet 0.495 0.872 0.631 0.495 — 19.110
AJANTPHARM DeConFuse 0.449 0.757 0.564 0.514 -44.660 -—- 29.150
ConvTimeNet 0.469 0.757 0.579 0.498 — 22.320
TimeNet 0.577 0.214 0.312 0.603 - 35.460
ALBK DeConFuse 0.485 0.776 0.597 0.550 -23.800 -5.890
ConvTimeNet 0.461 0.766 0.575 0.495 - 17.440
TimeNet 0.478 0.411 0.442 0.516 29.660
AMARAJABAT DeConFuse 0.549 0.718 0.622 0.568 19.460 41.830
ConvTimeNet 0.463 0.321 0.379 0.502 — 19,990
TimeNet 0.667 0.026 0.049 0.549 — 27.870
AMBUJACEM DeConFuse 0.486 0.829 0.613 0.576 -8.970 - 10.080
ConvlimeNet 0.457 0.410 0.432 0.503 — 1.310
TimeNet 0.448 0.533 0.487 0.470 16.490
ANDHRABANK ~— DeConFuse 0.391 0.753 0.515 0.479 -21.850 4.660
ConvTimeNet 0.401 0.763 0.526 0.513 1.060
TimeNet 0.446 0.484 0.464 0.548 - 18.610
APOLLOHOSP DeConFuse 0.447 0.921 0.602 0.510 23.140 6.820
ConvTlimeNet 0.432 0.812 0.564 0.509 1.440
TimeNet 0.436 0.941 0.596 0.493 4.630
APOLLOTYRE DeConFuse 0.502 0.920 0.650 0.536 - 13.140 2.730
ConvTimeNet 0.600 0.027 0.051 0.606 - 2.810
TimeNet 0.482 0.973 0.645 0.468 0.950
ARVIND DeConFuse 0.513 0.936 0.662 0.571 16.320 19.560
ConvTimeNet 0.603 0.376 0.463 0.637 — 33.780
TimeNet 0.476 1.000 0.645 0.445 0.000
ASHOKLEY DeConFuse 0.532 0.849 0.654 0.520 47.650 — 16.530
ConvlimeNet 0.524 0.092 0.157 0.502 — 14.800
TimeNet 0.522 0.798 0.631 0.551 - 13.550
ASIANPAINT DeConFuse 0.523 0.868 0.652 0.595 32.770 1.250
ConvTimeNet 0.500 0.245 0.329 0.539 4.400
TimeNet 0.463 1.000 0.633 0.487 0.000
AUROPHARMA ~— DeConFuse 0.511 0.835 0.634 0.532 3.370 4.430
ConvlimeNet 0.484 0.679 0.565 0.509 — 5.900
TimeNet 0.468 0.954 0.628 0.548 — 8.060

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 23 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC TrueAR Predicted AR
AXISBANK DeConFuse 0.527 0.843 0.649 0.535 3.830 26.440
ConvTimeNet — 0.503 0.643 0.565 0.485 12.060
TimeNet 0.500 0.835 0.625 0.525 — 6.970
BAJAJ-AUTO DeConFuse 0.491 0.776 0.601 0.552 12.590 15.430
ConvTimeNet 0.431 0.234 0.303 0.518 — 9,380
TimeNet 0.463 0.355 0.402 0.512 — 10.670
BAJFINANCE DeConFuse 0.570 0.934 0.708 0.487 21.610 -4.050
ConvTimeNet 0.526 0.440 0.479 0.441 16.480
TimeNet 0.569 1.000 0.725 0.568 0.000
BANKBARODA ~ DeConFuse 0.584 0.473 0.523 0.569 -21.990 2.880
ConvTimeNet 0.485 0.573 0.525 0.495 — 3.680
TimeNet 0.286 0.018 0.034 0.539 - 16.310
BANKINDIA DeConFuse 0.463 0.925 0.617 0.428 - 29.380 - 2.880
ConvTimeNet 0.491 0.757 0.596 0.500 — 25.090
TimeNet 0.571 0.224 0.322 0.567 — 19.800
BATAINDIA DeConFuse 0.523 0.693 0.596 0.494 63.650 34.340
ConvTimeNet — 0.000 0.000 0.000 0.522 0.000
TimeNet 0.520 0.456 0.486 0.547 6.090
BEL DeConFuse 0.457 0.892 0.604 0.592 -17.530 —- 22.310
ConvTimeNet 0.421 0.785 0.548 0.560 - 18.020
TimeNet 0.405 0.985 0.574 0.566 1.220
BHARATFORG DeConFuse 0.496 1.000 0.663 0.507 - 2.210 3.800
ConvTimeNet 0.400 0.035 0.065 0.578 -3.510
TimeNet 0.493 0.982 0.657 0.496 1.500
BHARTIARTL DeConFuse 0.486 0.817 0.609 0.563 9.350 — 10.08
ConvlimeNet 0.580 0.279 0.377 0.527 - 7.500
TimeNet 0.493 0.327 0.393 0.535 — 7.6/0
BHEL DeConFuse 0.540 0.857 0.662 0.578 -3.050 10.340
ConvlimeNet 0.555 0.589 0.571 0.576 — 32.780
TimeNet 0.481 0.562 0.519 0.494 - 7.340
BIOCON DeConFuse 0.523 0.780 0.626 0.487 30.340 — 9,750
ConvlimeNet 1.000 0.051 0.097 0.540 11.350
TimeNet 0.539 0.407 0.464 0.520 — 0.280
BOSCHLTD DeConFuse 0.437 0.938 0.596 0.550 -5.430 4.380
ConvlimeNet 0.464 0.481 0.473 0.513 3.330
TimeNet 0.000 0.000 0.000 0.496 0.000
BPCL DeConFuse 0.525 0.850 0.649 0.509 - 0.640 - 0.740
ConvlimeNet 0.535 0.611 0.570 0.561 — 2.290
TimeNet 0.482 0.956 0.641 0.466 — 1.660
BRITANNIA DeConFuse 0.604 0.871 0.714 0.550 17.710 4.930
ConvTimeNet 0.558 0.258 0.353 0.492 17.400
TimeNet 0.500 0.043 0.079 0.572 42.380
CAIRN DeConFuse 0.558 0.682 0.614 0.540 38.310 — 14.830
ConvTimeNet 0.833 0.059 0.110 0.483 69.850
TimeNet 0.000 0.000 0.000 0.480 63.040
CANBK DeConFuse 0.500 0.798 0.615 0.552 -2.440 - 9.350
ConvTimeNet 0.472 0.862 0.610 0.471 15.920
TimeNet 0.485 0.908 0.633 0.528 — 20.500
CASTROLIND DeConFuse 0.468 0.843 0.602 0.502 -12.570 -17.840
ConvTimeNet 0427 0.800 0.557 0.464 - 15.310
TimeNet 0.800 0.057 0.107 0.516 — 20.970

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 24 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC True AR Predicted AR
CEATLTD DeConFuse 0.474 0.797 0.595 0.598 -10.760 5.010
ConvTimeNet 0.413 0.725 0.526 0.509 — 27.600
TimeNet 0.434 0.957 0.597 0.520 - 10.180
CENTURYTEX  DeConFuse 0.575 0.807 0.672 0.602 -18.430 - 21.330
ConvTimeNet = 0.513 0.675 0.583 0.535 - 16.620
TimeNet 0.498 1.000 0.665 0.483 0.000
CESC DeConFuse 0.489 0.789 0.604 0.562 2.300 - 4.980
ConvTimeNet 0.550 0.101 0.171 0.535 — 5.930
TimeNet 0.458 0.844 0.594 0.476 — 7,060
CIPLA DeConFuse 0.472 0.810 0.596 0.564 -3.130 — 12.260
ConvlimeNet 0.508 0.619 0.558 0.541 — 7,370
TimeNet 0.462 0.867 0.603 0.536 - 3.270
COALINDIA DeConFuse 0.557 0.462 0.505 0.528 -2.370 - 1.030
ConvTimeNet — 0.500 0.051 0.093 0.433 — 5.210
TimeNet 0.667 0.051 0.095 0.489 6.380
COLPAL DeConFuse 0.519 0.893 0.657 0.436 16.070 6.000
ConvlimeNet 0.643 0.074 0.133 0.562 1.580
TimeNet 0.566 0.355 0.437 0.524 — 3.180
DABUR DeConFuse 0.542 0.791 0.643 0.560 21.590 — 3.260
ConvTimeNet — 0.500 0.026 0.050 0.503 41.300
TimeNet 0.500 0.983 0.663 0.560 — 4.800
DHFL DeConFuse 0.513 0.836 0.635 0.553 -8.590 - 8.510
ConvTimeNet 0.449 0.726 0.555 0.501 7.130
TimeNet 0.456 1.000 0.627 0.547 0.000
DISHTV DeConFuse 0.497 0.815 0.618 0.512 -14.010 20.570
ConvlimeNet 0.507 0.954 0.662 0.536 - 12.220
TimeNet 0.469 0.981 0.635 0.539 0.700
DIVISLAB DeConFuse 0.505 0.867 0.638 0.485 2.800 - 0.920
ConvTimeNet 0.460 0.513 0.485 0.474 14.960
TimeNet 0.489 0.973 0.651 0.567 — 4.620
DLF DeConFuse 0.583 0.903 0.709 0.545 14.530 32.160
ConvTimeNet 0.605 0.395 0.478 0.565 3.060
TimeNet 0.539 0.992 0.699 0.524 2.690
DRREDDY DeConFuse 0.518 0.870 0.649 0.586 -2.060 10.640
ConvTlimeNet 0.492 0.774 0.601 0.470 5.080
TimeNet 0.507 0.991 0.671 0.487 — 10.530
EICHERMOT DeConFuse 0.515 0.936 0.664 0.519 - 7.280 - 1.070
ConvTimeNet 0.478 0.681 0.561 0.494 — 7.780
TimeNet 0.503 0.904 0.646 0.553 0.750
ENGINERSIN DeConFuse 0.568 0.659 0.610 0.612 -3.150 — 40.820
ConvTimeNet 0.456 0.439 0.447 0.508 - 9.830
TimeNet 0.500 0.085 0.146 0.473 — 27400
EXIDEIND DeConFuse 0.525 0.850 0.649 0.603 22.020 9.570
ConvTimeNet 0.629 0.195 0.297 0.542 2.170
TimeNet 0.484 0.788 0.599 0.506 17.840
FEDERALBNK = DeConFuse 0.479 0.810 0.600 0.551 -23.270 -20.170
ConvTlimeNet 0.434 0.790 0.560 0.511 16.480
TimeNet 0.428 0.860 0.571 0.508 — 9,940
GAIL DeConFuse 0.554 0.683 0.612 0.496 35.420 1.670
ConvTimeNet — 0.000 0.000 0.000 0.564 0.000
TimeNet 0.714 0.083 0.149 0.521 29.550

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 25 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC True AR ‘Predicted AR
GLENMARK DeConFuse 0.543 0.847 0.662 0.546 -12.340 = 7.550
ConvlimeNet 0.640 0.483 0.551 0.580 - 14.500
TimeNet 0.529 0.780 0.630 0.490 4.890
GMRINFRA DeConFuse 0.512 0.792 0.622 0.547 8.230 3.520
ConvlimeNet 0.424 0.132 0.201 0.537 46.980
TimeNet 0.000 0.000 0.000 0.523 0.000
GODREJIND DeConFuse 0.551 0.932 0.692 0.572 12.190 6.320
ConvTimeNet 0.528 0.957 0.681 0.569 — 4.980
TimeNet 0.515 1.000 0.680 0.584 1.560
GRASIM DeConFuse 0.451 0.854 0.590 0.571 11.750 31.030
ConvTimeNet 0.457 0.552 0.500 0.563 4.060
TimeNet 0.421 1.000 0.590 0.563 — 0.660
HAVELLS DeConFuse 0.562 0.886 0.688 0.534 25.080 4.960
ConvTimeNet 0.929 0.106 0.190 0.553 1.110
TimeNet 0.586 0.724 0.647 0.621 28.710
HCLTECH DeConFuse 0.596 0.862 0.704 0.510 26.700 — 4.330
ConvlimeNet 0.573 0.331 0.42 0.477 13.730
TimeNet 0.566 0.985 0.719 0.529 1.090
HDFC DeConFuse 0.557 0.817 0.662 0.530 - 13.020 11.470
ConvlimeNet 0.677 0.175 0.278 0.551 7.020
TimeNet 0.515 0.575 0.543 0.492 18.220
HDFCBANK DeConFuse 0.551 0.851 0.669 0.561 -0.470 9.890
ConvlimeNet 0.569 0.306 0.398 0.510 — 7.500
TimeNet 0.529 0.992 0.690 0.522 0.520
HDIL DeConFuse 0.466 0.883 0.610 0.565 -51.190 -11.390
ConvTimeNet 0.448 0.915 0.601 0.482 27.120
TimeNet 0.445 1.000 0.610 0.474 0.000
HEROMOTOCO = DeConFuse 0.482 0.796 0.601 0.556 - 19.99 — 1.530
ConvlimeNet 0.529 0.350 0.421 0.570 - 18.100
TimeNet 0.418 0.573 0.484 0.442 — 0.910
HEXAWARE DeConFuse 0.577 0.879 0.697 0.518 41.150 — 3.690
ConvTimeNet 1.000 0.015 0.030 0.495 20.850
TimeNet 0.570 0.955 0.714 0.458 5.010
HINDALCO DeConFuse 0.495 0.872 0.631 0.547 6.020 — 6.310
ConvlimeNet 0.474 0.679 0.558 0.524 1.110
TimeNet 0.494 0.807 0.613 0.540 — 19.450
HINDPETRO DeConFuse 0.459 0.931 0.615 0.507 - 18.980 —- 1.200
ConvlimeNet 0.446 0.775 0.566 0.549 - 14.200
TimeNet 0.445 1.000 0.615 0.457 0.000
HINDUNILVR DeConFuse 0.594 0.956 0.733 0.512 8.010 2.820
ConvTimeNet — 0.500 0.030 0.056 0.497 10.740
TimeNet 0.623 0.696 0.657 0.578 20.200
HINDZINC DeConFuse 0.529 0.894 0.664 0.581 -8.970 7.870
ConvlimeNet 0.617 0.257 0.362 0.570 — 26.230
TimeNet 0.495 0.938 0.648 0.511 0.920
IBREALEST DeConFuse 0.613 0.642 0.627 0.562 50.250 4.550
ConvTlimeNet 0.750 0.028 0.055 0.560 2./20
TimeNet 0.000 0.000 0.000 0.455 0.000
IBULHSGFIN DeConFuse 0.534 0.814 0.645 0.574 -33.740 3.660
ConvTimeNet 0.488 0.907 0.634 0.562 2.130
TimeNet 0.447 0.837 0.583 0.491 4.990

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 26 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Filscore AUC True AR Predicted AR
ICICIBANK DeConFuse 0.514 0.664 0.580 0.554 -15.240 -7.840
ConvlimeNet 0.528 0.262 0.350 0.530 7.090
TimeNet 0.455 0.047 0.085 0.471 32.410
IDBI DeConFuse 0.545 0.757 0.634 0.596 -38.100 - 4.320
ConvTimeNet — 0.503 0.775 0.610 0.529 - 14.250
TimeNet 0.577 0.505 0.538 0.538 — 11.360
IDEA DeConFuse 0.450 0.949 0.610 0.571 -26.870 -12.280
ConvTimeNet 0.415 0.667 0.512 0.549 10.310
TimeNet 0.395 0.495 0.439 0.414 26.380
IDFC DeConFuse 0.460 0.737 0.566 0.500 10.310 7.640
ConvTimeNet 0.511 0.232 0.319 0.550 — 28.470
TimeNet 0.444 0.040 0.074 0.538 — 11.310
IFCI DeConFuse 0.541 0.653 0.592 0.612 -21.380 -10.090
ConvTimeNet 0.429 0.713 0.535 0.489 9.880
TimeNet 0.450 0.178 0.255 0.542 1.240
IGL DeConFuse 0.505 0.955 0.660 0.545 -17.220 - 4.580
ConvTimeNet 0.489 1.000 0.657 0.425 0.000
TimeNet 0.483 0.902 0.629 0.500 3.870
INDIACEM DeConFuse 0.512 0.796 0.623 0.607 3.720 — 15.030
ConvTimeNet 0.450 0.537 0.489 0.452 — 17.100
TimeNet 0.473 0.907 0.622 0.541 - 1.060
INDUSINDBK = DeConFuse 0.510 0.896 0.650 0.485 2.350 — 2.270
ConvTimeNet — 0.250 0.026 0.047 0.483 7.450
TimeNet 0.502 1.000 0.669 0.455 0.000
INFY DeConFuse 0.590 0.803 0.677 0.519 19.370 23.970
ConvTimeNet — 0.590 0.348 0.440 0.501 23.650
TimeNet 0.577 0.598 0.587 0.482 33.220
loc DeConFuse 0.546 0.848 0.664 0.560 7.260 — 8.670
ConvTimeNet 0.495 0.938 0.648 0.527 — 1.190
TimeNet 0.477 0.946 0.635 0.452 1.350
IRB DeConFuse 0.528 0.920 0.671 0.567 -14.090 -15.820
ConvTimeNet — 0.517 0.821 0.634 0.509 — 20.420
TimeNet 0.489 1.000 0.657 0.491 0.000
ITC DeConFuse 0.515 0.785 0.622 0.550 16.580 8./80
ConvTimeNet 0.482 0.383 0.427 0.509 16.990
TimeNet 0.465 0.935 0.621 0.550 — 2.330
JINDALSTEL DeConFuse 0.547 0.894 0.679 0.497 34.970 19.940
ConvTimeNet 0.440 0.089 0.149 0.434 114.370
TimeNet 0.535 0.187 0.277 0.548 35.170
JISLJALEQS DeConFuse 0.495 0.877 0.633 0.480 -26.510 -9.490
ConvTimeNet 0.495 0.412 0.450 0.521 — 6.950
TimeNet 0.477 0.623 0.540 0.455 18.140
JPASSOCIAT DeConFuse 0.467 0.324 0.383 0.465 -15.680 = - 7.23
ConvTimeNet 0.545 0.056 0.101 0.503 - 22.480
TimeNet 0.000 0.000 0.000 0.504 0.000
JSWENERGY DeConFuse 0.537 0.784 0.637 0.573 28.740 25.080
ConvTimeNet — 0.509 0.569 0.537 0.512 — 2.810
TimeNet 0.494 0.873 0.631 0.472 11.840
JSWSTEEL DeConFuse 0.567 0.850 0.680 0.559 17.590 3.040
ConvTimeNet — 0.560 0.425 0.483 0.522 — 20.500
TimeNet 0.520 0.975 0.678 0.476 — 4.980

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 27 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC True AR Predicted AR
JUBLFOOD DeConFuse 0.586 0.882 0.704 0.554 24.580 7.970
ConvTimeNet 0.520 0.205 0.294 0.501 22.900
TimeNet 0.750 0.071 0.129 0.582 110.88
JUSTDIAL DeConFuse 0.570 0.450 0.503 0.560 29.030 1.600
ConvTimeNet 0.643 0.248 0.358 0.560 — 20.230
TimeNet 0.000 0.000 0.000 0.439 0.000
KOTAKBANK DeConFuse 0.584 0.910 0.711 0.526 27.020 — 5.120
ConvTimeNet 0.000 0.000 0.000 0.502 0.000
TimeNet 0.579 0.955 0.721 0.511 0.900
KSCL DeConFuse 0.545 0.838 0.660 0.572 4.420 - 15.790
ConvTimeNet = 0.535 0.575 0.554 0.545 46.130
TimeNet 0.522 0.300 0.381 0.473 30.920
KTKBANK DeConFuse 0.494 0.784 0.606 0.518 -16.590 -15.610
ConvTimeNet 0.488 0.532 0.509 0.530 — 6.400
TimeNet 0.491 0.730 0.587 0.545 — 13.540
L&TFH DeConFuse 0.468 0.906 0.617 0.550 -20.710 -7.5/70
ConvlimeNet 0.453 0.906 0.604 0.550 - 19.540
TimeNet 0.432 0.958 0.595 0.478 — 1.020
LICHSGFIN DeConFuse 0.517 0.852 0.640 0.549 -21.250 12.680
ConvlimeNet 0.471 0.676 0.555 0.514 14.560
TimeNet 0.476 0.980 0.640 0.530 — 1.150
LT DeConFuse 0.525 0.810 0.637 0.524 7.670 — 3.420
ConvTimeNet 0.562 0.078 0.136 0.553 - 1.190
TimeNet 0.519 0.241 0.329 0.534 21.790
LUPIN DeConFuse 0.562 0.860 0.680 0.545 -46.000 -13.900
ConvTimeNet 0.534 0.645 0.584 0.504 — 9.170
TimeNet 0.518 0.702 0.596 0.515 — 13.000
M&M DeConFuse 0.576 0.760 0.656 0.556 26.670 — 5.130
ConvTimeNet 1.000 0.062 0.117 0.550 9.230
TimeNet 0.596 0.791 0.680 0.570 5.390
M&MFIN DeConFuse 0.505 0.819 0.625 0.435 57.200 3.640
ConvTimeNet 0.000 0.000 0.000 0.539 0.000
TimeNet 0.590 0.310 0.407 0.519 26.820
MARUTI DeConFuse 0.527 0.883 0.660 0.574 10.200 3.950
ConvlimeNet 0.508 0.559 0.532 0.562 6.680
TimeNet 0.485 1.000 0.653 0.500 0.000
MINDTREE DeConFuse 0.521 0.718 0.604 0.483 51.140 34.170
ConvlimeNet 0.625 0.097 0.168 0.528 1.180
TimeNet 0.577 0.291 0.387 0.498 37.020
MOTHERSUMI = DeConFuse 0.510 0.863 0.641 0.504 - 0.320 4.730
ConvTimeNet 0.510 0.537 0.526 0.519 — 22.050
TimeNet 0.489 0.979 0.653 0.535 2.480
MRF DeConFuse 0.489 0.571 0.527 0.463 - 3.020 2.370
ConvTimeNet 0.500 0.089 0.152 0.520 — 6.320
TimeNet 0.482 0.964 0.643 0.480 — 3.050
NHPC DeConFuse 0.531 0.520 0.526 0.598 -10.570 -3.130
ConvlimeNet 0.556 0.255 0.350 0.564 13.660
TimeNet 0.000 0.000 0.000 0.474 0.000
NMDC DeConFuse 0.550 0.783 0.646 0.557 -10.800 5.560
ConvTimeNet 0.540 0.558 0.549 0.528 — 16.940
TimeNet 0.528 0.633 0.576 0.500 - 10.610

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 28 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC True AR Predicted AR
NTPC DeConFuse 0.487 0.862 0.623 0.480 1.690 - 4.410
ConvlimeNet 0.535 0.349 0.422 0.560 --7. 440
TimeNet 0.497 0.789 0.610 0.534 3.830
OFSS DeConFuse 0.500 0.723 0.591 0.453 21.390 33.070
ConvTimeNet 0.593 0.134 0.219 0.518 6.100
TimeNet 0.667 0.017 0.033 0.419 6.190
OIL DeConFuse 0.533 0.731 0.616 0.525 -21.220 = - 15.240
ConvTimeNet 0.495 0.577 0.533 0.541 - 17.520
TimeNet 0.496 0.769 0.603 0.465 — 13.470
ONGC DeConFuse 0.526 0.750 0.618 0.604 20.320 13.740
ConvTimeNet 0.496 0.519 0.507 0.498 — 13.270
TimeNet 0.447 0.704 0.547 0.543 11.750
ORIENTBANK ~— DeConFuse 0.466 0.880 0.609 0.553 -10.110 -17.560
ConvlimeNet 0.435 0.740 0.548 0.492 16.580
TimeNet 0.430 0.490 0.458 0.518 38.720
PAGEIND DeConFuse 0.503 0.935 0.655 0.489 39.910 8.130
ConvTimeNet 0.375 0.195 0.256 0.400 0.630
TimeNet 0.447 0.545 0.491 0.521 1.050
PETRONET DeConFuse 0.520 0.929 0.669 0.525 12.330 — 9.760
ConvTimeNet 0.520 0.241 0.331 0.539 — 12.190
TimeNet 0.485 0.982 0.649 0.546 1.450
PFC DeConFuse 0.503 0.733 0.597 0.532 2.150 7.310
ConvTimeNet 0479 0.657 0.554 0.551 11.310
TimeNet 0.458 0.667 0.543 0.497 — 14.680
PIDILITIND DeConFuse 0.602 0.773 0.677 0.596 30.150 11.440
ConvTimeNet 0.564 0.500 0.530 0.501 12.880
TimeNet 0.550 1.000 0.710 0.518 0.000
PNB DeConFuse 0.512 0.644 0.570 0.572 -23.580 - 14.500
ConvTimeNet 0.496 0.634 0.557 0.560 - 18.650
TimeNet 0.495 0.455 0.474 0.550 — 6.780
POWERGRID DeConFuse 0.491 0.757 0.595 0.560 10.340 — 5.810
ConvTlimeNet 0473 0.777 0.588 0.531 — 6.230
TimeNet 0.481 0.495 0.488 0.511 0.420
PTC DeConFuse 0.526 0.766 0.624 0.610 -19.080 -30.920
ConvTimeNet 0471 0.607 0.531 0.518 — 34.44
TimeNet 0.449 0.907 0.601 0.537 — 5.060
RCOM DeConFuse 0.474 0.540 0.505 0.524 -29.600 -38.130
ConvTimeNet 0.091 0.010 0.018 0.489 — 39.190
TimeNet 0.000 0.000 0.000 0.495 0.000
RECLTD DeConFuse 0.439 0.863 0.582 0.511 -29.540 -32.010
ConvTimeNet 0.407 0.621 0.492 0.491 — 2.110
TimeNet 0.420 0.495 0.454 0.500 — 25.420
RELCAPITAL DeConFuse 0.497 0.843 0.625 0.575 -31.650 12.140
ConvTimeNet 0.481 0.704 0.571 0.563 — 14.590
TimeNet 0.471 0.981 0.637 0.491 - 15.740
RELIANCE DeConFuse 0.588 0.870 0.702 0.574 4.780 9.430
ConvTimeNet — 0.000 0.000 0.000 0.524 0.000
TimeNet 0.571 0.802 0.667 0.506 4.880
RELINFRA DeConFuse 0.535 0.868 0.662 0.528 -12.910 -11.870
ConvTimeNet 0.493 0.930 0.644 0.505 — 3.650
TimeNet 0.493 0.974 0.655 0.482 1.860

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 29 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC True AR Predicted AR
RPOWER DeConFuse 0.541 0.860 0.660 0.588 - 40.030 = 2.300
ConvTimeNet 0.512 0.947 0.660 0.605 9.490
TimeNet 0.500 0.904 0.644 0.529 - 15.860
SAIL DeConFuse 0.541 0.748 0.628 0.498 10.540 41.720
ConvTimeNet 0.576 0.276 0.374 0.498 12.370
TimeNet 0.562 0.146 0.232 0.482 11.190
SBIN DeConFuse 0.518 0.673 0.585 0.569 -1.370 - 3.740
ConvTimeNet 0.491 0.486 0.488 0.545 6.730
TimeNet 0.463 0.467 0.465 0.484 29.460
SIEMENS DeConFuse 0.520 0.919 0.664 0.574 -6.930 3.220
ConvlimeNet 0.540 0.613 0.574 0.559 8.280
TimeNet 0.485 0.991 0.651 0.505 - 1.080
SOUTHBANK DeConFuse 0.492 0.492 0.492 0.628 -34.640 - 42.640
ConvTimeNet 0.407 0.559 0.471 0.542 — 48.180
TimeNet 0.000 0.000 0.000 0.568 0.000
SRF DeConFuse 0.543 0.809 0.649 0.569 -15.350 42.110
ConvTimeNet 0.471 0.205 0.286 0.484 - 37.280
TimeNet 0.479 0.859 0.615 0.487 — 19.230
SRTRANSFIN DeConFuse 0.575 0.780 0.662 0.564 2.810 32.140
ConvTimeNet 0.765 0.106 0.186 0.578 - 0.030
TimeNet 0.517 0.862 0.646 0.441 9.070
STAR DeConFuse 0.474 0.881 0.617 0.581 -38.200 - 34.590
ConvTimeNet 0.453 0.631 0.527 0.531 — 50.680
TimeNet 0.462 0.512 0.486 0.482 — 56.200
SUNPHARMA DeConFuse 0.476 0.908 0.625 0.521 24.640 — 0.220
ConvTimeNet 0.468 0.743 0.574 0.472 15.310
TimeNet 0.476 0.734 0.578 0.520 -15.150
SUNTV DeConFuse 0.502 0.928 0.652 0.513 - 9.190 - 7.630
ConvTimeNet 0.588 0.423 0.492 0.600 — 16.630
TimeNet 0.485 0.892 0.629 0.533 — 6.620
SYNDIBANK DeConFuse 0.450 0.786 0.572 0.552 -52.140 —- 9.720
ConvTimeNet 0.443 0.755 0.558 0.540 -17.660
TimeNet 0.222 0.020 0.037 0.533 -34.330
TATACHEM DeConFuse 0.538 0.739 0.620 0.565 8.710 8.700
ConvTimeNet 0.700 0.061 0.112 0.504 3.210
TimeNet 0.530 0.765 0.620 0.548 2.060
TATACOMM DeConFuse 0.516 0.855 0.644 0.581 2.020 — 9,330
ConvTimeNet 0.488 0.936 0.642 0.537 — 14.430
TimeNet 0.480 0.964 0.640 0.562 - 4.480
TATAGLOBAL DeConFuse 0.573 0.850 0.685 0.574 29.440 14.710
ConvTimeNet = 0.530 0.733 0.615 0.564 — 6.740
TimeNet 0.536 0.850 0.660 0.534 — 9,600
TATAMOTORS ~— DeConFuse 0.522 0.761 0.619 0.576 -30.220 —- 1.920
ConvTimeNet 0.483 0.633 0.548 0.511 - 6.140
TimeNet 0.450 0.450 0.450 0.491 — 2.090
TATAMTRDVR ~— DeConFuse 0.478 0.854 0.610 0.518 -35.66 — 1.030
ConvTimeNet 0.447 0.738 0.557 0.502 — 17.550
TimeNet 0.455 0.971 0.610 0.502 — 8.180
TATAPOWER DeConFuse 0.540 0.514 0.527 0.564 -4.090 — 17.580
ConvTimeNet 0.558 0.276 0.369 0.549 — 25.530
TimeNet 0.333 0.010 0.019 0.450 — 8.550

 
Maggu et al. EURASIP Journal on Advances in Signal Processing

(2020) 2020:26

Table 6 Stock-wise trading results (Continued)

Page 30 of 32

 

Computational model performance

Financial evaluation

 

 

Stock name Method
Precision Recall Fiscore AUC TrueAR Predicted AR
TATASTEEL DeConFuse 0.552 0.807 0.655 0.528 17.240 - 10.030
ConvTimeNet 0.562 0.613 0.586 0.551 — 22.210
TimeNet 0.518 0.958 0.673 0.474 — 13.700
Tcs DeConFuse 0.573 0.746 0.648 0.557 33.910 2.550
ConvTlimeNet 0.636 0.056 0.102 0.453 2.990
TimeNet 0.573 0.714 0.636 0.565 6.360
TECHM DeConFuse 0.555 0.835 0.667 0.480 49.080 29.110
ConvlimeNet 0.578 0.496 0.534 0.507 47.820
TimeNet 0.572 0.685 0.624 0.563 — 5.130
TITAN DeConFuse 0.562 0.744 0.641 0.562 18.960 4.930
ConvTimeNet — 0.000 0.000 0.000 0.560 0.000
TimeNet 0.528 0.628 0.574 0.477 24.260
TVSMOTOR DeConFuse 0.453 0.953 0.614 0.504 -4.950 — 11.570
ConvTimeNet 0.431 0.802 0.561 0.438 — 19.260
TimeNet 0.441 1.000 0.612 0.473 0.000
UBL DeConFuse 0.609 0.664 0.635 0.550 59.870 32.690
ConvTimeNet 1.000 0.049 0.094 0.568 17.720
TimeNet 0.600 0.221 0.323 0.515 47.160
ULTRACEMCO DeConFuse 0.569 0.757 3 0.649 0.585 26.850 — 0.390
ConvTlimeNet 0.556 0.522 0.538 0.581 — 16.410
TimeNet 0.500 0.991 0.665 0.542 0.860
UNIONBANK DeConFuse 0.497 0.689 0.577 0.553 - 4.350 30.000
ConvTimeNet 0.453 0.709 0.553 0.494 — 25.830
TimeNet 0.408 0.194 0.263 0.511 — 34.140
UPL DeConFuse 0.480 0.897 0.627 0.553 5.600 2450
ConvTimeNet 0.480 0.738 0.585 0.526 5.010
TimeNet 0.459 0.841 0.594 0.501 — 3.330
VEDL DeConFuse 0.475 0.864 0.613 0.599 3.610 —4.550
ConvTimeNet 0.433 0.636 0.515 0.570 — 28.800
TimeNet 0.360 0.136 0.198 0.491 — 18.810
VOLTAS DeConFuse 0.497 0.864 0.631 0.483 60.280 8.460
ConvTimeNet 1.000 0.009 0.018 0.541 4.970
TimeNet 0.480 1.000 0.649 0.528 0.000
WIPRO DeConFuse 0.503 0.780 0.612 0.535 - 11.360 3.560
ConvlimeNet 0.532 0.615 0.570 0.562 - 13.570
TimeNet 0.492 0.872 0.629 0.575 8.590
WOCKPHARMA ~— DeConFuse 0.516 0.899 0.656 0.559 -7.170 18.620
ConvlimeNet 0.523 0.517 0.520 0.515 59.110
TimeNet 0.487 0.865 0.623 0.524 - 2.650
YESBANK DeConFuse 0.522 0.828 0.640 0.565 0.050 3.180
ConvTimeNet 0.494 0.664 0.566 0.523 — 7.690
TimeNet 0.507 1.000 0.672 0.559 0.000
ZEEL DeConFuse 0.557 0.900 0.688 0.535 4.660 — 8.240
ConvTimeNet 0.607 0.375 0.464 0.622 — 6.830
TimeNet 0.527 0.900 0.667 0.569 12.100

 
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 31 of 32

Abbreviations

TL: Transform learning; CTL: Convolutional transform learning; CNN: Convolutional neural network; LSTM: Long short-term
memory; GRU: Gated recurrent unit; ReLU: Rectified linear unit; SELU: Scaled exponential linear units; NSE : National Stock
Exchange; AUC: Area under curve; ROC: Receiver operating characteristics; NAV: Net asset value; RDF: Random decision
forest; EEG: Electroencephalogram; ECG: Electrocardiogram; AR: Annualized returns; MAE: Mean absolute error

Authors’ contributions

Ms. Pooja Gupta has introduced the CTL within the fusion framework and performed all the numerical experiments. Ms.
Jyoti Maggu originally formulated the transform learning model and the deep version for it. Dr. Angshul Majumdar has
helped with the model formulation and the assessment of the experimental part. Dr. Emilie Chouzenoux and Dr.
Giovanni Chierchia have contributed in the formulation of the model and the optimization algorithms. All the authors
have contributed to the writing and proofreading of the paper. The authors read and approved the final manuscript.

Funding
This work was supported by the CNRS-CEFIPRA project under grant NextGenBP PRC2017.

Availability of data and materials
The dataset used is a real dataset of the Indian National Stock Exchange (NSE) of past 4 years and is publicly available. We
have shared the data with our implementation available at https://github.com/pooja290992/DeConFuse.git.

Ethics approval and consent to participate
Not applicable.

Consent for publication
Not applicable.

Competing interests
The authors declare that they have no competing interests.

Author details

"Indraprastha Institute of Information Technology, Delhi, India. 7TCS Research, Kolkata, India. >Université Paris-Saclay,
CentraleSupélec, Inria, CVN, Gif-sur-Yvette, France. “LIGM, Université Gustave Eiffel, CNRS, ESIEE Paris, Noisy-le-Grand,
France.

Received: 8 November 2019 Accepted: 28 April 2020
Published online: 29 May 2020

References

1. N.Kriegeskorte, Deep neural networks: a new framework for modeling biological vision and brain information
processing. Annual Rev. Vis. Sci. 1, 417-446 (2015)

2. R.W. Guillery, S.M. Sherman, Thalamic relay functions and their role in corticocortical communication: generalizations
from the visual system. Neuron. 33(2), 163-175 (2002)

3. J. Cudeiro, A. M. Sillito, Looking back: corticothalamic feedback and early visual processing. Trends Neurosci. 29(6),
298-306 (2006)

4. C.Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, in Proceedings of the
IEEE conference on computer vision and pattern recognition, Going deeper with convolutions, (2015), pp. 1-9. https://
doi.org/10.1109/cvpr.2015.7298594

5. |.Daubechies, R. DeVore, S. Foucart, B. Hanin, G. Petrova, Nonlinear approximation and (deep) ReLU networks. arXiv
preprint arXiv:1905.02199 (2019)

6. P. Petersen, F. Voigtlaender, Optimal approximation of piecewise smooth functions using deep ReLU neural
networks. Neural Netw. 108, 296-330 (2018)

7. F.Schroff, D. Kalenichenko, J. Philbin, in Proceedings of the IEEE conference on computer vision and pattern recognition,
FaceNet: a unified embedding for face recognition and clustering, (2015), pp. 815-823. https://doi.org/10.1109/cvpr.
2015.7298682

8. Y.Taigman, M. Yang, M. A. Ranzato, L. Wolf, in Proceedings of the IEEE conference on computer vision and pattern
recognition, DeepFace: close the gap to human-level performance in face verification, (2014), pp. 1701-1708. https://
doi.org/10.1109/cvpr.2014.220

9. S.Nagpal, M. Singh, R. Singh, M. Vatsa, A. Noore, A. Majumdar, in Proceedings of the IEEE International Conference on
Computer Vision, Face sketch matching via coupled deep transform learning, (2017), pp. 5419-5428. https://doi.org/
10.1109/iccv.2017.579

10. J. Maggu, E. Chouzenoux, G. Chierchia, A. Majumdar, in International Conference on Neural Information Processing,
Convolutional transform learning (Springer, Cham, 2018), pp. 162-174

11. H.1. Fawaz, G. Forestier, J. Weber, L. \doumghar, P. A. Muller, Deep learning for time series classification: a review.
Data Min. Knowl. Discov. 33(4), 917-963 (2019)

12. S.Hochreiter, J. Schmidhuber, Long short-term memory. Neural Comput. 9(8), 1735-1780 (1997)

13. J.Chung, C. Gulcehre, K. Cho, Y. Bengio, in International Conference on Machine Learning, Gated feedback recurrent
neural networks, (2015), pp. 2067-2075

14. Z. Wang, W. Yan, T. Oates, in 2017 international joint conference on neural networks (UCNN), Time series classification
from scratch with deep neural networks: a strong baseline (IEEE, 2017), pp. 1578-1585

15. P. Malhotra, V. TV, L. Vig, P. Agarwal, G. Shroff, TimeNet: pre-trained deep recurrent neural network for time series
classification. arXiv preprint arXiv:1706.08838 (2017)
Maggu et al. EURASIP Journal on Advances in Signal Processing (2020) 2020:26 Page 32 of 32

20.

21,

22.

23.

24,

25.

26.

2/.

28.

29.

30.

31.

32.
33.

34.

35.

36.

37.
38.

39.

40.

N. Hatami, Y. Gavet, J. Debayle, in Tenth International Conference on Machine Vision (ICMV 2017), vol 10696,
Classification of time-series images using deep convolutional neural networks (International Society for Optics and
Photonics, 2018), p. 1O6960Y

K. Kashiparekh, J. Narwariya, P. Malhotra, L. Vig, G. Shroff, ConvTimeNet: a pre-trained deep convolutional neural
network for time series classification. arXiv preprint arXiv:1904.12546 (2019)

Z. Wang, T. Oates, in Twenty-Fourth International Joint Conference on Artificial Intelligence, |maging time-series to
improve classification and imputation, (2015)

O.B. Sezer, A. M. Ozbayoglu, Algorithmic financial trading with deep convolutional neural networks: time series to
image conversion approach. Appl. Soft Comput. 70, 525-538 (2018)

A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, A. losifidis, in 2077 IEEE 19th Conference on Business
Informatics (CBI) vol. 1, Forecasting stock prices from the limit order book using convolutional neural networks (IEEE,
2017), pp. 7-12

M. U. Gudelek, S. A. Boluk, A. M. Ozbayoglu, in 2017 IEEE Symposium Series on Computational Intelligence (SSCI), A deep
learning based stock trading model with 2-D CNN trend detection (IEEE, 2017), pp. 1-8

S. Ravishankar, Y. Bresler, Sparsifying transform learning with efficient optimal updates and convergence guarantees.
IEEE Trans. Sig. Process. 63(9), 2389-2404 (2015)

P. L. Combettes, JC. Pesquet, in Fixed-Point Algorithms for Inverse Problems in Science and Engineering. Springer
Optimization and Its Applications, vol 49. ed. by H. Bauschke, R. Burachik, P. Combettes, V. Elser, D. Luke, and H.
Wolkowicz, Proximal splitting methods in signal processing (Springer, New York, 2011)

J. Yang, M. N. Nguyen, P. P. San, X. L. Li, S. Krishnaswamy, in Twenty-Fourth International Joint Conference on Artificial
Intelligence, Deep convolutional neural networks on multichannel time series for human activity recognition, (2015)

S. Yao, S. Hu, Y. Zhao, A. Zhang, T. Abdelzaher, in Proceedings of the 26th International Conference on World Wide Web,
DeepSense: a unified deep learning framework for time-series mobile sensing data processing, (2017), pp. 351-360

Y. Zheng, Q. Liu, E. Chen, Y. Ge, J. L. Zhao, in International Conference on Web-Age Information Management, Time
series classification using multi-channels deep convolutional neural networks (Springer, Cham, 2014), pp. 298-310

J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, A. Y. Ng, in Proceedings of the 28th international conference on machine
learning (ICML-11), Multimodal deep learning, (2011), pp. 689-696

C. Feichtenhofer, A. Pinz, A. Zisserman, in Proceedings of the IEEE conference on computer vision and pattern
recognition, Convolutional two-stream network fusion for video action recognition, (2016), pp. 1933-1941

A. Eitel, J. T. Soringenberg, L. Spinello, M. Riedmiller, W. Burgard, in 2015 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), Multimodal deep learning for robust RGB-D object recognition (IEEE, 2015), pp. 681-687

Y. Chen, C. Li, P. Ghamisi, X. Jia, Y. Gu, Deep fusion of remote sensing data for accurate classification. IEEE Geosci.
Remote Sens. Lett. 14(8), 1253-1257 (2017)

N. Antropova, B. Q. Huynh, M. L. Giger, A deep feature fusion methodology for breast cancer diagnosis
demonstrated on three imaging modality datasets. Med. Phys. 44(10), 5162-5171 (2017)

S. Ravishankar, Y. Bresler, Learning sparsifying transforms. IEEE Trans. Sig. Process. 61(5), 1072-1086 (201 2)

H. Attouch, J. Bolte, B. F. Svaiter, Convergence of descent methods for semi-algebraic and tame problems: proximal
algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Math. Program. 137, 91-129 (2011)

E. Chouzenoux, J. C. Pesquet, A. Repetti, A block coordinate variable metric forward-backward algorithm. J. Glob.
Optim. 66(3), 457-485 (2016)

J. Bolte, S. Sabach, M. Teboulle, Proximal alternating linearized minimization for nonconvex and non-smooth
problems. Math. Program. 146(1-2), 459-494 (2014)

P. L. Combettes, J.-C. Pesquet, Deep neural network structures solving variational inequalities. Set-valued variational
anal. (2018). https://arxiv.org/abs/1808.075 26

S. J. Reddi, S. Kale, S. Kumar, On the convergence of adam and beyond. Proc. ICLR (2018)

A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, A. Lerer, Automatic
differentiation in PyTorch. NIPS Autodiff Workshop (2017)

G. Klambauer, T. Unterthiner, A. Mayr, S. Hochreiter, Self-normalizing neural networks. Adv. Neural Inf. Process. Syst.
30, 971-980 (2017)

A. Criminisi, J. Shotton, E. Konukoglu, Decision forests: a unified framework for classification, regression, density
estimation, manifold learning and semi-supervised learning. Found. Trends Comput. Graph. Vis. 7(2-3), 81-227 (2012)

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
