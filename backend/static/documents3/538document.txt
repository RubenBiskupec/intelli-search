International Journal of Computer Vision (2020) 128:2962-2978
https://doi.org/10.1007/s11263-020-01352-9

®

Check for
updates

Rooted Spanning Superpixels
Dengfeng Chai'®

Received: 25 April 2018 / Accepted: 9 June 2020 / Published online: 20 July 2020
© The Author(s) 2020

Abstract

This paper proposes a new approach for superpixel segmentation. It is formulated as finding a rooted spanning forest of a
graph with respect to some roots and a path-cost function. The underlying graph represents an image, the roots serve as seeds
for segmentation, each pixel is connected to one seed via a path, the path-cost function measures both the color similarity and
spatial closeness between two pixels via a path, and each tree in the spanning forest represents one superpixel. Originating
from the evenly distributed seeds, the superpixels are guided by a path-cost function to grow uniformly and adaptively, the
pixel-by-pixel growing continues until they cover the whole image. The number of superpixels is controlled by the number
of seeds. The connectivity is maintained by region growing. Good performances are assured by connecting each pixel to the
similar seed, which are dominated by the path-cost function. It is evaluated by both the superpixel benchmark and supervoxel
benchmark. Its performance is ranked as the second among top performing state-of-the-art methods. Moreover, it is much

faster than the other superpixel and supervoxel methods.

Keywords Superpixels - Segmentation - Spanning forest

1 Introduction

Superpixels have become effective alternative to pixels in
the past decade. They result from image oversegmentation,
which is dedicated to reducing image complexity while
avoiding undersegmentation (Ren and Malik 2003). An
image is oversegmented into many perceptually meaning-
ful segments such that each segment covers a local region
consisting of some connected similar pixels, and each seg-
ment is called as a superpixel. Superpixels have two prime
advantages over pixels. One advantage is the perceptual
meaning. In contrast with raw pixels generated by digi-
tal sampling, superpixels are formed by pixel grouping,

Communicated by Yuri Boykov.

This work was supported by the National Natural Science Foundation
of China (No. 41571335).

The source code for RSS algorithm can be found at https://github.
com/dfchai/Rooted-Spanning-Superpixels.

CX! Dengfeng Chai
chaidf@zju.edu.cn

Key Laboratory of Geoscience Big Data and Deep Resource
of Zhejiang Province, School of Earth Sciences, Zhejiang
University, No. 38, Zheda Road, Hangzhou 310027,
Zhejiang, China

Q) Springer

whose principles are based on the classical Gestalt theory
(Wertheimer 1938) assuring superpixels enhanced percep-
tual meaning. This characteristic facilitates defining higher
order potentials, high order conditional random fields and
associative hierarchical random fields (Arnab et al. 2016).
The other advantage is the complexity. Since many pixels
are grouped into one superpixel, the number of superpixels is
much smaller than that of pixels. When superpixels instead
of pixels serve as atoms, the size of an image is reduced
greatly. The size reduction can accelerate the processing
in subsequent tasks, and in turn, it is possible to employ
some advanced methods which might be computationally
infeasible for the huge number of pixels. For example, com-
pared with pixel-based convolutional neural network (CNN),
superpixel-based CNN (SuperCNN) enables efficient analy-
sis of large context (He et al. 2015). Moreover, superpixels
can be further grouped to generate some object proposals
(Uijlings et al. 2013). It can dramatically reduce the number
of possible candidates to be checked in object detection. For
example, both R-CNN (Girshick et al. 2014) and Fast RFCNN
(Girshick 2015) benefit from such reduction of candidate
numbers. Such advantages lead to successful applications in
many vision problems covering image segmentation (Boix
et al. 2012; Liu et al. 2018), video segmentation (Tsai et al.
2016), semantic segmentation (Farabet et al. 2012; Mostajabi
International Journal of Computer Vision (2020) 128:2962-2978

et al. 2015; Gadde et al. 2016), stereo computation (MicuSik
and KoSecka 2010; Guney and Geiger 2015), object tracking
(Wang et al. 2011), objectness measuring (Alexe et al. 2012),
object proposal generation (Hosang et al. 2015), etc.
Although superpixel segmentation is application orien-
tated in some way, some general characteristics are expected:

1. locality a superpixel covers a local region;

2. coherency a superpixel is composed of similar pixels;

3. connectivity a superpixel is composed of connected pix-
els;

4. compactness a superpixel is compact in absence of edges,
it is expected to be square in an area of constant color;

5. adherence superpixel boundaries adhere well to object
boundaries;

6. uniformity superpixels are homogeneous in sizes and
shapes;

7. adaptivity compactness and adherence are maintained
adaptively;

8. efficiency segmentation should be computationally and
memory efficient.

9. scalability supervoxel segmentation can be achieved in
the same way;

These characteristics follow principles of perceptual
grouping and support general applications. They are the
criteria for developing superpixel segmentation methods.
Although there are many approaches available for use, none
of them satisfy all the aforementioned characteristics. For
example, Watersheds generate superpixels of irregular sizes
and shapes (Vincent and Soille 1991), which conflict with
the compactness and uniformity.

1.1 Existing Superpixel Methods

Three types of formulations can be distinguished in the exist-
ing literature: graph partitioning, boundary evolution and
feature space analysis.

Graph partitioning is the most common formulation for
superpixel boundary determination. A vertex denotes a pixel,
an edge links two neighboring pixels, and all the vertices and
edges constitute a graph representing an image. Superpixel
segmentation is achieved by partitioning the graph into a set
of connected subgraphs, each of which denotes one super-
pixel. The pioneering work is based on Normalized Cut (NC)
(Ren and Malik 2003). It measures both the total dissimi-
larity between the different subgraphs as well as the total
similarity within the subgraphs by a criterion based on nor-
malized cut, and solves a generalized eigenvalue system to
find the optimal cut partitioning the graph (Shi and Malik
2000). It produces uniform, compact and coherent superpix-
els. However, its computational and memory requirements
are high and its boundary adherence is relatively poor. Some

2963

methods such as Superpixel Lattice (SL) (Moore et al.
2008), Lattice Cut (LC) (Moore et al. 2010) Superpixels via
Pseudo-Boolean Optimization (SPBO) and Superpixels via
Quaternary Labeling (SQL) (Chai 2019) find both horizontal
and vertical boundaries to produce a regular grid of super-
pixels. However, the segmentation performance is sacrificed
by these constraints. Compact Superpixels (CS), Variable
Patch Superpixels (VPS), and Constant Intensity Superpixels
(CIS) work similarly but generate superpixels without lattice
structure (Veksler et al. 2010). These approaches optimize
an objective function to find the graph cuts. The objective
function consists of a data term and a smooth term. The data
term favors coherent superpixels and the smooth term encour-
ages the boundaries to align with intensity edges. Their
computation costs are high and their performances are not
good. Entropy Rate Superpixels (ERS) result from greedy
optimization of an objective function consisting of entropy
rate of random walk on graph and a balancing term (Liu
et al. 2011). The entropy rate favors compact and homoge-
neous superpixels, while the balancing function encourages
uniform superpixels. It achieves good segmentation perfor-
mance with a relatively high computational cost. Minimum
spanning tree is an alternative to graph cut. Graph Based
Superpixels (GBS) (Felzenszwalb and Huttenlocher 2004)
performs an agglomerative clustering such that each clus-
ter is the minimum spanning tree of the constituent nodes.
The generated superpixels adhere well to image boundaries.
The clustering is very fast. However, the size and number of
superpixels cannot be controlled explicitly, and the shapes
of superpixels are arbitrary. The locality, compactness and
uniformity are poor.

Boundary evolution is an alternative to graph partition-
ing for boundary determination. Two ways of evolution have
been developed for superpixel segmentation. One is growing
superpixels from some given centers, the other is adjusting
some given boundaries. Turbopixel (TP) is a representative
of the first category (Levinshtein et al. 2009). Originating
from the given centers, Turbopixels grow with their bound-
aries evolving step by step. Boundary evolution is driven by
geometric flow calculated from grown regions and the rest
regions. In this framework, geodesic distance is introduced
as a measure of structure and layout of superpixels, and cen-
ters are relocated to generate structure-sensitive superpixels
(Wang et al. 2013). The main drawback of these methods
is very high computational cost, restricting practical appli-
cations. Superpixel Extracted via Energy-Driven Sampling
(SEEDS) falls into the second category (Van den Bergh et al.
2015). Starting from an initial partitioning, it adjusts the
boundaries by exchanging pixels or blocks of neighboring
superpixels. An objective function based on superpixel colors
and boundary shapes is defined to favor coherent superpixels.
A simple hill-climbing approach is employed to optimize the
objective function efficiently. However, it may not converge

Q) Springer
2964

 

 

Fig.1 Rooted spanning superpixels. Starting from the seed pixels, the
superpixels grow up pixel by pixel until they cover the whole image. The
initial state, intermediate states, and final state are shown respectively

to optimal segmentation in some cases, and it lacks adaptiv-
ity between compactness and adherence. By constructing an
objective function based on boundary and topology preserv-
ing Markov Random Fields, Efficient Topology Preserving
Segmentation (ETPS) achieves excellent performance (Yao
et al. 2015). But it is not fast enough to support realtime
applications.

Feature space analysis is another formulation for super-
pixel segmentation. Working in feature space, it determines a
superpixel by finding its pixels instead of its boundary. There
are two ways to achieve this goal. One is mode seeking, and
the other is k-means clustering. Mean Shift (MS) and Quick
Shift (QS) are two techniques for searching modes of a den-
sity function (Comaniciu and Meer 2002; Vedaldi and Soatto
2008). Once modes in the image feature space are found,
pixels converging to same mode constitute one superpixel.
They are slow. The size and number of superpixels cannot be
controlled explicitly and the shapes of superpixels are usu-
ally irregular. Simple Linear Iterative Clustering (SLIC) is a
constrained k-means clustering (Achanta et al. 2012), which
searchs pixels in a limited region instead of the whole image
to generate compact superpixels efficiently. VCell is also
built upon k-means clustering (Wang and Wang 2012). Edge-
Weighted Centroidal Voronoi Tessellations is developed to
constrain superpixel boundary. The weak point of k-means
clustering is no guarantee of connectivity. SLIC relies on
post-processing to repair connectivity whereas VCell relies
on two special mechanisms to maintain connectivity. As
an improved SLIC, Simple Non-Iterative Clustering (SNIC)
updates clusters by incorporating pixels connected to the
clusters (Achanta and Siisstrunk 2017). It is faster than SLIC
since no iteration is involved. The k-means clustering is
based on the color difference and spatial distance between
pixels and cluster centers, which facilitates shape regular-
ization. However, only mean value of the superpixels are
referred in clustering, more cues can be utilized to improve
performance.

Q) Springer

Ba" BA
H- 8s

International Journal of Computer Vision (2020) 128:2962-2978

  

 

from left to right. Their boundaries and constituting pixels are depicted
as green lines and colored regions in the top and bottom images respec-
tively (Color figure online)

 

Superpixel has been extended to supervoxel, which is a
three-dimensional version and has many applications in vol-
umetric image segmentation (Lucchi et al. 2012) and video
preprocessing (Xu and Corso 2012). Some methods treat
video as a series of images, and segment them frame by
frame. The others treat video as volumetric image. An evalua-
tion of supervoxel methods for video processing is presented
in Xu and Corso (2012), where, Mean Shift (MS) (Comani-
ciu and Meer 2002), Graph Based (GB) (Felzenszwalb
and Huttenlocher 2004), Hierarchical Graph Based (GBH)
(Grundmann et al. 2010), and NC (Shi and Malik 2000), Seg-
mentation by Weighted Aggregation (SWA) (Sharon et al.
2000) and Temporal Superpixels (TSP) (Chang et al. 2013)
have been evaluated and compared.

1.2 Motivation and Contribution

The above formulations have both positive and negatives. For
examples, GBS is fast but does not allow number and shape
controlling, k-means clustering based methods allow such
controlling but needs advanced measures for the differences
between pixels and superpixel centers.

This paper adapts minimum spanning tree and proposes
a formulation similar to image foresting transform (Falcao
et al. 2004) to integrate the positives of different formulations.
First, it adapts the underlying graph to represent vertex values
instead of edge weights. Second, it selects some vertices to
serve as roots of trees in spanning forest. Third, it introduces
a path-cost function to measure both color similarity and
spatial closeness between the seeds and the remaining pixels.
Fourth, superpixel segmentation is formulated as searching a
rooted spanning forest of the underlying graph with respect to
the roots and path-cost function. Finally, a set of first-in-first-
out (FIFO) queues are introduced to maintain the candidates
to search the forest efficiently.

Based on this formulation, superpixel segmentation is
achieved via region growing as depicted in Fig. |. Starting
International Journal of Computer Vision (2020) 128:2962-2978

ttt His
Hy tite
(a) (b) (d)

Fig. 2 Minimum spanning forest and rooted spanning forest. Vertices
and edges are illustrated as circles and lines. Edge weights are indi-
cated by their thickness, and vertex values are indicated by their colors.
b Minimum spanning forest of the disconnected graph in a, d is a mini-

 

from the evenly distributed seeds, the superpixels are guided
by a path-cost function to grow uniformly in homogeneous
regions and adaptively when they touch object boundaries,
the superpixels grow pixel by pixel until they cover the whole
image. The number of superpixels is controlled by the num-
ber of seeds. Good performances are assured by connecting
each pixel to the similar seed, which are dominated by the
path-cost function. Benefiting from the FIFO queues, super-
pixels grow efficiently and the proposed algorithm is much
faster than the other superpixel and supervoxel methods.

The rest of this paper is organized as follows: background
and adaptation of rooted spanning forest are presented in
Sect. 2, rooted spanning superpixels are formulated in Sect. 3,
experiments with evaluations and comparisons are presented
in Sect. 4, and conclusions are drawn in Sect. 5.

2 Rooted Spanning Forest
2.1 Minimum Spanning Forest

Our formulation is based on an undirected graph, which is
called as a graph for simplicity. A graph is an ordered pair
G = (Vg, Eg), where Vg is a nonempty set and E is a set
of unordered pairs of elements of Vg, i.e. Eg C VG x Va.
Elements of Vg and Eg are called the vertices and the edges
respectively. Each edge e = (s,t) € Eg links two vertices
s € Vg andt € Vg. If two or more edges link the same
two vertices, the edges are called multiple edges. If an edge
links a vertex with itself, the edge is called a Joop. A graph is
simple if it has no loops and no multiple edges. A weighted
graph associates a weight with every edge in the graph.

A simple path in a graph G(VgG, EG) is an alternating
sequence of distinct vertices and edges m = vje)U2e2...
Up—1€p—1Up, where e; = (vj, vi41) € Eg,i =1,..., p—1.
v, and vp are the origin and the destination of the path, and
they are denoted as vj = org(z) and vp = dst(z) respec-
tively. When p = 1, the path is called a trivial path. The
weight of a path is the sum of the weights of the traversed
edges. A graph is connected if any two vertices are connected
by one or more paths. If any two vertices are connected by

by de
oY
(h)

  

(f)

mum spanning forest of the connected graph in ¢, fis a rooted spanning
tree of the connected subgraph (with green vertices) in e, h is a rooted
spanning forest of the connected graph in g, the roots are illustrated as
filled circles (Color figure online)

exactly one path, the graph 1s called a tree. The weight of a
tree is the sum of the weights of all its edges. A forest is a
disjoint union of trees.

A spanning tree of a connected graph is a tree that con-
nects all the vertices and each edge of the tree is an edge of
the underlying graph. One graph usually has many different
spanning trees. A minimum spanning tree of a weighted con-
nected graph is a spanning tree whose weight is no more than
the weight of every other spanning tree. A minimum span-
ning forest of a weighted disconnected graph is a union of
minimum spanning trees for its connected components. As
demonstrated in Fig. 2b, the minimum spanning forest con-
sists of three trees; each is a minimum spanning tree of one
connected component of the disconnected graph in Fig. 2a.

Minimum spanning forest has been applied to image seg-
mentation (Felzenszwalb and Huttenlocher 2004). However,
the graph representing an image is a connected graph as in
Fig. 2c rather than a disconnected one as in Fig. 2a. It is par-
titioned into some components by removing all edges whose
weights are above a weight threshold. Fig. 2d is a minimum
spanning forest of the graph in Fig. 2c based on a weight
threshold indicated by the thick edges. This approach deals
with the edge weights derived from the pixel values. These
derived values may introduce some extra errors in segmenta-
tion. Moreover, the number of segments and their coverages
are not controlled explicitly but depend on the thresholds
implicitly.

2.2 Rooted Spanning Forest

This paper extends minimum spanning tree and forest to
rooted spanning tree and forest. The underlying graph is
adapted to represent the pixel colors instead of image edges.
Some roots are introduced to control the number and shape
of superpixels.

An intuitive picture of the above concepts is depicted in
Fig. 2, where Fig. 2f, h are a rooted spanning tree and forest
of the underlying graph Fig. 2e, g respectively. As indicated
by the same thickness, no weights are assigned to the edges.
Graph partitioning is based on the vertex values (colors).

Q) Springer
2966
2.2.1 Rooted Spanning Tree

Letr € Vg be aroot, f(*) be a path-cost function, 1.e., f (77)
is the cost of a path z. A rooted spanning tree of a graph G
with respect tor, f(*) 1s a tree T such that:

— V(T) = Ve, E(T) € Ee;

— ris the root of T;

— any root-originated path z in T meets one of the two
conditions:

l.w=r;
2. m = tev subject to f(tev) < f(t’e’v) for any t’ € 7.

Where tev and t’e’v are one-edge-extensions of path t and
path t’ respectively, [7 is the set of existing root-originated
paths.

2.2.2 Rooted Spanning Forest

Let R = {rj|r; € Vg,i = 1,2,..., K} be a set of roots,
f (x) be a path-cost function. A rooted spanning forest F of
a graph G with respect to R and f(*) is an union of trees
F =T; UT) U---UTx such that:

— V(T;) C Vg, E(7;) © Eg fori=1,..., K;
— V(T\) U---UV(Tx) = Vg, 7 AT; = 9 fori 4 Jj;

— r; 18 the root of 7; fori = 1,..., K;

— any root-originated path z in F meets one of the two
conditions:

l. w=r;

2. m = tev subject to f(tev) < f(t’e’v) for any t’ € 7.

where tev and t’e’v are one-edge-extensions of path t and
path t’ respectively, [7 is the set of existing root-originated
paths.

Rooted spanning tree is extended to rooted spanning forest
by injecting multiple roots instead of a single root.

3 Rooted Spanning Superpixels

This paper formulates superpixel segmentation as finding a
rooted spanning forest F of a graph G representing an image
I with respect to a set of roots R and a path-cost function
f(x). Each tree 7; C F represents one rooted spanning
superpixel (RSS). G can represent a volumetric image or a
video, then each tree represents one rooted spanning super-
voxel,

Q) Springer

International Journal of Computer Vision (2020) 128:2962-2978
3.1 Implicit Graph

A graph G(V, E) is employed to represent an image /(p).
Each vertex vp € V represents a pixel p, and each edge
€n,q = (Up, Ug) € E links a pair of neighboring pixels p
and q. It is not necessary to explicitly store the edges since
they are recoverable based on a given neighborhood system.
Therefore, no explicit graph is constructed and the image
is dealt with directly. Each pixel is treated as a vertex and
it is linked to its 8 nearest neighbors based on the second-
order neighborhood system employed in this paper. When the
underlying graph represents a volumetric data, each vertex
represents one voxel and it has 26 neighbors.

3.2 Roots Selection

A set of seed pixels are selected to serve as the roots of span-
ning forest to control the number and locality of superpixels.
They are selected regularly and evenly on the image plane
as shown by the initial state in Fig. 1. Given the expected
number of superpixels K, the expected width of a superpixel
is w = ./N/K, where N is the total number of pixels. Seeds
are selected by sampling the rows and columns with an inter-
val of w. A schema similar to that of SLIC can be adopted
to adjust the seeds within their 3 x 3 windows, however, no
performance gains are found in our experiments.

3.3 Cost Function

A path-cost function f (*) is developed to measure the color
similarity and spatial closeness between a seed pixel and
another pixel through a path. A general function for a path
WT = Vjej{v2... Up—1€p—-1Up 1S

f(w) = f(,..., Up), (1)

where the edges are excluded from the variable list since they
have no weights as declared in Sect. 2.2.
An existing path-cost function is the geodesic distance

P P
f8r) = YW) — L@i—-Dll2 #4 YS vr-svilla,

i=2 i=2

where ||/(v;) — [(v;—-1)||2 1s the L2 norm of color differ-
ence between two successive pixels, and ||vj;—1 0; ||2 is their
Euclidean distance. However, this color term fails to mea-
sure similarity between two ends of a path shown in Fig. 3a.
The alternating values amount to a large geodesic distance
for two ends of the same value.

As illustrated in Fig. 3b, this paper utilizes global char-
acteristics of a path instead of sum of local ones to measure
color similarity of path ends, and proposes two novel path-
cost functions.
International Journal of Computer Vision (2020) 128:2962-2978

Without loss of generality, assume that the image has a
single channel. Let /(v) be the value of v, the maximal differ-
ence between the origin and rest pixels on path is calculated
as

f(r) = max [I (v1) — 1(v)I, (3)
I<i<p

and the range of values of all pixels on path is calculated as

f' (a) = max I[(v;) — min [(v;). (4)
l<i<p I<i<p

The maximal difference acts as a barrier between the ori-
gin and destination. It reflects the cost of going from origin
to destination along the path. Similarly, the range of values
also reflects such cost. They are more robust than geodesic
distance since they avoid summing local derivatives. More-
over, they outperform a simple difference between origin and
destination, which does not take the intermediate pixels into
account.

For images of multiple channels, the pixel value is a vec-
tor, the differences in Eqs. 3 and 4 are replaced by their Lg
norms. Since only comparisons are involved, it is quite effi-
cient. Moreover, by experiments, it is found to outperform
alternative norms such as L2 norm, which need floating-point
operations.

Spatial closeness is measured in the same way by treating
pixel coordinates as extra channels:

Iy+i(v) = 24-x(v)
Iy42(v) =A-y(v) , (5)
Im43(v) = d'- z(v)

where x(v), y(v), z(v) are three coordinates of v, Jjy+1, [+2,

Iyy+3 are three additional channels, 4 and i’ are two scaling
factors. [jy+3(v) is employed only in supervoxel segmenta-
tion. Usually, the third dimension has different meaning (e.g.
time in video), and it needs a different scaling factor i’.

For each pixel, its color value is in {0, 1, ..., 255}, but its
coordinates can be large when the size of an image is very
large. By normalizing pixel coordinates using the expected
width of a superpixel w = ./N/K, Eq. 5 is written as:

Imai) =A-x(v) =Aw-x(v)/w =A-X(v)
Imy2(v) =-yv) =Aw-yvr)/w =A-S) , ©
I43(v) =X! + z(v) = Mw! + e(v)/w! = 4! - (0)
where i, A’ are two normalized scaling factors, x(v), y(v),
Z(v) are three normalized coordinates of v.
The unified cost functions for maximal difference and
range of values are

Foor) = max (v1) ~ 1(2) loo: (7)

2967
foo() = || max I(v;) — min I(v;)|loo. (8)
l<i<p l<i<p

The cost is calculated efficiently by comparisons and sub-
tractions. When the path z extend to a new pixel vp+1, its
maximal difference cost is calculated incrementally as

ax |[Z(v1) — L(v,)lloo,

d ( /
zr)= m
Too l<i<p+l

= max (max IZ (v1) — La) loo, IF) — Hepa) Is]

= max (f(r), II) = Hep )lloe) (9)

Similarly, the range of values can also be computed incre-
mentally. The costs of all root-originated paths can be
computed very efficiently based on the incremental comput-
ing.

3.4 Global Objective Function

The global objective function is defined as the sum of path-
costs for all the vertices (pixels):

YS) f(r(vp)). (10)

UpEV

where z(vp) is a path connecting one root and vp, f(z)
takes either fh (z) or f3,(7). Unlike the variables in the
existing objective functions for superpixel segmentation, the
variables are paths. Therefore, a path connecting to a root
denoting a superpixel need to be determined for each pixel.
Based on the inductive definition in Sect. 2.2.2, the paths for
all pixels must be determined progressively. In each step, the
existing root-originated paths are extended one step to reach
a new pixel up by:

(Up) = T*eUp (11)
ee 12
T= argmin f(tevp), (12)

where Tevp is an one-edge-extension of T to vp, IT is the set
of existing root-originated paths, t € /7 means that tT is an
existing root-originated path.

The root-originated paths for a graph with two roots
illustrated in Fig. 3c and a path-cost function in Eq. 4 are
found as follows: First, JT = {v,, v2} as they are triv-
ial. Second, since f’(vjeyv2) = 1 < 2 = f'(v3e202),
w(v2) = vyeyv2, TT = {vy, v2, vyeyv2}. Third, w(v5) =
arg MINy ey vre6u5 f (V1E1V2E6U5) = Ve, V2e6Us, then JT =
{v1, V2, Vje, V2e6Us}. Fourth, 1 (V4)= arg MINy e504
f(vje5v4) = vyes5va, then JT is updated to be JT =
{v2, Vje]U2e6U5, Vie5v4}. Finally, w(v6) = arg MiNy3¢7y,
f (v3e7U6) = 03e7V6.

Q) Springer
2968

(a)

Fig. 3 Path-cost function. a Depicts a path with alternating values, b
two path-cost functions, where, d is the maximal difference between
origin and the remaining pixels on path, and r is the range of values of

Without the requirement t € J7, a path tT = v3e2uU2 € IT
can be extended to vs such that f(v3e2U2e6U5) = 3 <4 =
f (vj e1v2e6 U5). In this case, v3e2v2e6Us5 has minimal cost but
v3€2U2 1S not a root-originated path. With this contradiction,
the paths with optimal costs do not form a forest at all. The
algorithms in Falcao et al. (2004) cannot deal with this con-
tradiction. Instead, they accept only monotonic-incremental
and smooth path-cost functions. By enforcing t € JT, the
root-originated paths are extended step by step, and the above
contradiction is resolved.

3.5 Rooted Spanning Superpixel Algorithm

The inductive definition of root-originated paths allows the
rooted spanning forest to be found progressively in a manner
as Dijkstra’s algorithm (1959). In each step, it needs to sort
all candidates to select the best one as Eq. 12. Although a
balanced queue can be employed to store the candidates,
the total time complexity of Dijkstra’s algorithm is O(m +
n logn), where m and n are the numbers of edges and vertices
respectively.

The proposed solution is motivated by counting sort and
bucket sort (Cormen et al. 2009). First, it divides the cost
range into a set of equal-sized intervals (buckets) as bucket
sort does. Second, it assigns an integer to each interval as
required by counting sort. When only colors are consid-
ered, the proposed path-cost functions directly take integers
in {0,1,...,255}. When spatial coordinates are consid-
ered, real intervals need to be quantized into integers as the
path-cost functions may take nonnegative real values. This
quantization has little influence on segmentation.

A FIFO queue w, is employed for a bucket to store the
candidates whose costs are y. The candidates in one queue
are served by FIFO schema, which resolves the sorting oper-
ation of bucket sort. The candidates in a queue w,, are served
before those in wy, when yj < yp. Since one and only
one FIFO queue is employed for each bucket (integer), the
order of queues is fixed. Such fixed order resolves the sort-
ing operation of counting sort. Based on these queues, the

Q) Springer

 

(b)

International Journal of Computer Vision (2020) 128:2962-2978

 

the pixels on path. c Illustrates a small graph, in which the vertex values
are shown in the circles and the selected roots are gray vertices

Algorithm 1 Rooted Spanning Superpixels Algorithm

Require: /, f(*), g(*), R= {r1,72,...

Ensure: L;

1: set label L(*) to zero for all pixels; {labels of pixels}

2: set state 6(+) to true for all pixels; {true/false: unlabelled/labelled}

3: set cost y(*) to maximal value for all pixels; {path-costs of labeled
pixels }

4: set cost @(*) to maximal value for all pixels; {path-costs of candi-
dates }

5: empty queue w,., for all costs and groups; {store the candidates }

6: for k = 1 to K do

7

8

TKS;

x <— g(rx); {get its group index}
pushback(w9_x, 7%); {push it into a corresponding queue}
9: Lrg) <—k; {assign a label to it}
10: wW(rg) <— 0; {record its path-cost}
11: (rg) <0; {record its path-cost}
12: end for
13: for y =Oto Y do
14: for x =Oto X do
15: while w, » 4% do

16: p < popfront (wy); {pop the first pixel in the queue}

17: if 5(p) then

18: 6(p) <— false; {mark as labeled}

19: k <— L(p); {get its label}

20: for all i = 0 to 8 do

21: q <neig(p, 1); {get its i-th neighbor}

22: t — max(w(p), ||/(¢) —1 (rz) |]o0); {calculate its cost}

23: if 5(q) and ¢(q) > t then

24: L(q) <k; {propagate the label to it}

25: w(q) <t; {update its path-cost}

26: o(q) <t; {update its path-cost}

27: pushback(@¢(g),x, 7); {push it into a corresponding
queue }

28: end if

29: end for

30: end if

31: end while

32: end for

33: end for

34: return L.

root-originated paths extend very efficiently since no sort-
ing operation is needed. Since candidates with lower costs
are served before those with higher costs, the existing root-
originated paths always extend to their most similar pixels.
International Journal of Computer Vision (2020) 128:2962-2978

  

 

0 1 xX

Fig.4 Path extending based on groups of queues. wy,, 1s a queue for
cost y and group x. The one-step-extension of paths in different groups
are searched simultaneously. However, they are synchronized by the
path-cost indicated by the dashed lines. Only after all paths with same
cost are found, can it start to search paths with higher costs

In other words, all pixels are connected to their most similar
seeds.

Multiple paths originated from different roots can extend
simultaneously. Without loss of generality, let the seeds be
partitioned into some groups indexed by 0,1,..., X and
the costs be quantized into 0,1,..., Y. Then, the number
of seeds in x-th group is denoted as K,, the queue for cost y
and xth group is denoted as w,_,. Path extending in the same
group is carried out in serial while those in different groups
can be carried out in parallel. As shown in Fig. 4, the simul-
taneous extending is synchronized by the path-cost. Such
synchronization guarantees fair competition among differ-
ent groups and assures coherent superpixels.

The seeds can be put into either one group or K groups.
The former results in a serial algorithm. The latter requires
a huge memory for the K « Y queues and is not useful in
practice. Since 4 or 8 CPU cores are usually available in a
personal computer, it is natural to partition the seeds into
4 or 8 groups, each being processed by one CPU core. A
simple way is to group the seeds according to the 4 quadrants
separating along central row and central column of the image.
The key to achieve maximum speedup is the balanced groups
in terms of the number of seeds.

Algorithm 1 describes the Rooted Spanning Superpixels
(RSS) Algorithm based on the maximal difference. In this
algorithm, line 1—5 initialize all variables, loop 6—12 labels
the seed pixels, loop 13—33 propagates the labels to all the
remaining pixels. It seems that the complexity depends on X
and Y, however, many queues are usually empty. Actually,
every pixel is labeled once and only once. It means that the
complexity is O(N), where WN is the total number of pixels.
Therefore, the computation time is stable with respect to X,
Y and K. The inner loop 14—32 can be carried out either
in serial or in parallel. To adopt the range of values, it is
necessary to record the maximum/minimum of each channel

2969

in w(x) and revise line 22 correspondingly. For supervoxels
segmentation, line 20 need to loop over all 26 neighbors.

As in Fig. 1, different labels indicated by different col-
ors are assigned to different seeds, they propagate pixel by
pixel as the root-originated paths extend step by step, until
all pixels are labeled. In a homogeneous region, all candi-
dates have the same color as seeds and have a zero cost. The
path extending and superpixel growing depends on the order
of candidates pushed into the queue. It assures superpixels
the uniform growing in homogeneous regions since the pix-
els close to seeds are served before the pixels far away. In
a non-homogeneous region, they have different colors and
costs. Pixels similar to seeds have lower costs and receive
a priority. Therefore, they grow adaptively after they touch
object boundaries. When the superpixel growing is paral-
lelized, the order of service for candidates of the same cost
may change. However, the final superpixels are influenced
only by the boundary pixels in two narrow strips along the
central row and central column respectively. Only a few of
these pixels have the same cost with respect to neighboring
seeds in different groups. Therefore, parallelization has little
influence on the final superpixels.

RSS is scalable as it is capable to deal with different kinds
of data in a unified way. On one hand, each pixel can take a
gray value, a RGB vector, a RGBD vector, or even a vector of
features. On the other hand, the data can be a 2-dimensional
image, a 3-dimensional volumetric data, or a video.

4 Experimental Results

This section analyzes the effects of path-cost functions
and scaling factor, and then compares RSS algorithm with
five state-of-the-art superpixel methods using the superpixel
benchmark! (Stutz et al. 2018), which employs the Berkeley
Segmentation Dataset 500 (BSDS) (Arbelaez et al. 2011),
Fashionista dataset (Fash) (Yamaguchi et al. 2012), NYU
Depth Dataset V2 (NYU) (Silberman et al. 2012), Stan-
ford Background Dataset (SBD) (Gould et al. 2009) and Sun
RGB-D dataset (SUN) (Song et al. 2015). All codes for the
methods run on a laptop with Intel Xeon(R) CPU E3-1575M
@ 3.00 GHz x8 to segment the images.

Boundary Recall (Rec), Undersegmentation Error (UE),
Explained Variation (EV) and Compactness (CO) are
employed to measure segmentation performance. Rec mea-
sures superpixels’ adherence to boundary by counting the
coincidence between superpixel boundary and ground truth
boundary. UE measures segmentation accuracy by cal-
culating the fraction of superpixels leaking ground truth
boundary. EV measures superpixels’ coherence by com-
puting the proportion of image variation that is explained

' https://github.com/davidstutz/superpixel- benchmark.

Q) Springer
2970

when superpixels are compressed as units of representa-
tion. CO measures the compactness of superpixels. Average
Miss Rate (AMR), Average Undersegmentation Error (AUE)
and Average Unexplained Variation (AUV) are calculated
by averaging 1-Rec, UE and 1-EV over a set of K €
[Kmin, Kmax], and they are employed to summarize the algo-
rithm performance. Algorithm ranking is based on the sum
of AMR and AUE. Please refer Stutz et al. (2018) for the
details.

RSS algorithm is also compared with state-of-the-art
supervoxel methods using the supervoxel benchmark” (Xu
and Corso 2016). The supervoxels generated by the top per-
forming methods are available in the benchmark, they are
employed in the comparison directly. To demonstrate its scal-
ability, RSS treats a video as 3-dimensional volumetric data
and segments it as a whole instead of frame by frame.

Boundary Recall Distance (BRD), 3D Undersegmentation
Error (UE3D), Explained Variation (EV), 3D Segmentation
Accuracy (SA3D), Mean Size Variation (MSV) and Tempo-
ral Extent (TEX) are employed to evaluate the segmentation
performances. The former three correspond to Rec, UE and
EV, but BRD and UE3D are based on different versions.
SA3D indicates the achievable segmentation accuracy and
it is correlated to UE. TEX measures the average temporal
extent of supervoxels. MSV measures the size variation of
supervoxels along temporal axes. Please refer Xu and Corso
(2016) for the details.

The difference between superpixels by serial RSS and par-
allel RSS is ignorable, and their performance curves overlap
with each other. Serial RSS is employed for comparison as
the other methods are not parallel.

4.1 Path-Cost Function

Different path-cost functions can be adopted to define root-
originated paths and to generate RSS. This experiment
compares five cost functions: f* (+) (Eq. 2), fh (x) (Eq. 7),
f5.(*) (Eq. 8), fE(«) and f; (*), where, FE («), fy (*) are
based on Lo instead of Log norm.

The bare comparison is based on pixel colors but not
pixel coordinates since the latter depend on a scaling factor 2
whose effects will be analyzed in the following section. The
overall performances on BSDS dataset are reported in Table
1. The geodesic distance is outperformed by the other four
functions as lower AMR, AUE and AUV are better. Since the
geodesic distance is a sum of derivatives, itis sensitive to local
variations and noises. The other four functions are based on
global characteristics, they overcome this shortcoming and
significantly improve the performance.

One can observe that f d(x) leads to lower AMR, lower
AUV and higher AUE than f’(«) does. It means that bet-

> http://www.cs.rochester.edu/~cxu22/d/libsvx/.

Q) Springer

International Journal of Computer Vision (2020) 128:2962-2978

Table 1 Overall performances on BSDS dataset for RSS based on dif-
ferent path-cost functions

AMR AUE AUV AMR + AUE
FL (®) 3.88974 7.39789 7.17524 11.28763
fo (8) 3.78788 7.60433 6.97447 11.39221
fo(®) 4.62260 7.17840 7.34485 11.80100
fE C8) 4.88122 7.25717 7.22384 12.13839
f8 (x) 8.76662 7.28618 9.49756 16.05280

The performances are measured by Average Miss Rate (AMR), Aver-
age Undersegmentation Error (AUE), Average Unexplained Variation
(AUV) and AMR + AUE

ter boundary adherence and coherence is achieved by f@(«)
while higher segmentation accuracy is achieved by f’ (x).
These two functions compensate for each other, however,
f d(x) outperforms f/f’ («) for all five dataset except for Fash
according to the sum of AMR and AUE.

Moreover, fh (*) and f2,(*) perform a little better than
fi (*) and f; («) respectively except for EV, which is based
on Lz norm. Red and white are as similar as red and black
under L., norm. In contrast, red is similar to black more
than white under L2 norm since it averages the differences in
three channels. It indicates that more coherent superpixels are
assured by Loo. Another advantage of 14, norm is its compu-
tational cost. The Lo, norm only needs integer comparisons
while the L2 norm needs floating-point computations. fh (x)
is employed in the rest experiments.

4.2 Balancing Factor

As shown in Fig. 5, the shapes of superpixels can be regu-
larized by the scaling factor din Eq. 6. This factor balances
the color similarity and spatial closeness. Given the expected
number of superpixels K, i is the only parameter to be spec-
ified for RSS algorithm.

When i = 0, superpixel growing depends only on pixel
colors and is sensitive to noises. The superpixels have irregu-
lar shapes. When di > 0, superpixel growing depends on both
colors and distances, superpixels are regularized to have reg-
ular shapes. As i increases, the shapes become more regular.
When i > 512, the superpixels are squares as superimposed
on rightmost image. The maximal normalized coordinate dif-
ference between a pixels v at a square’s borders and its center
Uc is 0.5, i.e., max(|xX(v) —X(v-)|, |¥(v) — V(ve)]) = 0.5. The
normalized coordinate difference between all the pixels out-
side a square and the center of this square are larger than
0.5. By multiplying a i > 512, their coordinate differences
are larger than 256, which is the maximum value for color
difference. Since the maximal difference is dominated by the
pixel coordinates, the generated superpixels are squares.
International Journal of Computer Vision (2020) 128:2962-2978

 

 

Fig.5 Shape regularization. The original image and superpixels generated using d = 0, 10, 512 are depicted respectively from left to right

 

 

 

 

1.0
0.18
0.15
0.9
0.12
0.8 0.10
0.08
0.7 . . 0.05
0 5 10 «615s 0 5 10 #615 ~—20
(a) Rec (b) UE
— 200

— 1200

 

 

 

 

2971
anel a
elle Tn
Cc Te | ttt
| rr | | USA
fn ft hae
0.95
0.93 0.5
0.90 0.4
0.88 0.3
0.85 0.2
. . . 0.1 : . -
0 5 10 15 20 (@) 5 10 15 20
(c) EV (d) CO
— 6000

Fig.6 Performances based on different scaling factors. Rec, UE, EV and CO vary with respect to i. and K, which are indicated by the horizontal

axis and line colors respectively

Figure 6 reports Rec, UE, EV and CO of superpixels on
BSDS dataset. The segmentations are carried out on a coarse,
middle and fine level based on K = 200, 1200, 6000 respec-
tively, and each is based on A= 0, 1,..., 20. As shown, finer
segmentation outperforms coarser segmentation as blue lines
are better than green lines and green lines are better than red
lines. However, for a fixed K, 4 has minor impacts on seg-
mentation performance especially for fine segmentation. As
i increases, UE is stable while Rec and EV decrease slowly
when A € [0, 5]. As indicated by their compactnesses, super-
pixel shapes are regularized by dr significantly.

The overall segmentation performance gets better as K
increases. Given a K, it gets worse as i. increases since
coherency and adherence receive less priority to uniformity
and compactness. As performance is improved with the cost
of increasing K, it is important to choose a well-balanced
K. Given a K, it is convenient to select an ANE [0,5]. A
smaller A gives segmentation performance more priority to
superpixel regularity. However, a large id can be employed if
uniform and compact superpixels are preferred.

4.3 Comparison with State-of-the-Art Superpixel
Methods

ERS (Liu et al. 2011), ETPS (Yao et al. 2015), SEEDS
(Van den Bergh et al. 2015), SNIC (Achanta and Siisstrunk
2017) and GBS (Felzenszwalb and Huttenlocher 2004) are
employed for comparison. The former three are TOP 3 algo-
rithms reported in Stutz et al. (2018). SNIC is an improved
SLIC. Both SLIC and GBS are used very widely. Typical
superpixels produced by these methods are demonstrated in
Fig. 7.

GBS is based on minimum spanning forest of a graph
whose edge weights measure the similarities of neighbor-
ing pixels. The segmentation is controlled by a threshold for
the edge weights and a threshold for the segment sizes. It
does not allow to control the number of superpixels directly,
and it generates some widespread superpixels of arbitrary
shapes. By introducing the roots of forest into the underlying
graph, the number of superpixels can be easily controlled by
RSS. Coupling the regularly and uniformly distributed seeds
with the path-cost function, the manner of path extending and
superpixel growing facilitates region competition and assures
superpixels the expected characteristics as demonstrated by
the examples. Each RSS covers a local, connected and coher-
ent region. They adhere to the boundaries of flowers, face,
shoulder, arms, white and black strips of shirt. Meanwhile,
they are neither too large nor too small, and they appear to
be compact or even square in homogeneous regions. They
allow further regularization by a scaling factor.

As a method based on boundary evolution, SEEDS over-
segments near object boundaries while under-segments in
the homogeneous regions. ETPS and ERS overcome this
shortcoming to some extent. However, noisy superpixel
boundaries can still be observed in homogeneous areas.
By clustering based on color and distance, SNIC produces
compact superpixels with clean boundaries at the cost of sac-
rificing some segmentation performances. As shown in the
last row, some white and black strips of the shirt are mixed
in one superpixel. The advantages of RSS over SNIC is that
superpixels grow along optimal paths, which assures better
coherence and adherence.

The above methods are evaluated and ranked based on
all five datasets. The optimal parameters for all methods

Q) Springer
——
we,
pos

N) @t

Or

Fae OT ERP.
SR ANNA,

a

gee NNN a
ye
aw SH al

we

fa As!
AA
IN

ne

 

ea
aaa

(a) ERS

Fig. 7 Visual comparison of different superpixels. Two images in the
BSDS dataset are segmented respectively into about 200 superpixels.
Superpixel boundaries are depicted as green lines on the original images.

Table 2 Overall performances
of superpixel methods

Q) Springer

  
 
   
 

MS 2 , =
—_— ae
a

 

(b) ETPS

International Journal of Computer Vision (2020) 128:2962-2978

a eS ER
es Mee Bikes
TY ay Pad’ re a A resi SN
Br te “ag oe Oe cer 0 oy

S

i

a 4
ae

      
    

   

  
    
 

   
    

oa
at ea
“, | WN 4 ’ »\3 a Ww
call mal <> All . 2 By. BZ
<4 Vc A pada a a
: ae we. \2' mn \ Naaman 2) W aa
= a ko
CA
SS ‘J “a
se ® ed
OT ae a. ‘=

(d) SNIC (e) GBS
Two images and their zoomed windows indicated by white rectangles
are depicted row by row. The superpixels by different methods are shown
column by column (Color figure online)

Rank AMR AUE AMR -+ AUE
Avg. BSDS Fash NYU SBD SUN

ETPS 1 1 1 1 1 1 1.91 5.94 7.85

RSS 2.8 2 3 3 4 2 2.23 6.56 8.79

ERS 3.0 4 4 2 3 3 3.07 6.16 9.23

SEEDS 3.8 3 5 5 2 4 1.14 8.47 9.61

SNIC 4.2 5 2 4 5 5 3.67 6.32 9.99

GBS 6.0 6 6 6 6 6 8.31 8.43 16.74

The performances are measured by Average Miss Rate (AMR), Average Undersegmentation Error (AUE) and
AMR -+ AUE. The first column list the superpixel methods. The metrics averaged over 5 datasets are listed in
the right 3 columns. The ranks for 5 datasets and their average are listed in the middle 6 columns
International Journal of Computer Vision (2020) 128:2962-2978

 

 

 

 

 

1.00
0.95
0.90
ne 2000 4000 6000
(a) Rec
1.0
0.9
0.8
eat 2000 4000 6000
(d) min Rec
0.06
0.05
0.04
0.03
0.02
0.01
0.00

2000 4000 6000

(g) std Rec

1000

500

 

PS

2000 4000 6000

(j) std K

0.10

 

0.09

0.08

0.07

0.06

0.05
2000 4000 6000

0.30

O22

0.20

0.15

 

2000 4000 6000

(e) max UE

 

 

1000 2000 3000 4000 5000 6000

(h) std UE

 

2000 4000 6000
(k) CO

 

 

 

 

 

2973
0.95
0.90
0.85
a 2000 4000 6000
(c) EV
0.8
0.7
0.6
0.5
ae 2000 4000 6000
(f) min EV
0.12
0.10
0.08
0.06
0.04
0.02 + : . ‘
1000 2000 3000 4000 5000 6000
(i) std EV
—— ERS
—=— ETPS
=—_= SEEDS
— SNIC
— GBS
— RSS

Fig. 8 Performance comparison of superpixel methods on the BSDS dataset. It reports BR, UE, EV, their minimums/maximums and standard
deviations, standard deviation of superpixel numbers and CO with respect to the number of superpixels

Q) Springer
 

 

 

2974
0.8 —
— ERS
0.6: _
04 — "ETPS
——~  SNIC
02 — ~GBS
1.0 —_——————— ~—-—C— R9SS

 

2000 4000 6000

Fig. 9 Efficiency comparison of superpixel methods. The total time
consumed by each method to segment all the images in each dataset is
divided by the number of images in the dataset to calculate the aver-
aged runtime in seconds for each method and each dataset. The last

are selected by the optimization schema employed in the
benchmark. Firstly, for each method and each dataset, AMR,
AUE and AUV based on thirteen K € [200, 6000] are com-
puted. Secondly, for each dataset, all six methods are ranked
according to their AMR + AUE. Thirdly, for each method, an
average rank is calculated by averaging its ranks over all five
datasets. Fourthly, for each method, the average AMR, AUE,
and AMR + AUE are also calculated by averaging them over
all datasets. Finally, their overall performances are sorted
based on their average ranks. As reported in Table 2, it results
in an order same as that based on average AMR+AUE.
According to the ranking, RSS outperforms all the methods
except for ETPS.

The detailed reports for BSDS dataset are presented in Fig.
8. Rec, UE and EV along with their minimums/maximums
and standard deviations are presented. In addition, CO and
standard deviation of superpixel numbers are also presented.
Four facts can be observed from these reports. First is the
balance among metrics. Rec, UE and EV are well-balanced
by RSS as all these metrics are ranked as the second or third
among six. In contrast, SEEDS does not assure a good bal-
ance since it has the best Rec and almost the worst UE. The
second concerns stability. RSS performs stably from coarse
to fine segmentation as indicated by its smooth curves. It per-
forms more stable than ETPS and SNIC. GBS and SEEDS
perform unstably as their curves vary sharply. The third is that
RSS generates more compact superpixels than SEEDS and
ETPS. The last concerns the number of superpixels. Given
an expected number, RSS generates a fixed number of super-
pixels for all images of the same size since it is fixed in
segmentation. SNIC and ERS also guarantee a fixed number.
But SEEDS and ETPS generate different number of super-
pixels for different images, which lead to nonzero std K. The
number by GBS varies significantly since it depends on the
other parameters indirectly.

RSS algorithm (serial version) is much faster than the
other methods as depicted in Fig. 9. The time consumed by
each method to segment all the images in each dataset is
divided by the number of images to get the averaged time
to segment one image of each dataset. For each method,

Q) Springer

 

International Journal of Computer Vision (2020) 128:2962-2978

Ave.
0.655
SEEDS 0.644

0.555

0.1
0.087
0.083

0.023

column lists for each method the runtime averaged over 5 datasets. The
figure depicts the averaged runtime in seconds to generate the expected
number of superpixels for a NYU image

the above averaged runtime is further averaged over all five
datasets and presented in the last column. Details for the NYU
dataset are illustrated in the figure, where the runtime with
respect to the number of superpixels is depicted. As shown,
the runtime of RSS is stable with respect to varying number
of superpixels while those of ERS, ETPS, SEEDS and GBS
are not.

The above reports are based on the optimal parameters
including the number of iterations for ETPS and SEEDS.
By reducing it to one, the averaged runtime over all five
datasets by either SEEDS or ETPS is 0.102s. It is still much
slower than RSS. Besides, it significantly impairs SEEDS’s
performance. Moreover, 40% computation time can be saved
when 4 CPU cores are employed to parallelize RSS.

4.4 Comparison with State-of-the-Art Supervoxel
Methods

GBH (Grundmann et al. 2010), SWA (Corso et al. 2008),
TSP (Chang et al. 2013) and GBS (Felzenszwalb and Hutten-
locher 2004) are employed for comparison. The former three
are top performing methods reported in (Xu and Corso 2016).
GBH 1s based on GBS, which is called as GB in this bench-
mark. A same scaling factor is employed for both spatial and
temporal dimensions, 1.e. NM =) = 3. Their supervoxels on
six successive frames in the ice video of the Chen xiph.org
dataset are demonstrated in Fig. 10.

One can observe the adaptivity of RSS in Fig. 10. On one
hand, RSS are uniform and compact in the homogeneous
regions. On the other hand, they adhere to object boundaries
and indicate the peoples’ outlines. GBS and GBH segment
the videos into a set of nonuniform and irregular volumes,
some cover large homogeneous areas while some others con-
tain only a few boundary pixels. Although TSP are uniform
and compact, their adherence to object boundaries are not
good enough to indicate the peoples’ outlines. The overall
appearance of supervoxels by SWA is similar to RSS.

Figure 11 reports their performances. RSS outperforms
the other methods according to three metrics for segmenta-
tion performance. It achieves the best BRD (lower is better)
International Journal of Computer Vision (2020) 128:2962-2978

 

Fig. 10 Visual comparison of different supervoxels. The 80 frames
ice video in the Chen xiph.org dataset is segmented into about 2000
supervoxels. Different supervoxels are depicted as different colors. Six

and EV. Its UE3D is good, but its SA3D is low. It seems to
contradict the correlation between SA3D and UE3D. Actu-
ally, SA3D also depends on the temporal dimension. When a
supervoxel covers more frames, the size of correct segment
increases and a higher accuracy is achieved. The other tempo-
ral metrics TEX and MSV are not good. Better SA3D, TEX
and MSV can be achieved when the scaling factors d, A! are
optimized for the video. A better solution is to treat a video
as a sequence of images instead of a volumetric data.

successive frames are shown from top to bottom. The original image,
and supervoxels generated by GBS, GBH, SWA, TSP and RSS are
shown from left to right respectively (Color figure online)

As demonstrated in Fig. 9, RSS is three times faster than
GBS, which is almost an order of magnitude faster than GBH,
SWA and TSP as reported in Xu and Corso (2016).

5 Conclusion

This paper proposes a new approach for superpixel segmenta-
tion. An image is represented as a graph, some regularly and

Q) Springer
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

2976 International Journal of Computer Vision (2020) 128:2962-2978
2.00 30 — 0.950
1.75 251 0.925 |
1.50 201 0.900
1.25 154 0.8754
1.00 104 0.8504
0.75 5 ic 0.825}
aa 500 1000 1500 2000 05 500 1000 1500 2000 ial 500 1000 1500 2000
(a) BRD (b) UE3D (c) EV
0.85 0.8 150
0.80 0.7; a 100
1001
0.75 0.64
75 |
0.70 0.54
501
0.65 0.44 25,
0.60 0.3 9} _
0 500 1000 1500 2000 0 500 1000 1500 2000 0 500 1000 1500 2000
(d) SA3D (e) TEX (f) MSV
— GBS — GBH — SWA —— TSP — RSS

Fig. 11 Performance comparison of superpixel methods on the Chen xiph.org dataset. It reports BRD, UE3D, EV, SA3D, TEX and MSV with

respect to the number of supervoxels

evenly distributed seed pixels serve as the roots, the maxi-
mal difference and range of values are proposed as path-cost
functions to measure both the color similarity and spatial
closeness between the seeds and rest pixels via some paths,
superpixel segmentation is formulated as searching an rooted
spanning forest of the graph with respect to the roots and cost
function, and it is achieved by extending the root-originated
paths pixel by pixel.

The new formulation integrates some positives of differ-
ent formulations and achieves a good balance among the
expected characteristics: the number and locality of super-
pixels are controlled by the seeds; the coherency, adherence
and adaptivity are assured by root-originated path extending
dominated by a cost function defined on paths; the unifor-
mity and compactness are regularized by a scaling factor;
and connectivity is maintained by the region growing. Its
segmentation performance is ranked as the second among
state-of-the-art superpixel methods. Moreover, it is the fastest
algorithm and allows parallelization. Finally, RSS algorithm
is scalable as it deal with different kinds of data in the same
way.

Its advantages are reinforced by some potential applica-
tions. RSS can be applied to various fields reviewed in the
introduction. In addition, it is able to find a region for a
given seed and a cost threshold, the regions based on dif-
ferent seeds and cost thresholds can serve as global object

Q) Springer

proposals, which is especially useful for network extraction.
Moreover, in light of recent work such as Tu etal. (2018), RSS
promises new applications in CNN-based semantic segmen-
tation as it is capable to deal with high dimensional features
extracted by CNN.

Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing, adap-
tation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indi-
cate if changes were made. The images or other third party material
in this article are included in the article’s Creative Commons licence,
unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copy-
right holder. To view a copy of this licence, visit http://creativecomm
ons.org/licenses/by/4.0/.

References

Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., & Siisstrunk,
S. (2012). Slic superpixels compared to state-of-the-art superpixel
methods. [EEE Transactions on Pattern Analysis and Machine
Intelligence, 34(11), 2274-2282.

Achanta, R., & Siisstrunk, S. (2017). Superpixels and polygons using
simple non-iterative clustering. In 2017 IEEE conference on com-
International Journal of Computer Vision (2020) 128:2962-2978

puter vision and pattern recognition (CVPR) (pp. 4895-4904).
IEEE.

Alexe, B., Deselaers, T., & Ferrari, V. (2012). Measuring the objectness
of image windows. JEEE Transactions on Pattern Analysis and
Machine Intelligence, 34(11), 2189-2202.

Arbelaez, P., Maire, M., Fowlkes, C., & Malik, J. (2011). Contour detec-
tion and hierarchical image segmentation. JEEE Transactions on
Pattern Analysis and Machine Intelligence, 33(5), 898-916.

Arnab, A., Jayasumana, S., Zheng, S., & Torr, P. H. (2016). Higher order
conditional random fields in deep neural networks. In European
conference on computer vision (pp. 524-540). Berlin: Springer.

Boix, X., Gonfaus, J. M., Van de Weijer, J., Bagdanov, A. D., Serrat, J.,
& Gonzalez, J. (2012). Harmony potentials. International Journal
of Computer Vision, 96(1), 83-102.

Chai, D. (2019). SQL: Superpixels via quaternary labeling. Pattern
Recognition, 92, 52-63.

Chang, J., Wei, D., & Fisher, J. W. (2013). A video representation using
temporal superpixels. In Proceedings of the IEEE conference on
computer vision and pattern recognition (pp. 2051-2058).

Comaniciu, D., & Meer, P. (2002). Mean shift: A robust approach toward
feature space analysis. [EEE Transactions on Pattern Analysis and
Machine Intelligence, 24(5), 603-619.

Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009).
Introduction to algorithms. Cambridge: MIT Press.

Corso, J. J., Sharon, E., Dube, S., El-Saden, S., Sinha, U., & Yuille,
A. (2008). Efficient multilevel brain tumor segmentation with
integrated bayesian model classification. JEEE Transactions on
Medical Imaging, 27(5), 629-640.

Dijkstra, E. W. (1959). A note on two problems in connexion with
graphs. Numerische Mathematik, 1(1), 269-271.

Falcao, A. X., Stolfi, J., & de Alencar, L. R. (2004). The image foresting
transform: Theory, algorithms, and applications. JEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 26(1), 19-29.

Farabet, C., Couprie, C., Nayman, L., & LeCun, Y. (2012). Learning hier-
archical features for scene labeling. JEEE Transactions on Pattern
Analysis and Machine Intelligence, 35(8), 1915-1929.

Felzenszwalb, P. F., & Huttenlocher, D. P. (2004). Efficient graph-based
image segmentation. /nternational Journal of Computer Vision,
59(2), 167-181.

Gadde, R., Jampani, V., Kiefel, M., Kappler, D., & Gehler, P. V. (2016).
Superpixel convolutional networks using bilateral inceptions. In
European conference on computer vision (pp. 597-613). Berlin:
Springer.

Girshick, R. (2015). Fast R-CNN. In 20/5 IEEE international confer-
ence on computer vision (ICCV) (pp. 1440-1448). IEEE.

Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature
hierarchies for accurate object detection and semantic segmenta-
tion. In Proceedings of the IEEE conference on computer vision
and pattern recognition (pp. 580-587).

Gould, S., Fulton, R., & Koller, D. (2009). Decomposing a scene into
geometric and semantically consistent regions. In 2009 IEEE 12th
international conference on computer vision (pp. 1-8). IEEE.

Grundmann, M., Kwatra, V., Han, M., & Essa, I. (2010). Efficient hierar-
chical graph-based video segmentation. In 20/0 IEEE conference
on computer vision and pattern recognition (CVPR) (pp. 2141-
2148). IEEE.

Guney, F., & Geiger, A. (2015). Displets: Resolving stereo ambiguities
using object knowledge. In Proceedings of the IEEE conference
on computer vision and pattern recognition (pp. 4165-4175).

He, S., Lau, R. W., Liu, W., Huang, Z., & Yang, Q. (2015). Supercnn:
A superpixelwise convolutional neural network for salient object
detection. International Journal of Computer Vision, 115(3), 330—
344,

Hosang, J., Benenson, R., Dollar, P., & Schiele, B. (2015). What makes
for effective detection proposals? IEEE Transactions on Pattern
Analysis and Machine Intelligence, 38(4), 814-830.

2977

Levinshtein, A., Stere, A., Kutulakos, K. N., Fleet, D. J., Dickinson, S.
J., & Siddiqi, K. (2009). Turbopixels: Fast superpixels using geo-
metric flows. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 31(12), 2290-2297.

Liu, M. Y., Tuzel, O., Ramalingam, S., & Chellappa, R. (2011). Entropy
rate superpixel segmentation. In 20/1 IEEE conference on com-
puter vision and pattern recognition (CVPR) (pp. 2097-2104).
IEEE.

Liu, Y., Jiang, P. T., Petrosyan, V., Li, S. J., Bian, J., Zhang, L., et al.
(2018). DEL: Deep embedding learning for efficient image seg-
mentation. In JJCAI (pp. 864-870).

Lucchi, A., Smith, K., Achanta, R., Knott, G., & Fua, P. (2012).
Supervoxel-based segmentation of mitochondria in em image
stacks with learned shape features. JEEE Transactions on Med-
ical Imaging, 31(2), 474-486.

MicuSik, B., & KoSecka, J. (2010). Multi-view superpixel stereo in
urban environments. Jnternational Journal of Computer Vision,
89(1), 106-119.

Moore, A. P., Prince, S. J., & Warrell, J. (2010). “lattice cut’-
constructing superpixels using layer constraints. In 2010 IEEE
conference on computer vision and pattern recognition (CVPR)
(pp. 2117-2124). IEEE.

Moore, A. P., Prince, S. J., Warrell, J.. Mohammed, U., & Jones, G.
(2008). Superpixel lattices. In IEEE conference on computer vision
and pattern recognition, 2008. CVPR 2008 (pp. 1-8). IEEE.

Mostajabi, M., Yadollahpour, P., & Shakhnarovich, G. (2015). Feed-
forward semantic segmentation with zoom-out features. In Pro-
ceedings of the IEEE conference on computer vision and pattern
recognition (pp. 3376-3385).

Ren, X., & Malik, J. (2003). Learning a classification model for seg-
mentation. In Null (p. 10). IEEE.

Sharon, E., Brandt, A., & Basri, R. (2000). Fast multiscale image seg-
mentation. In IEEE conference on computer vision and pattern
recognition, 2000. Proceedings (Vol. 1, pp. 70-77). IEEE.

Shi, J., & Malik, J. (2000). Normalized cuts and image segmentation.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
22(8), 888-905.

Silberman, N., Hoiem, D., Kohli, P., & Fergus, R. (2012). Indoor seg-
mentation and support inference from RGBD images. In European
conference on computer vision (pp 746-760). Berlin: Springer.

Song, S., Lichtenberg, S. P., & Xiao, J. (2015). Sun RGB-D: A RGB-D
scene understanding benchmark suite. In Proceedings of the IEEE
conference on computer vision and pattern recognition (pp. 567—
576).

Stutz, D., Hermans, A., & Leibe, B. (2018). Superpixels: An evaluation
of the state-of-the-art. Computer Vision and Image Understanding,
166, 1-27.

Tsai, Y. H., Yang, M. H., & Black, M. J. (2016). Video segmentation via
object flow. In Proceedings of the IEEE conference on computer
vision and pattern recognition (pp. 3899-3908).

Tu, W. C., Liu, M. Y., Jampani, V., Sun, D., Chien, S. Y., Yang, M.
H., et al. (2018). Learning superpixels with segmentation-aware
affinity loss. In JEEE conference on computer vision and pattern
recognition (CVPR).

Uijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W.
(2013). Selective search for object recognition. International Jour-
nal of Computer Vision, 104(2), 154-171.

Van den Bergh, M., Boix, X., Roig, G., & Van Gool, L. (2015). Seeds:
Superpixels extracted via energy-driven sampling. International
Journal of Computer Vision, 111(3), 298-314.

Vedaldi, A., & Soatto, S. (2008). Quick shift and kernel methods for
mode seeking. In European conference on computer vision (pp.
705-718). Berlin: Springer.

Veksler, O., Boykov, Y., & Mehrani, P. (2010). Superpixels and
supervoxels in an energy optimization framework. In European
conference on computer vision (pp. 211—224). Berlin: Springer.

Q) Springer
2978

Vincent, L., & Soille, P. (1991). Watersheds in digital spaces: An
efficient algorithm based on immersion simulations. JEEE Trans-
actions on Pattern Analysis & Machine Intelligence, 6, 583-598.

Wang, J., & Wang, X. (2012). VCells: Simple and efficient superpix-
els using edge-weighted centroidal voronoi tessellations. JEEE
Transactions on Pattern Analysis and Machine Intelligence, 34(6),
1241-1247.

Wang, P., Zeng, G., Gan, R., Wang, J., & Zha, H. (2013). Structure-
sensitive superpixels via geodesic distance. International Journal
of Computer Vision, 103(1), 1-21.

Wang, S., Lu, H., Yang, F., & Yang, M. H. (2011). Superpixel tracking.
In 2011 IEEE international conference on computer Vision (ICCV)
(pp. 1323-1330). IEEE.

Wertheimer, M. (1938). Laws of organization in perceptual forms. In A
source book of Gestalt psychology (pp. 71-88).

Xu. C., & Corso, J. J. (2012). Evaluation of super-voxel methods for
early video processing. In 20/2 IEEE conference on computer
vision and pattern recognition (CVPR) (pp. 1202-1209). IEEE.

Q) Springer

International Journal of Computer Vision (2020) 128:2962-2978

Xu, C., & Corso, J. J. (2016). Libsvx: A supervoxel library and
benchmark for early video processing. International Journal of
Computer Vision, 119(3), 272-290.

Yamaguchi, K., Kiapour, M. H., Ortiz, L. E., & Berg, T. L. (2012).
Parsing clothing in fashion photographs. In: 2012 IEEE conference
on computer vision and pattern recognition (CVPR) (pp. 3570—
3577). IEEE.

Yao, J., Boben, M., Fidler, S., & Urtasun, R. (2015). Real-time coarse-
to-fine topologically preserving segmentation. In Proceedings of
the IEEE conference on computer vision and pattern recognition
(pp. 2947-2955).

Publisher’s Note Springer Nature remains neutral with regard to juris-
dictional claims in published maps and institutional affiliations.
