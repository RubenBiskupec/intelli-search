ALMarwi et al. J Big Data (2020) 7:39 ; °
https://doi.org/10.1186/s40537-020-00310-z oO Journal of Big Data

RESEARCH Oy oT-Ta waa -55 4

. . . ®
A hybrid semantic query expansion ay

approach for Arabic information retrieval

Hiba ALMarwi!, Mossa Ghurab! and Ibrahim Al-Baltah?

 

*Correspondence:

Hebh.Almarwi@gmail.com Abstract

computer Scfence In fact, most of information retrieval systems retrieve documents based on keywords
University, Sanaa, Yemen matching, which are certainly fail at retrieving documents that have similar mean-

Full list of author information ing with syntactical different keywords (form). One of the well-known approaches to
avaiable at the end of the overcome this limitation is query expansion (QE). There are several approaches in query

expansion field such as statistical approach. This approach depends on term frequency
to generate expansion features; nevertheless it does not consider meaning or term
dependency. In addition, there are other approaches such as semantic approach which
depends on a knowledge base that has a limited number of terms and relations. In

this paper, researchers propose a hybrid approach for query expansion which utilizes
both statistical and semantic approach. To select the optimal terms for query expan-
sion, researchers propose an effective weighting method based on particle swarm
optimization (PSO). A system prototype was implemented as a proof-of-concept, and
its accuracy was evaluated. The experimental was carried out based on real dataset.
The experimental results confirm that the proposed approach enhances the accuracy
of query expansion.

Keywords: Query expansion, Word embeddings, Particle swarm optimization,
Information retrieval, WordNet, Term frequency

 

Introduction

Information retrieval (IR) is an active research field that aims at extraction of the most
relevant documents from large datasets. User query plays an important role in this pro-
cess. A numerous efforts have been done to retrieve the relevant documents which are
written in English language. Nevertheless, Arabic language has not received the deserved
effort due to some inherent difficulties with the language itself. In fact, Arabic language
is one of the richest human languages in its terms, varieties of sentence constructions,
and diversity of meaning [1]. The sentence in Arabic language is made up of intercon-
nected terms based on grammatical relation [2—4]. User query in most cases is too short
which may neither be sufficient nor effective enough to express what the user needs [2].
Vocabulary mismatch is one of the most critical issues in IR where the user and indexer
use different terms [5, 6]. Consequently, IR systems could not retrieve the documents
which match the user needs. A well-known and effective strategy to resolve this issue is

to perform query expansion (QE).

. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
GO) Springer O pen adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the
— source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this
article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not
included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permit-
ted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.

org/licenses/by/4.0/.
ALMarwiet al. J Big Data (2020) 7:39 Page 2 of 19

Query expansion is a technique that expands the initial query by adding more terms
which are semantically similar to the original user query. As a result, several approaches
have been introduced to process user queries. Traditional query expansion methods rely
on statistical models such TF/IDF, and BM25 [7, 8]. The statistical methods depend ona
term-based document retrieval which generates queries that capture the user’s interests
from a collection of documents. Although these methods are effective, they are not able
to provide accurate information to the user query. Since those methods consider terms
as atomic units of information, disregarding syntactic and semantic similarities between
terms. An alternative to the statistical method is the semantic method, which attempts
to find the candidate terms based on the representative meaning of the query in its con-
text [9]. The semantic methods rely on external semantic resources such as WordNet
and domain ontology. However, a complex language such as Arabic language suffers
from the lack of semantic resources. Therefore, a hybrid method is needed to enhance
the performance of query expansion especially for Arabic language.

Therefore, this paper proposes a hybrid semantic query expansion approach for Ara-
bic information retrieval which calculates the weight of each term based on three infor-
mation retrieval evidences namely word embedding, WordNet, and term frequency. To
remove noise from the generated terms, a particle swarm optimization (PSO) is used
as a semantic filtering to avoid query drift problems. The rest of the paper is organized
as follows: “Background and preliminaries” section presents a brief background and
preliminaries. “Related work” section reviews the related query expansion studies. The
proposed approach is presented in “Framework for proposed approach” section. “Exper-
iments and evaluation” section presents the experimental results and discussion. Finally,

“Conclusion and future work” section concludes the study and outlines the future work.

Background and preliminaries
‘There are several approaches in query expansion. Query expansion methods which are
related to our approaches are presented in the following subsections.

Term frequency

In document retrieval theory, document and query are represented as a vector in vec-
tor space. Each term in the vector has a weight which represents the importance of the
term in the document as a whole. Several researchers have suggested different weighting
functions. Luhn [10] studied term distribution to assign a weight to a term according
to its frequency. A few years later, researchers enhanced term frequency performance
by computing number of terms related to the document length. Ounis [11] studied the
effect of the document length in the collection. Although this method is accepted due
to its simplicity and efficiency, yet it ignores the order and semantic relations between
terms. In addition, it suffers from data sparsity. As a result, this limitation makes its

usage undesirable to measure words similarity.

WordNet

Most query expansion methods utilize the knowledge resource such as WordNet. WordNet
is a global lexical database which organizes the terms holding identical meanings into sets
called synset [12]. These synsets are connected to each other through pre-defined lexical
ALMarwiet al. J Big Data (2020) 7:39 Page 3 of 19

relations. Arabic WordNet has been constructed by the adaption of the Euro WordNet con-
struction [13]. The Arabic wordNet contains 11,269 concepts [12], comparing with Eng-
lish wordNet which contains 155,287 concepts [14]. Arabic WordNet is commonly used for
query expansion where appropriate senses are linked to the original query to provide the
desired conceptual information. Voorhees [15] mentioned that this approach makes a little
difference in retrieval effectiveness when the initial query is not well molded. On the other
hand, the well molded query will improve retrieval effectiveness significantly. Furthermore,
using a lexical resource alone for query expansion can cause a topic drift. This is because,
the inappropriate changes in the query will cause query to match semantically other similar
terms. Unfortunately, using WordNet in a query expansion process generates some noisy
terms. Gong, Cheang, and Hou [16] used term semantic network to filter out the noisy

terms.

Word embeddings

To obtain effective semantic term representations, term representation may implicitly be
learnt by using latent dirichlet allocation [17] and latent semantic analysis [18]. These meth-
ods still consider corpora as “bag of words” Hence, they are not effective in capturing the
semantic behind the text. Recently, neural network language models [19] have been used
to model languages with promising results. Word embeddings are a set of language mod-
eling such as word2vec [20] and Glove [21]. Word embeddings map each word to a vector
of a real number. The vector values are learning in a way that resembles a neural network.
Consequently, the technique is regularly lumped into the field of deep learning. The main
idea behind word embeddings is to find dense, low-dimensions and real-valued vectors for
each term within its context. The generated embeddings represent the syntax and semantic
relations between terms. In embedding spaces, the words that have the similar meanings
should have the similar representations. In addition, the embedding spaces show straight
structure that generated word embeddings can be deciphered as relations [20]. This allows
vector-oriented reasoning based on the offsets between words.

Particles swarm optimization

Particle swarm optimization is a population stochastic nonlinear optimization technique.
It is inspired from the social behavior of birds. It looks for an optimal solution in search
space [22]. Each solution in a search space is called a particle. All particles are initialized
with velocity, position, and fitness value which are calculated by using an objective func-
tion. The algorithm is guided by personal experience (pbest), overall experience (gbest), and
the present movement of the particles to decide their next positions in the search space.
Further, the experiences are accelerated by two factors known as cl and c2, and two ran-
dom numbers are generated between [0, 1]. In each iteration, the pbest and gbest values are

calculated. After finding the two best values, particle updates its velocity and positions.

Related work

In order to overcome word mismatch problem in information retrieval, many popu-
lar solutions have been proposed by the researchers. Most early studies in Arabic lan-
guage in the field of IR have focused on morphological analysis of the documents. From
another point of view, many efforts have focused on developing Arabic stemmer such
ALMarwiet al. J Big Data (2020) 7:39 Page 4 of 19

as [23-25], which depends on a set of rules and uses lookup table for roots. Al-Serhan
and Ayesh [26] tackled this drawback by utilizing neural network to extract Arabic root.
Although it significantly increases the IR performance, most stemming techniques
introduce a large amount of noise in documents. Elayeb and Boun has [27] explained the
limitations of morphological analysis in Arabic IR. Traditionally, document and query
represent as a vector in a vector space, and each item in the vector has a weight which
reflects its importance. Different weighting functions have been suggested. Luhn [10]
assigned weight to the term based on its frequencies. Ung and Park [3] proposed a term
weighting function which considers the occurrence and the absence of terms. In spite
of the fact that has been gotten from these methods which is sensibly great, it does not
consider the semantic similar terms. Bai [28] selected expansion terms by computing
correlations between pairs of terms using the association rule [5].

One of the most well-known approaches to overcome the limitations of keyword-based
method is using thesaurus and domain ontology which attempt to rephrase the query
based on its context [29, 30]. Yokoyama and Klyuev [31] used Japanese WordNet for
query expansion. Alzahrani and Salim [32] used fuzzy concept to assign the value from 1
to 0 to reflect the degrees of similarities between Arabic documents based on ontology.
Chauhan, Zhai and Zhou [33, 34] exploited ontology of sport domain to develop seman-
tic IR system. Khan [35] developed semantic web search based on ontology. Although
these approaches are effective, most complex language like Arabic has scarce of seman-
tic resources like lexicons and ontologies. Traditional information retrieval models treat
queries as a set of unrelated terms, disregarding the semantic relationships interweav-
ing them. To enhance the performance of information retrieval, semantic methods uti-
lize document co-occurrence statistics [18], probabilistic latent semantic analysis [36] to
represent terms. Although these models have already achieved good performance, they
are very costly and time consuming. To learn a viable representation of term based on its
context, distributed word representation which is also known as a word embedding has
been introduced in information retrieval field [37, 38]. Diaz, Mitra, and Roy [39, 40] have
used contextually associated words which have been generated from word embeddings
to extend user query. Liu [41] used fuzzy rules to reweigh the expansion words which
have been generated from word embeddings.

As it can be seen from the reviewed studies, some limitations were found. Of these
limitations, some studies were focused on statistical method which depends on the
exact matching to generate the expansion terms. This is in turns neglected any potential
semantic matching. On the other hand, some studies attempted to tackle the aforemen-
tioned limitation by using semantic methods, which utilizes the knowledge base during
the process of expansion terms generation. Yet, this method it suffers from the limited
number of terms and relations that are included in the knowledge base. Therefore, this
study proposes a hybrid approach which utilizes statistical and semantic method in

order to overcome the mentioned limitations and to produces better results.

Framework for proposed approach
The architecture of the proposed approach is illustrated in Fig. 1 and Table 1 respec-
tively. An overall architecture of proposed approach is presented in Fig. la, b, give

detailed insight of proposed approach. First the query is submitted by user to retrieve
ALMarwiet al. J Big Data

(2020) 7:39

the desired results. This query represents the input of the proposed approach. Then,
the initial user query is handled. The meaningful concepts are extracted and processed
using Khoja algorithm [42]. In order to get a rich set of associated terms, the initial
user query is expanded. This can be done by combining candidate terms from various
kinds of information sources aforementioned including WordNet, word embeddings
and term frequency. The query is refined in three different stages as shown in Fig. 1b.
First, the synonyms for each ¢; in a user query are obtained using Arabic WordNet.
They are combined with the seed query terms for further expansion in second stage.
In second stage, Word2Vec is used to extract more semantic similar terms for each

t; from the previous stage by computing its cosine distance from the original word

 

Ranked candidate term
based on its final weight

 

 

Expanded Query
End
Third Stage second Stage First Stage
— , i...
original query with Larder laarlur eeeettan
at < nitemost <— each term from —_ T1,t2,....,1 —_ to indexe: —_ Ds {ts,tz,...,te)
wordnet
Selecin top —
D: (t:,tz,...,t)
a =
similar term tee eeemeeeteeeesee es,
Apply cosine tr (S,S2)-)5) > =
similarity between Dp ‘ts,ts,---,te)
expanded query Q°
and word2vec secre
Tr (S2.S2.--5)
vit wee i
term weighting
function OD: (ts,tz,...,te)
New Query
D; (ts,tz,...,te)

Fig. 1 Overall architecture for the proposed system

 

 

 

Page 5 of 19
ALMarwiet al. J Big Data (2020) 7:39 Page 6 of 19

Table 1 Pseudo code of proposed system

 

 

Step 1 For each term t; in query q Construct set of synonyms g- based on wordnet
Step 2 Create extended query set g’ by unifying original query q with qe.
Step 3 For each term tj ing’

Extract the most similar relevant sense of the term within query Context based on word2vec(c)
Step 4 Select the most frequent m terms from PRD
Step 5 Unify m with ¢ for generating final candidate term that produce the Sense of query context
Step 6 For each final candidate term tf from step 5

Compute average weight using Eqs. 6, 7, 8
Step 7 Select optimal average weight for each term in final candidate term using PSO algorithm
Step 8 Unify the top optimal term from step 7 with original query q

 

in the vector space. Word2Vec (skip Gram) is chosen for our word embedding pro-
cess because it has proven to be useful in capturing intensive representations of word
based on its context [2]. In order to find further nominee expansion terms, most fre-
quent terms are calculated in third stage. Frequent terms are calculated using rapid
miner tool on a collection of documents that are retrieved at top ranks in response to
the initial query.

The generated expanded terms from the three stages are called as candidate terms
and build a candidate term pool. The values of three IR evidence namely word embed-
dings, WordNet and term frequency are computed for each term in candidate pool. Each
evidence has its weight which represents the importance of candidate terms. PSO based
term weighting approach is applied to find optimal weights for all the three IR evidences
and to determine the final weight of each candidate term as it is shown in proposed PSO
term-weighting approach section. After computing the weights of original query terms and
candidate terms, all the terms are arranged in descending order of their final weights. And
the top K terms are selected for query expansion. Finally, the selected expanded terms are
added to original query.

Proposed AQE approach

Researchers proposed approach aims to retrieve more relevant documents. It is providing a
convenient way of finding terms that are semantically related to any given query. In this sec-
tion, researchers describe how extended query term set is obtained based on three IR evi-
dence namely word embeddings, WordNet and term frequency. The researchers construct
Qc, the set of synonyms for each ¢; in a user query, giving a query Q consisting of m terms
{tj,...5---sbm}as Eq. 1

Ac = {{f1, syn(t1)}, {f2, syn(t2)}, see peeey {fn, syn(ty) }}. (1)

First the synonyms for each ¢; in a user query are obtained using Arabic WordNet, where
syn(t;) are the synonyms of term ¢; in user query. Next, researchers define an extended
query term set (EQTS) q’as Eq. 2

q = qUa. (2)

q’ is sent to second stage for further expansion. In this stage, Word2Vec is used to

extract more semantically similar terms for each ¢; from the previous stage by computing
ALMarwi et al. J Big Data (2020) 7:39 Page 7 of 19

its cosine distance from the original word in the vector space. Word2Vec (skip Gram)
is chosen for our word embedding process because it has shown an efficient learning
in generating high-quality word embeddings in large-scale unstructured text data [2].
Researchers define the set C of candidate expansion terms as Eq. 3

c= LJ NT(t) (3)

teq’

where NT(t) is the set of K terms that are the nearest to f; in the embedding space. Next,
researchers define an extended query term set (EQTS) q’ as Eq. 4

g =q Uc. (4)

In order to find further expansion terms, we select the most frequent m terms from a set
of pseudo-relevant documents (PRD)—which are retrieved at top ranks in response to
the initial query. The size of PRD and the number of selected terms M may be varied as
a parameter. All the expanded terms are added to the original query which constitute a
set of obvious candidates from which terms may be chosen and utilized to expand Q. In
fact, some of the obtained candidate terms may not be related to the meaning of query
as a whole. Therefore, term-weighting functions used to select most suitable terms by
assigning weights to each term in candidate pool. A term weighting function is math-
ematically represented by Eq. 5 and discussed in below section. It is based on three evi-
dence namely word embeddings, WordNet, and term frequency. Each IR evidence has

its own weight which is multiplied to corresponding IR evidence value.

Proposed PSO term-weighting approach

In fact, some of obtained candidate terms are proximate neighbours of individual query
terms, it is preferable to consider terms that are close to the meaning of query as a
whole. The proposed PSO term-weighting function aims to select most suitable terms.
It assigned weights to each term in candidate pool. It chose and included extra terms to
a query. The proposed term weighting function is based on three evidence namely word
embeddings, WordNet, and term frequency. The values of this evidence are computed
for each term in candidate pool which is mathematically represented in Eq. 5

sim(q,t) = Ss” w2v.wn.tf (5)

teq’

where sim(Q,t) is the similarity value between ¢; in candidate pool and all the terms in Q.
The first element of proposed term weighting function is w2v. The mathematical expres-
sion given as Eq. 6 is used to compute the mean cosine similarity between ¢; in candidate

pool and all the terms in Q in embedding space

1
sim(vee(t,q)) = = > | bai, (6)

tiEq

The second element of proposed term weighting function is wn. This element indicates
the mean cosine similarity between ¢; in candidate pool and all terms in Q which is

mathematically expressed as Eq. 7
ALMarwiet al. J Big Data (2020) 7:39 Page 8 of 19

; 1
sim(t,q) = 7 Ss” tdi. (7)
t;€q

The third element is tf which is one of the weighting IR evidence used in many term-
weighting function. This element indicates the number of occurrences of a term in the
collection. To restrict the search domain of candidate term, researchers consider only
the number of times the candidate terms appear within PRD

if (t) = LJ count(t) (8)

tePRD

tf{t) is the number of times the candidate term appears in pseudo-relevant documents
(PRD). Each evidence has its weight which represents the importance of candidate terms
where the values of weight are between 0 and, 1. The sum of all the weights is ensured
to be 1. The ideal values of the different evidences weights were founded out using PSO.
The initial values of weights are taken as positions of particles. Each weigh is multiplied
by its value. Then it is summed up to calculate the final weight of the term w_Score. That
is mathematically represented in Eq. 9

w_Score = w1 * w2v + w2 * wn + w3.ff. (9)

The initial values of cl, c2, population size, w, velocities and a maximum number of iter-
ation were set and w_Score used as the fitness value. In each iteration the fitness value of
each particle is compared with other particles to get best value (best). To obtain global
best value (gbest) the current population best fitness is compared with the previous pop-
ulation best fitness. At the end of each iteration, positions and velocities of each particle
are updated. The maximum iteration is checked. The maximum weight of each term was
maximized so the most appropriate candidate terms for query expansion could be iden-
tified. All terms were arranged in descending order according to their final weights. The
top M terms were selected for query expansion. Finally, the chosen expanded terms were
included to the original query.

Experiments and evaluation

In the following section, we present a set of experiments to evaluate the performance of
our proposed approach. The results show that our proposed approach have achieved the
best performance compared with all the other approaches.

Experimental environment
This experiment was run on a Dell desktop computer with a 64 bit i5-3470 processor
CPU running at 3.20 MHz, with an 8 Mb RAM, running Microsoft windows 7 profes-

sional with service pack 1.

Building corpora (index) and query designing

Due to the lack of the available Arabic corpora, Arabic corpus was collected from differ-
ent Arabic news websites using Vietspider program. Approximately 72 h were needed to
collect 8 GB of Arabic Web Pages from different known news websites such as Al-Alam,
BBC, CNN, and Al-Jazeera. The collected HTML pages were passed through a series of
ALMarwiet al. J Big Data (2020) 7:39 Page 9 of 19

pre-processes stages including removing non-essential HTML tags from HTML files. Ara-
bic stop words lists such as conjunctions, prepositions, and articles do not have any effect
in text mining process [43]. Furthermore, it increases the dimensionality of the text. There-
fore, stop words were removed to reduce text dimensionality. Although some Arabic stop
words lists are available in different studies such as [4], none of them has shown efficiency
in Arabic information retrieval. Therefore, our own Arabic stop words list was used. Non-
stop words were stemmed using Khoja algorithm [42]. For effective results, the process
of removing stop words was combined with stemming process [5]. These preprocessing
steps reduced corpora size to 1 GB. Statistical information about the dataset is provided
in Table 2. The processed files were used for training and indexing purposes. For indexing
creation purpose, the processed web pages were indexed using lucene. The processed files
were then dumped as raw text for the purpose of training the neural network of Word2Vec
framework. The parameters of Word2Vec were set as follows: word vector dimensional-
ity 300; negative samples 25; and window size five words. These are as part of the param-
eter setting described in below section. On another hand, 40 query documents (QDocs)
were designed manually by an expert of Arabic language to verify the correctness of our
approach. Due to the paper restriction, Table 3 presents only eight queries which were
selected randomly as an exemplary sample and shows the expansion terms that obtained by
w2v, WordNet and the selected expansion term from PSO.

Table 3 illustrates the selected random queries. It presents expansion terms that obtained
by WordNet, W2V, tf and the proposed approach. Table 4 presents the selected expansion
term and their fitness value.

Parameter setting

The proposed query expansion approach has two unique parameters. N, that is the num-
ber of ranked documents selected as most relevant document for query expansion (size of
PRD). M that is number of the candidate terms selected for query expansion. To find out
the best performance of proposed approach, a set of experiments are performed to select
suitable values of N and M. The results of these experiments are presented in following

subsections.

Number of top ranked documents (N)
It is important issue to select proper number of PRD documents for query expansion.
Therefore, set of experiments are performed to check size impact of PRD on IR perfor-
mance. Table 5 presents the results for size of PRD varying from 5 to 20.

As it can be seen clearly from Table 5, the best performance of proposed approach in
terms of a mean average precision MAP cannot be achieved effectively by a low or high
number of documents. However, the highest precision results were achieved when N

parameter is set to 10.

Table 2 Statistical about dataset

 

Size Number of documents Number of sentence Number of words

 

1 GB 6464 9561 48,305

 
ALMarwiet al. J Big Data

(2020) 7:39

Table 3 Selected terms for the randomly chosen query

Pret Candidates obtained | Candidates obtained | Top 5 frequent terms peer terms obtained
No by Wi ordNet_ ee W ord2Vec in PRD »PSO

L

Number of candidate terms selected for query expansion (M)

A number of candidate expansion terms were generated by the proposed approach.
Selection the most relevant candidate terms M from a whole candidates set is an impor-
tant issue. The number of candidate terms parameter, that will be added into the sub-
mitted queries, was tuned through performing several experiments as shown in Table 6.

4

what is the influence
of Hafter’s attack in
the middle of Libya?

pot ot Sey Usk
Syatalt Lana Suche bys
va Ja Hol pl
Spdy ad 3 dO

What are the
solutions can the
World Health
Organization (WHO
handle to illuminate
the spreading of
cholera in Taiz and
Makha?

Vyhe pill Gal Hi sa6 aS
4 ’
Se LI) Ub i»

ey. VV aja

How many victims
have been died
sinking in the Libyan
coast at the
beginning of 2017?

. oa Gr Ga ee, ee ae

$B 3S ow Yap
Sy YS eo yi

what does the Syrian
government
agreements state
from
implementation of
the agreement of the
Foe Kaftiya
Zabadani?

ys gh

Why did the King of
Morocco refuse
meet the President of
Tunisian
government?
do jadd) Dyed 7
Spel yd Jile boy
i adh ay
Sy) ey

what are the rapid
developments that
Haftar conducts to
face the possible
attacks in the south
and west of Libya?

os) taj BS
p fing pre! de gi y vs
Sade yi dig

Did the free media
which respect the
media career
accompany the
Massacre of
Kfariaien and

Alfoaien people?

AD Saseall lad
A cael GS Se
oy >
She le gh SNL

what are the
fequirements of the
President of
National Accord
goverment for the
restoration of
security and peace in
southem Libya?

Attempt, Assault,
Middle, Environment,
Surroundings. Center.

Boat, Its end,
beaches Sea, Start.

Martial, Tripoli,
Asa result, Led
, Soldiers. Militias,

Increase, Height,
Confrontation,
Musalloub, Qataba
, AL. Salou.

wwe ed ‘>
et ial gS

Wound, Death,
its end, were killed,
Starting, Beaches

Sea.

Benghazi, Tripoli,
Arm, Militias, Haftes.

Al Salov, Khais,
Score, Harvesting,
Confrontation.

 

Its end, Tripoli,
Beginning, were killed,
Arrive.

gt eel pe ls
SRG oye og Sue

Militias, Tripoli
, Led, Martial
, Soldiers, Benghazi.

Khais, Score,
Harvesting, Musalloub,
Al. Salou, Confrontation.

Arrive, Ig end, Wound,
beaches Sea, Starting.

 

wSoeled veh gal catat

Covenant,

Political system,

Government
performance,
Procedure,
Implementation

King, Royal, Meeting,
Dean, Director,
Emperor.

Contradictory,

Opposite, Against,
Confrontation, The

east.

‘> GF tuanlne oa Y

a tod

Escort, Accompanying
, Arabic, Free,
Libesal, Unrestricted.

Rule, State, System,
Political system.

Kip ge Md ask | Ge

wept «Seiad spall

Government, Political
system, Syria, System,
Damascus, Aleppo.

spe! oh
ald age oS alate

Majesty, constitutional
emperor, king, kings of
Saudi Arabia, Reign of
the king.

st AG
SHE! Rpm Apcighalh

The opposite, Gaddafi,
Colonel, The militias,
East side.

4d a nel OW 5
Amps 3 ye

Synchronize,
Attach, Accompany
Massacre, Butchery.

‘ Da) ela ha canon
SODY Ae glad 6 jm

Decisive, Diplomatic,
Liberal, Pasty,
Required, Opposition.

Synia, Aleppo,
Damascus,
Constitution,
Implementation.

Kings of Saudi Arabia,
King, President,
Government, Meeting.

pr eld! sito!
«te all ple

Assault, The militias,

Attack, Tripoli,
Gaddafi.

Massacre,

Coverage, Butchery,
Accompanying,
Victims

Decisive, Tripoli,
Military, Hafter
Opposition.

Ba Se ae
ele $0 Geel

System, Political system,
Government,
Implementation,
Damascus, Aleppo.

Majesty, king, Emperor,
Reign of the king,
Majesty, King.

Contradictory, Opposite,
Gaddafi, Colonel, The
militias, east side.

hae lee a Y cgul
some elgg

synchronize, Escort,
Accompanying,
Massacre, Unrestricted,

ALY! «gy Sauwe s all po
wide Fee Solel |

Tripoli, Military,
Coalition, Required,
Opposition, Decisive.

 

This is to ensure the accuracy of the obtained results.

Page 10 of 19
ALMarwiet al. J Big Data (2020) 7:39 Page 11 of 19

Table 4 Expansion terms and fitness values

Term | Fitness | Term | Fitness | Term | Fitness | Term | Fitness | Term Fitness | Term | Fitness
Value Value Value Value Value Value

Se] 079 [a | 062 | | 087 [Ge [ote [| oes |
ee ase| oe ose] al as fener S| og | oe oar
s[ S| ess| =| ose | em | ogo | ei |ozr| [08s [eT] 09

eochias
a
a

ses ore es Pee ee oar a oa ES ae
spe] 087 [ok | ota, SR] on [ET] 06g] ae] 068] ET 065

 

Table 5 Performance versus size of pseudo relevance documents

 

Size of pseudo relevance documents

 

5 10 15 20

 

MAP 0.2179 0.514 0.3475 0.1875

 

Table 6 Performance versus a number of expansions term

 

Number of expansions terms

 

2 4 6 8 10

 

MAP 0.44179 0.5175 0.534 0.3375 0.2937

 

As it can be seen clearly from Table 6, the best performance of proposed approach in
terms of a mean average precision MAP cannot be achieved effectively by a low or high
number of candidate terms. However, the highest precision results were achieved when
M parameter is set to 6. It is clear from the results of experiments that applying com-
position of parameters (size of pseudo relevance documents and number of candidate
terms) indeed effects on IR performance in terms of MAP positively.

Experimental results

To evaluate the performance of the proposed approach, two analyses is done. First,
researchers use standard evaluation metrics: precision, recall and F-measure. Second,
to make the result more reliable, statistical analysis is done. During the evaluation, the
designed queries were used. The query relevant documents were determined by the Ara-
bic domain specialist who provided the test queries.

Overall performance

F-measure for top ten retrieved documents are computed for the proposed approach
and compared with original query, TF, WordNet and W2V approaches respectively as
shown in Fig. 2. It demonstrates the higher F-measure values. It was obtained by the
proposed approach in compassion with other approaches. It is clear from the experi-

mental results shown in Fig. 2, that the proposed approach is performing better than the
ALMarwi et al. J Big Data (2020) 7:39 Page 12 of 19

original query for all forty queries. The proposed approach also obtains higher values
of F-measure for 38 and 36 queries in comparison to W2V and WordNet approaches
respectively. However, F-measure values are equal for two and four queries in compari-
son to W2V and WordNet approaches respectively. Furthermore, higher F-measure val-
ues are achieved by proposed approach over TF approach for 39 queries.

Four experiments were also conducted to evaluate the accuracy of proposed approach.
The accuracy of proposed approach was calculated and compared against the accuracy
of four different approaches including queries without expansion, w2v-based, TF-based
and the WordNet based approaches. The accuracy calculation was carried out using

Eq. 10. The following subsection presents the evaluation results.

relevant documents Q retrieved documents

 

Accuracy = (10)

retrieved documents

The comparison accuracy between the proposed approach

and without expansion

The comparison results between the proposed approach and queries without expan-
sion are represented in Fig. 3. As it is shown by this figure; the average accuracy per-
centage for queries without expansion is 11% which is relatively less than the accuracy
of the proposed approach. However, the highest accuracy obtained by without expan-
sion approach is for query 4, which is still lower than the accuracy of the same query
using proposed approach. It is clear from the experimental results shown in Fig. 3 that,
proposed approach is more accurate than the without expansion approach for all the

queries.

The comparison accuracy between the proposed approach and WordNet based
approach

The comparison results between the proposed approach and WordNet are represented
in Fig. 4. As it is shown by this figure; the average accuracy percentages for proposed
approach and the WordNet based are 53% and 19%, respectively. Accordingly, the Word-
Net Based approach obtains very low accuracy for more than 32 input queries, while

 

F-Vleasure
©
Ww

 

aol q3 q5 a7 q9 gli 913 qi5 gl7 919 q2i g23 925 q27 q29 931 q33 435 q37 q39
Queries

ee Original Query Ge TF ete WordNet —e—W2V —# Proposed Approach

 

 

Fig. 2 F-measure values of top ten retrieved documents with respect to 40 queries
XX SD
ALMarwiet al. J Big Data

(2020) 7:39

 

—e— Original Query —m— Proposed Approach

PRECSION
oO
in

O1 903 O5 O7 O901101301501701902102302502702903 2033035037039
QUERIES

 

 

Fig. 3 Accuracy of proposed approach versus queries without expansion
XM 7

 

—@— WordNet —i—Proposed Approach

4

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.2
0
Q2 Q5 O5 GF GF O119013.0150270179 02160250250270299031033 0355057053
QUERIES

Fig. 4 Accuracy of proposed approach versus wordNet based approach
XX S

PRECSION

 

 

the accuracy of proposed approach is higher than the WordNet-based for more than 13
input queries. It is worthy to point out that, WordNet based approach is more accurate
than without expiation approach, yet less accurate than the proposed approach.

The comparison accuracy between the proposed approach and TF based
approach

The comparison results between the proposed approach and TF based approach
are represented in Fig. 5. As it is shown by this figure; the average accuracy percent-
age for TF based approach is 14% which is relatively less than the accuracy of the pro-
posed approach. It is clear from the experimental results shown in Fig. 5 that, proposed
approach is more accurate than the TF based approach for all the queries expected for

query number 15.

The comparison results between the proposed approach and W2v based
approach

The comparison results between proposed approach and the w2v-based approach are
shown in Fig. 6. As depicted in this figure, the average accuracy percentage for pro-

posed approach is 53%. This means that proposed approach obtains very high accuracy

Page 13 of 19
ALMarwi et al. J Big Data (2020) 7:39 Page 14 of 19

 

—e— TF —i—Proposed Approach

b>
hm

~~

08 |
:
4 0.6
ww
cc
Cc
0.4
0.2
0
Q1 Q3 Q5 O7 O9 02102501501 701902 102502 502702%3 203 X03 5037039

QUERIES
Fig.5 Accuracy of proposed approach versus TF based approach
Ne

 

 

 

 

cr >)
—e— W2V —m Proposed Approach
i
0.9
0.8
0.7
5 0.6
sos \
cc
= O4
0.3
0.2 ‘
0
Q1 O3 Q5 Q7 O90110130150170190210230.2 50272903 1033035037039
Fig.6 Accuracy of proposed approach versus w2v-based approach

 

XN S

for more than 21 input queries. While the average accuracy percentage for w2v-based
approach is 27.37%. W2v based approach provided higher accuracy values for only 11
input queries. The w2v-based approach was compared to the WordNet-based approach
in terms of accuracy. It is worthy to point out that, the average accuracy percentage of
WordNet-based approach was only 19%. This means that w2v-base approach obtains
high accuracy for more than 3 input queries over wordNet based approach. As shown
in Fig. 6; for all tested queries, the accuracy values of the proposed approach are higher
than the accuracy values of the wordNet-based and w2v-based approaches.

Precision and recall values also computed and compared for the above selected que-
ries using above mentioned approaches as shown in Table 7.

To check the overall performance of the proposed approach, Recall and Precision
values are computed and compared with Original query, TF, WordNet, W2V based
approaches as shown in Table 8.

As it can be seen clearly from Table 8, the proposed approach outperforms all other
query expansion approaches. Figure 7 shows the comparison of Recall—Precision for all
approaches.
ALMarwiet al. J Big Data (2020) 7:39 Page 15 of 19

it is clear from the above results the proposed approach outperform other query
expansion approaches due to, proper selection of expansion terms from candidate pool.

Statistical analysis
To make the result more reliable, statistical paired t-test analysis is also computed.
Table 9 shows the improvement of proposed approach against other approach is sta-
tistically significant at «x = 0.05. The proposed approach statistically outperform other
approaches as p-values are 0.0257, 0.0258, 0.0295 and 0.0330 for Original query, tf-
based, WordNet-based and W2v-based approaches respectively (Fig. 8).

The results from different analysis demonstrate that proposed approach have achieved
the best performance compared with all the other approaches.

Discussions

Table 7 lists the accuracy-Recall values of the four approaches for eight randomly
selected input queries. By exploring these observations, there are two fundamental key
discoveries: the cases that the proposed approach outperformed the other approaches
and vice versa. First, for some queries such as queries Q#2, Q#7, Q#6, Q#3 and Q#1, the
proposed approach results outperformed the other approaches. For example, in case of
query no. 7 the term a¢ 4 is synonym to the third query term and term , > appears
with query term ertel in many documents. Therefore, these terms can be added as new
term for query expansion. Similarly, for query no. 2 term ,\.)\ and term J gle. comes
with the ;< in many documents (refer to Table 3). Consequently, these terms are added
in original query using proposed approach and it improves the accuracy.

In case of query no. 3 the term cles and term 3 3 bls generates as synonym for term
«| x, using wordNet method. Besides, the terms + ‘bl,., ol 59 jplgtS, >, (en
generates as expanded terms using w2v-base approach. The computed accuracy for
expanded query using wordNet is 0.4 and for modified query using w2v-base method is
0.5. The terms cules, J 3 dykes, <2%> is not selected as expanded terms using pro-
posed approach, hence it improves the accuracy from 0.4 and 0.5 to 0.7. The reason
behind this improvement is that, PSO plays an important role in selecting the suitable
terms for query expansion and make query more specific. Therefore, most of the
retrieved documents are relevant, and hence the results of the proposed approach were
better than the other approaches. Table 8 presents the comparison of MAP of proposed
approach with other query expansion approaches.

Second, for some queries including Q#4, Q#5, and Q#8, the WordNet, TF and w2v-
based approaches fetched better results than the proposed approach. For example, in
query Q#4 the terms eal, Lai, vl, 4ogX> is added as new term for query expansion
using proposed approach. In such cases, the proposed approach fails to remove inappro-
priate terms from the candidate pool which cause lags our approach behinds other

approaches.

Conclusion and future work
In this paper, a hybrid query expansion approach for Arabic information retrieval was
proposed. This approach combines statistical and semantic method to utilize the advan-

tages and strengths of each method. Thus, the term mismatch limitation of the statistical
Page 16 of 19

(2020) 7:39

ALMarwi et al. J Big Data

 

 

 

 

 

 

 

Lv'0 Ev'0 SS'0 85°0 LO CLYCO 910 8L°0 60°0 OLL'0 8
890 8Z0 CTO 870 99L'0 8910 8E0'0 OOL'O 760'0 SEL‘O L
vL0 080 6S°0 99°0 cv0 85°0 ce0 cv0 OL0 celO 9
S90 69°0 8Z°0 8Z°0 SZ0 L8'0 LL‘O ZV0 SZ0'0 OSLO S
Lv’ cv'0 89°0 89°0 LEELO 670 Z6E10 LSCO ZLV0 €S70 v

80 8'0 CO 670 9910 S670 790'0 9EL'O 9/00 VEL €
920 SZ8°0 SZV0 95°0 LS~O 09¢'0 9910 SScO 99L'0 SCO C

90 90 £0 9€°0 LO vvc0 €80°0 v6L'0 880'0 Sv LO L

UOISIDAId yyeray UOISIDAId yyeray UOISIDAd yyeray UOISIDAd yyeray UOISIDAId yjeray

yreoidde pasodoldg AZM YONPIOM 41 Aasanb jeurlbuco ‘ou Aiano

 

sayreoidde uoisuedxa Asanb uasayip Buisn salianb pa}dajas aAOge dU} 10j SaNj|eA UOISIDa1g puke [jeEr99y Z a1qeL
ALMarwi et al. J Big Data (2020) 7:39 Page 17 of 19

Table 8 Recall and Precision values using different query expansion approaches

 

 

Original query TF WordNet W2V Proposed
approach
Recall Precision Recall Precision Recall Precision Recall Precision Recall Precision

 

0.17375 0.1194 0.1985 0.1488 0.2163 0.1974 0.2737 0.2706 0.53062 04728

 

 

 

C >
it
0.9 °
0.8 o
0.7 2 a
6 0.6 o e@°@ ee
va]
5 0S e e. ® =
= 0.4 ° ® ® a ®
3 2 a °
me ee e® $
0.2 8 8 a a ° >
0.1 ga-° 6 $ g
oe
0 0.2 0.4 0.6 0.8 i
Recall
® Original Query @ TF e@ WerdNet @ W2V @ Proposed Approach
Fig. 7 Recall—Precision graph for different approaches

 

XN /

Table 9 Paired t-test results

 

 

 

 

 

 

Approach h-value p-value
Original query 1 0.0257
tf-based approach 1 0.0258
WordNet-based approach 1 0.0295
W2v-based approach 1 0.0330
C >)
0.035
——
0.03 a
0.025
. 0.02
= 0.015
0.01
0.005
0
Tf-based approach W2v-based approach
Original Query WordNet-besed...
Approach
Fig.8 Paired t-test results
NX

 
ALMarwiet al. J Big Data (2020) 7:39 Page 18 of 19

method is tackled by allowing semantically similar words to contribute to the scoring
function. The proposed approach generates expansion terms closely related to the mean-
ing of a query as a whole. To find the degree of the importance of the expanded terms to
the query as a whole, the weights of each candidate term were computed based on three
evidence namely word embedding, WordNet, and term frequency. Moreover, the par-
ticles swarm optimization was used to select the most suitable terms for query expan-
sion. The experimental results that were carried out on real dataset confirmed that the
proposed approach increased the value of accuracy in terms of information retrieval and
demonstrated the effectiveness of the proposed approach.

An interesting point for future work would be to extend the evaluation to include
more domains to ensure the applicability of the proposed approach in different domains.

Abbreviations
QE: Query expansion; PSO: Particle swarm optimization; IR: Information retrieval; W2v: Word to vector; TF/IDF: Term
frequency/inverse-document frequency; TF: Term frequency; pbest: Personal best value; gbest: Global best value.

Acknowledgements

This paper and the research behind it would not have been possible without the exceptional support of my supervisor,
Dr. Mossa Ghurab. His enthusiasm, knowledge and exacting attention to detail have been an inspiration and kept my
work on track. | have been extremely lucky to have a supervisor who cared so much about my work, and who responded
to my questions and queries so promptly. | would also like to thank the Dr. Ibrahim Al-Baltah for his valuable and con-
structive suggestions during the editing of this research work.

Authors’ contributions

HA took on the main role performed the literature, designed, performed experiments, analyzed data and wrote the
paper. MG supervised the research and co-wrote the paper. IA reviewed the manuscript language and helped in edit
manuscript. All authors read and approved the final manuscript.

Funding
Not applicable.

Availability of data and materials
Data will not be shared.

Ethics approval and consent to participate
Not applicable.

Consent for publication
Not applicable.

Competing interests
Not applicable.

Author details
' Computer Science Department, Sanaa University, Sanaa, Yemen. * Information Technology Department, Sanaa Univer-
sity, Sanaa, Yemen.

Received: 19 June 2019 Accepted: 22 May 2020
Published online: 29 June 2020

References

1. Atwan J, Mohd M, Rashaideh H, Kanaan G. Semantically enhanced pseudo relevance feedback for arabic information
retrieval. J Inf Sci. 2016;42(2):246-60.

2. Sadowski C, Stolee KT, Elbaum S. How developers search for code: a case study. In: Proceedings of the 2015 10th joint
meeting on foundations of software engineering. 2015. p. 191-201.

3. Jung Y, Park H, Du D-Z. An effective term-weighting scheme for information retrieval. Computer Science Technical
Report TROO8. Department of Computer Science, University of Minnesota, Minneapolis, Minnesota. 2000. p. 1-15.

4. LauT, Horvitz E. Patterns of search: analyzing and modeling web query refinement. In: UM99 user modeling. Springer;
1999. p. 119-28.

5. Carpineto C, Romano G. A survey of automatic query expansion in information retrieval. ACM Comput Surv.
2012;44(1):1-50.

6. Furnas GW, Landauer TK, Gomez LM, Dumais ST. The vocabulary problem in human-system communication. Commun
ACM. 1987;30(1 1):964-71.

7. Robertson S, Zaragoza H, Taylor M. Simple bm25 extension to multiple weighted fields. In: Proceedings of the thirteenth
ACM international conference on information and knowledge management. 2004. p. 42-9.
ALMarwi et al. J Big Data (2020) 7:39 Page 19 of 19

10.
11,

13.
14.
15,
16.

17,
18.

20.

21,

22.

23.

24,

25.

26.

2/.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

39.

 

Beil F, Ester M, Xu X. Frequent term-based text clustering. In: Proceedings of the eighth ACM SIGKDD international
conference on knowledge discovery and data mining. 2002. p. 436-42.

Shaalan K, Al-Sheikh S, Oroumchian F. Query expansion based-on similarity of terms for improving Arabic information
retrieval. In: International conference on intelligent information processing. Springer; 2012. p. 167-76.

Luhn HP. The automatic creation of literature abstracts. IBM J Res Dev. 1958;2(2):159-65.

He B, Ounis |. Term frequency normalisation tuning for bm25 and dfr models. ln: European conference on information
retrieval. Springer; 2005. p. 200-14.

ElKateb S, Black W, Rodriguez H, Alkhalifa M, Vossen P, Pease A, Fellbaum C. Building a wordnet for arabic. ln: LREC. 2006.
p. 29-34.

Gonzalo J. Sense proximity versus sense relations. GWC. 2004;2004:5.

Fellbaum C. A semantic network of english verbs. WordNet Electron Lex Database. 1998;3:153-78.

Voorhees EM. Query expansion using lexical-semantic relations. In: SIGIR’94. Springer; 1994. p. 61-9.

Gong Z, Cheang CW, Hou UL. Web query expansion by wordnet. In: International conference on database and expert
systems applications. Springer; 2005. p. 166-75.

Blei DM, Ng AY, Jordan MI. Latent dirichlet allocation. J Mach Learn Res. 2003;3UJan):993-1022.

Deerwester S, Dumais ST, Furnas GW, Landauer TK, Harshman R. Indexing by latent semantic analysis. J Am Soc Inf Sci.
1990;41 (6):391-407.

Sergienko R, Gasanova T, Semenkin E, Minker W. Collectives of term weighting methods for natural language call rout-
ing. In: Informatics in control, automation and robotics. Springer; 2016. p. 99-110.

Mikolov T, Sutskever |, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their composi-
tionality. In: Advances in neural information processing systems. 2013. p. 3111-9.

Pennington J, Socher R, Manning CD. Glove: global vectors for word representation. In: Proceedings of the 2014 confer-
ence on empirical methods in natural language processing (EMNLP). 2014. p. 1532-43.

Clerc M, Kennedy J. The particle swarm-explosion, stability, and convergence in a multidimensional complex space. IEEE
Trans Evol Comput. 2002;6(1):58-73.

Kadri Y, Nie J-Y. Effective stemming for arabic information retrieval. In: Proceedings of the challenge of arabic for NLP/MT
conference, Londres, Royaume-Uni. 2006. p. 68-74.

Boudchiche M, Mazroui A, Bebah MOAO, Lakhouaja A, Boudlal A. Alkhalil morpho sys 2: a robust arabic morpho-syntac-
tic analyzer. J King Saud Univ Comput Inf Sci. 2017;29(2):141-6.

Pasha A, Al-Badrashiny M, Diab MT, El Kholy A, Eskander R, Habash N, Pooleery M, Rambow O, Roth R. Madamira: a fast,
comprehensive tool for morphological analysis and disambiguation of arabic. LREC. 2014;14:1094—-101.

Al-Serhan H, Ayesh A. A triliteral word roots extraction using neural network for arabic. In: 2006 International conference
on computer engineering and systems. IEEE; 2006. p. 436-40.

Elayeb B, Bounhas I. Arabic cross-language information retrieval: a review. ACM Trans Asian Low-Resour Lang Inf Process.
2016;15(3):1-44.

Bai J, Nie J-Y, Cao G, Bouchard H. Using query contexts in information retrieval. In: Proceedings of the 30th annual inter-
national ACM SIGIR conference on research and development in information retrieval. 2007. p. 15-22.

Shen X, XuY, Yu J, Zhang K. Intelligent search engine based on formal concept analysis. In: 2007 IEEE international
conference on granular computing (GRC 2007). IEEE; 2007. p. 669

Froud H, Lachkar A, Ouatik SA. Stemming versus light stemming for measuring the similarity between arabic words with
latent semantic analysis model. In: 2012 colloquium in information science and technology. IEEE; 2012. p. 69-73.
Yokoyama A, Klyuev V. Search engine query expansion using Japanese wordnet. In: 2010 3rd international conference
on human-centric computing. IEEE; 2010. p. 1-5.

Alzahrani SM, Salim N. On the use of fuzzy information retrieval for gauging similarity of Arabic documents. In: 2009
second international conference on the applications of digital information and web technologies. IEEE; 2009. p. 539-44.
Chauhan R, Goudar R, Sharma R, Chauhan A. Domain ontology based semantic search for efficient information retrieval
through automatic query expansion. In: 2013 international conference on intelligent systems and signal processing
(ISSP). IEEE; 2013. p. 397-402.

Zhai J, Zhou K. Semantic retrieval for sports information based on ontology and sparal. In: 2010 international conference
of information science and management engineering, vol. 1. IEEE; 2010. p. 395-8.

Khan HU, Saqlain SM, Shoaib M, Sher M. Ontology based semantic search in holy quran. Int J Fut Comput Commun.
2013;2(6):570.

Hong L. A tutorial on probabilistic latent semantic analysis. 2012. arXiv preprint arXiv:1212.3900.

Zhou G, HeT, Zhao J, Hu PR. Learning continuous word embedding with metadata for question retrieval in community
question answering. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the
7th international joint conference on natural language processing (Vol. 1: Long Papers). 2015. p. 250-9.

Zhang M, Liu Y, Luan H, Sun M, Izuha T, Hao J. Building earth mover's distance on bilingual word embeddings for
machine translation. In: Thirtieth AAAI conference on artificial intelligence. 2016.

Diaz F, Mitra B, Craswell N. Query expansion with locally-trained word embeddings. 2016. arXiv preprint arXiv
:1605.07891.,

Roy D, Paul D, Mitra M, Garain U. Using word embeddings for automatic query expansion. 2016. arXiv preprint arXiv
:1606.07608.

Liu Q, Huang H, Lut J, Gao Y, Zhang G. Enhanced word embedding similarity measures using fuzzy rules for query
expansion. In: 2017 IEEE international conference on fuzzy systems (FUZZ-IEEE). IEEE; 2017. p. 1-6.

Khoja S. Stemming arabic text. Lancaster: Computing Department, Lancaster University; 1999.

Jansen BJ, Booth DL, Spink A. Determining the informational, navigational, and transactional intent of web queries. Inf
Process Manage. 2008;44(3):125 1-66.

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
