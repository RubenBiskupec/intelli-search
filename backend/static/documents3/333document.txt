Automated Software Engineering (2020) 27:229-263
https://doi.org/10.1007/s10515-020-00274-7

m)

Check for
updates

Modeling user concerns in Sharing Economy: the case
of food delivery apps

Grant Williams! - Miroslav Tushev! - Fahimeh Ebrahimi! - Anas Mahmoud! ®

Received: 26 September 2019 / Accepted: 28 July 2020 / Published online: 9 August 2020
© Springer Science+Business Media, LLC, part of Springer Nature 2020

Abstract

Sharing Economy apps, such as Uber, Airbnb, and TaskRabbit, have generated a
substantial consumer interest over the past decade. The unique form of peer-to-peer
business exchange these apps have enabled has been linked to significant levels of
economic growth, helping people in resource-constrained communities to build
social capital and move up the economic ladder. However, due to the multidimen-
sional nature of their operational environments, and the lack of effective methods
for capturing and describing their end-users’ concerns, Sharing Economy apps
often struggle to survive. To address these challenges, in this paper, we examine
crowd feedback in ecosystems of Sharing Economy apps. Specifically, we present a
case study targeting the ecosystem of food delivery apps. Using qualitative analysis
methods, we synthesize important user concerns present in the Twitter feeds and
app store reviews of these apps. We further propose and intrinsically evaluate an
automated procedure for generating a succinct model of these concerns. Our work
provides a first step toward building a full understanding of user needs in ecosys-
tems of Sharing Economy apps. Our objective is to provide Sharing Economy app
developers with systematic guidelines to help them maximize their market fitness
and mitigate their end-users’ concerns and optimize their experience.

Keywords Sharing Economy - Domain modeling - Mobile applications

1 Introduction

The recent decade has witnessed a major shift in the way people deal services
and goods. This shift has been enabled by the emergence of a new form of busi-
ness exchange, known as Sharing Economy (SE). Unlike conventional business

bh) Anas Mahmoud
mahmoud @csc.lsu.edu

Grant Williams
grwillia@ microsoft.com

! Microsoft, Redmond, WA 98052, USA

Q) Springer
230 Automated Software Engineering (2020) 27:229-263

models, SE is focused on providing access to—rather than ownership of—
assets and resources via peer-2-peer (P2P) coordination (Martin 2016). This on-
demand, convenient, and sustainable form of resource consumption has attracted
consumers and investors around the globe. As of today, there are hundreds of
SE platforms, enabling consumers to sell, rent, swap, lend, and borrow services
and assets at unprecedented scales. According to PwC—the multinational profes-
sional services network—SE is projected to grow from 15 billion U.S. dollars in
2014 to close to 335 billion U.S. dollars by 2025 (PwC 2015).

Earlier adaptations of digital SE can be traced back to the early days of the
Internet. Ebay and Craigslist, both launched in 1995, are prominent examples
of platforms that enabled a collaborative P2P circulation of services and assets.
However, the real proliferation of SE can be attributed to mobile technology.
Global networks of mobile devices has created an ideal environment for SE appli-
cations (apps) to thrive and reach the mainstream culture. Using mobile apps as a
mediator, services such as Uber (ridesharing), Airbnb (lodging), and TaskRabbit
(freelancing) paved the way for a new form of disruptive innovations that trans-
formed the way we do business forever.

Despite their proven benefits, SE apps often struggle to survive. In addition to
competing with each other (e.g., multiple apps providing the exact same service
in the same geographical area), these apps have to compete with existing classi-
cal markets in ecosystems of finite resources (e.g., taxi, hotel, and retail indus-
tries) (Dogru et al. 2019). To survive this fierce competition, SE apps have to be
in a constant state of innovation, driven by a deep knowledge of their end-users’
expectations, preferences, and needs (Dillahunt et al. 2017). However, this knowl-
edge is often tacit, embedded in the complex interplay between the user, system,
and market components of the ecosystem of operation. In order to be effectively
utilized, such knowledge must be translated into an explicit form, or external-
ized (Nonaka 1994). Once domain knowledge is externalized, it can be used to
instantiate and sustain innovation (Glassey 2008).

To address these challenges, in this paper, we propose an automated approach
for modeling crowd concerns in ecosystems of SE apps. We define a user concern
as any functional or non-functional behavior of the app that might negatively
impact its users’ experience or their overall well-being. This abstract definition
includes any technical (bugs or crashes) or nontechnical (service, economic,
or social) issues that consumers of mobile apps may experience. The proposed
approach is demonstrated through a case study targeting the ecosystem of food
courier, or delivery, apps. These apps, typically classified in popular app stores
under the Food and Drink category, form a uniquely complex and dynamic eco-
system that consists of food consumers, drivers, and restaurants, operating in an
extremely competitive environment and under strict business and technological
constraints. The goal of our case study is to demonstrate how such a complex
ecosystem can be automatically analyzed and modeled. Specifically, our contribu-
tions in this paper can be described as follows:

e We qualitatively analyze a large dataset of user feedback collected from the
Twitter feeds and app store reviews of four prominent food delivery apps. Our

Q) Springer
Automated Software Engineering (2020) 27:229-263 231

objective is to understand and classify the main pressing user concerns in the
ecosystem of these apps.

e We propose, formally describe, and evaluate a fully automated procedure for
modeling user concerns in the ecosystem of food delivery apps along with their
main attributes and triggers. The generated model is intended to provide SE app
developers with a framework for assessing the fitness of their mobile apps and
understanding the complex realities of their ecosystem.

The remainder of this paper is organized as follows. Section 2 provides a brief
background of existing related research, motivates our work in this paper, and pre-
sents our research questions. Section 3 describes our qualitative analysis. Section 4
proposes an automated procedure for extracting and modeling crowd concerns in
the ecosystem of food delivery apps. Section 5 discusses our key findings and their
impact. Section 6 describes the main limitations of our study. Finally, Sect. 7 con-
cludes the paper and describes our future work.

2 Background, rationale, and research questions

In this section, we provide a brief summary of seminal related research, motivate
our work, and present our research questions.

2.1 Background: Sharing Economy

The research on SE has become a prominent subject of research across multiple dis-
ciplines (Dillahunt et al. 2017; Hossain 2020). This can be explained based on the
interdisciplinary nature of the problems often raised in this domain. In general, the
research on SE can be categorized into five main categories:

e Economic: Recent research revealed that adapting solutions of SE can foster
economic growth in big cities and local communities (Cheng 2016; Zhu et al.
2017). Specifically, SE can help to counter excessive spending and purchase hab-
its (Hiittel et al. 2018) while generating new sources of revenue (Matzler et al.
2015). However, major concerns are frequently raised about the impact of this
new business model on traditional long-established markets, affecting the reve-
nue and business practices of these markets and threatening to put millions (e.g.,
taxi drivers and employees in the hotel industry) out of work by making their
jobs obsolete (Aznar et al. 2017; Zervas et al. 2017).

e Social: Existing research often describe SE as a vehicle for building social capi-
tal and establishing social relationships within local communities (Benkler 2017;
Tussyadiah and Pesonen 2016). However, on the negative side, SE has paved
the way for a new form of social challenges, including problems such as digital
discrimination, which refers to scenarios where a business transaction is influ-
enced by race, gender, age, or other aspects of appearance of service provider or
receiver (Edelman and Luca 2014). For instance, a recent report by the National

Q) Springer
232 Automated Software Engineering (2020) 27:229-263

Bureau of Economic Research found that black riders using Uber waited 30%
longer to be picked up (Ge et al. 2017). Another study reported that non-black
Airbnb hosts were able to charge 12% more than black hosts (Edelman and
Luca 2014).

e Environmental: Several studies suggest that SE promotes environmental aware-
ness by enabling more sustainable consumption practices in modern-day socie-
ties (Ala-Mantilaa et al. 2016; Bonciu and Balgar 2016). Other studies argue that
this impact is not as substantial, suggesting that environmental factors are not as
important for consumers as economic factors (Acquier et al. 2017). In fact, some
other studies went even further to suggest that SE can lead to more environmen-
tal pressure and resource exploitation due to the more affordable alternatives it
provides (Tussyadiah and Pesonen 2016).

e Legal: This category of studies investigate existing regulations and suggest new
regulatory infrastructures for protecting users of SE platforms from unwanted
business practices. The main objective is to propose legislation to regulate the
relationship between the app (e.g., Uber or Airbnb), service providers (e.g., driv-
ers Or apartment owners), and service receivers (e.g., riders or renters) (Bond
2014; Murillo et al. 2017), especially when the terms-of-service are somehow
violated, such as in cases of drunk drivers, under-insured cars, unsafe apart-
ments, and fraud (Cannon and Summers 2014).

e Computing: In computing, studies of SE often tackle the problem from an algo-
rithmic and humancomputer interaction (HCI) perspectives (Dillahunt et al.
2017). Algorithmic papers are mainly concerned with proposing new and more
efficient algorithms for P2P matching, path planning in ride-sharing (Chow and
Yuan Yu 2015; He et al. 2012), platform fairness (Bistaffa et al. 2015; Thebault-
Spieker et al. 2015, 2017), and pricing (Bistaffa et al. 2015). HCI related study,
on the other hand, propose design solutions to optimize user experience (Dilla-
hunt and Malone 2015), including protecting their privacy (Goel et al. 2016; Xu
et al. 2017) and safety (Bellotti et al. 2015) and understanding their usage pat-
terns and motivations to participate in SE (Zhu et al. 2017).

2.2 Background: mining mobile app user feedback

The research on mining mobile app user feedback has noticeably advanced in the
past few years. The objective of this line of research is to help software developers
infer their end-users’ needs, detect bugs in their code, and plan for future releases of
their apps. In general, two main channels of feedback are often considered: app store
reviews and Twitter.

e App store reviews: A systematic survey of studies related to app store review
analysis 1s provided in Martin et al. (2017). In general, this line of research pro-
poses new tools [e.g., AR-Miner (Chen et al. 2014), MARA (Iacob and Harrison
2013), MARC (Jha and Mahmoud 2018), and CLAP (Villarroel et al. 2016)],
methods, and procedures for analyzing user reviews available on Google Play
and the Apple App Store. The main objective is to capture any actionable main-

Q) Springer
Automated Software Engineering (2020) 27:229-263 233

tenance requests in these reviews, such as bug reports and feature requests as
well as non-functional requirements concerns, such as usability, reliability, secu-
rity, and privacy (Groen et al. 2017; Jha and Mahmoud 2018).

To automatically identify informative user reviews, reviews are typically clas-
sified using standard text classification techniques, including Naive Bayes (NB),
Support Vector Machines (SVM), Random Forests (RF), and Decision Trees
(DT) (Jha and Mahmoud 2018; Panichella et al. 2015) as well as clustering algo-
rithms such as DBSCAN (Villarroel et al. 2016). Simpler techniques, which rely
on linguistic pattern and term matching have also been proposed in the litera-
ture (Guzman and Maalej 2014; Iacob and Harrison 2013; Panichella et al. 2015).
In terms of modeling, techniques such as Latent Direchlet Allocation (LDA), are
commonly used to infer meaningful high-level topics from reviews (Chen et al.
2014; Guzman and Maalej 2014). Text processing techniques, such as sentiment
analysis, lemmatization, and part of speech tagging, are also commonly used to
improve the accuracy of review classification and modeling techniques (Carrefio
and Winbladh 2013; Maalej and Nabil 2015; Mcllroy et al. 2016; Williams and
Mahmoud 2017). In addition, meta-data attributes of user reviews, such as their
star rating and author information, are used to improve the predictive capabilities
of review classifiers (Khalid et al. 2015; Maalej and Nabil 2015).

e Twitter: Twitter enables large populations of end-users of software to publicly
share their experiences and concerns about their apps in the form of micro-
blogs. Analysis of large datasets of tweets collected from the Twitter feeds of
software systems revealed that around 50% of collected tweets contained action-
able maintenance information (Williams and Mahmoud 2017). Such informa-
tion was found to be useful for different groups of technical and non-technical
stakeholders (Guzman et al. 2017), providing complementary information to
support mobile app developers during release planning tasks. The results also
showed that text classifiers, such as SVM and NB, summarization methods, such
as Hybrid TF.IDF and SumBasic, and modeling methods, such as LDA, can be
effectively used to categorize, summarize, and cluster software-related tweets
into semantically related groups of technical feedback (Williams and Mahmoud
2017).

2.3 Research gap and motivation

Our review shows that systematically analyzing and synthesizing user feedback at
a domain level can help app developers to critically evaluate the current landscape
of competition and to understand their end-users’ expectations, preferences, and
needs (Coulton and Bamford 2011; Finkelstein et al. 2014; Harman et al. 2012; Pal-
omba et al. 2018; Svedic 2015). Understanding the domain of competition is criti-
cal for the survival of SE apps. Specifically, the clusters of functionally-related SE
apps form distinct micro-ecosystems within the app store ecosystem. A software
ecosystem can be defined as a set of actors functioning as a unit and interacting with
a shared market for software and services, together with the relationships among
them (Jansen et al. 2009).

Q) Springer
234 Automated Software Engineering (2020) 27:229-263

However, the majority of existing research on mining crowd feedback in the
mobile app market is focused on individual apps, with little attention paid to how
such information can be utilized and integrated to facilitate software analysis at an
ecosystem, or application domain, level (Martin et al. 2017; Panichella et al. 2015).
Extracting concerns at a domain level can be a more challenging problem than
focusing on single apps, which typically receive only a limited number of reviews
or tweets per day (Mcilroy et al. 2017). Furthermore, existing crowd feedback min-
ing techniques are calibrated to extract technical user concerns, such as bug reports
and feature requests, often ignoring other non-technical types of concerns that origi-
nate from the operational characteristics of the app (Jha and Mahmoud 2019; Mar-
tin et al. 2017). These observations emphasize the need for new methods that can
integrate multiple heterogeneous sources of user feedback to reflect a more accu-
rate picture of the ecosystem. Jo bridge the gap in existing research in this paper,
we present a case study on modeling crowd feedback in ecosystems of SE apps.
Our case study targets the ecosystem of food delivery apps. Emerging evidence has
shown that, unlike other SE apps, the demand for food delivery services has sig-
nificantly increased after the COVID-19 shelter-in-place order (Chen et al. 2020).
In fact, according to The New York Times, while use of Uber’s ride-sharing ser-
vice went down by 80% in April of 2020, UberEats has experienced 89% increase in
demand (Conger and Griffith 2020). This makes food delivery a particularly inter-
esting SE domain to be targeted by our analysis.

2.4 Case study setup and research questions

The first major food courier service to emerge was Seamless, in 1999. A product of
the internet boom, seamless allowed users to order from participating restaurants
using an online menu, a unique innovation that granted the service considerable
popularity. Following seamless, Grubhub was also met with success when it began
offering web-based food delivery for the Chicago market in 2004. As smart phones
became more popular, a number of new food couriers took advantage of the new
demand for a more convenient mobile app-based delivery services. Of these com-
petitors, UberEATS rose to the top, leveraging their experience with ride-sharing to
adapt to food delivery. By the end of 2017, UberEATS became the most downloaded
food-related app on the Apple App Store.

The set of food delivery apps along with their consumer (e.g., restaurant patrons
and drivers) and business (e.g., restaurants) components represent a uniquely com-
plex and dynamic multi-agent ecosystem. This complexity imposes several chal-
lenges on the operation of these apps. These challenges, which can also be often
found in other SE ecosystems, can be described as follows:

e Fierce competition: users often have multiple services to choose from within a
given metropolitan area. Switching from one app to another is trivial, and users
are highly impatient with late or incorrect orders. For instance, food delivery ser-
vices have less than one hour for delivery. This forces developers to constantly
innovate to provide faster delivery than their rivals.

Q) Springer
Automated Software Engineering (2020) 27:229-263 235

e Decentralized fulfillment: the drivers are generally independent contractors
who choose whom to work for and when to work. This creates challenges,
not only for job assignment, but also for predicting when and where human
resources will become available.

e Multi-lateral communication: in order to fulfill an order, the delivery app
must communicate with users, drivers, and restaurants to ensure that the
food order is ready when the driver arrives, and that the user knows when to
expect delivery. Each channel of communication presents an opportunity for
failure.

The main objective of our analysis is to demonstrate the feasibility of auto-
matically generating an abstract conceptual model of user concerns in such a
dynamic and complex ecosystem. Such model is intended to provide systematic
technical and business insights for app developers as well as newcomers trying
to break into the SE market. To guide our analysis, we formulate the two follow-
ing research questions:

e RQ: What types of concerns are raised by users of food delivery apps?
Mobile app users are highly vocal in sharing suggestions and criticism.
Understanding this feedback is critical for evaluating and prioritizing poten-
tial changes to software. However, not all concerns, especially in business-
oriented apps, are technical in nature. Therefore, developers must also be
aware of business discussions, such as talk of competitors, poor service, or
issues with other actors in their ecosystems. Therefore, the first phase of our
analysis is focused on systematically externalizing and classifying crowd
feedback available in the Twitter feeds and app store reviews of food delivery
apps.

e RQ,: How can user concerns in the ecosystem of food delivery apps be auto-
matically and effectively modeled? The second phase of our analysis 1s focused
on automatically externalizing and modeling user concerns in the ecosystem
of food delivery apps. Modeling such information can provide valuable infor-
mation for SE app developers, enabling them to discover the most important
user concerns in their ecosystem, along with their defining attributes and trig-
gers. To answer this question, we propose an automated procedure for generat-
ing a new form of user feedback analysis models and we compare its perfor-
mance to LDA, a commonly used technique for generating topics of app user
concerns from online user feedback.

3 Qualitative analysis

To answer our first research question (RQ,), in this section, we qualitatively
analyze a large dataset of app store reviews and tweets, sampled from the crowd
feedback of four popular food delivery apps. In what follows, we describe our
data collection process as well as the main findings of our analysis.

Q) Springer
236 Automated Software Engineering (2020) 27:229-263

3.1 Data collection

In order to determine which apps to include in our case study, we used the top charts
feature of the Apple App Store and Google Play. These charts keep the public aware
of the top grossing and downloaded apps in the app store. As of September of 2018,
UberEats is the most popular food delivery app on the App Store. Among the top ten
apps in the Food category, there are three additional competing delivery apps: Door-
dash, GrubHub, and PostMates. If we broaden our focus to the top twenty-five apps,
only one additional food delivery app is found, Eat24. Eat24 was recently acquired
by GrubHub, and have redirected users to their parent app, allowing us to exclude
it from the analysis.! The Google Play Store shows the top 25 most popular apps
in an arbitrary order. However, we find that UberEats and its three main competing
apps are also present within the top 25. Therefore, the apps UberEats, Doordash,
GrubHub, and PostMates covers the most popular food delivery services available
on both platforms.

It is important to point out that there are several other food delivery apps in the
app market. These apps often operate in very limited geographical areas or have
smaller user base. In our analysis, we are interested in apps with the biggest market
share (as quantified by their app store download numbers), thus we narrowed down
our ecosystem to its fittest elements from a user perspective. Popular apps receive
significantly more crowd feedback on app stores and social media in comparison to
smaller apps (Mcilroy et al. 2017). Furthermore, selecting mature apps gives smaller
and newcomer apps a chance to learn from the mistakes of the big players in the
market (Pagano and Maalej 2013).

After the list of apps is determined, the second step in our analysis is to identify
and classify the main user concerns in the ecosystem. Prior research has revealed
that software-relevant feedback can be found in tweets and app store reviews (Maale}
and Nabil 2015; Panichella et al. 2015; Sorbo et al. 2016; Williams and Mahmoud
2017). To extract reviews, we used the free third-party service AppAnnie.” This
service allows reviews up to 90 days old to be retrieved from Google Play and the
Apple App Store.

To collect tweets, we limited our search to tweets directed to the Twitter account
of our apps. For example, to retrieve tweets associated with UberEats, we searched
for to: ubereats. Our previous analysis has revealed that this query form yields
a large rate (roughly 50%) of meaningful technical feedback among the resulting
tweets (Williams and Mahmoud 2017). In our analysis, we collected tweets in the
period from September 4th to December 4th of 2018. In total, 1833 tweets, 13,557
App Store reviews, and 29,674 Google Play reviews were extracted. Table | sum-
marizes our dataset. Collecting data from multiple sources of feedback (multiple app
stores and twitter) and over a long period of time is necessary to minimize any sam-
pling bias that may impact the validity of the analysis (Martin et al. 2015).

 https://www.eat24.com/.
2 https://www.appannie.com/en/.

Q) Springer
Automated Software Engineering (2020) 27:229-263 237

Table 1 Experimental data,

. . App Tweets Apple app store Googleplay Total

including the number of posts

(tweets and reviews) collected Doordash 344 6685 5273 12,302

from each channel of user

feedback GrubHub 414 1058 2863 4335
Postmates 450 1467 2820 4737
UberEats 625 4347 18,718 23,690

3.2 Analysis

To conduct our qualitative analysis, we sampled 900 posts (300 tweets, 300 10S
reviews, and 300 Android reviews) from the data collected for each app in our
domain. Sampling 900 posts from the population of posts for each app ensures a
confidence level of 99%. To perform the sampling, we developed a Ruby program to
first execute a shuffle () method on the lists of tweets and reviews to randomize
the order, taking the time of the post into consideration to avoid selecting posts from
the same time period (e.g., tweets from one week only). The first 300 posts from
each source of user feedback were then selected.

To manually classify our data, we followed a systematic and iterative coding pro-
cess. Specifically, three judges participated in the data classification process. The
judges have an average of three years of industrial software engineering experience.
For each post (tweet and review), each judge had to answer three main questions:
(a) does the post raise any concerns (informative vs. uninformative)?, (b) what is
the broad issue raised in the post?, and (c) what is the specific concern raised in the
post? The manual classification process was carried over four sessions, each ses-
sion lasted around 6 h, divided into two periods of three hours each to avoid any
fatigue issues and to ensure the integrity of the data (Wohlin et al. 2012). A final
meeting was then held to generate the main categories of concerns as they appeared
in the individually classified data. Conflicts were detected in less than 5% of the
cases, mainly on the granularity level of the classification. For example, concerns
about refunds and promo codes were considered two separate categories by one
judge, while another judge classified them under the same concern category (money
issues). Such conflicts were resolved after further discussion and eventually using
majority voting. In what follows, we describe the results of our qualitative analysis
in greater detail.

3.3 Results

A post during our manual classification task was considered informative if it raised
any form of user concerns. The rest of the reviews were considered miscellaneous.
Posts containing spam (e.g.,“#UberEats Always late!! Check bit.ly/IxTaYs’’) or
context-free praise or insults (e.g., “I hate this app!” and “This app is great!’”’) were
also considered irrelevant. In general, the following general categories and sub cat-
egories of concerns were identified in the set of informative posts:

Q) Springer
238 Automated Software Engineering (2020) 27:229-263

e Business concerns: This category includes any concerns that are related directly
to the business aspects of food delivery. In general, these concerns can be subdi-
vided into two main subcategories:

e Human: these concerns are related to interactions with employees of the apps.
Users often complained about orders running late, cancellations, restaurant
workers being rude, and drivers getting lost on the way to delivery. Human
related reviews were on average the longest (30 words), often narrating multi-
paragraph sequences of human (mainly driver) failures that led to undesirable
outcomes.

e Market: the apps in our dataset generally make money either through flat-
rate delivery charges or surcharges added to the price of individual menu
items. Users are highly sensitive to the differences between what they would
pay at the restaurant versus at their doorstep. Posts complimenting low fees
and markups were rare. Price complaints were not the only form of market-
related feedback. Other posts included generic discussions of market-related
concerns such as business policy (such as refunds), discussion of competi-
tors, promotions, and posts about participating restaurants and delivery zones.
Requests for service in remote areas were fairly common too.

e Technical concerns: This set of concerns includes any technical issues that are
related to the user experience when using the app itself. As have been shown
before (Maalej and Nabil 2015), technical concerns often revolve around two
subcategories:

e Bug reports: Posts classified under this category contain descriptions of soft-
ware errors, or differences between the described and the observed behaviors
of the app. Bug reports commonly consist of a simple narration of an app fail-
ure. In our dataset, we observed that the most common bugs were related to
payments (174 out of 533) while crashes and service outages counted for 53
posts.

e Feature requests: These posts contain requests for specific functionality to
be added to the app, or discussions of success/failure of distinct features.
For example, some users of DoorDash complained about being forced to tip
before the order was delivered. Users of Eat24 lament a recent update which
removed the ability to reorder the last meal requested through the app. Under
this category, we also include non-functional requirements (NFRs), or aspects
of software which are related to overall utility of the app rather than its func-
tional behavior (e.g., usability, reliability, security, and accessibility) (Cle-
land-Huang et al. 2005; Glinz 2007). Ease-of-use was the most common NFR
cited by users, followed by user experience (UX).

In terms of specific concerns, nine different concerns were identified: driv-
ers, customer service, refund, service outage, promo code, communication,
security, routing, and order. Thorough descriptions as well as examples of these
concerns are shown in Table 2. In Table 3, we show the number of posts clas-
sified under each category of concerns in the sampled dataset. In general, our
qualitative analysis revealed that, based on the total number of relevant posts,

Q) Springer
239

Automated Software Engineering (2020) 27:229-263

.poeq Jouyny Surysnd
jdoy SUIT) poyeUT]sd oY) pue MOpUIM AIOATTIp oy} puocog INoY ue se
yey} AJOAT[Op & jNoge ‘punore ABM J9yIO 9Y} OU ‘qnyqnAsH# 19eJUOD 0} pry J,,
S|GIPOUT SVM I DLT OS DUBS POO} dL], ‘UOTIDOIIp atsoddo ajoyd
-W09 9} UI SAOIP UDY} ‘SUOTIDAIIP OIS V JO} OU Yse 0} pvy ISO] 103 JOATIC,,
_junosoe AU UO SUTIapsO [Is oe ATJUNOD 9} JOAO
[je oJdood pue promssed Aw jasos [ ‘poyory sem yuNodoe Aur SoyeU}sog @,,
.JUSTUO} poo} 393 JUD
] Mou pue Japso Au [IDURD 0} pRY | OS pasoyTs dovyd ay} Jo onjord v ou
puos 0} aJeuNsod AW JO} $7 JUads sontq Aqeq polapsO | OS soJVUNSOgo,,

.S[QISTTO JOU SBM | JY) SuTkes ApOJeINDOvUT ‘poyoofal seM Ipod OWO.d sy,
_O8eINO UP IOF SUTUT} JVIIDH,, pue , jUMOP IIB SIOAIOS OU I,,,

_jueineysor oy} woIy ABMe UTUT ¢ SAT] [ UOYM Aroq
-qnJ pu P[OD POATIIV Jey} POO} JOF punjol B OU 393 0} B[GQvuUN o19M ADT,

«AT PUY 0} 9JSOOH Yse Oo} pey |
‘puUNO] 9q 0} SIOYMOU ST JOQUINU JORIUOD IY], jSOIAIS SITY} ISN J9A9 JOU OG,,

_ OUI Je PINoYs pur JUdIIST][Iq SVM ‘SUOTJONISUT JOPIO OY} MOTLOF
JOU pIp JOATIP OY], YSeGI00gd@,, pue Jo] Surysed yuoroyIp ATa}0Tdur09
B 0} JOALIP 9} YOO) syeqJogn# 0} Daves | Jey} Ssorppe oy} ‘UOTIppe U,,—

ysod oydurexy

yorq MopuIM AJOATTOp ay} poysnd ATTenpess Aoy} “Tow0}sNd 9} 119Te
ULY} JOYIVI PUL ‘IOPIO UL 0} JOATIP B IJNOI 0} PITT} SIOTAIIS ‘SOUUTOUIOS JOplO

pouaddey sity} uoym josdn oJam siasn Aueyy ‘dyoy JoF srosn yse 0}
SIOALIP Sutsned ‘pores sdde S1darIp oy} Ur sud}sAS SqH ay} AT[BUOISeIDO SUIINOY

S]UNODDB ITI} 0} sosieYyoO poured
-xoun poyJodar siosn [eIdAdg “‘UOUWOD ATSuIsSTIdIns o19M SIOIIO AVLINIIS AyLamnse¢

uorye1odo0-Jo-snoy
pue suroj nuow SuIpresal ATTeroadso 4uBINe}sSoI OY} puv IdTAIaS AIOATTOP
oY} UIOMJOg UONVITUNWIUOD poyIe} WOIF poyeuIst1o ATUOWUIOD ssng  uUOTROTUNUTUOD
Aj\901109
SIOpsO 0} pordde Sutog jou sopoo uonowOld sem yodal Sng UOWIUIOS WY Ipod OWOIg

ure[duros 0}
BIPOUW [VIDOS 0} PoUN} ATOJIPOUUUT SIASN ‘UMOP SBM DOIAIIS B IOAQUIY AA, 93RINO DOIAIIS

ow AIOAT[OP SUOCT 0} ONP I[QIPOUT poapUdI SBM POOF
OY} Jl UDA “JOpIO OY} SuIpNyoxs ‘asseyo AIOAI[IP IY} JOJ Spunysor posayjo
ATUO AT[VIOUIS SIOTAIIS JY} IOAODSIP 0} PoyeYSNIF UdIJO SIOM SIIS) punjoy

SIOMSUP BATIIAI 0} YOO} II SUT MOY puke sJOquIOUT
QOIAIOS JO SSOUT[PUSTIF OY) YIM UOTORIsHessip passoaidxa ATUOUIUOD SIOS~]  SOTAIJOS JawIO}sND

uon
-ddlIp SUOIM OY} JUIM SIOATIP UOYM Josdn ATTeID0dsa 319M SIS) “SOUT
WEM SUOT SUISND ‘sIOpJO PoUTqUIOD JO ‘ATUSTOYFoUT poyoyedsip a13aM
SIOATIP ‘ATTBOYIOOdS “SIOALIP YUM SVM WdTGOId UOWIUOS JSOW 9I[SUIS OU], SIOATICY

uondiiosaq uia0u07Z

sdde AIDAI[IP POOF JO Wd}SASOI9 9} UI SUIIOUOD JasN JO UOTBOYISsv[d pouleis-ouy YW 7 aqeL

pringer

DS
240 Automated Software Engineering (2020) 27:229-263

Table 3 The number of posts
(tweets and reviews) classified
under each category of user

Tweets 10S Android Total Length

 

 

 

 

Human 443 649 276 1368 30
feedback
Market 392 563 340 1295 28
Business 704 931 522 2157 27
Bug 244 175 114 533 27
Feature 54 106 77 237 24
Technical 292 258 186 736 25
Length is the average number of words in the posts classified under
each category
300
250
200 [ |
WM
Y
WM
}
Oy 150
ey
3
oD
a 100
q
s
ZA, 50 ]

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

UberEATS GrubHub DoorDash Postmates

Fig.1 The distribution of concern categories for each app. Y-axes is the number of posts (reviews and
tweets)

Android reviews were the least informative in comparison to other sources of
feedback. One potential explanation for this phenomenon is that Google Play
does not pose any restriction on the number of times an app can request users to
leave a review for the app, while the Apple App Store limits app in this respect.
As aresult, many Android reviews were terse, with statements such as “I’m only
posting this because the app keeps nagging me” being common.

Finally, the results also show that the distribution of concerns over the apps
was almost the same. As Fig. | shows, concern types spread almost equally
among apps, highlighting the similarity between the apps in their core features
and user base. It is important to point out that our identified categories were
considered orthogonal: each post could be any combination of human, market,
bug, and feature issues. Therefore, there was considerable overlap between cat-
egories. This overlap is shown in Fig. 2.

Q) Springer
Automated Software Engineering (2020) 27:229-263 241

Feature Market

eye

Fig.2 A Venn diagram of the distribution of classification labels and their overlap in the dataset. For
example, the diagram shows that there is 1368 unique Human-related concerns. Among those, 506 were
also classified as Market concerns, 23 as Human, Market, and Feature, 137 as Human and Feature, 16
as Human, Bug, and Feature, 57 as Human, Market, and Bug, 142 as Human and Bug, and 8 as Human,
Market, Feature, and Bug

Bug Human

4 Modeling crowd feedback

In the first phase of our analysis, we qualitatively analyzed a large dataset of
crowd feedback, sampled from the set of app store reviews and tweets directed
to the apps in our ecosystem. Our results showed that user concerns tend to over-
lap and extend over a broad range of technical and business issues. Furthermore,
these concerns tend to spread over multiple feedback channels and apps in the
domain, which makes it practically infeasible to collect and synthesize such feed-
back manually. This emphasizes the need for automated tools that developers can
use to make sense of such data. To address these challenges, our second research
question in this paper (RQ,) aims at proposing automated methods for generat-
ing representative models of the data. To answer this question, we first investi-
gate the performance of LDA as one of the most commonly used topic modeling
techniques in app user feedback analysis (Chen et al. 2014; Gomez et al. 2015;
Guzman and Maalej 2014; Iacob and Harrison 2013). We then propose a novel
frequency-based approach for generating more expressive models of the data. The
performance of both techniques is evaluated based on their ability to capture the
main concerns of food delivery app users as well as their main attributes and trig-
gers (Table 2).

Q) Springer
242 Automated Software Engineering (2020) 27:229-263

 

054 0.255
052 0.25
© 0.5 ©
3 3
A 048 A 0.245
8 8
& 0A6 5
S S 0.24
6 044 6
oO oO
0A2 0.235
0.4
038
Ly
2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20
The number of topics The number of topics
(a) LDA (b) Assisted LDA

Fig.3 The impact of the number of topics on the coherence score

4.1 Baseline: modeling user concerns with LDA

Introduced by Blei et al. (2003), LDA is an unsupervised probabilistic approach
for estimating a topic distribution over a text corpus. A topic consists of a group
of words that collectively represents a potential thematic concept (Blei et al.
2003; Hofmann 1999). Formally, LDA assumes that words within documents are
the observed data. The known parameters of the model include the number of
topics k, and the Dirichlet priors on the topic-word and document-topic distribu-
tions f and a. Each topic f¢, in the latent topic space (t, € T) is modeled as a mullti-
dimensional probability distribution, sampled from a Dirichlet distribution f, over
the set of unique words (w; € W) in the corpus D, such that, ¢,,,, ~ Dirichlet(f).
Similarly, each document from the collection (d; € D) is modeled as a probability
distribution, sampled from a Dirichlet distribution @ over the set of topics, such
that, 0, ~ Dirichlet(a). 6,4 and @,,, are inferred using approximate inference
techniques such as Gibbs Sampling (Griffiths and Steyvers 2004). Gibbs sampling
creates an initial, naturally weak, full assignment of words and documents to top-
ics. The sampling process then iterates through each word in each document until
word and topic assignments converge to an acceptable (stable) estimation (Blei
et al. 2003).

4.1.1 Topic extraction

We use Gensim® to extract topics from our dataset of user posts (reviews and
tweets) (Rehurek and Sojka 2010). Gensim is a Python-based open-source toolkit
for vector space modeling and topic modeling. We apply lemmatization and stop-
word removal on the posts to enhance the quality of generated topics. For lemma-
tization we use the spaCy library for Python* and to remove stop-words we use

> https://radimrehurek.com/gensim/.
* https://spacy.io.

Q) Springer
Automated Software Engineering (2020) 27:229-263

Table 4 Topics generated by LDA for our dataset of use feedback

Topic 1
Word

Uber
Nice
Number
See
Payment
Super
Unable
Check
User

Many
Topic 5
Word

Ever
Company
Mobile
Steal
Apply
Wish
Create
Quick
Scam

Basically

Prob.

0.078
0.036
0.021
0.032
0.025
0.019
0.018
0.017
0.017
0.016

Prob.

0.081
0.051
0.022
0.021
0.021
0.017
0.016
0.015
0.014
0.014

Topic 2
Word

Delivery
Food
Even
Fee
Love
Pay

Go
Want
Come
Cold

Topic 6
Word

Order
Service
Get

Time
Customer
Food

Say
Driver
Never
Call

Prob.

0.092
0.073
0.034
0.031
0.026
0.024
0.020
0.020
0.019
0.015

Prob.

0.090
0.045
0.044
0.037
0.030
0.022
0.021
0.020
0.019
0.019

Topic 3
Word

Option
Horrible
Location
Home
Dollar
Live
Night
Download
Awful
Part

Topic 7
Word

Good

Card
Awesome
Use
Everything
Think
Sign

Next
Amazing

Delay

Prob.

0.076
0.055
0.039
0.028
0.028
0.026
0.022
0.018
0.014
0.014

Prob.

0.186
0.052
0.038
0.038
0.025
0.024
0.022
0.017
0.017
0.015

Topic 4
Word

Well
Terrible
Fast
Actually
Be
Speak
Life
Report
Name
Buy
Topic 8

Word

App
Great
Restaurant
Try
Uber-eats
Work

Place

243

Prob.

0.077
0.066
0.028
0.027
0.018
0.018
0.016
0.014
0.014
0.013

Prob.

0.127
0.033
0.032
0.028
0.025
0.024
0.023
0.018
0.018
0.017

Gensim’s built-in stop-word removal function. LDA’s hyper-parameters a and f are
optimized by Gensim, where a is automatically learned from the corpus and f 1s set
to be 1/(number of topics). To determine the number of topics, we rely on Gensim’s
coherence score. Topic coherence provides a convenient measure to judge how good
a given topic model is. Our analysis shows that at around 8—10 topics, our data will
generate the most cohesive topics (Fig. 3a).

4.1.2 Results

The list of generated topics are shown in Table 4. In general, the topics are of poor
quality, in other words, they do not seem to capture any of the major concerns iden-
tified either by our qualitative analysis. For example, while the second topic in
Table 4 includes words such as delivery, food, and fee, it fails to represent a coherent
concern due to the mixture of words from more than one concern category. Other
topics in Table 4 also contain almost no words collectively representative of any of
the concern categories identified during our qualitative analysis phase.

Q) Springer
244 Automated Software Engineering (2020) 27:229-263

Table 5 Topics generated by assisted LDA for our dataset of user feedback

Topic 1 Topic 2 Topic 3

Word Prob. Word Prob. Word Prob.
Order 0.002 Order 0.042 App 0.037
App 0.001 Food 0.022 Order 0.030
Food 0.001 App 0.020 Food 0.023
Get 0.001 Get 0.019 Great 0.020
Service 0.001 Service 0.019 Time 0.018
Delivery 0.001 Time 0.017 Love 0.017
Driver 0.001 Delivery 0.016 Delivery 0.017
Time 0.001 Customer 0.012 Get 0.015
Restaurant 0.001 Driver 0.012 Service 0.015
Customer 0.001 Never 0.010 Good 0.014
Topic 4 Topic 5 Topic 6

Word Prob. Word Prob. Word Prob.
Order 0.002 Order 0.023 App 0.037
App 0.001 Get 0.021 Good 0.030
Food 0.001 Food 0.015 Order 0.023
Get 0.001 App 0.012 Food 0.020
Delivery 0.001 Service 0.011 Delivery 0.018
Service 0.001 Driver 0.009 Service 0.017
Customer 0.001 Customer 0.009 Nice 0.017
Time 0.001 Guy 0.009 Time 0.015
Driver 0.001 Doordash_help 0.008 Get 0.015
Never 0.001 Delivery 0.008 Bad 0.014

These poor results can be explained based on the limited length of user reviews
and tweets. Recent research has shown that LDA does not perform well when the
input documents are short in length (Bing et al. 2011; Hong and Davison 2010; Yan
et al. 2013). Specifically, LDA is a data-intensive technique that requires large quan-
tities of text to generate meaningful topic distributions. However, due to the sparsity
attribute of short-text, applying standard LDA to short-text data (e.g., user reviews
or tweets) often produces incoherent topics (Hong and Davison 2010; Zhao et al.
2011). To overcome this problem, researchers use supplemental strategies to effec-
tively train LDA in short-text environments. Such strategies, often known as pool-
ing, are based on merging (aggregating) related texts together and presenting them
as single pseudo-documents to LDA, thus, increasing the amount of text per docu-
ment to work with. In our analysis, we aggregate posts from each source (App Store
reviews, Google Play reviews, and Twitter) for each app in a single document, thus
producing 3 x 4 documents. We then generate topics for our aggregated data. Using
this data, the coherence score hits a local maxima at six topics (Fig. 3b). The gener-
ated topics are shown in Table 5.

Q) Springer
Automated Software Engineering (2020) 27:229-263 245

In general, aggregating user posts resulted in producing very similar topics. Gen-
erated topics are more redundant, providing only incomplete representations of the
user concern in our data. The poor generalization ability of LDA can be attributed to
two main reasons. First, due to the overlapping nature of the different concern cat-
egories, the classes are not separable by LDA. As a result, we see a mixture of words
from different concern categories in the same topic. Second, LDA is a data-intensive
technique that requires large quantities of text to generate meaningful topic distribu-
tions (Blei et al. 2003). However, our dataset is relatively small, consisting of only
3600 user posts, and even much less documents when these posts are aggregated.

In summary, our attempt to automatically generate our list of concerns using LDA
was relatively unsuccessful. In order to generate meaningful topics, LDA requires a
balance between the number and length of text artifacts being modeled (Tang et al.
2014). While we had a relativity large number of artifacts, their length was limited.
Our attempt to generate larger artifacts using Assisted LDA resulted in only few
lengthy artifacts (12). This has negatively impacted LDA’s ability to converge, or
generate meaningful latent topic structures. Our expectation is that, applying more
fine-grained text aggregation strategies that can produce sufficiently long, but not too
long, documents (e.g., aggregating tweets based on hashtags) would help to improve
the quality of generated topics (Hong and Davison 2010; Mimno et al. 2011).

4.2 Proposed modeling approach

The first part of our modeling analysis showed that LDA comes with several inher-
ent limitations related to its computational complexity and the nature of our data.
These limitations prevent LDA from producing meaningful representations of crowd
feedback. To overcome these limitations, in this section, we propose a fully auto-
mated procedure for generating succinct representations of crowd feedback in the
ecosystem of food delivery apps. In general, our automated model generation proce-
dure can be divided into four main steps:

Informative feedback is captured.

Important concepts (domain entities) in the feedback are identified.
Relationships between the domain entities are determined.

Entities and relations are consolidated to automatically generate the model.

PwWnN>

In what follows, we describe these steps in greater detail.

4.2.1 Identifying informative feedback

The first step in our procedure is to separate informative user feedback from unin-
formative feedback. A large body of research exists on classifying mobile app user
feedback into different categories of software maintenance tasks, such as feature
requests and bug reports (Maalej and Nabil 2015; Panichella et al. 2015; Williams
and Mahmoud 2017). Our classification configurations can be described as follows:

Q) Springer
246 Automated Software Engineering (2020) 27:229-263

e Classification algorithms: To represent our data, we experiment with three dif-
ferent classification algorithms: Support Vector Machines (SVM), Naive Bayes
(NB), and Random Forests (RF). These algorithms have been extensively used
to classify crowd feedback in the app market (Maalej and Nabil 2015; Panichella
et al. 2015). Their success can be attributed to their ability to deal effectively
with short text (e.g., tweets, user reviews, YouTube comments, etc.) (Wang and
Manning 2012).

e Training settings: to train our classifiers, we used 10-fold cross validation. This
method creates 10 partitions of the dataset such that each partition has 90% of
the instances as a training set and 10% as an evaluation set. The benefit of this
technique is that it uses all the data for building the model, and the results often
exhibit significantly less variance than those of simpler techniques such as the
holdout method (e.g., 70% training set and 30% testing set).

e Text pre-processing: English stop-words were removed and stemming was
applied to reduce words to their morphological roots. We used Weka’s built-in
stemmer and stop-word list to pre-process the posts in our dataset (Lovins 1968).
It is important to point out that lemmatization is sometimes used instead of stem-
ming in app review classification tasks (Panichella et al. 2015). The results often
show a marginal impact of these techniques on the precision of classification.
In our analysis, we use stemming for its lower overhead. Specifically, lemma-
tization techniques are often exponential to the text length, while stemming 1s
known for its linear time complexity (Bird et al. 2009).

e Sentiment Analysis: sentiment analysis is often used in app user feedback clas-
sification tasks as a classification feature of the input data (Williams and
Mahmoud 2018; Jha and Mahmoud 2019). The underlying hypothesis is that
user concerns are often expressed using negative sentiment (Lin et al. 2018). To
calculate the sentiment of our data, we used SentiStrength (Thelwall et al. 2010).
SentiStrength assigns positive (p) and negative (m) sentiment scores to input text,
using a scale of — 5 to + 5, based on the emotional polarity of individual words.
To convert SentiStrength’s numeric scores into these categories, we adapted the
approach proposed by Jongeling et al. (2017) and Thelwall et al. (2012). Spe-
cifically, a post is considered positive if p+n> 0, negative if p+n <0, and
neutral if p +n =O. It is worth mentioning that other sentiment analysis tech-
niques, such as VADER and the Stanford CoreNLP are also used in related stud-
ies. However, the difference in performance between these tools is often mar-
ginal (Panichella et al. 2015; Jongeling et al. 2015; Williams and Mahmoud
2017).

e Text representation: to classify our data, we experimented with simple bag-of-
words with lowercase tokens. The bag-of-words representation encodes each post
as a vector. Each attribute of the vector corresponds to one word in the vocabu-
lary of the dataset. A word is included in the vocabulary if it is present in at least
two posts. Words that appear in a single post are highly unlikely to carry any
predictive value to the classifier. An attribute of one in a post’s vector indicates
that the corresponding word is present, while a zero indicates absence. This rep-
resentation can be extended to treat common sequences of adjacent words, called
n-grams, a gram is a single word; n is the number of adjacent words, so two adja-

Q) Springer
Automated Software Engineering (2020) 27:229-263 247

wW App User 7
( app user

| updated the app, but now it !

<O, 1, 0, O, 1, O, O, QO, : O;, 2S

Fig.4 A visual representation of an N-Gram encoded tweet

cent words are a bi-gram. For example, the phrase “this app 1s good” contains
four words, and three b-grams (“this app’, “app is’, “is good”). Figure 4 illus-
trates how bag-of-words and n-gram representations work; “updated”, “‘app’’, and
“crashes” are the key words that occur in the tweet “I updated the app, but now it
crashes”. “Now it crashes” is a tri-gram that is also included. Each ‘1’ in the vec-
tor representation at the bottom corresponds to one of the highlighted n-grams,
while each ‘O’ corresponds to a vocabulary word that is not found in the tweet.
To generate this representation, we utilized the n-gram tokenizer in Weka, which
allowed uni-gram, bi-gram, and tri-gram tokens to be included in a single data-
set.

We trained two set of classifiers to categorize our data. One classifier for detect-
ing business posts and one classifier for detecting technical posts. The standard
measures of Precision (P), Recall (R), and F-Score (F p) are used to evaluate the
performance of our classification algorithms. Assuming f, is the set of true positives,
f, is the set of false positives, and f,, 1s the set of false negatives; precision 1s cal-
culated as: t, / (t, + tf) and recall is calculated as: t, / (t, +f). The F-measure is the
weighted harmonic mean of P and R, calculated as: Fi, = (1 + B°)PR)/(p7P +R).
In our analysis, we use # = 2 to emphasize recall over precision (Berry 2017).

All tweets and reviews in our original dataset were stored in ARFF format, a
common text-based file format often used for representing machine learning data-
sets, and then fed to Weka.° Table 6 shows the performance of NB, SVM, and
RF in terms of P, R, and F,. SVM provided the best average classification per-
formance in separating the different types of concerns, in comparison to NB and
RF respectively. The best SVM results were obtained using the Pearson VII func-
tion-based universal kernel (Puk) with kernel parameters o = 8 and m = 1 (Ustiin
et al. 2006). Universal Kernels are known to be effective for a large class of clas-
sification problems, especially for noisy data (Steinwart 2001). RF was evalu-
ated with 100 iterations. Raising iterations above this number did not improve

> A replication package is available at: http://seel.cse.lsu.edu/data/ASEJ2019.zip.

D) Springer
248

Automated Software Engineering (2020) 27:229-263

Table 6 A comparison of the performance of our classifiers (SVM, NB, and RF) with lower-casing (LC),
stemming (ST), stop-word (SW) removal, and sentiment analysis (SEN)

Business

LC

LC + SEN

LC + SW

LC + SW + ST
Human

LC

LC + SEN

LC + SW

LC + SW + ST
Market

LC

LC + SEN

LC + SW

LC + SW + ST
Technical

LC

LC + SEN

LC + SW

LC + SW + ST
Bugs

LC

LC + SEN

LC + SW

LC + SW + ST
Features

LC

LC + SEN

LC + SW

LC + SW + ST

NB
P

0.85
0.85
0.83
0.84

0.68
0.68
0.69
0.68

0.56
0.56
0.55
0.56

0.38
0.38
0.42
0.39

0.31
0.31
0.34
0.33

0.17
0.17
0.19
0.18

0.79
0.79
0.84
0.83

0.79
0.78
0.83
0.81

0.69
0.69
0.75
0.74

0.67
0.67
0.69
0.71

0.65
0.65
0.66
0.68

0.65
0.63
0.62
0.67

Fy

0.82
0.82
0.83
0.84

0.73
0.73
0.75
0.74

0.62
0.62
0.63
0.64

0.49
0.49
0.52
0.51

0.42
0.42
0.45
0.45

0.27
0.27
0.29
0.28

SVM

0.89
0.89
0.89
0.89

0.83
0.83
0.83
0.83

0.72
0.73
0.75
0.75

0.60
0.59
0.58
0.61

0.51
0.51
0.52
0.53

0.50
0.50
0.50
0.51

0.85
0.85
0.85
0.85

0.79
0.79
0.79
0.79

0.66
0.67
0.67
0.68

0.55
0.55
0.52
0.55

0.53
0.53
0.57
0.57

0.51
0.51
0.52
0.52

Fy

0.87
0.87
0.87
0.87

0.81
0.81
0.81
0.81

0.69
0.70
0.71
0.71

0.57
0.57
0.55
0.58

0.53
0.51
0.53
0.54

0.51
0.50
0.50
0.51

RF
P

0.85
0.86
0.88
0.87

0.85
0.85
0.87
0.86

0.78
0.80
0.81
0.79

0.93
0.95
0.88
0.91

1.00
0.52
0.91
0.98

0.00
0.00
1.00
1.00

0.87
0.87
0.86
0.88

0.70
0.69
0.74
0.75

0.47
0.43
0.54
0.53

0.07
0.05
0.22
0.16

0.03
0.03
0.14
0.11

0.00
0.00
0.01
0.01

0.86
0.86
0.87
0.87

0.77
0.76
0.80
0.80

0.59
0.56
0.65
0.64

0.13
0.10
0.35
0.27

0.05
0.06
0.24
0.19

0.00
0.00
0.03
0.02

the performance. We also notice that almost all classifiers achieved better perfor-
mance when classifying the reviews and tweets into generic categories of Busi-
ness and Technical. The performance deteriorated when the data was classified
at a subcategory level (Human, Market, Bug, and Feature) due to the fact that the
classifier had to deal with a larger set of classes (labels). Separating concerns at
this level can be challenging, especially when the data is relatively unbalanced.

Q) Springer
Automated Software Engineering (2020) 27:229-263 249

Table 7 A performance
comparison of SVM using ee
ordinary bag-of-words versus Prec. Recall F, Prec. Recall F,
N-grams —_—_—_—_——_ha_—aooasox_xssss—

Uni, bi, and tri-grams Bag-of-words

Business 0.89 0.85 0.87 0.89 0.85 0.87
Human 0.84 0.80 0.82 0.83 0.79 0.81

Market 0.71 0.66 0.69 0.78 0.68 0.71
Technical 0.61 0.55 0.58 0.61 0.55 0.58
Bug 0.58 0.52 0.55 0.53 0.57 0.54

Feature 0.48 0.32 0.38 0.51 0.52 0.51

In general, business-related posts were easier to classify than technically-
related posts. This phenomenon is driven by the quantity of each class. Table 3
shows that technical posts were rare. The prior-probability of any given post
being technical is less than 25%, negatively impacting the performance of all
three classifiers. This problem was exacerbated for the individual technical cate-
gories, with feature requests only occurring in 6.5% of posts. The relative sparsity
of technical posts in comparison to other application domains can be explained
based on the fact that the domain food delivery is a business domain in nature,
thus, users had so many more business-related issues to discuss. For instance,
Food courier services would often fail behind the scene, causing drivers to be dis-
patched to incorrect locations, or customer support to fail to call. These failures
often caused customers to discuss competition and pricing. As a result, business
concerns crowded out technical concerns. In other domains, failures are more
immediate and visible to consumers, meaning that user concerns are more likely
to take the form of bug reports.

We further experimented with the bag-of-words representation of text, and
then allowing bi- and tri-grams to be included alongside individual words. Nei-
ther approach improved the performance. Table 7 shows a comparison between
the uni-gram encoding (1.e., bag-of-words), and the encoding which included bi-
and tri-grams. The lack of improvement partly stems from the fact that the addi-
tional composite tokens often had the same class implications as their constituent
words. For example, the term account was found to have a negative implica-
tion on the business class, meaning that posts containing the word account were
unlikely to be business-related. Most of the related N-grams, including account
got hacked and account was hacked had the same implication, except with a sub-
stantially smaller weight. Therefore, they were essentially irrelevant to classifica-
tion. In some other cases, bi- and tri-grams did not have the same implication as
their constituent words. For example, promo was positively implicated to busi-
ness, but promo code had a negative implication. However, the single word in
this case, and in many others, had a higher weight than the bi- and tri-grams,
and occurred in substantially more posts. Often times, the bi-grams had the same
weight and occurrence as the tri-grams, making the tri-grams superfluous.

Our results also show that the sentiment polarity of posts had almost no impact
on the classification accuracy. Specifically, the results show that miscellaneous posts

Q) Springer
250 Automated Software Engineering (2020) 27:229-263

e
oO
oO

Proportion of Sentiment (%)

50
S aS . x & @
S een Oe @ + <
Sg eS <

L Negative IS Neutral @ Positive

Fig.5 The distribution of sentiment over the different types of posts

(posts not business or technically-relevant) were detected as having more positive
sentiment than any other category. These result were expected; non-miscellaneous
posts often described problems users were having. Otherwise, as Fig. 5 shows, the
categories had substantially similar sentiment scores overall. For future work, we
suspect that enhancing SentiStrength’s dictionary with emotion-provoking software-
related words (crash, uninstall, etc.), or using customized sentiment analysis classifi-
ers (e.g., Williams and Mahmoud 2017) would help to better estimate the emotional
polarity of posts.

4.2.2 Identifying model entities

In order to specify the main entities (nodes) of our model, we look for important
words in the set of reviews and tweets classified as informative in the previous step.
Our assumption is that such words capture the essence of user concerns in the eco-
system. In Object Oriented software design, when generating conceptual models
from requirements text or any textual data, nouns are considered candidate classes
(objects), verbs are considered as candidate operations (functions), while adjec-
tives commonly represent attributes (Abbott 1983; Elbendak et al. 2011). Based on
these assumptions, we only consider important nouns, verbs, and adjectives in our
analysis.

To extract these parts of speech (POS), we utilize the Natural Language Toolkit
(NTLK) (Bird et al. 2009) POS tagging library. We further apply lemmatization to
reduce the morphological variants of words in our dataset down to their base forms.
For example, drink, drinks, drinking, drank, and drunk, are all transformed to sim-
ply drink. By applying lemmatization, we avoid the problem of morphological vari-
ants being treated as entirely different words by our model. After lemmatization,
we merge words together under each part of speech category. For example drive
and drives are merged to simply drive when used as verbs. However, the word drive
can also be a noun (e.g., “that was a long drive’). Therefore, we only merge words

Q) Springer
Automated Software Engineering (2020) 27:229-263 251

Table 8 The top 10 most

important nouns, verbs, and Noun verb Adjective

adjectives in our dataset Food Order Good
App Use Great
Service Say Terrible
Delivery Deliver Horrible
Time Charge Wrong
Consumer Wait Last
Driver Cancel Bad
Restaurant Give Free
Money Want Easy

within the same part of speech to avoid losing this semantic distinction. Extracted
parts of speech are then ranked based on their Hybrid 7F.JDF scores (Inouye and
Kalita 2011). Formally, TF.JDF can be computed as:

[RI

: A EPRI (1)
Ir, 2: w, Er, Ar, ER|

TF .IDF(w,) = TF(w,) X lg

where TF(w,) is the term frequency of the word w, in the entire collection, IAI is the
total number of posts in the collection, and Ir; Ww, Er, AT, € R| is the number of
posts in R that contain the word w;. The purpose of TF.JDF is to score the overall
importance of a word to a particular document or dataset. In general, TF.IDF bal-
ances general frequency and appearance in number of posts. High frequent words
appearing in few documents have higher TF.IDF. After defining TF.JDF, we extract
important POS from the set of informative business and technical posts. The top ten
nouns, verbs, and adjectives in our dataset are shown in Table 8.

4.2.3 Identifying model relations

Our model generation procedure depends on the co-occurrence statistics of words in
the data to capture their relations. For example, in our dataset, the words customer
and refund appear in a very large number of user reviews and tweets. Therefore, the
procedure assumes there is a relation connecting these two entities. To count for
such information, we use pointwise mutual information (PMI).

PMI is an information-theoretic measure of information overlap, or statistical
dependence, between two words (Church and Hanks 1990). PMI was introduced
by Church and Hanks (1990), and later used by Turney (2001) to identify synonym
pairs using Web search results. Formally, PMI between two words w, and w, can be
measured as the probability of them occurring in the same text versus their prob-
abilities of occurring separately. Assuming the corpus contains N documents, PMI
between two words w, and w, can be calculated as:

Q) Springer
252 Automated Software Engineering (2020) 27:229-263

 

 

 

 

 

 

Arrive =—— Cold
Deliver ———— Food Hot
Prepare Late

 

 

 

Fig.6 The key elements of the entity-action-property relations represented by our model

C(w,,W>)
PMI =1 TN _\_ l Pw Wa)
—_ 08> C(w,) C(w3) — O85 P(w,)P(v>) (2)
N N

where C(w,, w,) is the number of documents in the collection containing both w, and
w,, and C(w,), C(w,) are the numbers of documents containing w, and w, respec-
tively. Mutual information compares the probability of observing w, and w, together
against the probabilities of observing w, and w, independently. Formally, mutual
information is a measure of how much the actual probability of a co-occurrence of
an event P(w,,w,) differs from the expectation based on the assumption of inde-
pendence of P(w,) and P(w,) (Bouma 2009). If the words w, and w, are frequently
associated, the probability of observing w, and w, together will be much larger than
the probability of observing them independently. This results in a PMI > 1. On the
other hand, if there is absolutely no relation between w, and w,, then the probability
of observing w, and w, together will be much less than the probability of observing
them independently (1.e., PMI < 1). PMI is intuitive, scalable, and computationally
efficient (Mihalcea et al. 2006; Newman et al. 2010). These attributes have made it
an appealing similarity method to be used to process massive corpora of textual data
in tasks such as short-text retrieval (Mihalcea et al. 2006), Semantic Web (Sousa
et al. 2010; Turney 2001), source code retrieval (Khatiwada et al. 2017).

To generate the relations in our model, we computed PMI between every pair
of words to determine their relatedness. One potential pitfall of relying on PMI as
a measure of relatedness is that PMIs hits a maximum with words occurring only
once. This happens often with misspellings and irrelevant words. In order to pre-
vent this phenomenon, we restrict our analysis to only words that occur at least
ten times. Ten was chosen due to being the point at which sensitivity to additional
increases became less noticeable (i.e., changing 10—11 would not substantially alter
the results).

4.2.4 Model representation

To generate our model, we extract the top 10 nouns ranked by TF.JDF and then
use PMI to extract the three most related verbs and adjectives with each noun. An
example of a node, or an atomic entity in our model, is shown in Fig. 6. This node
consists of three main parts:

e Concern: the middle part of the node represents the concern’s name (food),
which is basically one of the important nouns (based on TF-.JDF) in our dataset.

Q) Springer
Automated Software Engineering (2020) 27:229-263 253

e Properties: directly attached to the entity’s name from the right is the top
three adjectives associated with the entity (based on PMI). In our example,
food could be cold, hot, or late.

e Triggers: on the left side of the node, we attach the list the top three verbs
frequently associated (based on PMI) with the noun (concern’s name). Verbs
often represent triggers, or leading causes of concerns. In our example, the
verbs arrive, deliver, and prepare are commonly associated with the word
food.

Formally, our model generation process can be described as follows, given a set
of Words, containing all words in the dataset occurring at least ten times, we
define the parts of speech of a word, or pos(word), Adjs, Verbs, and Nouns as
follows:

Adjs = {word € Words | pos(word) = Adj}
Verbs = {word € Words | pos(word) = Verb} (3)
Nouns = {word € Words | pos(word) = Noun}

We define three helper sets to help us express our graph mathematically. Se/Nouns
is the list of the top 10 selected nouns when ranked by Hybrid TF.JDF. Verbs,, and
Adjs,, are the sets of three most closely related (by PMI) verbs and adjectives for a
given word w. These sets are defined, using the function top(n, pred) to retrieve the
top n words after words are sorted based on the predecessor function pred(word).
We use two functions to sort words: TF.IDF for nouns and PMI for verbs and adjec-
tives. We express this using A notation for defining anonymous functions, such that,
Ax.TFIDF(x) means define a function that takes an x and returns its TF.IDF. This
results in the following expressions:

SelNouns = top(10, Nouns, Ax.TFIDF(x))
Verbs,, = top(3, Verbs, Av.PMI(v, w)) (4)
Adjs,, = top(3, Adjs, Aa.PMI(a, w))

We define a graph, (V, £), expressed as a tuple of vertices and edges, as follows:

V= tw € SelNouns | {w} U Verbs, U Adjs,, }

FE = {w € SelNouns, v € Verbs,, | (w,v)} U (5)
{w € SelNouns,a € Adjs,, | (w,a)}

The set of vertices (V) is constructed by creating a smaller set containing each
selected noun and its related adjectives and verbs, and then taking the union of these
smaller sets to form the entire set of relevant entities, properties, and actions. The set
of edges (£) is simply the union of associations of nouns to adjectives and nouns to
verbs. Applying this process to our informative posts in the domain of food delivery
apps results in the model in Fig. 7.

Q) Springer
254 Automated Software Engineering (2020) 27:229-263

 

 

 

 

         
  
  

 

 

 

 

 

 

 

 

 

Second Place Atrive Cold
Full Order Refuse Deliver Food Hot
Disappointed ‘| Prepare

 

 

 

 
 
  
           
       
 
      
  
 
 
    

App Download|] | [Hack Service

 

Fre = ——,—Ss«dDeellivery
‘High

 

 
 

 

 

 

Customer Ss Driver
Big |
Difficult
Restaurant .——__ Happen
Hard Call Long

 

Fig.7 A suggested model diagram depicting the relationships between important nouns (entities of the
ecosystem), adjectives (attributes), and verbs (concern triggers)

4.3 Model evaluation and interpretation

Due to the lack of a priori ground-truth, evaluating domain models can be a
challenging task. In general, a domain model is an abstraction that describes a
specific body of knowledge. Therefore, the quality of the model can be assessed
based on its completeness, or its ability to encompass the main concepts pre-
sent in the knowledge it models (Mohagheghi and Dehlen 2019; Rubén 1990).
These concepts are often determined manually by domain experts. To evaluate
our model generation procedure, we examine the main concepts captured in the
model. Specifically, we assess the extent to which the noun—verb—adjective asso-
ciations presented in our model reflect the main concerns identified by our quali-
tative analysis:

e Customer Service: Concerns about customer service frequently appeared when
an order was not delivered on time, when the order was inaccurate, or when
refunds were denied. The model identified both customer and service as impotr-
tant nouns along with the relations <customer, refund> and <customer, incor-
rect>. Furthermore, both customer and service were associated with the adjec-
tives poor and terrible in the model.

e Orders: Orders were commonly associated with delays. Users complained about
receiving cold food as a result. Users were disappointed whenever food was left
waiting at the restaurant to be picked up. The model identified <order, refuse>
whenever restaurants refused to cancel orders or the app refused to take action
when things went wrong. In addition, <order, place> was a common occurrence
as these two words often appeared together (e.g., “place order’). The relation
<order, second> originated from posts of users complaining about having to re-

D) Springer
Automated Software Engineering (2020) 27:229-263 255

order for the second time and the relation <order, full> originated from people
asking for full refunds.

e Food: Food was directly related to arrival. This was captured in the relations
<food, arrive>, <food, deliver>, and <food, prepare>. Food was also associated
with temperature, mainly due to the number of complaints about receiving cold
or hot food (e.g., <food, cold> and <food, hot>). Complaints about orders being
late were common, resulting in the relation <food, late>.

e Delivery: Delivery was associated with a number of complaints about incor-
rect estimated times, explaining the relation <delivery, estimate>. The rela-
tion <delivery, prepare> occurred due to issues with orders being stuck in the
preparation stage and never being dispatched for delivery. The relation <deliv-
ery, choose> primarily occurred in the context of users stating that they would
“choose a different delivery service”.

e Time: Time was primarily present in complaints about delivery delays. The rela-
tions <time, estimate> and <time, prepare> appeared for the same reasons they
appeared with delivery. A common occurrence was <time, waste> due to unex-
pected delays and order cancellations. The relation <time, long> occurred in
similar contexts, as in “it took longer than the estimated time’”’.

e App: App appeared alongside comments about ease-of-use, resulting in the rela-
tion <app, easy>. The relation <app, ridiculous> was a general complaint about
poor policies or bad usability. The relation <app, delete> appeared when users
discussed deleting an app after a poor experience. A common association was
<app, look>, appearing due to phrases such as “look into this” and “looks like”.
The relation <app, end> appeared from posts were users complained that they
“ended up” eating cold or incorrect food, or not eating at all.

e Money: Money issues were captured by the relation <money, waste>. This rela-
tion stems from incidents were users ordered food that ended up being inedi-
ble and being unable to obtain a refund, which also yielded the model relation
<money, refund>. The verb take was associated with money in posts such as
“you take my money but did not deliver’, resulting the relation <money, take>.

e Drivers: Drivers are a critical component of the ecosystem. All services strug-
gled with their drivers’ timing, directions, and friendliness. Users frequently
complained about drivers combining orders. The model successfully identified
the relation <driver, find> from posts discussing a driver’s inability to find their
destination. Lack of friendliness is captured in the relation <driver, awful>.

e Restaurants: Users often asked services to add new restaurants as well as dis-
cussed problems that occurred between the app, restaurant, and driver. The rela-
tion <restaurant, show> appeared in the model partly due to users stating that
the restaurant they wanted did not “show up” in the app. However, this phrase
was more often associated with the driver not appearing at the restaurant. Com-
munication problems between restaurants and consumers were captured through
the <restaurant, call> relation.

In summary, to answer RQ,, in terms of completeness (the omission of domain

concepts and relationships), our model was able to recover a large number of con-
cepts in the data. Missed concerns were rare (e.g., inability to find a customer

Q) Springer
256 Automated Software Engineering (2020) 27:229-263

Feedback classification Identifying model entities Identifying model relations
aan
yea Y —— a ——_
ann
TF.IDF analysis PMI Analysis @

 

 

 

 

 

 

 

 

 

 

Stemming
S258 Sentiment analysis
oa Classification
| Improved, concern-aware, Sharing Economy apps |

 

Fig.8 A diagram of the proposed approach

service number). In terms of clarity, some of the captured relations, such as <food,
cold> or <money, waste> were more obvious than others, for example <restaurant,
show>. Incorrect, or hard to explain, relations were also present in the model. For
example, the relations <driver, big> and <money, long> did not seem to reflect any
issues that were identified by our qualitative analysis of the data, rather they origi-
nated from posts such as “not a big fan of the driver” or “no longer interested”.
While these relations were relatively rare, they can be eliminated by compiling a list
of such common English adjectives to filter them out before they make their way to
the model. Another observation is that technical concerns, despite not being accu-
rately classified, have also found their way into the model. For instance, hacking was
a popular technical concerns. The verb hack appeared in association with the nouns
customer and service.

5 Discussion and impact

A summary of the main steps of the proposed approach is depicted in Fig 8. The
first phase of our analysis has revealed that user concerns in SE extend beyond the
technical issues of mobile apps to cover other business and service oriented matters.
These results emphasize the importance of studying user feedback in the app market
at an ecosystem-level. Specifically, apps should be analyzed in bundles, or clusters,
of functionally related apps rather than studied individually. In fact, such clusters
can be automatically generated using app classification techniques (AlSubaihin et al.
2016).

Once these fine-grained categories of semantically-similar apps are identi-
fied, automated data clustering, classification, and modeling techniques should be
employed to consolidate and analyze user feedback and identify the main pressing
user concerns in these clusters. Our analysis has also provided an additional evi-
dence on the value of considering multiple sources of user feedback to get the full
picture of user concerns. For instance, in the domain of food delivery apps, users
preferred to use Twitter as low latency method to get instant reactions from app
developers or operators. These complaints were very common whenever any of the
services in our ecosystem went down for some reason. Understanding how users uti-
lize different sources of feedback can help developers to focus their attention on the
right channels of feedback while planning for their next release.

Q) Springer
Automated Software Engineering (2020) 27:229-263 257

In the second phase of our analysis, we proposed an automated procedure for gen-
erating conceptual models of user concerns in the ecosystem of food delivery apps.
According to Yu (2009), “conceptual modeling frameworks aim to offer succinct
representations of certain aspects of complex realities through a small number of
modeling constructs, with the intent that they can support some kinds of analysis”.
Our procedure adapted assertions from Object-Oriented programming and text pro-
cessing to extract the main entities of our ecosystem. An underlying tenet is that the
vocabulary of a domain provides an easily accessible supply of concepts. An infor-
mation theoretic approach, which utilizes term co-occurrence statistics, was then
used to establish a structuring mechanism for assembling and organizing extracted
concepts. Our evaluation showed that relying on these techniques can generate a
high quality model which captures most of the latent concepts in the domain knowI-
edge. By changing the 7F.JDF and PMI thresholds and the number of nouns, verbs,
and adjectives in Eq. (4), domain entities and relations can be included or excluded,
thus, giving app developers the flexibility to generate domain models at different
levels of granularity. The simplicity and configurability of our procedure gives it an
advantage over other more computationally expensive methods, such as LDA (Blei
et al. 2003), which requires large amounts of data and a calibration of several hyper-
parameters in order to produce meaningful topics (Chen et al. 2014; Guzman and
Maalej 2014).

In terms of impact, our generated model can provide valuable ecosystem-wide
information to SE app developers, acting as a vehicle to facilitate a quick transi-
tion from domain knowledge to requirements specifications. For instance, startups,
or newcomers, trying to break into the food delivery app market, can use our proce-
dure to quickly generate a model for their micro-ecosystem of operation. Through
the model entities and relations, they can get insights into the complex realities of
their operational environments. Such information can help them to redirect their
effort toward innovations that can help to avoid these issues in their apps. For exam-
ple, developers can work on more accurate driver dispatching procedures to avoid
delays, add new features for payments and refund to reduce amount of money and
time wasted, add more security measures to prevent hacking, and implement smarter
rating systems of drivers, customers, and restaurants, to control for the quality of
service provided through the app. After release, developers can further use our
model to automatically track users’ reactions to their newly-released features.

6 Limitations and validity

Our analysis takes the form of a case study. Case studies often suffer from external
validity threats since they target specific phenomena in their specific contexts (Woh-
lin et al. 2012). For instance, our case study only included four apps. These apps
might not represent the entire domain of food delivery. However, as mentioned ear-
lier, our analysis was focused only on the fittest actors in the ecosystem. These popu-
lar apps often receive significantly more feedback than smaller apps (Mcilroy et al.
2017). Furthermore, to minimize any sampling bias, our data collection process
included multiple sources of user feedback and has extended over a long period of

Q) Springer
258 Automated Software Engineering (2020) 27:229-263

time to capture as much information about the apps in our ecosystem as possible. In
terms of generalizability, we anticipate that our proposed approach could be applied
to other application domains beyond SE, especially for apps operating in complex
multi-agent ecosystems. However, independent case studies need to be conducted
before we can make such a claim.

Internal validity threats may stem from the fact that we only relied on the textual
content of user posts and their sentiment as classification features. In the literature,
meta-data attributes, such as the star-rating of the review or the number of retweets,
have also been considered as classification features (Guzman and Maalej 2014). The
decision to exclude such attributes was motivated by our goal of maintaining sim-
plicity. Specifically, practitioners trying to use our procedure do not have to worry
about collecting and normalizing such data, especially that the impact of such attrib-
utes on the quality of classification was found to be limited (Guzman and Maale}
2014).

Threats might also stem from our model evaluation procedure. Specifically, our
generated LDA topics and models was only evaluated intrinsically, based on how
well the generated model correlated with the results of the qualitative analysis.
While such evaluation can be sufficient for model generation and calibration tasks, it
does not capture the practical significance of the model. Therefore, a main direction
of future work will be dedicated to the extrinsic evaluation of our model. Extrinsic
evaluation is concerned with criteria relating to the system’s function, or role, in
relation to its purpose (e.g., validation through experience). To conduct such analy-
sis, our model will be provided to selected groups of app developers to be used as
an integral part of their app development activities. Evaluation data will be collected
through surveys that will measure the level of adaptation as well as the impact of
such models on idea formulation and the success or failure of mobile app products.

7 Conclusions

SE has come with a set of unconventional challenges for software engineers. Under-
standing these challenges begins with understanding end-users’ needs, and then
using such knowledge to develop a better understanding of the internal dynamics of
such a complex and dynamic software ecosystem. To achieve this goal, in this paper,
we proposed an automated approach for modeling crowed feedback in ecosystems
of SE apps. The proposed approach is evaluated through a case study targeting the
ecosystem of the food delivery apps. Our results showed that users tend to express a
variety of concerns in their feedback. Such concerns often extend over a broad range
of technical and business issues. The results also showed that, in our ecosystem of
interest, business concerns were more prevalent than technical concerns. In the sec-
ond phase of our analysis, we proposed an approach for automatically generating an
abstract conceptual model of the main user concerns in the ecosystem of food deliv-
ery apps. The results showed that a descriptive model can be generated by relying
on the specificity, frequency, and co-occurrence statistics of nouns, verbs, and adjec-
tives in textual user feedback. The results also showed that, despite being relatively
rare and hard to classify, dominant technical concerns were reflected in the model.

Q) Springer
Automated Software Engineering (2020) 27:229-263 259

We further compared our generated model’s entities with topics generated using the
topic modeling technique LDA. The results showed that, due to the short nature and
lack of structure in user feedback text, LDA failed to generate any cohesive topics
that were representative of valid user concerns.

In addition to extrinsically evaluating our generated model, our future work in
this domain will include conducting more case studies, targeting SE apps operating
in dynamic and multi-agent ecosystems, such as ridesharing or freelancing. These
models will be enriched with more information such as the priority of user concerns,
or the magnitude/direction of the relation between two ecosystem entities. Such
information will enable us to understand the SE app market at a micro level and pro-
vide more succinct representations of its complex realities.

Acknowledgements This work was supported in part by the U.S. National Science Foundation (Award
CNS 1951411) and LSU Economic Development Assistantships awards.

References

Abbott, R.: Program design by informal English descriptions. Commun. ACM 26(11), 882-894 (1983)

Acquier, A., Daudigeos, T., Pinkse, J.: Promises and paradoxes of the sharing economy: an organizing
framework. Technol. Forecast. Soc. Chang. 125, 1-10 (2017)

Ala-Mantilaa, S., Ottelina, J., Heinonenb, J., Junnilaa, S.: To each their own? The greenhouse gas impacts
of intra-household sharing in different urban zones. J. Clean. Prod. 135, 356-367 (2016)

AlSubaihin, A., Sarro, F., Black, S., Capra, L., Harman, M., Jia, Y., Zhang, Y.: Clustering mobile apps
based on mined textual features. In: International Symposium on Empirical Software Engineering
and Measurement, pp. 38:1—38:10 (2016)

Aznar, J., Sayeras, J.M., Rocafort, A., Galiana, J.: The irruption of Airbnb and its effects on hotels’ profit-
ability: an analysis of Barcelona’s hotel sector. Intang. Cap. 13(1), 147-159 (2017)

Bellotti, V., Ambard, A., Turner, D., Gossmann, C., Demkova, K., Carroll., J.M.: A muddle of models of
motivation for using peer-to-peer economy systems. In: Annual ACM Conference on Human Fac-
tors in Computing Systems, pp. 1085-1094 (2015)

Benkler, Y.: Peer production, the commons, and the future of the firm. Strateg. Org. 15(2), 264-274
(2017)

Berry, D.: Evaluation of tools for hairy requirements and software engineering tasks. In: International
Requirements Engineering Conference Workshops, pp. 284—291 (2017)

Bing, L., Lam, W., Wong, T.L.: Using query log and social tagging to refine queries based on latent top-
ics. In: International Conference on Information and Knowledge Management, pp. 583-592 (2011)

Bird, S., Klein, E., Loper, E.: Natural Language Processing with Python. O’Reilly Media, Newton (2009)

Bistaffa, F., Farinelli, A., Chalkiadakis, G., Ramchurn, S.: Payments for large-scale social ridesharing. In:
The ACM Conference on Recommender Systems, pp. 139-146 (2015)

Blei, D., Ng, A., Jordan, M.: Latent Dirichlet allocation. J. Mach. Learn. Res. 3, 993-1022 (2003)

Bonciu, F., Balgar, A.: Sharing economy as a contributor to sustainable growth. An EU perspective. Rom.
J. Eur. Aff. 16(2), 36-45 (2016)

Bond, A.: An app for that: local governments and the rise of the sharing economy. Notre Dame Law Rev.
90, 77 (2014)

Bouma, G.: Normalized (pointwise) mutual information in collocation extraction. German Society for
Computation Linguistics and Language Technology, pp. 31-40 (2009)

Cannon, S., Summers, L.: How Uber and the sharing economy can win over regulators. Harvard Bus.
Rev. 13, 24-28 (2014)

Carreno, G., Winbladh, K.: Analysis of user comments: an approach for software requirements evolution.
In: International Conference on Software Engineering, pp. 343-348 (2013)

Chen, G., Cheng, M., Edwards, D., Xu, L.: Covid-19 pandemic exposes the vulnerability of the sharing
economy. Res. Square. (2020) https://doi.org/10.21203/rs.3.rs-26460/v 1

Q) Springer
260 Automated Software Engineering (2020) 27:229-263

Chen, N., Lin, J., Hoi, S., Xiao, X., Zhang, B.: AR-Miner: Mining informative reviews for developers
from mobile app marketplace. In: International Conference on Software Engineering, pp. 767-778
(2014)

Cheng, M.: Current sharing economy media discourse in tourism. Ann. Tour. Res. 60(C), 111-114 (2016)

Chow, Y., Yuan Yu, J.: Real-time bidding based vehicle sharing. In: International Conference on Autono-
mous Agents and Multiagent Systems, pp. 1829-1830 (2015)

Church, K., Hanks, P.: Word association norms, mutual information, and lexicography. Comput. Linguist.
16(1), 22—29 (1990)

Cleland-Huang, J., Settimi, R., BenKhadra, O., Berezhanskaya, E., Christina, S.: Goal-centric traceability
for managing non-functional requirements. In: International Conference on Software Engineering,
pp. 362-371 (2005)

Conger, K., Griffith, E.: The results are in for the sharing economy. They are ugly. (2020). https://www.
nytimes.com/2020/05/07/technology/the-results-are-in-for-the-sharing-economy-they-are-ugly.
html

Coulton, P., Bamford, W.: Experimenting through mobile apps and app stores. Int. J. Mob. Hum. Comput.
Interact. 3(4), 55-70 (2011)

Dillahunt, T., Malone, A.: The promise of the sharing economy among disadvantaged communities. In:
Annual ACM Conference on Human Factors in Computing Systems, pp. 2285-2294 (2015)
Dillahunt, T., Wang, X., Wheeler, E., Cheng, H.F., Hecht, B., Zhu, H.: The sharing economy in comput-

ing: a systematic literature review. ACM Hum. Comput. Interact. 1(38), 1-26 (2017)

Dogru, T., Mody, M., Suess, C.: Adding evidence to the debate: quantifying Airbnb’s disruptive impact
on ten key hotel markets. Tour. Manag. 72, 27-39 (2019)

Edelman, B., Luca, M.: Digital discrimination: The case of Airbnb.com. Harvard Business School NOM
Unit Working Paper No. 14-054 (2014)

Elbendak, M., Vickers, P., Rossiter, N.: Parsed use case descriptions as a basis for object-oriented class
model generation. J. Syst. Softw. 87(7), 1209-1223 (2011)

Finkelstein, A., Harman, M., Jia, Y., Martin, W., Sarro, F., Zhang, Y.: App store analysis: mining app
stores for relationships between customer, business and technical characteristics. Technical report,
University of College London, Tech. Rep. rN/14/10 (2014)

Ge, Y., Knittel, C., MacKenzie, D., Zoepf, S.: Racial and gender discrimination in transportation network
companies (NBER Working Paper No. 22776) (2017)

Glassey, O.: Method and instruments for modeling integrated knowledge. Knowl. Process Manag. 15(4),
247-257 (2008)

Glinz, M.: On non-functional requirements. In: Requirements Engineering, pp. 21—26 (2007)

Goel, P., Kulik, L., Ramamohanarao, K.: Privacy-aware dynamic ride sharing. ACM Trans. Spat. Algo-
rithms Syst. 2(1), 1-41 (2016)

Gomez, M., Martineza, M., Monperrus, M., Rouvoy, R.: When app stores listen to the crowd to fight bugs
in the wild. In: International Conference on Software Engineering, track on New Ideas and Emerg-
ing Results (NIER), vol. 2, pp. 567-570 (2015)

Griffiths, T., Steyvers, M.: Finding scientific topics. In: The National Academy of Sciences, pp. 5228—
5235 (2004)

Groen, E., Kopczynska, S., Hauer, M., Krafft, T., Doerr, J.: Users: The hidden software product qual-
ity experts?: A study on how app users report quality aspects in online reviews. In: International
Requirements Engineering Conference, pp. 80-89 (2017)

Guzman, E., Ibrahim, M., Glinz, M.: A little bird told me: mining tweets for requirements and software
evolution. In: International Requirements Engineering Conference, pp. 11—20 (2017)

Guzman, E., Maalej, W.: How do users like this feature? A fine grained sentiment analysis of app reviews.
In: Requirements Engineering, pp. 153-162 (2014)

Harman, M., Jia, Y., Zhang, Y.: App store mining and analysis: MSR for app stores. In: Conference on
Mining Software Repositories, p. 2012 (108-111)

He, W., Li, D., Zhang, T., An, L., Guo, M., Chen, G.: Mining regular routes from GPS data for rideshar-
ing recommendations. In: The ACM SIGKDD International Workshop on Urban Computing, pp.
79-86 (2012)

Hofmann, T.: Probabilistic latent semantic indexing. In: International Conference on Research and Devel-
opment in Information Retrieval, pp. 50-57 (1999)

Hong, L., Davison, B.: Empirical study of topic modeling in Twitter. In: Workshop on Social Media Ana-
lytics, pp. 80-88 (2010)

Q) Springer
Automated Software Engineering (2020) 27:229-263 261

Hossain, M.: Sharing economy: a comprehensive literature review. Int. J. Hospital. Manag. 87,
102470 (2020)

Hiittel, A., Ziesemer, F., Peyer, M., Balderjahn, I.: To purchase or not? Why consumers make eco-
nomically (non-) sustainable consumption choices. J. Clean. Prod. 174, 827-836 (2018)

Iacob, C., Harrison, R.: Retrieving and analyzing mobile apps feature requests from online reviews.
In: Mining Software Repositories, pp. 41-44 (2013)

Inouye, D., Kalita, J.: Comparing Twitter summarization algorithms for multiple post summaries. In:
International Conference on Social Computing (SocialCom) and International Conference on
Privacy, Security, Risk and Trust (PASSAT), pp. 298-306 (2011)

Jansen, S., Finkelstein, A., Brinkkemper, S.: A sense of community: a research agenda for software
ecosystems. In: International Conference on Software Engineering—Companion Volume, pp.
187-190 (2009)

Jha, N., Mahmoud, A.: Using frame semantics for classifying and summarizing application store
reviews. Empir. Softw. Eng. 23(6), 3734-3767 (2018)

Jha, N., Mahmoud, A.: Mining non-functional requirements from app store reviews. Empir. Softw.
Eng. 34, 3659-3695 (2019)

Jongeling, R., Datta, S., Serebrenik, A.: Choosing your weapons: On sentiment analysis tools for soft-
ware engineering research. In: International Conference on Software Maintenance and Evolu-
tion, pp. 531-535 (2015)

Jongeling, R., Sarkar, P., Datta, S., Serebrenik, A.: On negative results when using sentiment analysis
tools for software engineering research. Empir. Softw. Eng. 22(5), 2543-2584 (2017)

Khalid, H., Shihab, E., Nagappan, M., Hassan, A.: What do mobile app users complain about? IEEE
Softw. 32(3), 70-77 (2015)

Khatiwada, S., Tushev, M., Mahmoud, A.: Just enough semantics: an information theoretic approach
for IR-based software bug localization. Inf. Softw. Technol. 93, 45-57 (2017)

Lin, B., Zampetti, F., Bavota, G., Di Penta, M., Lanza, M., Oliveto, R.: Sentiment analysis for soft-
ware engineering: how far can we go? In: International Conference on Software Engineering
(2018)

Lovins, J.: Development of a stemming algorithm. Mech. Transl. Comput. Linguist. 11, 22—31 (1968)

Maalej, W., Nabil, H.: Bug report, feature request, or simply praise? On automatically classifying app
reviews. In: Requirements Engineering Conference, pp. 116-125 (2015)

Martin, W., Harman, M., Jia, Y., Sarro, F., Zhang, Y.: The app sampling problem for app store mining.
In: Working Conference on Mining Software Repositories, pp. 123—133 (2015)

Martin, C.: The sharing economy: a pathway to sustainability or a nightmarish form of neoliberal
capitalism? Ecol. Econ. 121, 149-159 (2016)

Martin, W., Sarro, F., Jia, Y., Zhang, Y., Harman, M.: A survey of app store analysis for software
engineering. Trans. Softw. Eng. 43(9), 817-847 (2017)

Matzler, K., Veider, V., Kathan, W.: Adapting to the sharing economy. MIT Sloan Manag. Rev. 56(2),
71-77 (2015)

Mcilroy, S., Shang, W., Ali, N., Hassan, A.: User reviews of top mobile apps in apple and google app
stores. Commun. ACM 60(11), 62—67 (2017)

Mcllroy, S., Ali, N., Khalid, H., Hassan, A.: Analyzing and automatically labelling the types of user
issues that are raised in mobile app reviews. Empir. Softw. Eng. 21(3), 1067-1106 (2016)
Mihalcea, R., Corley, C., Strapparava, C.: Corpus-based and knowledge-based measures of text
semantic similarity. In: National Conference on Artificial Intelligence, pp. 775-780 (2006)
Mimno, D., Wallach, H., Talley, E., Leenders, M., McCallum, A.: Optimizing semantic coherence
in topic models. In: Conference on Empirical Methods in Natural Language Processing, pp.

262-272 (2011)

Mohagheghi, P., Dehlen, V.: Existing model metrics and relations to model quality. In: ICSE Work-
shop on Software Quality, pp. 39-45 (2019)

Murillo, D., Buckland, H., Val, E.: When the sharing economy becomes neoliberalism on steroids:
unravelling the controversies. Technol. Forecast. Soc. Chang. 125, 66—76 (2017)

Newman, D., Noh, Y., Talley, E., Karimi, S., Baldwin, T.: Evaluating topic models for digital librar-
ies. In: Annual Joint Conference on Digital Libraries, pp. 215-224 (2010)

Nonaka, I.: A dynamic theory of organizational knowledge creation. Organ. Sci. 5(1), 14-37 (1994)

Pagano, D., Maalej, W.: User feedback in the appstore: an empirical study. In: Requirements Engi-
neering Conference, pp. 125-134 (2013)

Q) Springer
262 Automated Software Engineering (2020) 27:229-263

Palomba, F., Linares-Vasquez, M., Bavota, G., Oliveto, R., Di Penta, M., Poshyvanyk, D., De Lucia, A.:
Crowdsourcing user reviews to support the evolution of mobile apps. J. Syst. Softw. 137, 143-162
(2018)

Panichella, S., Di Sorbo, A., Guzman, E., Visaggio, C., Canfora, G., Gall, H.: How can I improve my
app? Classifying user reviews for software maintenance and evolution. In: International Confer-
ence on Software Maintenance and Evolution, pp. 767—778 (2015)

PwC: The sharing economy: consumer intelligence series. PricewaterhouseCoopers LLP (2015)

Rehurek, R., Sojka, P.: Software framework for topic modelling with large corpora. In: Workshop on New
Challenges for NLP Frameworks (2010)

Rubén: Domain analysis: an introduction (1990)

Sorbo, A.D., Panichella, S., Alexandru, C., Shimagaki, J., Visaggio, C., Canfora, G., Gall, H.: What
would users change in my app? Summarizing app reviews for recommending software changes. In:
International Symposium on Foundations of Software Engineering, pp. 499-510 (2016)

Sousa, D., Sarmento, L., Rodrigues, E.: Characterization of the twitter replies network: Are user ties
social or topical? In: International Workshop on Search and Mining User-generated Contents, pp.
63-70 (2010)

Steinwart, I.: On the influence of the kernel on the consistency of support vector machines. J. Mach.
Learn. Res. 2, 67—93 (2001)

Svedic, Z.: The effect of informational signals on mobile apps sales ranks across the globe. Ph.D. thesis,
School Business Faculty, Simon Fraser University (2015)

Tang, J., Meng, Z., Nguyen, X., Mei, Q., Zhang, M.: Understanding the limiting factors of topic modeling
via posterior contraction analysis. In: International Conference on Machine Learning, pp. 190-198
(2014)

Thebault-Spieker, J., Terveen, L., Hecht, B.: Avoiding the south side and the suburbs: the geography
of mobile crowdsourcing markets. In: The ACM Conference on Computer Supported Cooperative
Work and Social Computing, pp. 265-275 (2015)

Thebault-Spieker, J., Terveen, L., Hecht, B.: Toward a geographic understanding of the sharing economy:
systemic biases in uberx and taskrabbit. ACM Trans. Comput. Hum. Interact. 24(3), 40 (2017)

Thelwall, M., Buckley, K., Paltoglou, G., Cai, D., Kappas, A.: Sentiment strength detection in short infor-
mal text. J. Assoc. Inform. Sci. Technol. 61(12), 2544-2558 (2010)

Thelwall, M., Buckley, K., Paltoglou, G.: Sentiment strength detection for the social web. J. Am. Soc.
Inform. Sci. Technol. 63(1), 163-173 (2012)

Turney, P.: Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In: European Conference on
Machine Learning, pp. 491-502 (2001)

Tussyadiah, I.P., Pesonen, J.: Impacts of peer-to-peer accommodation use on travel patterns. J. Travel
Res. 55(8), 1022-1040 (2016)

Ustiin, B., Melssen, W., Buydens., L.: Facilitating the application of support vector regression by using a
universal pearson vii function based kernel. Chemometrics and Intelligent Laboratory Systems 81,
29-40 (2006)

Villarroel, L., Bavota, G., Russo, B., Oliveto, R., Di Penta, M.: Release planning of mobile apps based on
user reviews. In: International Conference on Software Engineering, pp. 14—24 (2016)

Wang, S., Manning, C.: Baselines and bigrams: simple, good sentiment and topic classification. In:
Annual Meeting of the Association for Computational Linguistics, pp. 90-94 (2012)

Williams, G., Mahmoud, A.: Analyzing, classifying, and interpreting emotions in software users’ tweets.
In: International Workshop on Emotion Awareness in Software Engineering, pp. 2—7 (2017)
Williams, G., Mahmoud, A.: Mining Twitter feeds for software user requirements. In: International

Requirements Engineering Conference, pp. 1-10 (2017)

Williams, G., Mahmoud, A.: Modeling user concerns in the app store: a case study on the rise and fall of
Yik Yak. In: International Requirements Engineering Conference, pp. 64—75 (2018)

Wohlin, C., Runeson, P., Hést, M., Ohlsson, M., Regnell, B., Wesslén, A.: Experimentation in Software
Engineering. Springer, Berlin (2012)

Xu, L., Shah, N., Chen, L., Diallo, N., Gao, Z., Lu, Y., Shi, W.: Economy: Privacy respecting contract
based on public blockchain. In: The ACM Workshop on Blockchain, Cryptocurrencies and Con-
tracts, pp. 15-21 (2017)

Yan, X., Guo, J., Lan, Y., Cheng, X.: A biterm topic model for short texts. In: International Conference on
World Wide Web, pp. 1445-1456 (2013)

Yu, E.S.: Conceptual modeling: Foundations and applications. chap. Social Modeling and i*, pp. 99-121.
Springer (2009)

Q) Springer
Automated Software Engineering (2020) 27:229-263 263

Zervas, G., Proserpio, D., Byers, J.: The rise of the sharing economy: estimating the impact of Airbnb on
the hotel industry. J. Mark. Res. 54(5), 687-705 (2017)

Zhao, W., Jiang, J., Weng, J., Lim, E., Yan, H., Li, X.: Comparing Twitter and traditional media using
topic models. In: European Conference on Advances in Information Retrieval, pp. 338-349 (2011)

Zhu, G., Kam Fung So, K., Hudson, S.: Inside the sharing economy: understanding consumer motiva-
tions behind the adoption of mobile applications. Int. J. Contemp. Hosp. Manag. 29(9), 2218-2239
(2017)

Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.

Q) Springer
