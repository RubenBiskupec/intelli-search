NP] | Digital Medicine

PERSPECTIVE OPEN

www.nature.com/npjdigitalmed

® Check for updates

Challenges for the evaluation of digital health solutions—A
call for innovative evidence generation approaches

Chaohui Guo’, Hutan Ashrafian@®’, Saira Ghafur?, Gianluca Fontana’, Clarissa Gardner* and Matthew Prime @'

The field of digital health, and its meaning, has evolved rapidly over the last 20 years. For this article we followed the most recent
definition provided by FDA in 2020. Emerging solutions offers tremendous potential to positively transform the healthcare sector.
Despite the growing number of applications, however, the evolution of methodologies to perform timely, cost-effective and robust
evaluations have not kept pace. It remains an industry-wide challenge to provide credible evidence, therefore, hindering wider
adoption. Conventional methodologies, such as clinical trials, have seldom been applied and more pragmatic approaches are
needed. In response, several academic centers such as researchers from the Institute of Global Health Innovation at Imperial College
London have initiated a digital health clinical simulation test bed to explore new approaches for evidence gathering relevant to
solution type and maturity. The aim of this article is to: (1) Review current research approaches and discuss their limitations; (2)
Discuss challenges faced by different stakeholders in undertaking evaluations; and (3) Call for new approaches to facilitate the safe

and responsible growth of the digital health sector.

npj Digital Medicine (2020)3:110; https://doi.org/10.1038/s41 746-020-003 14-2

INTRODUCTION

Digital health has evolved rapidly since the concept was first
introduced in 2000 by Seth Frank'’. The FDA considers digital
health as a broad scope that includes categories such as mobile
health, health information technology, wearable devices, tele-
health and telemedicine, and personalized medicine’, a definition
we follow in this article. Indeed, the numbers of digital health
solutions are booming, for example, more than 300,000 health
applications exist with more than 200 added daily*. Digital
solutions can be grouped as follows, based on potential risk to
patients”: (1) Solutions that improve system efficiency but with no
measurable patient outcome benefit; (2) Mobile digital health, that
inform or deliver basic monitoring, and encourage behavior
change and self-management; (3) Clinical decision support (CDS),
and prediction models, that guide treatment, deliver active
monitoring, calculate and/or diagnose.

The evidence requirements of regulators are determined by a
product’s intended use claims, as such, a large proportion of
digital health solutions (e.g. administrative tools and wellness
apps) fall outside of their jurisdiction. Therefore, a huge challenge
for end users, such as patients and providers (e.g. healthcare
professionals, hospital administrators), is how to determine a new
solution's credibility and compliance with standards. Furthermore,
end users have different thresholds for acceptance of innovation
and can be grouped into five archetypes: innovators, early
adopters, early majority, late majority, and laggards®. In addition,
aging adults, considered amongst the most digitally divided
demographic group’, present unique challenges and dedicated
efforts exist to develop strategies for implementation’ '®.
Conversely, challenges exist for healthcare innovators to best
demonstrate solution impacts and to ensure compliance with
standards, these include: unclear end-user expectations; uncer-
tainty of evidence generation approaches; and, keeping up to date
with the evolving compliance landscapes.

This article discusses the challenges for providing timely and
robust evidence, to meet end-user expectations, in the context of

digital health solutions. Specifically, we consider how the cadence
of traditional research approaches are misaligned with the “fail
fast, fail often” mantra espoused by technology start-ups. In
addition, we introduce clinical simulation-based research as a
potential opportunity to bridge the evidence gap.

A RAPIDLY EVOLVING GUIDANCE AND REGULATORY
LANDSCAPE

Over the last 10 years a plethora of guidance has been developed
for digital health innovators. In Table 1, we highlighted 10 of the
key guidance (e.g., Continua Design Guidelines 2010, WHO
monitoring and evaluating digital health solutions 2016, NICE
evidence standards framework 2019; US FDA pre-certification
program—a working model 2019, and FDA Proposed Regulatory
Framework for modifications to Artificial intelligence/Machine
learning-based Software as a Medical Device 2019). We ordered
them by date first published and provided for each guidance a
brief summary, applicable areas within digital health, releasing
organization, and its main activities (Table 1). We observed that
development of such documents follows a_ pattern: initial
development by industry, optimization by non-government
organizations, and finally refinement by government agencies. In
addition, academic initiatives and institutions have produced
critical thought leadership, often acting as counterbalance to
industry proposals (Table 2; The digital health scorecard 2019). In
Table 2, we highlighted five academic recommendations relevant
to undertaking evidence generation studies for digital health
solutions.

Until recently regulators relied upon modifications to existing
medical device (software) regulations and innovators were
encouraged to conform to development standards, as shown in
Table 3, where we highlighted eight regulations and standards
relevant to digital health solutions (e.g., IEC Medical device
software, ISO Health informatics—requirements for an electronic
health record architecture). However, the speed of development,

 

‘Roche Diagnostics, Basel, Switzerland. “Imperial College London, London, UK. “email: matthew.prime.mp1@roche.com

Scripps Research Translational Institute

np} nature partner

journals
C. Guo et al.

Np)

 

‘SOdIANP |edIPawW “Ba

‘SBuly} JaYyJO JsHuOWe ‘UOISIAJadNs
pue JO1}U0D 9y} JYHNoIU}

yyeay d1qQnd Hurowoijd pue
Huljda}01d 10} ajqisuodsad sl WG 4 eUL

‘a0UaHJaAUOD

pue uoleziuouwey Ajojejnbai ad1Aap
JeD1patu jeuUOHeUJa}U! d}e19}999e

0} pue “(41HD) Sed!Aaq |e2!pew

UO 9dJO4 ySe] UONeZIUOWWeH Jeqo|y5
3} JO yJOM JeEUONepUNo} Buoys

394} UO PjING 0} 494}960} awW0D
aAeYy OYM P]JOM ay} puNnole Woy
$10}e|NBad ad1Aap jed1paew jo dnosb
Aje\UunjOA e SI 3 ‘UoezIUOWeY
Aioye|n6ai ad1Aap |ed1paw

Ul SUO!DOJIP JINN} SSNISIP 0} LUNIOY
ese LLOc Ayeniqa4 ul paAladuod
SEM (4YOQWI|) WINsJO4 s1Oye;NHBay
DAI [Pd payy |euONeUsa}U] SYL

uoluA ueedoing
34} JO YDUeIG BAIINDEXF

YYeaH Dyjqng jeuoleusazu|

‘SOdIANP |edIPawW “Ba

‘SBuly} JaYyJO JsHuOWe ‘UOISIAJadNs
pue JO1}U0D 9y} JYHNoIU}

yyeay d1qQnd Hurowoijd pue
Huljda}01d 10} ajqisuodsad sl WG 4 eUL

‘ABojouyre}

pue uOoleuUJojul Jo UOIed1;dde

3y} YBnosy} YyyYeay jo UOI}eWOJSUeI}
3y} Buloddns sapes|

1yBnou} pue JosiApe jeqo)6 e sI
SSWIH ‘Auedwo> uonerouu| (SSWIH)
Ayald0sg swiaysks Juawabeuely

pue UO!eWJOJU] a1eD4YeEdH
paseq-diysiaquiaw e SI WHDd

uoleziuebio
Hulseajas Jo saijiAnoe uleyy

(V4) “uoneysiulWpY
Bnig 79 poof

(4HQWII) Winsoy suojejnbay
BdIAVQ JerdIpeyy |euo!zeuUsE}U]

UOISSILULUOD URSdOINA

(OHM) YoHeziuebiC
YI22H PLOM

(V4) “uoneysiulWpY
Bnig 79 poof

(WHDd) a>2uel|y a1eD4eaH
pa}DaUUOD paZzI|eUuOssad

ajqisuodsai uoneziuebio

paysijqnd
Jsuy 91eq

8LOZ = suoljedijdde jedipaw ajiqoyy)

(GWeS) 22!A0q
Z10Z JeD1Ipayy e se aeMmYosS

9L0C BIeMYOS aeYyIedH

9L0Z SUOIINJOS YJeay [eUbIP [IV

9L0Z SUOIINJOS YJeay [eUbIP [IV

(spuepuejs e}ep pue

Ayyjiqesadouajul uo siseydwa
Jeyndied) suonediyjdde ajiqow

OLOZ pue sadlAap payauUOD

yyeay je1bip
UIUUM seale ajqerjddy

‘aynjeys jesapajy JapUN sadIAap

|PED1IPALU SE SUO!}IPUOD 4840 JO aseasip

juaAaid Jo ‘ayebijiw ‘ain>d ‘esoubeip
‘yed1} 0} papusaju! ase Jey} sdde
a|IqowW ysOoW saasJaAO ADUabe aul
‘(uolze]NBa1 Wq4 0} pafqns aie sdde
ajiqow |e Jou) YDeoidde paseq-ysiy

saljUunoD

SNOHEA Ul SUOYa AojejnBas Gwes 404
siseq dU} Se ({YQWI) WNIOY suojejnbay

Jd1Aaq |edIpayy jeuONeUsazU|
ay} Aq padojanap s}uauns0q

BDIAVP DI}sOUbeIp OJUA UI
JO jedIpau e se Ajijenb you se0p
pue sa0p aiemyos uayM Bulqiidseq

9]DAD-a J!
ainyew ynNpoid sy Huoje suoinjos
jeu1Hip Jo uolepyeA pue UoHenjerd

UY} JOJ YIOMOWe [eEIBUDH e SAPIAOId

(‘Ja ‘YSU JO BDUeIIIO}Z JUAIZeEd

‘syINSa1 BAIWEHaU-as|e} JO DAITISOd
-asjey ‘YSU JO pooy!ay!| ‘AWeAes ys
se YNs ‘SUOISUBUUIP /) SS pue (D}9
‘s}youaq d10W JO aUO BuldUuaadxa
sjuaijed Jo poou!jay!| ‘apnyubew

‘SadAj se yons ‘SuOoIsSUuaWIP /) s}yoUaq

9} YO UI! SAadIAap jedIpaw ajenjerAd
0} yomaWweJ [2e49UDH e HUIPIAOId

"SSOUIJOM
pue yyeay jeuosiod HuroWUOW
suolerijdde 10} pasn s}uaUOdWOD Jo

Ayyiqesadoia}ul 9y} aiNsua 0} pouinbeai

ase JeY} CLa}ID pue spuepuejs
BHulAjJapunN Jo yIomawes e Huluysaq

SUOIdIDSap JUaLUNIOG

7 ,e2uepinb
sdde jesipew ajiqow Yq4 Sn

eg( LVN) UOHenjeag

jediul] GWesS AYGWI pue “(€ZN)
suay}sKks Juswabeue~= Aujeno
GWwes 4YGWI “(ZLN) 44OMawel4
uoljeziobaje> ¥s!1y¥ GWeS JHCWI
‘(OLN) suontuyeg Aey Gwes JHGWI
g6(9/L'S AAAGGAW) SE2!AEq [eI!PeW
Jo yAomMawel4 Aloje|NBay ay}
UIYUM adedyYeaH Ul PpasA a1eMYOS
BUO|Y Pues JO UOHedYISSe|>

pue uoledyI]eNO ay} UO sauljapIND

9 SUONUeMa}U! Yyyeey je61p
BHulyenjeas pue BulsoyUOW OHM

| S82!Aap Jed1pau
JO} YIOMaWWeI YSU-JYaUEq SYC4

ig(DD)
SAUI|apIND ublsaq enuljuoy

3]4. JUaWINDOG

 

‘(AANSNeYXa JOU) SUOINJOS YYyeay je61p 0} JUeAa|a S}JUaLUNDOP UOIssndsip pue aduepPINH pa}da|aS

“L e1qeL

Scripps Research Translational Institute

npj Digital Medicine (2020) 110
Np}
3

‘SOdIANP |edIPawW “Ba

‘SBuly} JaYyJO JsHuOWe ‘UOISIAJadNs
pue JO1}U0D 9y} JYHNoIU}

yyeay d1qQnd Hurowoijd pue
Huljda}01d 10} ajqisuodsad sl WG 4 eUL

 

C. Guo et al.

‘SOdIANP |edIPawW “Ba

‘SBuly} JaYyJO JsHuOWe ‘UOISIAJadNs
pue JO1}U0D 9y} JYHNoIU}

yyeay d1qQnd Hurowoijd pue
Huljda}01d 10} ajqisuodsad sl WG 4 eUL

/(SHN) 22!\49S YyeeH

jJEUONeN 9y} UIYUM salHojouyrda}
yyeey Jo asn ay} ‘sbulyy

Jayjyo ysbuowe ‘sauljapinb saysijqnd
yaiym wopbuly payun ey}

Ul YEAH JO JUBWYed|q 9y} Jo Apog
dNQnd jeyUawWyedap-uOU dAINDEXy
SO|EM 8

puejbuq ul siayQew ased |eID0s

pue yyeay uo AdI|Od JO} ajqisuodsay

uoleziuebio
Hulseajas Jo saijiAnoe uleyy

(V4) “uoneysiulWpY
Bnig 79 poof

(V4) “uoneysiulWpY
Bnig 79 poof

(JDIN) 22uUa]J/e2xq sey pue
YYedH JOJ d3NWISU] JeUOHeN

adeD [eID0S
7 YYedSH JO JUsUUed|q YN

ajqisuodsai uoneziuebio

(yoeqpasy
JO} paseajad
yUawuNdOp

Yep) 6L0Z

6LO7

paysijqnd
Jsuy 91eq

SUOINJOS
paseq-swiyywobye yw pue |v

SUOINJOS
yyeay jeubip 4s MoT

(suuywobye

aduabijja}U! jeIye
aAidepe yim spnpoid
"]Px9) SUOINjOS YyYeey jewWbig

suOHINjos Yyyeay [eubip [IV

yyeay je1bip
UIUUM seale ajqerjddy

sAemujed WWd pue ‘OAON

3d ‘(A)OLS a4} Hulpnyjoul ‘sueiBoud
JoyJeWasId JUaIND WoO} Sad1}DeI1d
sabelanag] osje 3 “wesbold (aD

-31d) UONeEIYIWIIIIg BIEMYOS YYe2aH
jeu16iq ay} Ul pauOlsiAUS se Ydeoidde
JidL paseg-uoneziuebio ay}

pue ‘aouepinb suonedyipow aiemyos
9} Ul Sejdiduud JUsUaHeuew

YSU ‘YOMaWeI 4SU-]YoUdq

SVG4 ‘se|diouud uonezioba}e>

ASU (4UQWII) Windoy ssojejnbay
Bd1AVQ Jedpeyy JeUOIeUsA}U]
peziuowuey Ajjeuoneusea}U! 9y} UO
poseq si! yeYi QWeS peseq -TW/IV 03
SUOI}EIYIPOW JO} YIOMAWIEI PasOdold
DDUCWOJIad PyiOM-jead HulOWUOW 0}
Pa}ILUWOD ase pue (ODD) BuUa||adxa
jeuoljeziuebio pue Ayjenb jo ainyn>
Jsnqod e pajesjsSUOWap sAeY OUM
SJaINJDeJNUeWW LOI SBDdIABP |ed1|PawW
paseq-aieMyos JO JYBISIaAO JUaIDYYo
Joy Aemyyed AleyUnjoA e sapiAold

‘(1)u) [242 payjosju0s

peziwopuel Jo Apnjs uoljUuaAJe4u!
Ayyenb-yBiy e si ssauanipeaya

W194} Hurenjead 10} puepueys pjob au}
‘Suolpuny Jal JaYHIY YUM syNpoid
JO4 *(SJ9SN 0} YSU UO paseg Jal}
-3914}) JNpod Jay} JO UOHeDYISSe|D
jeuolouny ay} BulAynuUap!

yBnoiy} Jasn ay} Hulpinyd ‘(g uolD~aS)
spuepuej}s DIWOUODS puUe (\Y UO!}DaS)
SPIPPUP}s SSOUDAIPAJo JOJ UO!}EIBUSB
BDUaPIAD UO BdUePINH HulpiAoid

SUOIINJOS YYeay jeywHip Huryenjers pue
BHuldojanap 10} sajdiouid OL SaplAoid

SUOIdIDSap JUaLUNIOG

oo (GWEeS)

DIAIaq Jedpay e se a1eEMYoS paseq
-Buluseay aulydeyy/aUebI|}E}UI!
JelyIy 0} suoedYIPOW

JO} yIOMaWe4 AJoyejnBay pesodoid

, (le@pow Bupom e)
weiBoid uonedyiyad-a1d Wq4 Sn

omawedy
SpJePUR}S SIUAPIAS DIN

- Abojouyre} aued pue yiyeay
UdALIP-e}ep JO} JONPUOD jo apo>

3]4. JUaWINDOG

 

panuljUoD | aIqeL

npj Digital Medicine (2020) 110

Scripps Research Translational Institute
np}

C. Guo et al.

 

Table 2.

Tool/framework Document descriptions

Selected academic recommendations relevant to undertaking evidence generation studies for digital health solutions (not exhaustive).

Applicable areas within digital health Date first
published

 

Quality in prognosis studies (QUIPS)'°'

6 factors to consider when evaluating validity and
bias in studies of prognostic factors: participation,

Prognosis models (incl., 2006

individualized predictive model)

attrition, prognostic factor measurement,
confounding measurement and account, outcome
measurement, and analysis and reporting

The Cochrane risk-of-bias tool for
randomized trials (RoB2)'°7

The risk of bias in nonrandomized
studies of interventions (ROBINS-I)'°?

participant selection, etc.)
PROBAST: A Tool to Assess the Risk of
Bias and Applicability of Prediction
Model Studies!

Set of domains of bias to guide the evaluation
about features of a trial that are relevant to risk of
bias based on answers to the signaling questions

Tool to assess risk of bias in non-randomized
studies over 7 domains (e.g., missing data,

Tool to assess the risk of bias and applicability of
prediction model studies (20 questions). Informed
by a Delphi procedure involving 38 experts and

Randomized studies (suitable for
individually randomized, parallel-
group trials)

2008 (updated
in 2011)

Non-randomized studies 2016

Predictive models (incl., CDS
algorithms)

refined through piloting. It is not suitable for

comparative studies.

The digital health scorecard?

Academic developed framework that proposes

All digital health solutions

validation should include three aspects: (1)
technical validation (e.g., how accurately does the
solution measure what it claims?), (2) clinical
validation (e.g., does the solution have any support
for improving condition-specific outcomes?), (3)
system validation (e.g., does the solution integrate
into patients’ lives, provider workflows, and

healthcare systems).

diversity of interventions, and potential risks has finally prompted
policy-makers to produce more targeted guidance on solution
classification and evidence requirements”’'~'* (Tables 1 and 3).
For example, one initiative, the FDA Pre-certification Program",
seeks to streamline the approval of Software as a Medical Device
(SAMD), and proposes to assess both development organization
and product capabilities. Notwithstanding, current guidance does
not go far enough to enable innovators and end-users to know
what evidence generation approaches are appropriate, and
practical, for all classes of digital health solutions throughout the
product lifecycle.

TRADITIONAL APPROACHES TO EVALUATION OF DIGITAL
HEALTH SOLUTIONS

The most commonly recognized evidence for healthcare inter-
ventions is the randomized controlled clinical trial (RCT)'®'®, yet,
only a handful of products have been tested in this way as shown
by recent systematic review'’ and our searching results in Table 4,
where we illustrated recent studies evaluating digital solutions
and their methods (including study designs, study length, sample
size, etc.). Indeed, a recent systematic review of publications
between 1995 and 2016 identified just 24 RCTs for the high-risk
CDS category'’”. In our opinion, this lack of studies indicates that
these methods are no longer practicable, likely due to the speed
of digital product development and iterative upgrading. In Fig. 1,
we mapped existing approaches along two dimensions; strength
of evidence and study duration, which demonstrated the current
methodological gap to evidence needs and opportunity for more
innovative and agile approaches. In this section we highlight a few
of the more common methodologies, discuss strengths and
limitations, and provide examples of their application (Table 4).

Surveys and interviews

In the early stages of development innovators seek to establish
product usability, feasibility, and efficacy'®. Surveys and/or inter-
views are often employed, which are low-cost, efficient, scalable

npj Digital Medicine (2020) 110

 

tools to collect attitudes, user experience, and suitability insights.
Commonly used methods include usability testing, user-center
design, net promoter score survey (e.g. to rate likelihood to
recommend a product), online surveys, and log-file data analyses
(e.g. to evaluate how users interact with the digital solution)'’.
Such approaches have been used to explore user views on the
usefulness of digital storytelling°, to assess a web-based network
for MS patients*', and to collect attitudes towards digital
treatment for depression**. Despite being common, few efforts
are turned into peer-reviewed publications'”, likely because the
main purpose was to generate insights for internal use (e.g.
product development) or external customer communication (e.g.
case studies, presentations), and can be challenging to pass the
peer-review for such work due to its relatively lower evidence
strength '°.

A key approach for digital solution development is usability
testing which has been widely utilized to examine whether
specified users can achieve intended use effectively and
efficiently**-*°. Typically, an intended user completes tasks
and is observed for where they encounter problems. This can
be exploratory, to identify new features or functionalities, or
comparative testing A vs. B*”’*®. Studies are conducted by UX
researchers, who synthesize results and translate to actions
(e.g. product improvements). Data collected can be qualita-
tive (e.g. observations of problems) and/or quantitative (e.g.
task time, task success rates). Evidence strength depends
upon study design, for example, task-based and controlled
studies that collect quantitative data and can be replicated in
other settings/sites, generate stronger evidence, whilst
surveys and self-reported behaviors provide weaker evidence,
as suggested by UX practitioners*’. Controversy exists
regarding the appropriate number of participants. Whilst
there is no “single correct number”, for formative testing 5
participants is common (“the magic number 5”), compared
with 20 participants for summative tests, which offer a tighter
confidence interval?°.

Scripps Research Translational Institute
Np}
5

aaoge sy

aaoge sy

uolu¢ UeadoNy ay} Jo YdUeIG aAI}NIaxy

 

C. Guo et al.

aaoge sy

aaoge sy

ABojouypay

ase>D YyyeaH Ul suoledjddy || 0} suleyed
(08'0vZ'SE) OSI ‘SuoHeziueb1o spiepuejs
JEUOIeU SNOLWeA WO} SaAI}E}UaSaIdaI

Jo pasodwod Apog buljjes-psepue}s
jEUO!}eUJA}U! Ue S| UO!eZIPJepUeIsS

JO} UOHeZIUeHIC jeUO!eUJE}U] SYL

UOISSILIWUO>)
JPED1UYII}O1}Da]/q JEUONeUIA}U|

uoleziuebsio Hulseajas Jo salAnoe uleyy

(OSI) YoHezIUebIC
SpJePUe}S JEUO!}EUIS}U]

(OSI) YoHezIUebIC
SpJePUe}S JEUO!}EUIS}U]

UOISSILULUOD URSdOINA

JUSPISA1dg
ay} Aq Me] O}U! paubis
pue ssaibuo> Aq passed

(OSI) YoHezIUebIC
SpJePUe}S JEUO!}EUIS}U]

(OSI) YoHezIUebIC
SpJePUe}S JEUO!}EUIS}U]

(OSI) YoHezIUebIC
SpJePUe}S JEUO!}EUIS}U]

(OSI) YoHezIUebIC
SpJePUe}S JEUO!}EUIS}U]

a|qisuodsead uoneziuebio

ABojouyre}
ase dyyeay

Buiwosdn ul suoiedijdde ||

ABojouyre}
asedyyeay
ul suonesidde ||

(ZL 2194) (awes)
ad1Aaq Jedpa
e se aleMyos

SUOIJNIOS
yyeay je16ip |iVv

ABojouyre}
asedyyeay
ul suonesidde ||

ABojouyre}
asedyyeay
ul suonesidde ||

ABojouyre}
asedyyeay
ul suonesidde ||

aJeMYOS
90072 BIIASP [EII|Pe

yyeay jeubip ulyyuM
seaie ajqeaijddy

paysijqnd
Jsuy a1eq

‘(AANSNeYXa JOU) SUOINJOS YYyeay jeHIp 0} JUeAa|a Spyepue}s pUe SUO!e]NBa pa}da]|aSg

jUaWdojaAap Japun

“UOIJELWUOJUI
yyeay jeUOSJad JO UOIID9}OJd JY} JO} SOIIAJAS
uoleziuuiAuopnasd Bulsn uolya}01d AdeAd

JO} sjUaWAaJINbas pue sajdisuiid sure}uo>D

spnpoid

DIREMYOS 0} saijdde Ajjedyideds 71 any
uoleryissej> WaAIMOY ‘s}oNpoid qwyes JO}
ajqeaijdde si uone;nbay ay} Jo Ajasnua ay

(SODIABP |edIPawW SAIZEAOUU!

UleyaD JO MAaIAdJ JY} Paeds 0} paubisap)
weiboid sadiAap ybnosyyeeAg By} pue
‘Adeidy} padueApe JUIDIPAaW SAI}eJBUDHaL
34} Bulpnyjdu! ‘suuesHboid JUsWUdoOjaAap
ynpoid paypedxs HulaAod Wed duo YUM
‘yyeay 0} pajyejas adods peoigq BuIpiAojg

(S-UH3) Waysks psoray
YedH D1U0I}D9]/9 Ue Ul JUdSIId aq AewW
JeY} SUOIJDUNJ JO SI] PDUDJBJOI & SBPIAOId

‘spaau ssauisng
JI9Y} Jos 0} sdew Hulkjdde Jo Hulyeas> YUM
pabuey> suoneziuebio Oj aduepinb sapiAoid

94N}D9}1YDIe
YHA Ue :UO!IJEWOJU! (YHA) PAoOde1 YWesy
DIUOI}DaJa S9}eD1IUNWIWWOD pue sabeuew
‘sassad0jd yey} Wayshs e JO dIN}Da}YDIe
UY} JO} s}JUdDWaIINbaJ JO Jes ay} SAUYaq

(SN) s27e1S peuuy ayy

pue (qq) uolufM UesdoiNy 9y} Ag peziuouwey
SI }| ‘SODIASP |ed1IPawW UIYUM aIeEMYOS

pue aieMyos jed1|pawW Jo JUBWUdOJaAep

DY} JOJ SJUDWAIIND|. B]DAD af] HUIPIAOIg

SUOIdIDsSap JUuaLUNDOG

2 C1W)

Hulusea] aulydeyy Huisq swiayshs (|v)
a.uabyjazu jeIyy 10J ysomawes4

€SOET CD DAl/OSI
|, WoeziuAuopneasd

—SsDIJEWOJU! YYe2aH
LLOC@:ZECSC OSI

o19PZ/ZL0Z (NJ) YORe|NBay

—(YGW) YORE|NBay a21A0q |ed1IPpay

so LPW Send Ainquad 3SLZ SN

30 (W4 YH4) Z aseajay ‘apoyy)
jeuolDUNY Wa}sks-sproday YyeeaH

31U01}9 3/9 ZH —SDeW LOU] YYeaH

IZ1H] SLOZ:L8ZOL Z1H/OSI

jeds1Hbojoulwa} UseMjeq Hbuiddew
Jo sajdiduug —soneuWJojU! YYedH
VLOC:OOECL Y1/OSI

oo NPENYyre
pJodaJ YYeay DWuUoIda]a ue

JO} SJUaUaIINbaY —sdIWeULOJU! YYeaH

LLO@:80€8L OSI

59 |S8SS220Id B]DAD ayt] BseMIJOS

-dIEMYOS DDIAVP [EDJIPS|] -VOETI DAI

3]4. JUaWINDOG

“€ 91qeL

 

npj Digital Medicine (2020) 110

Scripps Research Translational Institute
C. Guo et al.

Np)

 

peyodai
you yyHuea] Apnjs pus-0}-pua
‘a6e}s UONDNPUOD ay} JO} SYBOM Z|

9LOCT-S LOZ

VN

Jaded au}
Ul PUNO} JOU Y}HuUa] Apnys puas-7-puy

(ZLOZ-9LOZ) S4eak Z

sueak ¢

(LLOZ-Z00Z) sueek +

(QLOZ-V LOZ) S4eak Z

nny

nny

NW

nny

NW

NW

a|6uls

a|6uls

uolenjeAd
BuLnp paajoaul

yybHua| Apnis sails Jo Jaquinn

 

-39d1Y} SIU} UI pa|josud diam Adesay} 91 232 ‘AMANDE
ued yensn ul (Sais G) sajJoqeip jeso pue (SUO!}edIP SW jediskud ‘a.uasaype uolNsabul
sjuedinivied 67 pue ‘UOIIPUOD OWG Z<) BAISUaedAYyNue Huijiey uoljedipaw Hulinseaw (Wosuas

ay} Ul (seus Z) sjuedioed Og UIYUM (%Z2) ZLVGH pue (6H WW OL < ddS) 2/qsebu! YUM Uae} UO!eD1IPaW
W210} Ul (SAUS ZL) SJUedIDIWed GOL dg BOISAS pazeATja YUM sjuedIIWeg ‘OWQ) buLayo sulipew jewbiq
uoissaiddns
(SDIUI]D ZL Ul SJUBIJeEd GpZ =U) |eJIA AJH pa}uauINd.OGg UO sweIBOld
Jaye] weak | Ayyiqeyiene ‘sa (Uyeayea|a} yyeayajal Jo Ayiqejieae Jo edu

Huvayo sdiulj> aed Arewiid UY} aJeNjeAS O} UO!}e1}SIUILUPY
EL JO sease adIAJas Ul S}UaIed G7 = UYE2H SueJa}e/\ BY} Ul UONeNeAFZ
u) Ayjigejiene Yyeaysje} a}elpauuwy weibojg peziuopuey-saysn|> V ,sulesBboid Yyyeayajal

poled aed jensn
e Bulmoyjo} sdiuyd Uuled Ayeldads g
Ul pa}UdWa|duu! AjjenuUanbas

S| UONUSAJOIU! B|IGOW dVINIGEM 61 (dVWW92M)
34} YdIUM Ul UBIsAap abpam paddays weiboid Juswabeuew ubisap peziuuopuel
paynsde1 9q O} USJIPJIYD OZL e HulAojdwia 104 jesodojd j0d0301d -}jas uled paJaAljap-JaUsayU| Ja}snd :aAipadsold

(SADIBW OG wil4N YDNOL suC)
JO1JUOD pue (‘dja ‘S}sIBOJOULIDOPUAS Aq
poubisap aiemyos paseq-auoud ja)

UO!JUSAJA}U! UIdMJeq PeziwOpUeL

JO1}U0D AjuaaAd pue sed1pesd ueliskyd
Ul €L “SA dnos6 UuolUdaAJa}U! By} AWUNWWOD 9aJY} WO paynsde4 gel00} JUaUUabeueW
ul Sajaqeip z adA} YUM sjualjed ¢| aJaM sajaqeip 7 adA} YUM sjuaned S2}JBQLIP SIGOW .2OG||AaM

(uBIsap UOIUSAJ9}UI

ysod 7» -asd ‘d}IS-1]NW) UONUdAJA}U!

34} HuIAIade/ JOU JAaYJOUe pUe

SUS UOIUSAJA}U! JY} Je JUBWAC|dap
jsod ul 7Zp pue -aid ul Jaye pue aiojaq uolssiwipe ,| UewWebeuew
OLOL ‘AS JOUJUOD ‘sod Ul GE ‘SA aid = ADUaHJaWA UO [YY YUM S}Npe Wolds Aunfur Asupry eynde 104
ul Sjuai}ed /9/ ‘dus UOI}EJUSWWa|dW]| pa}a]]OD dJaM eJep BWIOD}NO JedIUI]y AeMUed died pajqeusa AjjeibIG

Jadue> \seaJq YUM USWOM
ul Adesay} soyiqiyul aseyewose
WL-uoU UO SFE pue ‘UOI}IPUOD Jo uonenuluodsip Ajiea adnpal 0}
(WL) Buibessaw yxa] UO sjuaijed gee = BulBbessaw 3x9} jo je} peziumopuey ,coulbessaw 3x21

SURIDIUI|D JO UOI}AJDSIP
ay} Je HuowWUOW Wo IdWAS UUM

(a1e> jensn) aJe> JeNsn “SA saWODINO payioda

UOI}IPUOD [O4JUOD BY} UI GZE  -JUaNed YIM SON} pl|os padueApe
pue uolipuouabeyyod uonuaAsa}U! 40} AdesayyouayD }Ua!}edjno aulnos oc}21d2} IA pay ra]}0>
QU} Ul Ly YUM ‘syuaned 99/ 0} paubisse AjWopues syuaned S@9WOIINO payodal-jualyed

asdejas ye snjeys

BDUCWOJJad Ja}}8q PUe UO!D9}eP
asdejas Ajsed 0} anp (AjaAiydedsal
‘Wwe |O1JUOD pue je}UBWLadxa

9} Ul SYJUOW Z| “SA SL) SO parosduul

swojdwhs payodai-jjas uo paseq ¢, Wyobje ubisap aanesedwos
Sjuaned OZ, Wwyyobje dn-mojjo} payeipaw-qam Vv dn-mojjo} payeipaw-qay, ss paziWuepuey :aAlDadsold
aZzIs ajdwes sjlejap poujyew/ubIsaq uolyNnjos jeubig yoeoidde uonenjeaq

‘(AANSNeYXe JOU) SUOIINJOS YYyeey jeubHip Huyenjers ul saibojopoyuyew snower Huizijiyn saipnys jUa..aYy “yp |e]

Scripps Research Translational Institute

npj Digital Medicine (2020) 110
Np}
7

(29Q-|Udy 9L0Z) syyUOWW 6

pewoda JON

paywodas JOU dUljaWI} pus
-O}-puad ‘Y QL 4OO} Jas} UO!E;NWIS

pewoda JON

 

C. Guo et al.

peyoda ON

pewoda JON

(6L0Z) UaWYsI|qnd 0}

(SLOZ) JULWNIDEI WOU SURdA f YOO}
} J2}0} Ul ‘syBaM Q YOO} UO!DNpUOD
Apnys ‘9107 Avenue 03 SL0Z
isnBny wos adejd yoo) JUSWINIDeY

yiGua} Apnis

a|6uls

a|6uls

a|6uls

nny

a|6uls

a|6uls

a|6uls

uolenjeAd
BuLnp paajoaul
SOUS JO JAQUINN

(UoNUaAsaqU! UI g| ‘AUI|aSeq U! OZ) SE

aseyd uoljeiNwis jedIUI|> JY} Ul
(SAaPIAOIC AjNde} G PUe JUAPISe/ €) g

pa}npuod ajam AjjUa JaPsJO JO
suo!eiNWIS /7Z ‘Wia}SAS UO!UIOJU!
BY} WOJJ Pa}eI]Xa9 JJaM e}ep
jualjed Jo sydam JNO} ‘peaysul ‘awy
-]C31 JOJ PSAJOAU! SEM JUedIDIWed ON

°%BZ JO B1ODS & YUM passed
A\jnjsa2ons pue ,aaauiwexa jeieads,
e se pasajua Ajjeloyyo sem wWayshs
YEPeW au} ZLOT U| ‘@DUeWOJEd
ajyenjers 0} yepayy Aq uayewapun
319M $}S9} 9919eId 7 (DIIWN) eulyuD
JO} UOIJeUIWWeXA BDUaII7 [edIPey
jeuonen Buryey Ajjeisyjo aiojag

SJUBAS payejas-UONeIIPaW ZZ

sjuaijed
Od Siapso Adesay}OW AY) 6/76

paynsdaJ Jam Sinpe yr

aZzIs ajdwes

pue sueldiskud 10} peo aaizi1ubod

‘YH4 pedueYyUs JO YHA auljaseq
0} poubisse AjWoOpues sueldiskud

(sased daijg pue eluowNaud)
SOMeUSIS [eDUI]D BY} Hulydeua s10}De
juaijyed poules} pazipyepueys Jo sdijd

OdSPIA YUM HuljdeJsa}U! SUeIDISKAUd

‘D}9 UO!}eED}UNWIWOD
paAoidu pue ‘sioua JO UO DNpaJ
‘JUBWAaAOJdUUI Ssad01d UO s}>edUUI

9Y} a}eWISe pue Wa}sAs UO!IEWOJUI

jeuidsoy jeybip e ojU!l palajua

ale SuapJO [ed1PewW YdIYM YHnosy}
SS9D01d dy} JUaSaIdd1 0} padojavsp
SEM JaPOW UO!}E|NWIS JaINdWOD

euly> ul

uoneuiuexy Bulsuadiy jedpayy
jJEUONeN JO }Sa} UBM payd0W

ul (wuaysks YOUOSIeM “a"!) Npoid
Hulpesay sejiuis pue saauiwexa
ueuuny jesauab ysulebe painseaw
SEM JDURWUIOJJad Si ‘Sssad01d
Huluosesas pue Hbulusesy ayIj]-uewNy
e SOZIJIIN YDIYM ‘(YEpsyp) YAOMaWed
Huluseaj-daap e Huljenyjeag

PpaiNsealw sJaM (S}UdAd

Ayayes Juaed payejas-uonesdipaw
4aquunu Japso dale “6'd)

syedui pue ‘jeyidsoy ay} noybnolU}
poa}uawajdu sem waysks JOdD

‘yeu1dsoyu

Aueizia} e yo Aeq Adesayjoway> paq
-O€ & Ul (OD-d) Japso Adesay}yOWAY>D
paseq Jaded snsiaA (QD-D)

Japio Aedsayjyoways pezueyndwoy

uolsebbns Aiejuapasijue

ue Jo Buryyjem e apiAoid 0} JaYIOUM
‘os ji ‘pue ‘uolnsabbns AyANDe

Ue SPIAOJd 0} JAYJBYM paziwuopues
sdaisueayH ‘Apnjs ay} Jo Aep

ydea uo JUedIDIWed Yea JO} ‘S@WI}
pa}Dea]as-Jasn ye Aep Jad saul} avy
0} dn palaaljap aq pjnod suonsebbns
posojie} Ajjen}xa}UOD ‘UOIUdAJ9}U!
94} azIWIdoO 0} suolsabb6ns

AWAIDe sdaysueayH jo AdedyJo ay}
a}eNjers 0} jel} peziwopuel-O1DIW VV
Apnjs paziuopue

-J9ISN|D ‘YaemM-ZL ‘Wwe

sjlejap poujyew/ubIsaq

sainjeay YM Wayshs spore
yyeay WuUO01Da]a padueyuy

og StaIsAs

YW Ul MOYpOM aed Ayewlid
OUI! SayN4 UOIDIPaid jedIul]D
Ppeppeqws Jey} [00O} SGD V

jgoWaysAS UOIJEWOJU!
JeDIPawW paseq-1a}NdwWOdD ou!
sueldisAyd Aq Aujua Japso WIIG

6 4Oddns
UOISIDap [eIIUI|D se (YEP)
ylomawes Hulusesj-daap vy

91 (dOdD) Asque
Japso JapIAoJd paziuayndwoy

7, (OD-D) Jap10
Adejsayyouay> jo Aujua Japso

ueldisAud pezisajndwod pie Oo}
wua3sKs yoddns uolsidap jediul|>

z¢}X9}UOD JUAN sjeENpIAIpU!
9} O} polojie} suolsabbns
AWAIDe VIA Buryyem sejnbe
sabeunodua Jey} UO!USAJE}U!
yyeaHw ue ‘sdaisveayH

uolyNnjos jeubig

uolze;NWIs
JEDIUN|D :dAIWDedsolg

uolze;NWIs
jeuoieyNdwod :aAiadsold

uBisap UOlUaAJe4UI
ysod-dig :aAlpadsold

ubisap uoleziuiopues
-OIDII) <BAIDAEdSOdg

yoeoidde uonenjeaq

PanulUo? % aIqeL

npj Digital Medicine (2020) 110

Scripps Research Translational Institute
C. Guo et al.

Np)

 

payodal ou y}buay Apnis

payodai
(8LOZ-9LOZ) sueak Z /ajqedijdde jon

(payodas

you yyHua] Apnjs pud-0}-pue—6 L0Z
Auenuer-piw pue 8107 Jaquuiadaq
-PpIW UdaMjeq peZzAjeue JIM
SBHUIPIOIAI DDIOA ||\V) SYJUOW +7

pewoda JON

sujJuOW +9 a|6uis

uolenjeAd
BuLnp paajoaul
yybHua| Apnis sails Jo Jaquinn

paynuap! asam SUSIA 716

sjuaijed JIIURD JSCDIGQ SEO

(I]N LUNAS
Se PaPJOIAI BDIOA) s}UedIDI Wed OF

sased jualjed
OOL Bunenjeas suelishud +

luayshs puepuejys Buisn sz pue yq
-dISd Buisn g¢g—suns uoNe|NWIs OS
pue (sjuaiyed g pue sio}D0P OL) SL

aZzIs ajdwes

uaye} Bulag

suol}de DYyIDads-wo0ydwAs

pue yeyd ay} Ul pe}yuaWNDOp
wojdwAs jo sajei Jayubiy

UUM payelsosse ase Saj0ds WO ydWwAS
SVSd Jeybiy YUM sUsiA jualjed
JOYJBYM SUIWeXd 0} JPAINQIsSIp
JO/pue 3}0WaJ9}Ua) JaDUeD

jeuolbai e ye S'USIA JUaIZed JaDUeD
UO SMAIAAI WEY dAIWDadsoljay

elpu] Jo sjeyidsoy
Ul BDUePJODUOD ay} aJenjeArad
0} paiedwod ajam pueog JoWwNn}

ay} pue (siadUeD JseaIq 89) OAM Aq
apewW suoMepuUaWUWWODAaJ JUaWU}eaL |

sayei ADeund3e

uoluUBOd91 ay} sIedwod

pue (UIs ‘jueysse 3/6006 ‘exa|v

“B'a) sjueysisse SDIOA 0} paAejd aiam
sjuedinivied 9p JO SHuIPsOD9I BDIOA,

(uBisap ysod-aid e “a"l) DIM} JaDUeD
ydeu pue peay jo jas e}ep uo!}epljeA
jUapuedapul! ue Ul sjual}ed QOL
B}ENIEAD O} P2}NID91 UBY} DBM SaAd|
adualadxe HulAuen YUM suelishud

SMAIAJ9}U! [2INIDNAYS

-IWas UO paseq pajenjeAa aM
Ajayes je>d1paw UO ~eduI! ‘sj}ualZed
dAY BY} UO PUNO PleM e WIOJad

QO} peyse aJaMm S10}DOp Huljedidijed
“(MG-dISd) Wass Mau ay} JO (Wua}sAs
jUdWaHueW Psepue}s }ed0]) auljaseq
0} poubisse AjWoOpues sueldiskud
UOI}PUOD Yea JO}

poyenjers ajam adueinzeypjWojiad

sjlejap poujyew/ubIsaq

ez(SWSJ) a]22s JUaussasse
Huluaasds WO WAS 31U04}D9]F

jghBojOIUO JO} UOS}EM

zz (SUoled1Ipaw
pasuadsip AjuOwWWOd
JO uoIUboOD~aJ) sjueysisse BdIO/A

1z,l2@POW UOHDIpaid
sso|-1y61am YUM SSqd

oz (Ad-dISd) ,Yones1pew

Ul aINPaD0I1g JUaHIJja}U; YBNoY}
Ayayes uated, se UMOUy
luaj\sks JUdWabeueW UO!}EDIPA|/\
jgoUONonNsysUu!l Yoddns uolsizap
pue Hulyos dIyewWO Ne se YyoNs

uolyNnjos jeubig

(Q@AI}Dedsoid YUM
pluqdhy ‘}ul) aAlpadsoujeyY

yoeoidde uonenjeaq

PanulUo? % aIqeL

 

Scripps Research Translational Institute

npj Digital Medicine (2020) 110
Methodological gap exists for faster
gy and high-quality evidence generation

Survey and interviews (incl.,
usability testing)

Increasing evidence level

Expert opinion / case example

   
 

Retrospective
observational studies

C. Guo et al.

np)

 

Prospective RCT studies

Prospective observational
studies

 

 

Fast-evidence
generation

Longer-time to
evidence generation

Fig. 1 Existing approaches for health digital solution evaluation, current methodological gap and emerging innovative pragmatic
approaches to fill such gap. Note, the position of each methodology is meant to be illustrative and reflecting general cases.

Prospective studies

Prospective RCTs are the most accepted method for evaluating
healthcare interventions®'. For end-users, not considered “early
adopters”, such studies are critical to justify adoption decisions.
The randomization unit can be individuals, groups (“clusters”), or
even specific solution components**. Choice of the study designs
heavily depends on the digital solution and objectives of the
evaluation.

Individual-randomization trials (IRTs) are well-suited for digital
solutions targeting an individual user, such as _ patient-level
randomization (e.g. symptom self-monitoring®*) or clinician-level
randomization (e.g. digital pathology algorithms for patholo-
gists**). This is traditionally the most commonly used experi-
mental design in healthcare research (e.g., clinical trials for the
development of drugs and diagnostic tests)*°, however for digital
health solutions, we found few studies employed strict individual
randomized designs (Table 4; e.g., refs. °° °°). One reason is that
individual randomization is not always possible or appropriate as
in the examples provided below.

Cluster-randomization trials (CRTs), by contrast, are better suited
for digital solutions supporting group efforts (e.g. solutions
supporting tumor board meetings*”), and this approach has been
increasingly adopted by public health researchers” **. CRTs are
often used in situations when contamination may occur; for
example, where individuals in the same cluster have been
randomized to different intervention groups, or for logistic,
feasibility or ethical reasons**. Attractive features include:
increased administrative efficiency; decreased risk of experimental
contamination (e.g. where control group individuals adopt the
intervention)’; and, enhancement of subject compliance®*. In
addition, CRTs allow both direct and indirect effects of an
intervention to be evaluated—a particular advantage when both
effects are hypothesized to be important, e.g., in vaccine field
trials*’. Disadvantages include: reduced statistical efficiency
relative to IRTs*°; overmatching; and, subsampling bias*”*®.
Analysis commonly employs multi-level modeling*””°.

Micro-randomization trials (MRTs) are helpful when researchers
want to determine empirically the efficacy of a specific
component (e.g., which component of an intervention should be
delivered, and whether it had the intended effect)**. MRT involves
randomly assigning an intervention option at each time point that
the component could be delivered (e.g., see examples in the ref. *'
on p. 5 and ref. °’)?'°*, and can be particularly powerful in the
early stages of product development?'. MRTs generate long-
itudinal data with repeated measures of participants’ behaviors,
context, and psychosocial factors, and can be analyzed by

Scripps Research Translational Institute

methods, such as multilevel models and generalized estimating
equation? >>",

The most commonly used method for evaluating digital health
solutions, however, is the pre-post design, as demonstrated by a
previous systematic review'’ and supported by our own searches
(Table 4). A standard approach of pre-post design involves: pre-
phase, which provides control data; “washout” period”? (i.e., with
no interventions implemented with a time gap up to several
months), to allow familiarization and to limit bias related to
implementation®’°°; post-phase to collect data on solution
effectiveness. Existing studies are often undertaken at a single
site (vs. multi-site), which is typically more practical and affordable.
Typically, this design requires a longer duration, making it difficult
to evaluate continuous solution upgrades (i.e. new features and/or
bug fixes), which are often observed in digital health products. In
addition, it is not optimal for testing medium-term or longer-term
clinical outcomes, because it is difficult to determine independent
effects when patients may appear in both pre-phase and post-
phase. Data analysis generally employs methods, such as analysis
of variance (ANOVA) and analysis of covariance (ANCOVA) or non-
parametric tests (depending on the underlying distributions)’.

Relatively few multi-site studies have been conducted'’ (we
also listed some examples in Table 4), nevertheless, a variety of
designs have been attempted in this context including:
pre-post’®, cross-sectional with non-equivalent control’, cross-
sectional with internal control®, and randomized controlled
trial®’. For multi-site RCTs, some sites are assigned as controls
and the rest as the experimental condition. For this approach,
control and experimental sites should be matched along key
characteristics (e.g., workflow, patient characteristics), which can
be difficult to achieve. The main advantage is reduction in study
duration. Disadvantages include: higher set-up efforts; increased
cost; and, challenges to identify matched sites. Various tests are
employed such as t-test, non-parametric tests, or other advanced
techniques (depending on the underlying distributions)®.

Retrospective studies

Retrospective studies can be employed to analyze pre-existing
data, such as patient charts or electronic medical records. Types of
retrospective studies include case series, cohort, or case-control
studies. They are typically quicker, cheaper, and easier®* than
prospective studies because data are already collected, and are
commonly used to generate hypotheses for further investigation
by prospective studies. The disadvantages are, that they are
subject to biases and confounding factors, such as patient
information loss or distortion during data collection®, risk factors

npj Digital Medicine (2020) 110
np}

C. Guo et al.

 

10

present but not captured, normal growth or maturation influence,
attrition bias (e.g. patients with unfavorable outcome(s) less likely
to attend follow-up)°*’, and selection bias due to non-random
assignment of participants®”°°®. Such biases threaten internal
validity, therefore, retrospective studies are considered (particu-
larly by the academic groups) inferior as compared to RCTs°*®®. It
remains as an open question whether this is still the case for
digital health solutions, particularly for the ones of lower-risk class.

To date, few publications have evaluated digital solutions with
retrospective data, likely due to limited use of digital solutions in
clinical practice, and challenges for data access (e.g. GDPR).
Nevertheless, one such study from India investigated concordance
between the treatment recommendations of an artificial intelli-
gence (Al) algorithm compared with actual tumor board
recommendations®” (Table 4). Strictly speaking this study was a
hybrid of retrospective (treatment recommendations from Tumor
Board 2014-2016) and prospective (treatment recommendations
from Al algorithm in 2016). A key limitation of the study was that
breast cancer treatment knowledge was not constant for the two
conditions, because of the evolving clinical practice standards.
Additional, prospective studies would be required to examine
impacts on clinical outcomes, efficiency, and mental fatigue of
clinicians.

Systematic reviews

Systematic reviews have a key role in evidence-based medicine
and the development of clinical guidelines®’’”°. Reviews on a
specific solution can provide stronger evidence for its impacts, but
require a sufficient number of individual evaluation studies. A
possible limitation for such work in digital health is that included
studies would need to be matched to the same mechanism of
intervention, disease area, and measurable outcome.

Systematic reviews of prediction models are a new and evolving
area and are increasingly undertaken to systematically identify,
appraise, and summarize evidence on the performance of
prediction models’'~”°. Frameworks and tools exist to facilitate
this including: prediction model risk of bias assessment tool
(PROBAST), quality in prognosis studies (QUIPS), revised Cochrane
randomized comparative design (ROB), risk of bias in nonrando-
mized studies of interventions (ROBINS-l). Details provided in
Table 2.

Economic evaluation

Demonstration of positive economic benefits are critical for the
majority of end-users to justify solution adoption. In addition, such
data is important for other critical actors (e.g. Payers, Government
agencies, Professional Societies) to endorse the need for change.
The World Health Organization (WHO) guidelines provide a good
overview of options for economic evaluation (Table 4.8 in WHO
guideline'®) including: cost-effectiveness analysis, cost-benefit
analysis, cost-consequence analysis, cost-minimization analysis,
etc. However, for all of the aforementioned methods, tracking
usage and performance data of users compared to non-users, is
required.

The critical evidence gaps for digital health solutions

In general, approaches for evidence generation at early stages of
product development deliver weaker evidence. Although, such
efforts may be enough to support internal needs, and can
convince “early adopters”, they are insufficient to satisfy the
“majority” of a solution’s potential beneficiaries. These groups
require, and expect, more robust, traditional evidence approaches.
Currently, and in our opinion, there is a gap between quick, lower-
cost approaches applied at the early stages of product develop-
ment and higher-cost approaches needed to convince the
majority of stakeholders.

npj Digital Medicine (2020) 110

THE CHALLENGE OF THE TRADITIONAL APPROACH FOR
DIGITAL HEALTH INNOVATORS

It is our opinion that traditional methods to develop more robust
evidence are incongruent with the agile approach taken in
software development (e.g., mismatch between the length of RCTs
and the typical development and update cycle of software). As
such, traditional approaches present fundamental limitations for
researchers to create evidence for digital health solutions. In fact,
evaluation of digital health solutions has been identified as
requiring improvement, and has been cited as a major obstacle for
wider adoption’* ”°. The paradox at the heart of this problem is
that, “without evidence healthcare providers would not adopt a
solution; without solution adoption it is very difficult to generate
evidence to convince healthcare providers”.

Digital solution evaluation requires collective efforts from
multiple parties, such as health authorities, healthcare providers
(incl., academic medical centers), and manufacturers such as small
and medium-sized enterprises (SMEs), multinational corporation
(MNCs). Whilst they face shared difficulties with the current
approaches for evidence generation (e.g. significant time and
cost), they also have circumstance-specific challenges.

SMEs—Limited resources to undertake clinical studies

SMEs typically prioritize and allocate their research and develop-
ment budget to product development. Anecdotal evidence
suggests that close relationships between innovator and adopter
are a critical driver of initial adoption decisions. Wider implemen-
tation requires robust evidence of benefit, yet this is difficult to
prioritize given the many challenges for establishing new
ventures. In addition, well designed and executed studies require
skilled researchers, often via collaboration with academia, adding
further complexity. Moreover, it has been estimated that the
timescale for submitting a research proposal and receiving ethical
approval for a pilot or trial study can take as long as 3 years'”. As
demonstrated in a recent report'’, the biggest obstacle for
providing evidence of effectiveness reported by companies, is the
cost and timeframe for evaluation.

MNCs—Out of date evidence not an investment priority

Larger corporations have more resources to develop evidence but
are equally limited by time. For internal budget allocation, it can
be difficult to provide rationale for investments into expensive
and time-consuming clinical studies for early-stage solutions when
such products are constantly evolving. Given it typically takes 2-3
years to conduct a study, evidence published today may reflect a
product that has been updated and refined multiple times.
Furthermore, for many companies’ investments in sales and
manufacturing, for example, are more tangible with more
predictable return on investment than those in clinical studies.

The same challenges (as SMEs) exist around navigating the
complex infrastructure of the healthcare system, dealing with the
cultural resistance to digital solutions, and identifying appropriate
principle investigators for the evaluation studies. Despite the long-
existing collaborations between large health abd life science
companies and principal investigators in, for example clinical trials
for drug development, this group of researchers may not
necessarily be willing to conduct studies to evaluate digital
solutions, as they require different settings, capabilities and also
deliver different scientific output—benefits on the operational
level impacting cost and indirectly patient outcome versus a drug
that can improve patient outcome directly.

Academic institutions—focus on research output not widespread
adoption

A growing number of academic centers have created digital
health research programs to develop and evaluate digital health

Scripps Research Translational Institute
solutions. However, such research units generally favor traditional
research methodologies because of the increased likelihood of
high-impact publication. As such, the timeliness of studies is
largely immaterial, therefore, potentially valuable solutions may be
delayed and/or are never implemented at scale. Obtaining
sufficient research funding can also be a challenge.

EVOLVING PRAGMATIC APPROACHES FOR EVIDENCE
GENERATION

In our opinion, large differences exist between the evidence
required for initial adopters (e.g., surveys and interviews, case
Studies), and that required for the majority (prospective RCT
studies). Other research areas, such as drug development, have
demonstrated that pragmatic approaches can be adopted to
control cost at early stages (pragmatic clinical trials, basket of
baskets, umbrella trials, etc.’”-’”). The “gold standard” RCT remains
but for later-stage final assessment.

The concept of “simulation” is not new and is the methodo-
logical foundation for human behavior experimental research (e.g.
neuroscience and experimental psychology). The assumption is
that people behave similar to real-life if key components of the
scenarios are extracted and _ fidelity maintained. Various
approaches for simulation could be applied to evaluate digital
solutions, such as, computational, system, and clinical simulation.

Computational simulation for software evaluation involves two
steps: verification and validation®”. Verification checks if a system
was built according to specification, and validation checks that a
system meets user expectations. The most common application of
computational has been for verification. Typically, this involves
simulated outcomes based on synthesized or real cases, before
involving users/clinicians. Recent efforts have extended its use to
non-regulated and on-market products (e.g., Google Alexa;
Table 4). This approach is more applicable for products where
the outputs can be evaluated for individual users, and not for
clinical management tools where a group of users are targeted
(e.g. multidisciplinary tumor boards).

System simulation adopts a system engineering view and
methodology to model the effect of an intervention on a
healthcare system (e.g. multi-site hospital network) without
disrupting the real health care setting®'. It has gained some
traction (ASCO QCS Keynote topic by Joe Simone, literatures®”**),
however, to date we are not aware of the use of system simulation
to evaluate a digital health solution, perhaps because of the
significant complexity to establish models that represent a
healthcare system.

Clinical simulation was traditionally developed and used in
training medical residents, and it was further developed as an
approach to test systems and digital solutions with representative
users doing representative tasks, in representative settings/
environments®*. In our opinion, can be complementary to many
of the traditional approaches reviewed above that require the use
of a digital solution in real clinical practice, and could bridge the
evidence needs between those of “early adopters” and the
“majority”. Clinical simulation provides a good balance between
the strength of evidence (e.g., “near-live” clinical scenarios), whilst
remaining cost-effective and timely for fast version updates
(Fig. 1). Previous work demonstrated, the total cost for such a
simulation was as little as 2750 USD, including set-up, subject and
personnel cost®’. A recent cost-effective analysis suggested that
introducing simulation into a product development lifecycle could
lead to cost savings of 37-79%°°. Other advantages include:
scalability’, flexibility in design of studies (e.g. different scenarios,
various types of participants), feasibility in being implemented as
remote and/or distributed®’, and ability to collect behavioral and/
or cognitive metrics. Sophisticated approaches and equipment
can be employed, such as eye-tracker analysis or measurement of
EEG, which would not be possible in real clinical practice.

Scripps Research Translational Institute

C. Guo et al.

np)

 

Furthermore, clinical simulation may also be helpful in facilitating
patient engagement and/or Patient and Public Involvement and
Engagement (PPIE), an initiative aiming to involving patients and/
or representatives from relevant public bodies in the research®®.

Clinical simulation has been increasingly used in evaluating
digital health solutions, including five studies in Table 4, and a
further twenty studies from ITX-lab evaluating clinical information
systems®’. For example, in one study” primary care physicians
interacted with videoclips of professional patient actors providing
standardized responses to clinical scenarios and utilized a CDS
tool of clinical prediction rules via an EMR system. In another
recently published study®', cognitive load and performance of
physicians was evaluated for different conditions by randomly
assigning participants to baseline EHR (control) or enhanced EHR
(simulated environment with features such as automatic sorting
and decision support instructions). Moreover, a recent interview
study of 10+ companies reported that they found this approach
feasible for evidence generation for their own digital solution'”.

Several academic centers have established clinical simulation
test environments, including: The School of Health Information
Science (University of Victoria); The Department of Development
and Planning (Aalborg University); The IT Experimentarium (ITX)
lab (Danish Institute for Medical Simulation)**; and, The Institute of
Global Health Innovation (IGHI) (Imperial Colleague London)’.
Indeed, researchers from IGHI have established a simulation test
bed specifically to explore application to test digital health
solutions. Initial work evaluated the impact of a digital solution on
the conduction of cancer multidisciplinary team (MDT) meetings.
56 healthcare professionals (e.g. pulmonologist, oncologists,
radiologists, clinical nurse specialists, and thoracic surgeons),
who were regular participants at lung cancer tumor boards, were
recruited to take 10 simulated MDT sessions. High-fidelity mock
patient cases were developed by the study team and clinical
experts’’. Participants discussed up to 10 patient cases, using a
standard UK approach to conduct MDTs (paper handout and PACS
system) in the control condition, compared with the NAVIFY
Tumor Board solution. A manuscript detailing the learnings and
results from this pioneer work is under development.

Whilst clinical simulation offers opportunities to prospectively
test a digital solution quickly, safely and cost-effectively prior to
implementation, there are a few limitations in its use. First, high-
fidelity is a prerequisite for generating valid and effective
evidence. Therefore, researchers should take efforts to create
scenarios representing real clinical practice, recruit the most
representative end-users as participants, and provide comprehen-
sive trainings of the digital solutions to the participants before
their simulation sessions. Second, while the regulatory space
evolves fast, we think clinical simulation results itself alone
probably are not adequate for approval application from Health
authorities, particularly for higher-risk group of digital solutions
that would need to be approved as SaMD. Nevertheless, in these
cases, clinical simulations can help to provide initial insights for
product development, reduce safety risk for patients, and guide
the design of large-scale real clinical studies. Third, for digital
solutions that are already adopted in clinical practice, leveraging
real-word data (RWD) is probably more suitable. RWD studies
could be systematically employed to undertake near real-time
evaluation during pilot implementation and post-market monitor-
ing. Indeed, studies utilizing real-world data (RWD) have been
encouraged to support regulatory decision making (e.g. The 21st
Century Cures Act; Table 3); have been used for clinical evidence
generation (e.g. diagnostic and treatment patterns)**”°; and can
demonstrate solution utility (e.g. meta-data associated with
solution features and functionalities).

Finally, we believe clinical simulation can be employed in
combination with traditional study designs, e.g., individual-
randomization, _cluster-level randomization, and micro-
randomization to examine different types of digital solutions.

npj Digital Medicine (2020) 110

11
np}

C. Guo et al.

 

12

For example, clinical simulation-based study with micro-
randomization design can be a powerful and pragmatic approach
to evaluate the digital solutions with multiple components at early
stage of the product development.

CONCLUSION

Innovators face significant challenges to overcome the “no
evidence, no implementation—no implementation, no evidence”
paradox in digital health. We believe that innovative approaches,
such as simulation-based research, can enable the generation of
higher-quality, lower-cost, and more timely evidence. By con-
sidering such methods, end-users will encourage developers to
undertake research activities, rather than be intimidated by the
complexity, cost, and duration of traditional approaches.

DATA AVAILABILITY
All data supporting the findings of this study are available within the paper.

Received: 15 December 2019; Accepted: 22 July 2020;
Published online: 27 August 2020

REFERENCES

1. Frank, S. R. Digital health care—the convergence of health care and the Internet.
J. Ambul. Care Manag. 23, 8-17 (2000).

2. Mathews, S. C. et al. Digital health: a path to validation. NPJ digital medicine 2,
1-9 (2019).

3. FDA. https://www.fda.gov/medical-devices/digital-health (2020).

4. IQVIA. IQVIA Institute for Human Data Science Study: Impact of Digital Health
Grows as Innovation, Evidence and Adoption of Mobile Health Apps Accelerate.
https://www.iqvia.com/newsroom/2017/1 1/impact-of-digital-health-grows-as-
innovation-evidence-and-adoption-of-mobile-health-apps-accelerate/ (2017).

5. NICE. https:/Awww.nice.org.uk/about/what-we-do/our-programmes/evidence-sta
ndards-framework-for-digital-health-technologies (2019).

6. Rogers, E. M. Diffusion of innovations. Simon and Schuster (2010).

7. Ball, C. et al. The physical—digital divide: exploring the social gap between digital
natives and physical natives. J. Appl. Gerontol. 38, 1167-1184 (2019).

8. Francis, J. et al. Aging in the digital age: conceptualizing technology adoption
and digital inequalities. In Ageing and digital technology, 35-49 (Springer,
Singapore, 2019).

9. Peek, S. et al. What it takes to successfully implement technology for aging in
place: focus groups with stakeholders. J. Med. Internet Res. 18, e98 (2016).

10. Wu, Y. -H. et al. Bridging the digital divide in older adults: a study from an
initiative to inform older adults about new technologies. Clin. Interv. Aging 10,
193-201 (2015).

11. FDA. https://www.fda.gov/media/98657/download.

12. Shuren et al. FDA regulation of mobile medical apps. JAMA, 320, 337-338
(2018).

13. https://www.gov.uk/government/publications/code-of-conduct-for-data-driven-
health-and-care-technology/initial-code-of-conduct-for-data-driven-health-and-
care-technology.

14. FDA. https://www.fda.gov/medical-devices/digital-health/digital-health-software-
precertification-pre-cert-program.

15. Chung, K. C. et al. Introducing evidence-based medicine to plastic and recon-
structive surgery. Plast. Reconstr. Surg. 123, 1385 (2009).

16. Song, J. W. et al. Observational studies: cohort and case-control studies. Plast.
Reconstr. Surg. 126, 2234 (2010).

17. Pawloski, P. A. et al. A systematic review of clinical decision support systems for
clinical oncology practice. J. Natl. Compr. Canc. Netw. 17, 331-338 (2019).

18. https://www.who.int/reproductivehealth/publications/mhealth/digital-health-
interventions/en/. WHO (2016).

19. Ghafur, S. et al. A simulation test bed: the solution to the obstacles of evaluating
the effectiveness of digital health interventions (in preparation).

20. Cumming, G. P. et al. Web-based survey on the effect of digital storytelling on
empowering women to seek help for urogenital atrophy. Menopause Int. 16,
51-55 (2010).

21. Lavorgna, L. et al. Health-care disparities stemming from sexual orientation of
Italian patients with Multiple Sclerosis: a cross-sectional web-based study. Mult.
Scler. Relat. Disord. 13, 28-32 (2017).

npj Digital Medicine (2020) 110

22.

23.

24.

25.

26.

27.

28.

29.

30.

31.

32.

33.

34.

35.

36.

37.

38.

39.

40.

41.

42.

43.

AA,

45.

46.

47.

48.

49.

50.

51.

52.

53.

54.

55.

Topooco, N. et al. Attitudes towards digital treatment for depression: a Eur-
opean stakeholder survey. Internet Interv. 8, 1-9 (2017).

Evans, D. Hierarchy of evidence: a framework for ranking evidence evaluating
healthcare interventions. J. Clin. Nurs. 12, 77-84 (2003).

Schneiderheinze, H. et al. Development and usability analysis of a multimedia
eConsent solution. Stud. Health Technol. Inform. In GMDS, 297-303 (2019).
Hardy, A. et al. How inclusive, user-centered design research can improve
psychological therapies for psychosis: development of SlowMo. JMIR Ment.
Health 5, e11222 (2018).

Maramba, I. et al. Methods of usability testing in the development of eHealth
applications: a scoping review. Int. J. Med. Inform. 126, 95-104 (2019).

Molich, et al. Comparative usability evaluation. Behav. Inf. Technol. 23, 65-74
(2004).

Zimmerman & Paschal. An exploratory usability evaluation of Colorado State
University Libraries’ digital collections and the Western Waters Digital Library
Web sites. J. Acad. Librarianship. 35, 227-240 (2009).
https://userfocus.co.uk/articles/strength-of-evidence.html.

Faulkner, L. Beyond the five-user assumption: benefits of increased sample sizes
in usability testing. Behav. Res. Methods 35, 379-383 (2003).

Juni, P. et al. Systematic reviews in health care: assessing the quality of con-
trolled clinical trials. BMJ 323, 42-46 (2001).

Kumar, S. et al. Mobile health technology evaluation: the mHealth evidence
workshop. Am. J. Prev. Med. 45, 228-236 (2013).

Baker, et al. Digital health: Smartphone-based monitoring of multiple sclerosis
using Floodlight. Nature (2019).

Kohlberger, T. et al. Whole-slide image focus quality: automatic assessment and
impact on Al cancer detection. J. Pathol. Inform. 10 (2019).

Chan, A.-W. & Altman, D. G. Epidemiology and reporting of randomised trials
published in PubMed journals. Lancet 365, 1159-1162 (2005).

Basch, et al. Symptom monitoring with patient-reported outcomes during
routine cancer treatment: a randomized controlled trial. J. Clin. Oncol. 34, 557
(2016).

Hershman, D. L. et al. Randomized trial of text messaging (TM) to reduce early
discontinuation of aromatase inhibitor (Al) therapy in women with breast can-
cer: SWOG $1105. Oral presentation at: 2019 ASCO Annual Meeting (2019).
Quinn, et al. WellDoc™ Mobile diabetes management randomized controlled
trial: change in clinical and behavioral outcomes and patient and physician
satisfaction. Diab. Tech. Therap. 10, 160-168 (2008).

Hammer, R. D. et al. Digital Tumor Board Solutions have significant impact on
case preparation. JCO Clinical Cancer Informatics (forthcoming).

Greaney, et al. Study protocol for Young & Strong: a cluster randomized design
to increase attention to unique issues faced by young women with newly
diagnosed breast cancer. BMC Public Health. 15, 1-11 (2015).

Ohl, et al. Impact of availability of telehealth programs on documented HIV viral
suppression: A Cluster-Randomized Program Evaluation in the Veterans Health
Administration. Open Forum Infect Dis. 6, of2206 (2019).

Arnup, et al. The use of the cluster randomized crossover design in clinical trials:
protocol for a systematic review. Syst. Rev. 3, 1-6 (2014).

Eldridge, S. & Kerry, S. A Practical Guide to Cluster Randomised Trials in Health
Services Research. Vol. 120 John Wiley & Sons, 2012).

Donner, A. & Klar, N. Pitfalls of and controversies in cluster randomization trials.
Am. J. Public Health 94, 416-422 (2004).

Halloran, M. E. et al. Design and interpretation of vaccine field studies. Epidemiol.
Rev. 21, 73-88 (1999).

Cornfield, J. Randomization by group: a formal analysis. Am. J. Epidemiol. 108,
100-102 (1978).

Torgerson, D. J. Contamination in trials: is cluster randomisation the answer?.
BMJ 322, 355-357 (2001).

Mazor, K. et al. Cluster Randomized Trials: opportunities and Barriers Identified
by Leaders of Eight Health Plans. Med. Care. S29-S37 (2007).

Raudenbush, S. W. Statistical analysis and optimal design for cluster randomized
trials. Psychol. methods 2, 173 (1997).

Campbell, M. K. et al. Analysis of cluster randomized trials in primary care: a
practical approach. Family Pract. 17, 192-196 (2000).

Klasnja, P. et al. Micro-Randomized Trials: an experimental design for developing
just-in-time adaptive interventions. Health Psychol. 34, 1220-1228 (2015).
Klasnja, P. et al. Efficacy of contextually tailored suggestions for physical activity:
a Micro-randomized Optimization Trial of HeartSteps. Ann. Behav. Med. 53,
573-582 (2018).

Bolger & Laurenceau. Intensive Longitudinal Methods: An Introduction to Diary
and Experience Sampling Research (Guilford Press, 2013).

Walls & Schafer. Models for intensive longitudinal data (Oxford University Press,
2006).

Evans, S. R. Clinical trial structures. J. Exp. Stroke Transl. Med. 3, 8-18 (2010).

Scripps Research Translational Institute
56.

57.

58.

59.

60.

61.

62.

63.

64.

65.

66.

67.

68.

69.

70.

71.

72.

73.

74.

75.

76.

77.

78.
79.

80.

81.

82.

83.

84.

85.

86.

Bowen, D. J. et al. How we design feasibility studies. Am. J. Prev. Med. 36,
452-457 (2009).

Dimitrov, D. M. & Rumrill, P. D. Pretest-posttest Designs and Measurement of
Change. (IOS Press, 2003).

Beriwal, S. et al. How effective are clinical pathways with and without online
peer-review? An analysis of bone metastases pathway in a large, integrated
National Cancer Institute-Designated Comprehensive Cancer Center Network.
Int. J. Radiat. Oncol. Biol. Phys. 83, 1246-1251 (2012).

Bouaud, J. et al. Physicians’ attitudes towards the advice of a guideline-based
decision support system: a case study with OncoDoc2 in the Management of
Breast Cancer Patients. Stud. Health Technol. Inform. 264-269 (2015).

Mattsson, T. O. et al. Non-intercepted dose errors in prescribing anti-neoplastic
treatment: a prospective, comparative cohort study. Ann. Oncol. 26, 981-986
(2015).

Berry, D. L. et al. Enhancing patient-provider communication with the electronic
self-report assessment for cancer: a randomized trial. J. Clin. Oncol. 29,
1029-1035 (2011).

Caselli, E. et al. Influence of sanitizing methods on healthcare-associated
infections onset: a multicentre, randomized, controlled pre-post interventional
study. J. Clin. Trials. 6, 1-6 (2016).

Sauerland, S. et al. Retrospective clinical studies in surgery: potentials and pit-
falls. J. Hand Surg. Br. 27, 117-121 (2002).

Kaji, A. H. et al. Looking through the retrospectoscope: reducing bias in emer-
gency medicine chart review studies. Ann. Emerg. Med. 64, 292-298 (2014).
Tofthagen, C. Threats to validity in retrospective studies. J. Adv. Pract. Oncol. 3,
181 (2012).

Geneletti, S. et al. Adjusting for selection bias in retrospective, case-control
studies. Biostatistics 10, 17-31 (2009).

Somashekhar, S. P. et al. Watson for Oncology and breast cancer treatment
recommendations: agreement with an expert multidisciplinary tumor board.
Ann. Oncol. 29, 418-423 (2018).

Graham, R. et al. Clinical Practice Guidelines We Can Trust: Committee on Stan-
dards for Developing Trustworthy Clinical Practice Guidelines. (National Academies
Press, 2011).

Goff, et al. ACC/AHA guideline on the assessment of cardiovascular risk: a report
of the American College of Cardiology. American Heart Association Task Force
on Practice Guidelines. Circulation 63, 2935-2959 (2014).

Rabar, S. et al. Guideline Development Group Risk assessment of fragility frac-
tures: summary of NICE guidance. BMJ 345, p.e3698 (2012).

Bouwmeester, W. et al. Reporting and methods in clinical prediction research: a
systematic review. PLoS Med. 9, 1001221 (2012).

Collins, G. S. et al. Transparent Reporting of a multivariable prediction model for
Individual Prognosis or Diagnosis (TRIPOD): the TRIPOD statement. Ann. Intern.
Med. 131, 211-219 (2015).

Debray, T. P. et al. A guide to systematic review and meta-analysis of prediction
model performance. BMJ 356, i6460 (2017).

Shaw, J. et al. Beyond “implementation”: digital health innovation and service
design. NPJ Digit. Med. 1, 1-5 (2018).

Moxey, et al. Computerized clinical decision support for prescribing: provision
does not guarantee uptake. J. Am. Med. Inform. Associat. 17, 25-33 (2010).
O'Sullivan, et al. Decision time for clinical decision support systems. Clin. Med.
14, 338 (2014).

Mentz, R. J. et al. Good Clinical Practice Guidance and Pragmatic Clinical Trials:
balancing the best of both worlds. Circulation 133, 872-880 (2016).

Ford, |. et al. Pragmatic trials. N. Engl. J. Med. 375, 454-463 (2016).

Cunanan, K. M. et al. An efficient basket trial design. Stat. Med. 36, 1568-1579
(2017).

Dahabreh, I. J. et al. Modeling and Simulation in the Context of Health Technology
Assessment: Review of Existing Guidance, Future Research Needs, and Validity
Assessment (2017).

Anderson, J. G. et al. Evaluation in health informatics: computer simulation.
Comput. Biol. Med. 32, 151-164 (2002).

Dong, Y. et al. Systems modeling and simulation applications for critical care
medicine. Ann. Intensive Care 2, 1-10 (2012).

Roberts, S. D. Tutorial on the simulation of healthcare systems. Proceedings of
the 2011 winter simulation conference (wsc), 1403-1414 (2011).

Kushniruk A. et. al. From usability testing to clinical simulations: bringing con-
text into the design and evaluation of usable and safe health information
technologies. Contribution of the IMIA human factors engineering for health-
care informatics working group. Yearb. Med. Inform. 22, 78-85 (2013).
Kushniruk, A. W. et al. Low-cost rapid usability engineering: designing and
customizing usable healthcare information systems. Healthc, Q. (2006).

Baylis, T. B. et al. Low-Cost Rapid Usability Testing for health information sys-
tems: is it worth the effort? Stud. Health Technol. Inform. (2012).

Scripps Research Translational Institute

C. Guo et al.

Np}

 

87.

88.

89.

90.

91.

92.
93.

94.

95.

96.

97.

98.

99.
100.
101.
102.
103.
104.
105.
106.
107.
108.
109.
110.
111.
112.
113.

114.

115.

116.

117.

118.

119.

120.

121.

122.

Yao, H. et al. Research and design on distributed remote simulation based on
Web. In /EEE International Conference on Information Management and Engi-
neering. pp. 522-525 (2010).
https://imperialbrc.nihr.ac.uk/patients-public/ppi-e-strategy/.

Jensen, S. et al. Clinical simulation: a method for development and evaluation of
clinical information systems. J. Biomed. Inform. 54, 65-76 (2015).

Li, et al. Integrating usability testing and think-aloud protocol analysis with
“near-live” clinical simulations in evaluating clinical decision support. /nt. J. Med.
Inform. 81, 761-772 (2012).

Mazur, et al. Association of the usability of electronic health records with cog-
nitive workload and performance levels among physicians. JAMA Netw Open. 2,
e€191709-e191709 (2019).
https://www.imperial.ac.uk/global-health-innovation/.

Gardner, et al. A mixed methods study for the evaluation of a digital health
solution for cancer multidisciplinary team meetings using simulation-based
research methods. ASCO 2020 Annual Conference. American Society of Clinical
Oncology. pp. e14063 (2020)

Khozin, et al. Real-world data for clinical evidence generation in oncology. J.
Natl. Cancer Inst. 109, djx187 (2017).

Calabria, et al. Open triple therapy for chronic obstructive pulmonary disease:
Patterns of prescription, exacerbations and healthcare costs from a large Italian
claims database. Pulmon. Pharmacol. Therap. 61, 101904 (2020).

Pal, et al. Real-world treatment patterns and adverse events in metastatic renal
cell carcinoma from a large US claims database. BMC Cancer 19, 548 (2019).
http://www.pchalliance.org/resources.
https://ec.europa.eu/docsroom/documents/1 792 1/attachments/1/translations.
http://www.imdrf.org/docs/imdrf/final/technical/imdrf-tech-170921-samd-n41-
clinical-evaluation_1.pdf.
https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-
intelligence-and-machine-learning-software-medical-device.

Hayden, et al. Evaluation of the quality of prognosis studies in systematic
reviews. Ann. Intern. Med. 144, 427-437 (2006).

Higgins, et al. The Cochrane Collaboration’s tool for assessing risk of bias in
randomised trials. BMJ 343, d5928 (2011).

Sterne, et al. ROBINS-I: a tool for assessing risk of bias in non-randomised studies
of interventions. BMJ 355 (2016).

Wolff, et al. PROBAST: a tool to assess the risk of bias and applicability of
prediction model studies. Ann. Intern. Med. 170, 51-58 (2019).
https://webstore.iec.ch/preview/info_iec62304%7bed1.0%7den_d.pdf. (2006).
ISO. https://www.iso.org/standard/52823.html.

ISO. https://www.iso.org/standard/51344.html.

ISO. https://www.iso.org/standard/57757.html.

FDA. https://www.fda.gov/regulatory-information/selected-amendments-fdc-act/21st-
century-cures-act.
https://www.eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32017R074
6&from=DE.

ISO. https://www.iso.org/standard/63553.html.

ISO. https://www.iso.org/standard/74438.html.

Denis, et al. Randomized Trial comparing a web-mediated follow-up with rou-
tine surveillance in lung cancer patients. J. Natl. Cancer Inst. 109 (2017).
Connell, et al. Evaluation of a digitally-enabled care pathway for acute kidney
injury management in hospital emergency admissions. Nature Digit. Med. 2, 1-9
(2019).

Palermo, et al. Mobile health intervention for self-management of adolescent
chronic pain (WebMAP mobile): Protocol for a hybrid  effectiveness-
implementation cluster randomized controlled trial. Contemp. Clin. Trials. 74,
55-60 (2018).

Frias, et al. Effectiveness of digital medicines to improve clinical outcomes in
patients with uncontrolled hypertension and type 2 diabetes: Prospective,
Open-Label, Cluster-Randomized Pilot Clinical Trial. J. Med. Internet Res. 19, e246
(2017).

Aziz, M. T. et al. Reduction in chemotherapy order errors with computerised
physician order entry and clinical decision support systems. Health Inf. Manag
44, 13-22 (2015).

Chen, A. R. et al. Computerized provider order entry in pediatric oncology:
design, implementation, and outcomes. J Oncol Pract. 7, 218-222 (2011).

Wu, J. et al. Master clinical medical knowledge at certificated doctor-level with
deep learning model. Nat. Commun. 9, 4352 (2018).

Ammenwerth, et al. Simulation studies for the evaluation of health information
technologies: experiences and results. Health Inf. Manag. J. 41, 14-21 (2012).
Cheng, et al. Utility of a clinical decision support system in weight loss prediction
after head and neck cancer radiotherapy. JCO Clin. Cancer Inform. 3, 1-11 (2018).
Palanica, et al. Do you understand the words that are comin outta my mouth?
Voice assistant comprehension of medication names. Nat. Digit. Med. 2, 1-6 (2019).

npj Digital Medicine (2020) 110

13
Np)

C. Guo et al.

 

14

123. Seow, H. et al. Do high symptom scores trigger clinical actions? An audit after
implementing electronic symptom screening. J. Oncol. Pract. 8, e142-e148
(2012).

ACKNOWLEDGEMENTS

We acknowledge Nate Carrington from Roche for his valuable input on the
regulations related to digital health solutions in the US and European markets.

AUTHOR CONTRIBUTIONS

All authors (C.H.G., H.A., S.G., G.F., C.G., M.P.) contributed to the conception of the
work, analysis and interpretation of the data, drafting the work and revising critically,
final approval of the completed version, and accountability for all aspects of the work
in ensuring that questions related to the accuracy or integrity of any part of the work
are appropriately investigated and resolved.

COMPETING INTERESTS

C.H.G. and M.P. are currently employees of Roche Diagnostics; H.A., $.G., G.F. and C.G.
are employees of Imperial College London. H.A., S.G., G.F., C.G. receive infrastructure
support provided by the NIHR Imperial Biomedical Research Center (BRC) at Imperial
College London; S.G. is a co-founder of Psyma Ltd, a mobile app that allows clients to
access psychological therapy through a live video call; M.P. is a co-founder of Open
Medical Ltd, a cloud-based patient management platform.

npj Digital Medicine (2020) 110

ADDITIONAL INFORMATION

Correspondence and requests for materials should be addressed to M.P.

Reprints and permission information is available at http://www.nature.com/
reprints

Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims
in published maps and institutional affiliations.

Open Access This article is licensed under a Creative Commons

7 Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this license, visit http://creativecommons.
org/licenses/by/4.0/.

© The Author(s) 2020

Scripps Research Translational Institute
