Machine Learning (2020) 109:2247-2281
https://doi.org/10.1007/s10994-020-05914-3

-)

Check for
updates

Learning with mitigating random consistency
from the accuracy measure

Jieting Wang! - Yuhua Qian'® - Feijiang Li’

Received: 13 April 2020 / Revised: 27 July 2020 / Accepted: 19 September 2020 /
Published online: 27 October 2020
© The Author(s) 2020

Abstract

Human beings may make random guesses in decision-making. Occasionally, their guesses
may generate consistency with the real situation. This kind of consistency is termed ran-
dom consistency. In the area of machine leaning, the randomness is unavoidable and ubiq-
uitous in learning algorithms. However, the accuracy (A), which is a fundamental perfor-
mance measure for machine learning, does not recognize the random consistency. This
causes that the classifiers learnt by A contain the random consistency. The random con-
sistency may cause an unreliable evaluation and harm the generalization performance. To
solve this problem, the pure accuracy (PA) is defined to eliminate the random consistency
from the A. In this paper, we mainly study the necessity, learning consistency and leaning
method of the PA. We show that the PA is insensitive to the class distribution of classifier
and is more fair to the majority and the minority than A. Subsequently, some novel gener-
alization bounds on the PA and A are given. Furthermore, we show that the PA is Bayes-
risk consistent in finite and infinite hypothesis space. We design a plug-in rule that maxi-
mizes the PA, and the experiments on twenty benchmark data sets demonstrate that the
proposed method performs statistically better than the kernel logistic regression in terms of
PA and comparable performance in terms of A. Compared with the other plug-in rules, the
proposed method obtains much better performance.

Keywords Random consistency - Accuracy - Pure accuracy - Bayes-risk consistent

Editors: Kee-Eung Kim, Vineeth N. Balasubramanian.
 Yuhua Qian
jinchengqyh@ 126.com

Jieting Wang
jietingwang @email.sxu.edu.cn

Feijiang Li
feijiangli@ email.sxu.edu.cn

Institute of Big Data Science and Industry, Shanxi University, Taiyuan 030006, Shanxi Province,
China

Q) Springer
2248 Machine Learning (2020) 109:2247-2281

1 Introduction

In the process of decision-making, human beings may make random guesses with-
out logical reasoning when they lack sufficient evidence or detailed knowledge. For
instance, intern doctors are likely to diagnose patients with colds during flu season, and
students are likely to choose a lucky option when faced with a difficult multiple-choices
question. Sometimes, these random guesses may generate consistency with the real situ-
ation. We term this consistency the random consistency.

In the area of machine learning, randomness is unavoidable and ubiquitous in con-
structing classifiers, such as collecting and labeling data, selecting the structures or
parameters of models and even in setting random operations (Ghahramani 2015). The
prediction results of the learning models may also contain the random consistency. The
random consistency produces dishonest feedback, misleads the decision direction and
harms the improvement of the generalization ability, especially when the tendency of
random guesses coincides with the class distribution of the real situation.

Eliminating the random consistency from evaluation measures has been well-studied
in the field of educational psychology, where researchers advocate that the expected
score for the accurate answer with no insight would be zero rather than one. This elimi-
nation has proven helpful in achieving a higher reliability and validity assessment and
increasing the performance of examinees (Sabers and Feldt 1968; Diamond and Evans
1973; Wu et al. 2017; Budescu and Bar-Hillel 1993; Espinosa and Gardeazabal 2010).
In the field of clustering evaluation, eliminating the random consistency has been an
increasingly employed method to improve the quality of clustering evaluation (Hubert
and Arabie 1985; Albatineh et al. 2006; Vinh et al. 2009, 2010; Albatineh and Niewia-
domska-Bugaj 2011; Qian et al. 2016; Li et al. 2018, 2019).

In the area of classification, the accuracy (A) is a vital performance measure in
model evaluation and learning theory. The original learning theories focus on searching
the generalization bounds for the error probability (one minus accuracy) (Valiant 1984;
Bartlett and Mendelson 2003). The traditional algorithms, including logistic regression,
support vector machine and Adaboost are designed to optimize convex surrogate loss
functions of the error probability (Zhang 2003; Bartlett et al. 2006). In ensemble learn-
ing, accuracy has been used as the preferential measure to evaluate the performance of
integration (Zhou et al. 2002; Martinezmunoz and Suarez 2006). Although it is a funda-
mental performance measure, the accuracy does not recognize the random consistency,
which may limit the performance of the algorithms based on it. In this paper, we aim to
define a performance measure that eliminates the random consistency from the accuracy
and to study the learning performance of the measure theoretically and experimentally.

1.1 Related work

The measure that eliminates the random consistency from the accuracy is referred to
as the pure accuracy (PA). The PA measure is a kind of non-decomposable measures.
The non-decomposable measures cannot be decomposed into each individual instance
(Waegeman et al. 2014; Kotlowski and Dembczynski 2017; Sanyal et al. 2018). Simi-
lar measures include the F-measure, AUC, and balanced error rate (Zhao et al. 2013).
For the non-decomposable measures, many learning theories and algorithms have been
developed.

Q) Springer
Machine Learning (2020) 109:2247-2281 2249

From the aspect of learning theory, Waegeman et al. (2014) investigated the generali-
zation bound in terms of the F-measure when optimizing the Hamming loss and subset
zero-one loss in a multi-label learning setting, and concluded that optimizing such losses
as a surrogate of the F-measure leads to a high worst-case regret. Bayes-risk consistency
guarantees that by increasing the amount of data, a rule can eventually learn the optimal
decision with high probability. Agarwal et al. (2005b) show the Bayes-risk consistency of
the AUC based on a new proposed combinatorial parameter. The key step of their proof is
the symmetrization by a ghost sample that is the same as that for the classification error
rate (Devroye et al. 1996). In this paper, to clarify the surrogate relation of PA and A, we
show the upper bound of PA value for A-optimal rule and the upper bound of A value for
PA-optimal rule. In addition, we give a Bayes-risk consistency analysis for the pure accu-
racy based on the Rademacher complexity in a finite hypothesis space and based on the VC
dimension in an infinite space.

In optimizing the non-decomposable measures, Musicant et al. (2003) extended the sup-
port vector machine to optimize the F-measure by setting appropriate parameters in the
standard SVM. Joachims (2005) proposed a large margin machine for maximizing a convex
lower bound of non-decomposable measures. Hazan et al. (2010) and Song et al. (2016)
trained deep neural networks by inferring the gradients of the non-decomposable meas-
ures. Narasimhan and Agarwal (2013) proposed a SVM model for optimizing the AUC
via a tight convex upper bound. Waegeman et al. (2014) proposed an exact algorithm for
optimizing the F-measure in the context of multi-label learning. Gao et al. (2016) proposed
a one-pass AUC optimization algorithm that needed to read the training data only once.
These methods directly optimize the non-decomposable measures. In addition to these
direct methods, the plug-in rule is an effective method that learns a posterior probability
function by the logistic regression method or some other mature methods, and searches a
threshold that optimizes the objective measure. For optimizing non-decomposable meas-
ures, Narasimhan et al. (2015) simply used the bisection method to determine a threshold.
The bisection method require the monotonicity of the function being solved. Then, there
is still much room for improving the effectiveness of the plug-in method. Here, we give an
interval search method to determine the threshold of the plug-in rule for optimizing the PA.

1.2 Contributions

We aim to verify the learning ability and Bayes-risk consistency of the PA in this paper.
First, with regard to the cost-sensitive loss function, we give a non-closed formulation of
the optimal rule w.r.t the PA. Based on this formulation, we illustrate that the PA is insen-
sitive to the class distribution of classifiers and gets a low bias in minority accuracy and
majority accuracy compared with A. Second, we give a novel lower and an upper bound
for the optimal rules w.r.t the A and PA, respectively. These bounds help us clarify the
surrogate relation between the PA and A. Furthermore, the generalization upper bounds of
the PA in the worst case are given to analyze the consistency. The proof of these bounds
employ the same symmetrization technique that was applied to prove the generalization
upper bound of the accuracy (Devroye et al. 1996) and AUC (Agarwal et al. 2005a). How-
ever, the difference is that the PA has fractional formulation. Thus, the consistent analysis
of the PA needs to handle the fractional formulation. Last, we design a plug-in rule in
terms of maximizing the PA and experimentally validate its performance.
Briefly, the major contributions of this paper are summarized as follows:

Q) Springer
2250 Machine Learning (2020) 109:2247-2281

e Some bounds for the optimal rules w.r.t the PA and A are given. These bounds theoreti-
cally show that the PA-optimal rule is capable of approaching a satisfactory A value for all
distributions.

e Second, we develop an inequality to handle the probability of large deviations of variables
in fractional form. The generalization bounds for the PA are shown in finite and infinite
hypothesis space. These bounds verify the Bayes-risk consistency of learning by PA.

e We propose a plug-in rule based on the interval search method for optimizing the PA.
Through it, we experimentally verify the fairness and performance of PA in learning.

The organization of this paper is presented as follows: We give the definition of the PA in
Sect. 2. In Sect. 3, two examples are given to show the necessity of evaluating classifiers by
the PA. In Sect. 4, a surrogate analysis between the PA and the A is conducted. In Sect. 5, the
generalization upper bounds of the PA are developed. We propose a plug-in rule for optimiz-
ing the PA and experimentally validate its performance in Sect. 6. We form a conclusion and
propose future work in Sect. 7.

In this paper, definitions and theorems which are tagged with a literature reference are
taken from the literature, while the original ones come without such a tag. All the proofs are
presented in the “Appendix”’.

2 Preliminaries

We consider the task of binary classification. Let 1 C R! and Y= {+l1,-1} be the fea-
ture space and the label space, respectively. The underlying distribution of 4 x Y is usually
unknown, and we only have a collection of empirical data Sy = {(X1,y1),.--.(%y.Yy)} that
are drawn independently from this distribution. The goal of classification is to learn a classi-
fier h(x) mapping from *¥ to Y via Sy. Let H be the hypothesis space, from which the clas-
sifier h(x) is learnt. To evaluate the performance of classifiers, the confusion matrix is usu-
ally employed. Let TP, FP, FN, TN denote the true positive P(A(X) = +1, Y = +1), false
positive P(h(X) = +1, Y =—-1), false negative P(A(X) = —1, Y = +1) and true negative
P(h(X) = —1, Y = —1), respectively. Let p and qg(h) denote the probability of P(Y = +1) and
P(h(X) = +1), respectively. The confusion matrix is shown in Table 1.

Based on the confusion matrix, the accuracy (A) and the error probability (L) are defined
as:

A(h) = P(A(X) = Y) = TP + TN, (1)
Lith) = P(h(X) 4 Y) = FP + FN. (2)
Table 1 Confusion matrix h(X) Y
Y=+1 Y=-1 Total (h)
h(X) = +1 TP FP qth)
h(x) =-1 FN TN 1—q(h)
Total (Y) Pp l-p 1

Q) Springer
Machine Learning (2020) 109:2247-2281 2251

2.1 The definition of PA

To define the pure accuracy (PA), we begin with giving the definition of random accuracy
(RA), which aims to measure the random consistency in accuracy. For the classifier h(x) to
be evaluated, let 1“ be the set of all possible binary partitions with the same class distri-
bution as it:

H® = {hl > P(h'(X) = 41) = gh), h'(X) € {+1,-1}}. (3)

Considering that the output preference of the classifier (tendency of predicting which
instances as positive) is unknown in advance, we suppose the partitions in 1H“ are uni-
formly distributed. Because the partitions in 1“ have the same output randomness as the
classifier to be evaluated, we define RA as the expectation accuracy over the partitions in
HI,

Lemma 1 When the partitions in H™ are distributed uniformly, the expectation accu-
racy of partitions in H™ is:

EnenanA(h’) = pg(h) + (1 — p)(1 — g(h)). (4)
Definition 1 The RA is defined as:
RA(h) = pq(h) + (1 — py — qth). (5)

Definition 2 The PA is defined as:

A(h) — RA(h
PA(h) = Sa (6)

Definition 3 The pure loss (PL) is defined as:

1 —A(h)

The denominator of PA guarantees the maximum value to be 1. Note that the formula-
tion of the PA coincides with the definition of Cohen’s x statistic (Cohen 1960; Scott 1955;
Goodman and Kruskal 1963). The difference between them is how to define the random
consistency. In the definition of Cohen’s x statistic, random consistency is called as chance
agreement. The chance agreement is the agreement degree that the two raters give their rat-
ings independently. The chance agreement between the classifier )(X) and the label label Y
iS:

PAX) =Y)= PAX) = Y = 1) = pqih) + (1 — pid — gh).
(8)

l={-1,+1}

The way we define the RA gives a general framework to measure the random consist-
ency in measures and is helpful to propose new performance measures.

Cohen’s x statistic has been successfully used in the area of psychology (Cameron
et al. 2003) and medicine (Blair and Stanley 2008). The advantage of correction for
the expected agreement by chance has made Cohen’s x statistic commonly be used as
a reliable performance measure in the area of machine learning (Ferri et al. 2009;). In
ensemble learning, Kappa-error diagrams have been used to gain insights about the

Q) Springer
2252 Machine Learning (2020) 109:2247-2281

effectiveness of classifier ensembles (Kuncheva 2013) and to prune classifiers (Margin-
eantu and Dietterich 1997). In addition, Cohen’s x statistic has been used for feature
selection (Vieira et al. 2010).

3 On the advantages of pure accuracy measure

A learning algorithm sensitive to the class distribution may get a decision boundary
that deviates from the optimal one. Thus, the learning objective should be insensi-
tive to the output class distribution. The extensively applied accuracy does not sat-
isfy this property. We employ Example | to show that the PA is satisfactory in this
respect.

Example 1 (Class distribution insensitivity) In this example, we aim to compare the eval-
uation result of the A and PA on the prediction results with different class distribution.
Under the settings of N = 100 and p = 0.3, we randomly generate a binary vector as the
true label vector. A partition with a fixed class distribution g can be generated by Algo-
rithm 1. The class distribution g is varied from 0 to | with a step of 0.05. Under each g, we
run Algorithm 1 1000 times to generate 1000 partitions and use A and PA to evaluate the
partitions, respectively. The distributions of the A value and PA value are shown in Fig. 1.
From Fig. 1, it is easy to observe that the value of A decreases with the increase of g, while
the value of PA is always near zero. This finding reflects that the A is sensitive to the class
distribution of classifiers, while the PA is not.

Algorithm 1 Generator of Partition with a Fixed Class Distribution

Require: Data set Sy = {(a, y;),7 = 1,2..., N}, class ratio q € [0,1].
1: for each 1 € N do
2: Generating go ~ uniform(0, 1).
3: if go < q then h(a) = +1;
4: else h(a;) = —1
5: end if
6: end for
Ensure: The predicted label h(a;), i = 1, 2...,.N.

Further, we give the classifier that maximizes A and PA, respectively.

 

Fig. 1 Distribution of A and PA under different g. Under each q, the box plot depicts the A values (left
panel) and PA values (right panel) of Algorithm 1

Q) Springer
Machine Learning (2020) 109:2247-2281 2253

Lemma 2 (Devroye et al. 1996) Let n(x) = P(Y = +1|X =x) be the conditional class
probability given X = x. The classifier that maximizes the A or minimizes the L is:

nore mampraay= {th 1b °
Correspondingly, the minimal error probability is
L* = L(h,) = Ey min{n(X), 1 — n(X)}. (10)
Theorem 1 The classifier that maximizes the PA is
hp, (x) = arg max PA(h) (11)
_ { +1, n(x) > (5 — p)PA* +p, (12)
—l, otherwise.

where PA* = PA(h,,) and p = P(Y = +1).

For the cost-sensitive loss L, = pFP + (1 — p)FN, it is known that when p is smaller,
more attention will be paid to the minority class to get a smaller L,. According to the proof
of Theorem |, PA is equivalent to L, with p = (1 /2 — p)PA* + p. Due to PA* < 1, a smaller
p value will generate a smaller (1/2 — p)PA* + p value. In this case, h,, will pay more
attention to the minority class. Thus, 4;,, may be insensitive to class distribution.

In learning classifiers, the minority class is often overwhelmed by the majority class to
guarantee a higher overall accuracy (He and Garcia 2009). Then the classifiers learnt by
optimizing the accuracy or error probability are usually biased to the majority class. This
phenomenon is particularly desirable to avoid because the minority class is precious and
inadequately represented. We employ Example 2 to show that the pure accuracy can miti-
gate the classification bias.

Example 2 (Fairness) To measure the bias of the classifier h(X), we use the absolute differ-
ence of the two class accuracy:

Bias(h) = |P(A(X) = -1|¥ = -1) —- P(X) = +1 Y = +1) (13)

Assume that two class data are generated from Gaussian distribution: M(,, >) and
N(py, X). The label of the minority class is corrupted by the instance-independent noise at
the level s,: PY =-1|Y =+1)= Sy:

For this learning task, the bias of hi iS:

d, + A/2 dy — A/2
Bias(h*.) = o uot) —] -o( 2") 14
‘ ( Va Va ”

where @(e) is the cumulative distribution function of the standard normal distribution,

A = (fy — My)’ 2 (My — My) and dy =1n 2 Due to the formulation of /,, is non-
2s,

closed, the bias of it is simulate through a large number of instances. First, a sample that

obey the distribution of this task are generated with a size of 10*. Then, the threshold that

optimizes the PA is searched from the range [0, 1] with a step 10~*, and the bias of hp, 1s

calculated through the sample.

Q) Springer
2254 Machine Learning (2020) 109:2247-2281

Let uw, = —1, » = land yp, vary from 0 to 2, p vary from 0.05 to 0.35 and the one-side
noise level s,; vary from 0 to 0.5. The bias curve of h) (the dashed line) and that of h7, (the
solid line) are shown in Fig. 2. As Fig. 2 shown, the dashed line is consistently lower than
the solid line in each case, which demonstrates that learning by PA is more fair than learn-
ing by A under different imbalance degree, overlap degree and noise level.

4 Surrogate analysis of the optimal rules

The task of classification is to predict the labels of future observations. The optimal clas-
sifier is usually obtained by minimizing a loss function. From the same hypothesis space,
different loss functions usually obtain different optimal classifiers. In this section, we focus
on giving some novel bounds for h;,,(x) and h’,(x) to clarify the substitution relationship
between them in learning classifiers. Theorem 2 (derived by Lemma 3) and Theorem 3
(derived by Lemma 4) are major results of this section.

Lemma 3 For all distributions, the plug-in rule with p as the decision threshold

1
h(x) _ { +1, yn(x)> p, where p€(0, 51, (15)
—1, otherwise,
satisfies:
1- Pax
Lih,) > - , (16)

when p = 1/2, the equality holds.

Lemma 3 gives an upper bound on the error probability of the plug-in rule. Accord-
ing to Lemma 3, we have:

Theorem 2 For all distributions, suppose that p = P(Y = +1) < 7 the error probability
of h,,, satisfies:

 

 

 

 

 

 

Fig. 2 Bias of 4) and h,, as a function of the ratio of majority class (left panel), the Mahalanobis distance

PA
of the two distributions (middle panel) and the one-side noise level (right panel). The dashed line is the bias
curve of hi and the solid line is that of hy A

D) Springer
Machine Learning (2020) 109:2247-2281 2255

1
L* < Lih,,) < | —————— - 1 ]L". 17
PA ( — p)PA* +p ( )

From Theorem 2, we can conclude that the error probability of the optimal classifier
learnt by PA satisfies L(h,,,) > L(hi,) as PA* — 1 for all distributions.

Loy 1 *
valves 4 For all distributions, suppose that p= P(Y = +1) < > the pure loss of h,
satisfies:

EE

« 18
p(s-p)-E(5-P) ”
Lemma 4 gives the upper bound of the pure loss of 14, with respect to L*. To obtain

the convergence relation between PL(h;,) with PL(h,,), we further amplifying L* in
Theorem 3.

PL(h*) <

Theorem 3 For all distributions, suppose p < > the pure loss of h’, satisfies:

PL(hp,) < PL(h, (19)

c 2(1 — p)
~ pGB — 2p) — L*(1 — 2p)
From Theorem 3, we can conclude that the pure loss of the optimal classifier learnt by
A satisfies PL(h,,) > PL(h;,,) as L* > 0 only when p = * Based on Theorems 2 and 3, we
can infer that learning by PA can obtain a satisfactory A for all distributions, while learn-

ing by A can obtain a satisfactory PA only when the class distribution is balanced. We also
employ Example 3 to reflect this phenomenon.

PL(hp,)- (20)

Example 3 (Surrogate analysis) In this example, we aim to analyse the surrogate relation
of A and PA. Under the settings of N = 100 and p = {0.1,0.2,0.5}, we enumerate all pos-
sible values of FP and FN and calculate the A values and PA values. The A value and PA
value of each pair of (FP, FN) under different p are shown in Fig. 3. From Fig. 3, we can
observe that under the settings p = {0.1,0.2}, when the PA value tends to 1, most of the A
values tends to 1, while when the A value tends to 1, most of the PA values are low. When
p = 0.5, the relation between A and PA is linear.

5 Bayes-risk consistency analysis of learning by the pure accuracy
measure

The underlying distribution of 1 x JY is usually unknown, and we only have a collection of
the empirical data Sy = {(X,, yj), +.» (®y. Yy)} that is drawn independently from the distri-
bution. In machine learning, the classifier is generally obtained by the principle of empiri-
cal risk minimization (ERM). The feasibility of the ERM is guaranteed by the property of
Bayes-risk consistency. The corresponding loss function of PA is PL. Therefore, in this
section, we validate the learnability of PA by analyzing the Bayes-risk consistency of PL.

Q) Springer
2256 Machine Learning (2020) 109:2247-2281

 

 

 

  

0 0.5 1
A A A

(a) (b) (c)

Fig.3 Surrogate analysis of A and PA when p = 0.1 (left panel), p = 0.2 (middle panel) and p = 0.5 (right
panel)

For the risk function R, let Ry(A) be the empirical risk calculated on Sy:
Ry(h) = Exyye sy RA), Y). ERM obtains the optimal rule he from a hypothesis space 71
N

by minimizing R nh):
he = arg min Ry(h). (21)

To guarantee the feasibility of the ERM, the property of Bayes-risk consistency is
defined as:

Definition 4 (Devroye et al. 1996) The rule he is Bayes-risk consistent, if for any small
N

enough é, it satisfies

Jim PUR ) — inf R(A)| > €) = 0. (22)

The Bayes-risk consistency requires that the empirical optimal hypothesis he has a
N
large probability of converging to the universal optimal hypothesis as the number of empir-

ical data tends to infinite.
To analysis the Bayes-risk consistency, the gap between Rh, ) and inf, R(/) is usually
N

upper bounded by (Devroye et al. 1996):

R(he ) — inf R(A) < 2 sup |Ry(h) — RD), (23)
Ry h hEH

which is known as the estimation error. The estimation error measures the performance gap
between the empirical data and the underlying distribution. The convergence of the estima-
tion error ensures that the rule learnt finite samples can be generalized to infinite samples.
The bound of the estimation error, the so-called generalization bound, is the key factor in
studying the property of the Bayes-risk consistency.

The Rademacher complexity (Bartlett and Mendelson 2003) and the VC Dimension (Vap-
nik and Chervonenkis 1971) are two complexity measures of the hypothesis space; they have
a crucial role in bounding the estimation error in the sense of accuracy. Here, we use the gen-
eralization bounds based on them to analyse the Bayes-risk consistency of learning by PA. To
save space, we omit the definitions of the Rademacher complexity, the VC dimension and the
corresponding generalization bounds.

D) Springer
Machine Learning (2020) 109:2247-2281 2257

5.1 The Bayes-risk consistency of the pure loss measure in a finite hypothesis space

The fractional form of the pure loss leads to that the empirical value of it is not an unbiased
estimation of the expected value. Therefore, the techniques in deriving the generalization
bounds of the error probability (Theorem 8 in Bartlett and Mendelson (2003) and Theorem 2
in Vapnik and Chervonenkis (1971) cannot be directly applied. Here, we establish a bridge
between the estimation error of the pure loss and that of the error probability; and then obtain
the Bayes-risk consistency of the pure loss in finite hypothesis space and infinite hypothesis
space based on Theorem 8 in Bartlett and Mendelson (2003) and Theorem 2 in Vapnik and
Chervonenkis (1971), respectively.

First, we give the formulation of the empirical error probability Ly(h) and the empirical
random accuracy RA yA) to analysis the Bayes-risk consistency:

N
Ly(h) = ))U{h(x;) # yi}. (24)
i=l
jn” | N
RAy(h) = ae 2 2 I{hi(x;) = y,}. (25)
where h, EH” | 6 ja is the cardinality of a set and I{e} is the indicator function. Then, the
empirical pure loss PLy(h)i iS
~ Ly(h)
PLy(h) = —X—_. (26)
— RAy(A)

In practice, according to Lemma |, the empirical random accuracy is computed by:

RAy(h) = 1 — py — (1 — By )qWy, (27)
where
N
Py = Yi Uy, = +1}. (28)
i=l
_ N
qh)y = YU he, = +1). (29)

i=1

Lemma 5 For two random variables Z,,Z,€[0,1], any e€€(0,1], let
a = EZ,EZ,/(2EZ, + EZ,), we have
Z,_ EZ, :)
(30)

P
( Z, EL,

< P(|Z, — EZ,| > ae) + 3P(|Z, — EZ,| > ae).

Z, EZ,

 

 

Lemma 5 links the probability of the large estimation error of the fractional variable to
that of the numerator and denominator. Based on Lemma 5, we obtain Theorems 4 and 5.

Q) Springer
2258 Machine Learning (2020) 109:2247-2281

Theorem 4 Suppose the cardinality of H is finite: |H| < 00, then for every h € H, any
Ee € (0, 1], we have

 

P| sup PLy(h) — PL(h) > e}
heH

2 (31)
< 8|H| exp {-29(« — a) \

where @ = MiNpex PLD and Rc(H) is the Rademacher complexity of H.

 

Theorem 4 provides the probability of the large estimation error in terms of the number
of the empirical data in finite hypothesis space. From Theorem 4, we can conclude that
learning by the PA is Bayes-risk consistency in a finite hypothesis space.

5.2 The Bayes-risk consistency of the pure loss measure in an infinite hypothesis
space

In this section, we consider the Bayes-risk consistency in an infinite hypothesis space. For
an infinite hypothesis space, the union bound cannot be utilized. We utilize the symmetri-
zation technical to bound the estimation error of the pure loss. Next, we divide the hypoth-
esis space into N + | subspaces according to the class probability of hypothesis functions,
to ensure that each hypothesis subspace has the same degree of random accuracy. Then,
we employ the VC bound of the error probability to bound the estimation error of the pure
loss.

be N
collection as Sy and PL,(h) is the corresponding empirical pure loss. Suppose

N > 5(6 + 4ae)a~7e~*, where a = min,<r pupa E € (0, 1], then we have
P| sup > Ee \
hEeH
< 2p sup |PLy(h) — PL yh) >< \
hEH 2

Theorem 5 As the same condition as Lemma 6 and suppose the VC dimension of H is
finite: d,.(H) < 00, we have:
> e}

{sup
heH
2 _ Inn _ 2

< 4(N + Dyexp { -(S NE wh

Lemma 6 Let Sy = {(x’ »Yy)> vey (x! Vy )d be an independent and identically distributed

PLy(h) — PL(h)

 

 

(32)

 

PLy(h) — PL(A)

 

 

(33)

Theorem 5 provides the probability of the large estimation error in terms of the number
of the empirical data in infinite hypothesis space. From Theorem 5, we can conclude that
learning by the PA is Bayes-risk consistent in an infinite hypothesis space.

Q) Springer
Machine Learning (2020) 109:2247-2281 2259

6 Performance validation of learning by the pure accuracy measure

By the Bayes-risk consistency, we have shown that the PA can be utilized to learn clas-
sifiers through minimizing PL. However, due to the fractional form, optimizing PL is a
challenging task. To handle this challenge, we introduce the plug-in rule and propose an
interval search method.

The plug-in rule refers to a rule with a formulation of h;.(x) = sign(q(x) — 6"), where
n(x) is an estimator of the posterior probability y(v) = P(Y = +1|X = x) and 6* is a thresh-
old (Koyejo et al. 2014). The plug-in method mainly contains the following steps: first,
randomly split the training data S, into S, and S,; second, learn 4(x) by minimizing a loss
function on S); third, determine 6* by maximizing the learning objective on S,.

In Narasimhan et al. (2014), it has been proved that assigning an empirical threshold to
a suitable posterior probability estimate can optimize the performance measures expressed
as a function of the 7P and TN and p. That is, the plug-in method can optimize a com-
plex performance measure through searching a decision threshold that optimizes the meas-
ure for the posterior probability estimate. The major focus of this section is developing an
method to search the threshold that optimizes PL rather than to learn the posterior prob-
ability 7(x).

In this section, first, we introduce the method to learn the posterior probability. Then,
we discuss some methods of determining the threshold that optimizes PL and propose a
interval search method. Finally, we experimentally validate the performance of the interval
search method and the classifier learnt by the PA.

6.1 Learning n(x)

Many methods can be employed to learn 7(x). Here, we introduce the kernel logistic
regression model, which is proven to be a suitable posterior probability estimate (Ingo
2005; Narasimhan et al. 2014; Menon et al. 2013). The kernel logistic regression model 1s:

ISi| 1S, IS; | IS;
max » de ay KO; XV; — 2 log (: + exp ( ay K(X;, »)) (34)

i i=l j=l

where a; are the variables to be solved and K(e, «) is kernel function. With the optimal a is
obtained by the gradient descent method, we have

1

(x) = ———__
1 + exp(- Dix ax; yjK@;,x))

(35)

6.2 The interval search method

As for determining 6*, different threshold settings correspond to optimizing different learning
objective functions.

To optimize the accuracy, the threshold 6* of the plug-in rule is 0.5, and this is the so-
called kernel logistic regression (KLR) method. To optimize the balanced accuracy (BA),
the threshold 6* of the plug-in rule is p (Menon et al. 2013).

For the measures in a fractional form, search strategies are effective and simple. An
intuitive approach to determine the optimal threshold is the point-wise search method,

Q) Springer
2260 Machine Learning (2020) 109:2247-2281

namely, evaluating the fractional measure at each possible threshold and outputting the best
performing threshold. There is no doubt that exhausting all possible thresholds is impos-
sible. The gird search is a method to handle this, which divide the range of the threshold
into multiple equal intervals and set the end points as the candidate thresholds. Besides,
the posterior probabilities on S, can also be set to the candidate thresholds. We term this
search strategy the S,-search. The gird search method and the S,-search method search
the threshold in a limited range. In addition to the point-wise search methods, the bisec-
tion method transforms the fractional measure to a one-dimensional function and obtains
the optimal threshold by solving the zero root of the one-dimensional function in binary
(Narasimhan et al. 2015). The bisection requires that the objective function be monotone
on the interval, while the fractional performance measures are usually non-monotonic with
respect to the threshold.

In this subsection, we develop a method for searching the optimal threshold via the
interval search method, and use this method to minimize PL. The interval search method
is an effective way to search the local minimum of a unimodal function (Chong and Zak
2011). For a unimodal one-dimensional function f(r) defined in [a, #], to obtain the mini-
mum 7*, the interval search method is based on the idea that it produces a series of inter-
vals [@,, B,.], where [a,41, Bp41] C [a,, B,J and lim,_,,, B, = lim,_,,, a, = r*. Specifically, the
interval search method inserts two points in each iteration and produces [@,, A,, M;,, B,]. If
SA) <fCu,), then a,,,; = a, and f,,, = w,; otherwise, a,,,; = A, and f,,,; = B,. When the
interval length is reduced by the ratio of 1 — (5 — 1)/2, the interval search method is so-
called gold section method.

For any plug-in rule h;(x) = sign(#(x) — 6), we briefly discuss about whether the PL is a
unimodal function of the threshold 6. According to the proof of Theorem 1, the PL is con-
sistent to the cost-sensitive loss with the optimal threshold as the cost weight:

L,.(6) = 6*FP(6) + (1 — 6*)FN(6), (36)
where 5* is the minimum of L;.(6):
é*= argmin L 5.) (37)
and FP(6) = P(n(X) > 6, Y = —1), FN(6) = P(n(X) < 6, Y = +1). Because
FP(6) = P(W(X) > 6, Y = -1) = P(n(X) > 6) — P(n(X) > 6, Y = +), (38)
we have:

L;.(6) = 6°FP(6) + (1 — 6°) FN(6)

= §*P(y(X) > 5) — 6*P(Y = +1) + POX) < 6,¥ = 41). @)
For 5, < 5», we have:
Lis-(5,) — Lye(6y)
= 5*P(n(X) € (6,,55]) — P(n(X) € (6,, 55], ¥ = +1). (40)
Thus, if
PUD) € Cr. eb VEtD — 5 an

P(m(X) € (6), 65])

we have L;.(6,) > Ls.(6y); otherwise, L5.(6,) < L5« (65).

Q) Springer
Machine Learning (2020) 109:2247-2281 2261

The unimodality of PL requires that for 6, < 6, < 6*, L;.(6,) > L5.(6)) and for
0° <6, < 65, L5.(6,) < L5.(65). Thus, when 6* < 6, <6,, the unimodality of L;.(6)
requires that the posteriori probability should satisfy condition (41), which signifies that
there exist a small number of positive objects in the objects with small posterior prob-
abilities. When 6, < 6, < 6”, the unimodal of L;..(6) requires that the posteriori probability
should satisfy the contrary case of condition (41), which signifies that there exist a large
number of positive objects in the objects with large posterior probabilities.

According to the above discussion, if the posteriori probability is sufficiently good, PL
is a unimodal function of 6. The interval search method is applied to obtain 6*. From Theo-
rem 1, we have

1 1 1
s=(5- )Pa® =>-(5- )PL*
5? +P=5-\a7P (42)
Then, we express the plug-in rule as:

h,(x) = sign] ace) — (5 - (5 -P)r)]. (43)
2 2
and apply the interval search method to finding the optimal r that minimizes PL 5,|(A,(x)).
A fixed reduction of the interval is employed. In each iteration, the interval length is
reduced by the t € (0,0.5) ratio. The interval search method is thus called as t-interval
search method and the ratio t is a parameter to be tuned. The interval search method for
minimizing the PL is shown as Algorithm 2. The time complexity of the t-interval search
method contains two parts, which are learning 7(x) and searching 6*. The time complex-
ity of learning 7(x) is the same as the gradient descent method, and the time complexity
of search 6* is O(N log, €), where N is the number of training data, 7 is the reduction ratio
of the interval and ¢ is the threshold of the stop condition. Learning 7(x) is the main time
consuming part. When handling large number of samples, it is suggested to utilize effective
gradient descent method.

Algorithm 2 The 7-Interval Search Method for Minimizing the PL

Require: The training data Sy
Randomly split the training data Sy into S; and S2 with a ratio of 8 : 2 and use S; to
estimate 7(x)
Set a= 0, 8 =1, t = 0,e = 0.0001
Let A=a+7(G-—a) and w= G—7(6—- a),
Obtain ha(2) = sign{f(a) — (+ — (+ — p)d)], hy(2) = signiq(a) — (4 — (4 — p)w)] and
calculate PL) s5,\(hy, Y) and PL) s,\(hy, Y) on S2
while 6—a>e,do
IF PL)s,\(hy(2)) < PL s,\(hu(2)), THEN update 6 = « ELSE update a = );
A=a+7(B—a), w= B—7T(GB—a) and calculate PL) sy\(hy(2)) and PLisy| (hy (a)) on
S2;
t=t+1; é&=5-(4-—p)d.
end while
Ensure: The optimal threshold 6*.

6.3 Experiments

We validate the performance of the t-interval search on a variety of benchmark data sets.
By the benchmark data sets, we show that learning by PA is more fair in majority accuracy

Q) Springer
2262 Machine Learning (2020) 109:2247-2281

and minority accuracy than A and compare the t-interval search method with some other
plug-in rules to show its effectiveness.

The benchmark data sets are downloaded from the KEEL Data Set Repository
(Alcalafdez et al. 2008) and the UCI Machine Learning Repository (Dua and Graff
2017). These data sets are briefly described in Table 2, including data ID, name, size,
number of attributes and the imbalance ratio(IR). The posterior probability is gen-
erated by the kernel logistic regression and the kernel function is the RBF kernel
K(x, x’) = exp(—y||x — x’||°).

Each data set is randomly divided into a training set, a validation set and a test set at
a ratio of 3:1:1. The methods are compared in the same division. We randomly divide
the data set 30 times to obtain an average performance. The parameter y is chosen from
{2-4 2-7, 2°, 27,24, 2°} and the t is chosen from {0.1,0.2,0.3,0.4} via the validation set.
Each attribute is linearly scaled to the range [0, 1] using the maximum and minimum
values in the training data. For each data, we also add 3% and 5% random uniform label
noise to increase the complexity of the data.

First, to show that learning by PA is more fair than A, we compare the bias [refer
to Eq. (13)] of KLR and the t-interval method. Figure 4 shows the comparison result.
Each bar of Fig. 4 is the difference of the mean bias over 30 times between KLR and
the r-interval method on each benchmark data set. As shown in Fig. 4, we observe that
16/20, 17/20, 16/20 bars are greater than zero under 0% noise, 3% noise, 5% noise,
respectively. That is, the bias of KLR is large than that of the t-interval method, which
reflects the classifiers learnt by PA is more fair than the classifiers learnt by A.

Table 2 Description of data sets

Data ID Data name Attribute Instance IR Download
1 First-order theorem proving 1 51 6118 1.02 UCI

2 First-order theorem proving2 51 6118 1.28 UCI

3 First-order theorem proving3 51 6118 1.16 UCI

4 First-order theorem proving4 51 6118 1.14 UCI

5 First-order theorem proving5 51 6118 1.27 UCI

6 Crx 15 653 1.21 KEEL
7 Heart 13 270 1.25 KEEL
8 Australian 14 690 1.25 KEEL
9 Wdbc 30 569 1.68 KEEL
10 Bands 19 365 1.70 KEEL
11 Ionosphere 33 351 1.79 KEEL
12 Wisconsin 9 683 1.86 KEEL
13 Pima 8 768 1.87 KEEL
14 Titanic 3 2201 2.10 KEEL
15 German 20 1000 2.33 KEEL
16 Segment 19 2308 6.02 KEEL
17 Dermatology 34 358 16.90 KEEL
18 Wilt 5 4839 17.54 UCI
19 Flare 11 1066 23.79 KEEL
20 Winequality-red 11 1599 29.17 KEEL

Q) Springer
Machine Learning (2020) 109:2247-2281 2263

Second, to validate the performance the proposed method, the A and PA are
employed as evaluation measures. The benchmark methods are KLR, p-cut (with the
proportion of the minority class in S, as the threshold), grid-search, S,-search and
bisection method. The KLR aims to optimize the A, and p-cut aims to optimize the
balanced accuracy. The grid-search and S,-search aim to optimize the PA. The bisec-
tion is used to optimize the F)-measure and PA, which are noted as Bisection-F, and
Bisection-PA, respectively. Tables 3, 5 and 7 show the mean and the standard deviation
of A over 30 time comparisons with 0%, 3% and 5% label noise, respectively. Tables 4,
6 and 8 show the mean and the standard deviation of PA over 30 time comparisons with
0%, 3% and 5% label noise, respectively. In each row of the tables, the method with the
maximal evaluation value is underlined and printed in bold type, and the method with a
dot indicates that the t-interval search is significantly better with regard to the pairwise
Student’s ¢ test with a level of 0.1. As shown in Tables 3, 4, 5, 6, 7 and 8, the evaluation
score obtained by the t-interval search is highlighted in bold and is underlined in most
of the comparisons. In many comparisons, the t-interval search is statistically better
than other methods.

To further analysis the statical performance of each method, for each method, we
calculate the gap between the times of the significant wins and the times of signifi-
cant loses. An algorithm a significantly wins b if its mean and standard deviation are
satisfied:

Og Op
Ui, — 1.96“ > w, + 1.962,

vi vi a4)

where f is the number of comparison times; otherwise, a significantly loses b (Please refer
to reference Li et al. (2016) for more details). Figure 5 shows the results of the statisti-
cal comparison. Each bar in Fig. 5 represents the gap between the times of the significant
wins and the times of significant loses. As shown in Fig. 5, we observe that the bar of the
t-interval search method is the highest w.r.t PA under different noise level. With respect
to A, the bar of the t-interval search is the highest when the noise level is % and 5%; and
when the label is not polluted by noise, the bar of the t-interval search is the second high-
est. In general, we can conclude that the t-interval search method can optimize the PA
value better and also can obtain a satisfactory A value.

 

 

°
h

 

°

oo

a
T

oO oO
au SO pp
a ve a w
T T T
° ° °
a4 OS wp © w&
a nN a oO a

oO

°
on
a

Bias(KLR) —Bias(7-interval)
°

Bias(KLR)—Bias(r-interval)

2°
2°
a

 

 

 

oO
°o

1
12345 67 8 9 1011121314151617181920 12345 67 8 9 1011121314151617181920

 

 

Data ID Data ID Data ID

(a) 0% noise (b) 3% noise (c) 5% noise

Fig.4 Bias Gap of KLR and the t-interval method under different noise level. Each bar is the mean gap
over 30 times on each data set

D) Springer
Machine Learning (2020) 109:2247-2281

2264

Le0'0
€c0'0
£900
€00°0
€00°0
€c0'0
L100
6c0°0
0100
ce0'0
0S0°0
L100
¢c0'0
8£0°0
1700
cl0'0
TT00
e100
v10'0
cl0'0

+H o+H OH
ao a oO
t+ y+ +
eCAD
a

 

+
S
S
=
=

 

+
e
a
Sy
S

 

+H sO
\o
wa
ae
So

 

+
eS
S
S
S

 

+
=
>
Sy
S

 

+
>
\e
x
S

 

+
st
Vel
er
S

 

+
1
eS
Sy
S

 

+
~~
in
x
S

 

+
b
=
x
S

 

HoH
im 2
S
es
S

 

+
>
‘e
S

 

+H +
Co QR
~ @
oS
oo

 

+
on
~
er
S

[BAJoyut-2

°cV0'0
°1c0'0
Sc0'0
7000
°S00°0
°9¢0'°0
9100
6c0°0
vIO0
*170'0
°L90°0
°cC0'0
6c0°0
*1S0'0
¢c0'0
vIO0
*1100
°c 100
e100
*c100

HtHHHa AnH HH HHH oH OH HH oH on 4 oH

+

SOV
vc6'0
Lc60
cs3'0
666°0
£660
cel 0
0820
cOL'0
9960
9780
609°0
cs60
0S8°0
880
198°0
€89°0
c99°0
€89°0
899°0
0L9'0

Vd-wonsestg

*7c0'0
°cC0'0
°L0C 0
°c100
°700°0
°8¢c0°0

6100

€£0°0

vIO0
°cV0'0
°VL0°0
°€c0'0
°cc0'0
°LS0°0
°cC0'0
©9100
°c100
*V100

vIO0

cl0'0

so’
S60
S60
LSL’O
1660
£660
9CL'0
6LL'0
9SL'0
696°0
Svs'0
L190
cS6'0
LV3'0
9LL'0
6S8°0
189°0
199°0
789°0
1L9°0
cL9'0

H+tHHxann HHH HHH HH oH oH oH on 4H oH

+

yoreas-°>

 

 

 

 

Or’? SLY os's STE yuey

*820'0 F 7E6'0 0€0'0 * Sr6°0 *€90'°0 F 8rL'0 Z00'0 * 8960 Oz
*720'0 F £60 E100 F Lr6'0 *7S0'0 F 008°0 L00°0 * 856° 61
*0@7'0 F EPL'0 €90°0 * 8Z8'0 9900 F 9IL'0 000°0 + 9F6°0 81
°900'0 * L660 900°0 F 966'0 *S00'0 F 866'0 7000 * 666°0 LI
*700'0 * 766°0 *S00'0 F 7660 S00'0 F 7660 *700'0 F €66'0 OT
«L700 F SZL'0 v70'0 F PrL'0 «L700 F OIL'0 *720'0 F TrL'0 SI
6100 * 6LL'0 9100 F LLL'0 *€70'0 F LOL'0 9100 F 6LL'0 vl
“S600 F SSL'0 *0€0'0 F 8SL'0 *160'0 F OSL'0 8z0'0 * €9L'0 EI
r10'0 * 0L6°0 *610°0 F 9F6'0 €10'0 * L960 *€10'0 F 9960 ral
-970'0 F 980 *Tr0'0 F 9P8'0 *Tr0'0 F Sr8'0 *Tr0'0 F 980 II
*€90'0 * S79°0 *090°0 ¥ 979°0 *690'°0 * 7Z09°0 *6S0'0 F LE9'0 Or
*€70'0 F S560 *720'0 F 7560 *720'0 F S560 *720'0 F S60 6
*160'0 F 8r8'0 *860'°0 F I7Z8'0 ZEO'O F 0S8'°0 O00 * OS8'0 8
*8r0'0 F OLL'0 *€S0'0 F 7Z6L'0 *0S0'0 F 98L'0 *6r0'0 F 6820 L
*1Z0'0 F 0980 *0€0'0 F 8080 €7O'O F 798'0 °970'0 * 6S8°0 9
°910°0 F 789°0 *Z10'0 F OL9°0 *r10'0 F 189°0 E10'0 F €89'0 ¢
‘7100 F 199°0 “1100 F 7S9°0 *€10'0 F 799°0 *110'0 F €99°0 v
r10'0 * 789°0 *€10°0 F OL9'0 *Z10'0 F 189°0 *€10'0 F €89°0 €
r10'0 * 0L9°0 r100 + T1909 *r10'0 F 699°0 €10'0 * 1190 Z
ZI0'O * ZL9°0 “1100 F Lr9'0 Z10'0 * OL9°0 ‘7100 F 699°0 I
yo.leas-plip 'y-wonoasig yno-d UM al

OSIOU NOY vyep UO AdvINDOR Jo UOSTIedWIOD ¢€ ajqey

pringer

Y) §S
2265

Machine Learning (2020) 109:2247-2281

9010
9ET'0 +
Le0°0
cc0'0
vI00
L90°0
cv0'0
990°0
cc0'0
€L0°0
8010
Le0°0
TS0°0
6L0°0
1700
cc0'0
€c0'0
9¢0°0
6c0°0
vc0'0

+
i)
a4
oun

 

+
>
in
Q
S

 

+
Ae
eS
S
S

 

+
Se
x
Sy
S

 

+
on
0
a
S

 

+H sO
as
m we
4 2
So

 

+
st
CS
+t
S

 

+
wn
se
Sy
S

 

+
4
H
S
S

 

+
wn
NN
Q
S

 

+
xt
Q
Sy
S

 

+
~
~
&
So

 

+
a
Q
S
S

 

HoH
eS

Ne
=
S

 

+
Q
on
“
S

 

+H oH
~ OM
QNQ &
a 9
oo

 

+
>
st
”
S

[eAJoyUI-2

L60°0
LOTO
Lv0'0
*cV0'0
*8100
6S0°0
cv0'0
cLO'0
°1C0'0
°860°0
°col 0
°LV0'0
6S0°0
“VOTO
0S0°0
¢c0'0
°€c0'0
°LC0'0
9c0°0
°Sc0'0

H+tHHxann HHH HHH HH oH oH oH on 4H oH

+

SSE
TOTO
Tec0
€S0°0
986°0
€L60
9LE0
OSV'0
OLV'0
£c60
9790
S80
£06°0
869°0
vLS0
ccL 0
c9e0
vce0
y9e'0
90
Ore'0

Vd-uonoestg

 

 

06°

CIO + CIO
celO + 8E70
Le0'0 + LS0'0
°C6l'0 + $880
°C10'0 + 8460
“S00 + L9E0
cr0'0 + ISO
cL0'0 + 09F'°0
Tc0'0 + Te6'0
°960°0 * 0S9°0
*8IT'O0 + 6LT0
°6r0'0 + 968°0
°f90'°0 + 069°0
“OIT'O + 8rS°0
“700 + 8IL0
°6c0'0 + I19€°0
*v70'0 + ST7E0
L700 * S970
870'0 + 6¢E°0
v70'0 + SPE0
yorras-°>

09'¢
LOT'O * 6010
SctLO + Iv70
Le0'0 + LS0°0
*1S0'0 + 7L6°0
*L10°0 + SL6°0
*rS0'0 + S9E0
vv0'0 + TSr'0
cLO'0 + 6S7'°0
Te0'0 + S¢6'0
“1010 + 6179°0
“e710 + 98T'0
°6r0'0 + 7060
*£90'0 + £69°0
°L60°0 * 6£€$°0
*cr0'0 + 6IL'0
6c0'0 + 79C'0
*V70'0 + STEO
970'0 + 99¢°0
6c0'0 + 87£°0
€70'0 + Sre0

Yoreas-plip

cll O
*8r1 0
°0S0°0
°890°0
°cC0'0
°0L0°0
°cv0'0
°¢80°0
*7r00
°cOT 0
*SIT 0
°870°0
°1800
“TIT 0
°790°0
°7c0'0
°€c0'0
°L70'0
°8¢c0'0
°cC0'0

H+tHHxann HHH HHH HH oH oH oH on 4H oH

+

0c'9
OIT'O
800
€£0°0
096°0
6960
vre'0
Lev'0
SIV 0
LL8°0
vr9'0
8810
968°0
8c9°0
TLS°0
£09°0
cle 0
£6c 0
Oce'0
S0e'0
£60

' J-uonoasig

°8¢0°0
°090°0
1700
*7r0'°0
°1c0'0
6r0°0
°cv0'0
690°0
6c0'0
°660°0
°col 0
°970'°0
790°0
“1010
Sv0'0
9¢0°0
°$C0'0
*7c0'0
8c0'0
£c0'0

H+tHHxann HHH HHH HH oH oH oH on 4H oH

+

SI?
TS0'0
c9T0
€90°0
£860
896°0
cLe 0
8c7'0
9970
Lc6'0
vr9'0
1810
£060
869°0
OLS'0
LCL'0
s9e'0
sce 0
c9¢'0
sce 0
Ivce0

yno-d

°9¢0'°0
°OV1 0
°000°0
*cV0'0
°Z100
°190°0
°0V0'°0
°1L0°0
°0¢0°0
°001'0
eCIT 0
°L70'0

090°0
°660°0
°cS0'0
°9¢0'°0
°cC0'0
°6¢0°0
°8c0°0
°$c0'0

H+tHHxann HHH HHH HH oH oH oH on 4H oH

+

STs
700°0
LVI0
000°0
986°0
€L60
Sve 0
vryv'0
6rr'0
vc6'0
LV9'0
9610
c06'0
L69°0
VLS'0
LILO
9S¢°0
vee0
19¢°0
81¢0
6ce'0

UM

que’

GOWAN OM TN OO OH OS
ee aN

7a NOM TN OR DW

dI

OSIOU JNOYIAM Sjas vyep UO Adv.NDOV Ind Jo uosIedWIOD page

pringer

Y) S
Machine Learning (2020) 109:2247-2281

2266

cel
870°0 + €68°0
6£0°0 + 8060
8L0°0 + 8820
0€0°0 + 0€6°0
020°0 + ZP6°0
970°0 + 0FZ'0
v10°0 + 89Z°0
ve0'0 + OFZ
610'0 + 6£6°
vr0'0 + 9€8°0
970°0 + ZE9°0
S¢0'0 + 0€6°0
970°0 + 6€8°0
crv0'0 + T08"0
870°0 + ZE8"0
£100 + 699°0
O10°0 + €99°0
£100 + 6Z9°0
£10'0 + 9690
TLO'0 + €99°0

 

 

 

 

0
0

 

 

 

 

 

 

 

 

 

[BAroyur-2

OLY

°6L0°0 + 1780
°790'0 + S80
TS0'0 + v6L'0

*cv0'0 + V160
°170'0 + Tr6'0
°970'0 + OIL'0
«C100 + P9L'0
*1€0'0 + 9€L'0
020°0 + 9£6°0

*7S0'0 + 808°0
«£900 + LLS°0
*£S0'0 + £060
*7£0'0 + 6780
°0S0'0 + 9920
970'0 + L780

£10'0 + 999°0

“C100 + 669°0
°C 100 + pL9'0
TLO'0 + 7$9°0

*110°0 + 669°0

Vd-uonoestg

00'V

6€1'0 + Lv8°0

«L700 + LL8°0
°L61'0 + 989°0
L¢0°0 + 0760

*£70'0 + TV6'0
°970'0 + LILO
°610°0 + 79L'0
*¢70'0 + IZTZ0
810°0 + 6£6°0

«L€0°0 + 808°0
°1S0'0 + 009°0
°1S0'0 + 7060
°870'0 + 6780
°$S0'0 + 0920
°870'0 + L780
vl0'0 + $99°0

*Z10°0 + 669°0
£100 + 8L9'0

£10'0 + S90

°010°0 + 099°0

yorras-°>

Of V
°9€T'0 + LV8'0
°970'0 + S180
°C6I'0 + 789°0

0v0'0 + LT6°0
*¢70'0 + 1760
°870'0 + LILO
610°0 + Z9L°0
*cr0'0 + 6IL0
0700 + LE6'0
*c70'0 + O18'0
°7S0'0 + 9650
°0S0°0 + 0060
*870'0 + I€80
°SS0'0 + V9L'0
970'0 + 0£8°0
S100 + $99°0
*c10°0 + 669°0
£100 + 8L9'0
vl0'0 + 9S9'0
“1100 + 669°0

Yoreas-plip

00'S

870°0 + 98870

*7£0'0 + £680
9S0°0 + €£8°0

*c70'0 + VI60
°0Z0'0 + 6760
°L70'0 + £7L'0
«C100 + £9L'0
6c0'0 + Ivl'0

°610°0 + T160
*vr0'0 + 9180
°190°0 + 686°0
L700 + £680
°S£0'0 + 0080
*C70'0 + LOL'0
°S£0'0 + T8L'0
*1 100 + 8690
*c10°0 + S790
°910°0 + 7990
T10°0 + 7$9°0

“C100 + 7790

' J-uonoasig

OLS

*170'0 + £L9°0
°7S0'0 + 80L°0
°860'0 + 859°0
*cr0'0 + VI16'0
°9€0'0 + 7880
°9€0'0 + 689°0
°020'0 + PrL'0
°620'0 + IZZ'0
610'0 + 6€6°0
°SS0'0 + 608°0
°L90°0 + TLS‘0
°0S0'0 + 868°0
£¢£0'0 + 0€8°0
°670'0 + S9L'0
L700 + L780
*Z10°0 + £99°0
“100 + L690
*T10°0 + SL9'°0
*c10°0 + 0S9'0
*110°0 + 669°0

 

yno-d

S6C
100°0 + 0F6°0
L000 + 676°0
000°0 + 616°0

*cr0'0 + VI16'0
610'0 + 760
°970'0 + LZL'0
S100 + S9L'0
0£0'0 + PrL'0
0z0'0 + S60
°0S0°0 + T18°0
*bS0'0 + 7650
*£S0'0 + 7060
*££0'0 + 6780
«6600 + OLL'0
670'0 + 9780
S100 + L990
*c10°0 + 6690
«C100 + PL9'0
TLO'0 + ZS9°0
*110°0 + 099'°0

 

 

 

 

UM

que’

Go WAN mM TN OO OND OS
ee aN

7a NOM TN OR DW

dI

ISIOU %¢ YIIM Sjas vyep UO AovINDOR Jo UOSTIedUIOD g¢ ajqeL

pringer

Y) §S
2267

Machine Learning (2020) 109:2247-2281

cel
LLO'0 + T7T0
8010 + £eT0
9€0°0 + 700
Ivl'0 + 8S$°0
cL0°0 + 008°0
LS0'0 + $9¢°0
ve0'0 + 8cr'0
vL0'0 + CEP
Tv0°0 + 898°
COLO + 7E9°0
[60'0 + €6T0
vL0°0 + TS8°0
£S0'0 + 9Z9°0
$80°0 + Z6S°0
9S0°0 + €Z9°0
970°0 + SEE
610°0 + SZE"0
L700 + SSE"
870°0 + 667°
v70'0 + 97E0

 

 

 

 

 

0
0

 

 

 

 

 

 

 

 

0
0

 

[BAroyur-2

OL
690°0 + 00T'0
9L0°0 + S€T0
£v0'0 + E00
6910 + ZOS'0
9L0°0 + £80

*PS0'0 + SPE0
S¢0'0 + OCr'0
*1L00 + SIV'0
£700 + 1980
eCIT'0 + 696°0
*crl'0 + 9TT0
*vIT'0 + £6L0
¢90°0 + LS9'0
*COT'0 + 8750
£S0'0 + €$9°0
Sz0'0 + I¢e0
970'0 + LIE0
°970'0 + LvE0
v70°0 + 667°0
°1Z0'0 + 61€0

 

Vd-uonoestg

O8"e

°840°0 + 960°0
T80°0 + 8710

c£0'0 + 9400

6ST'0 + 60S°0

6L0°0 + £8Z°0

°0S0'0 + 6€£°0
L¢e0'0 + O7r'0

°¢L0°0 + L6E0
6£0°0 + L980

°£80°0 + 7950
°160°0 + O€T0
eCIT'0 + 0620
°SS0'0 + 9S9°0
*rITO+ CIS 0
°960'0 + 7$9'0
870°0 + 87£°0

£700 + 61€0

S70'0 + €S€0

L700 + O00

°610°0 + O7E0

yorras-°>

Sve
080°0 + £010
£80°0 + 8710
£c0'0 + Zr00
8910 + 80S°0
6L0°0 + 18Z°0

£600 + OVE'0
Le0'0 + Itv'0
°7L0'0 + P6E0
£r0'0 + 7980
°[60'0 + 695°0
°160°0 + PZI0
eOIT'O + S8Z0
©9600 + 6$9°0
“TI TO + C750
TS0'0 + 6S9°0
6700 + 67£°0
£700 + 61€°0
S700 + €S€°0
L700 + TOE'O
°770'0 + 8IE0

 

 

Yoreas-plip

O19
880°0 + COTO
960°0 + TET‘

*870'0 + $700
eCLT'0 + S6V'0
*00T'0 + $69°0
°790'0 + £0€°0
«L€0°0 + CIV'0
*¢L0°0 + L8€0
°S70'0 + 0080
°860°0 + 7850
eOETO+ FIT O
*VOT'0 + P9L'0
°LL0°0 + 7850
°S60'0 + T7S'0
*7L0'0 + ISS'0
*V70'0 + 8870
°f70'0 + LL70
*££0'0 + POCO
*£70'0 + TLT0
°S70'0 + 7870

' J-uonoasig

OSV

*f70'0 + £90°0
°1S0°0 + OOT'0
Tv0'0 + Lv0°0

6910 + 00S°0

°6L0°0 + 1790
°690'0 + SEE0
°070'0 + 0070
°690'0 + 70r'0
0r0'0 + 698°0

eLITO + CLS0
*cvl0 + CZI0
*801°0 + r8L'0
990°0 + 6$9°0

*00T'0 + 8760
9S0°0 + 7$9°0

700 + 67£°0

°970'0 + 9TE0
S700 + OSE0

£700 + 9670

°770'0 + 61E0

 

yno-d

O'S

*770'0 + S00°0
°£80°0 + 9700
°000°0 + 000°0
6910 + 8670

6L0°0 + 78L'0

°790'0 + O7E0
8£0°0 + 8170

°690'°0 + OI7'0
cv0'0 + 6$8°0

*801'0 + SLS°0
*61T'0 + 8010
eCITO + S6L'0
°990'0 + 7$9°0
“CTI TO + 9€6°0
°6S0'0 + 7S9°0
*7£0'0 + STEO
*V70'0 + 9TE0
*1€0'0 + Cre'0
°S70'0 + 887°0
°770'0 + O7EO

UM

que’

GOWAN mM TN OO OND CO
ee aN

7a NOM TN OR DW

dI

OSIOU %E YIM Sjos Beep uo Advinooe oind jo uostiedwioy 9g ajqey

pringer

Y) S
Machine Learning (2020) 109:2247-2281

2268

gol
6L0°0 + 9€8°0
L700 + 7880
080°0 + 76L'0
L¢e0'0 + $88°0
£100 + re6'0
870°0 + TCZ'0
L100 + PSZ'0
T£0'0 + 97L'0
7700 + 616°0
870°0 + 708°0
0S0°0 + S6S°0
v70'0 + TT6°0
cE0'0 + C78"0
870°0 + 0ZZ°0
S700 + S€8°0
£100 + T99°0
T10°0 + 9F9°0
9100 + €Z9°
vl0'0 + 1S9°
T10'0 + T99°0

 

 

 

 

 

 

 

 

 

 

oo

 

[BAroyur-2

coV

°980°0 + 99L°0
°790'0 + S780
°670'0 + £920
L¢e0'0 + $880

9100 + 7£6'0

°V70'0 + VOL'0
*L10°0 + 0SL'0
°C70'0 + LILO
£700 + F160

*£90'0 + 0920
°8S0'0 + S7S'0
°S£0'0 + 688°0
°0¢0'0 + £180
°$S0'0 + OVL'0
°870'0 + S780
“C100 + LS9'0
TLO'0 + €79°0

°L10°0 + 899°0
S100 + 879°0

*c10°0 + S90

Vd-uonoestg

OI’

6010 + 6180

°L70'0 + 698°0
°681'0 + 790
670'0 + 9L8°0

S100 + vE6'0

°0£0'0 + TOL'0
810°0 + OSZ0

*160'0 + TIL0
Tc0'0 + p16'0

°790'0 + S9L'0
°6L0°0 + SSS0
*7£0'0 + 888°0
S00 + 6080
°LS0'0 + 87Z'°0
°L70'0 + 7780
TLO'0 + 8$9°0

T10'0 + €79°0

*810°0 + 699°0
vl0'0 + TS9°0

*c10°0 + LS9°0

 

yorras-°>

SCV

OTTO + LT8°0
°070'0 + 0480
°681'0 + £90
6£0°0 + 888°0
S100 + €£6°0
°0£0'0 + TOL'0
8100 + 0SZ°0
°0€0'0 + TIL 0
ve0'0 + r16'0
°L90°0 + Z9L'0
°990'0 + 695°0
*7£0'0 + 9880
°c£0'0 + O18'0
°6S0'°0 + 87Z'0
«L700 + 7780
TLO'0 + 8$9°0
T10°0 + €79°0
*810°0 + 0L9°0
vl0'0 + 0S9°0
*c10°0 + 9690

 

Yoreas-plip

067
vs0'0 + €r8°0
870°0 + 0680
940°0 + €08°0
L¢e0'0 + $880
°600°0 + ST6'0
£700 + LIL'0
°L10°0 + 870
0£0'0 + TEL'0
«L700 + 0880
°£90'0 + 9920
°6S0°0 + 7SS°0
°6c0'0 + €S8°0
*¢£0'0 + L8L'0
°SS0'0 + vEL0
°9€0'0 + 9LL'0
“100 + S90
“100 + 7£9'0
°910°0 + 6690
£10'0 + 0S9°0
*V10°0 + 6£9'0

' J-uonoasig

OLS

°7v0'0 + 6790
°9€0'0 + $69°0
*7S0'0 + 0S9°0
£v0'0 + 0880
°S70'0 + 898°0
«L700 + 989°0
°770'0 + TEL'0
*1€0°0 + £OL'0
S700 + S160
°090'0 + 6SL'°0
°SS0'0 + 6€5°0
*1€0°0 + $880
°L70'0 + 7180
°8S0°0 + IPL'0
°970'0 + 7780
*V10°0 + 7S9'0
“100 + 7790
«L100 + 9990
*V10°0 + 9790
*c10°0 + 7S9'0

yno-d

SO'e
c00°0 + T76°0
800°0 + 7160
000°0 + 206°0
LE0'0 + $880
7100 + S€6°0
7700 + 6TL'°0
L100 + 0SL'0
970°0 + TEZ"0
*£70'0 + 7160
°790°0 + £9L'0
°6S0°0 + ILS'°0
*7£0'0 + 888°0
°620'0 + TI8'0
*1S0°0 + CrL'0
°870'0 + 7780
“1100 + L690
“100 + 1790
«L10°0 + 899'°0

£100 + 0S9'0
*c10°0 + 7S9'0

 

 

 

 

 

UM

que’

Oo WAN MOM TN OO f OH O&O
aN

7a NOM TN OR DW

dI

ISIOU %¢ YIIM Sjas vIep UO AdvINDOR Jo UOSTIeduIOD 7 ajqeL

pringer

Y) §S
2269

Machine Learning (2020) 109:2247-2281

cV'l
vS0'0 + 970°0
OTTO + S6T°0
S¢c0'0 + €50°0
8710 + 8070
Lv0'0 + 8SZ°0
090°0 + Ss¢"0
cv0'0 + 00F'0
990°0 + F6E°0
670°0 + ¥78°0
CIT'0 + SS°0
v60°0 + TIT0
vS0'0 + Z08°0
790°0 + @h9°0
L60°0 + PES"
670°0 + 029°0
L700 + 6TE0
7700 + T62°0
£¢£0'0 + SHE
870°0 + 067"
£700 + CCEO

 

 

 

 

 

 

 

 

 

 

 

 

 

oo

 

[BAroyur-2

09'¢

870'0 + 8S0°0

*VOT'0 + COTO
¢S0'0 + 0700

crl'0 + OIV'0

£S0'0 + 7SL'0

TS0'0 + 6£¢'0

Ov0'0 + £6£°0

cS0'0 + L8€°0

°0S0'0 + TI8°0
*8ET'0 + 9970
«LITO + 0S0'0
*£L0°0 + P9L'0
°090'0 + 7790
“CIT 0 + PLv'0
°860'0 + 679°0
°970'0 + TIE 0
v70'0 + L870

°Sc0'0 + SECO
0€0°0 + 987°0

*V70'0 + 80€°0

Vd-uonsestg

OI’

8S0°0 + 1700

°COT'O + OST'O
0£0°0 + LV0'0

SST'0 + T6E0

¢S0'0 + SSL'0

*1L0°0 + 6I€'0
vr0'0 + €6£°0

£90'0 + I8¢'0

990°0 + ST8°0

*6cT'0 + 8970
°960°0 + 6S0°0
°890'0 + £940
°7L0°0 + F190
*LIT0 + 9000
*7S0'0 + 7790
1200 + vIE0

TZ0'0 + 6870

°9€0'0 + 9EE0
870'0 + 067°0

*€70'0 + VIE 0

 

yoreas-°>

Sve

LS0°0 + €70°0

860°0 + 6ST0

c£0'0 + 670'0

Lvl'0 + TTv'0

vS0'0 + pSL'0

°990'0 + O70
vr0'0 + €6¢'0

090°0 + 780

TL0°0 + 9T8°0

°Crl'0 + 6970
°760'0 + 790°0
°890°0 + 6SL'°0
°L90°0 + L190
*6I1'0 + 6rr'0
*7S0'0 + £790
770'0 + VIE0

1700 + 6870

°Sc0'0 + LEEO
870°0 + 0670

*€70'0 + CTO

 

Yoreas-plip

SLs
790°0 + T90°0
7600 + T9T‘0
SS0'0 + £700
9vT'0 + SOv'0
*cr0'0 + $690
°990'0 + £0€0
*170'0 + S80

6L0°0 + 9LE0
°790°0 + 87L'0
*6c1'0 + PLv'0
«LITO + 6700
°890°0 + 7L9°0
°690'0 + 6SS°0
*SITO + ISv'0
°CL0°0 + 6£6°0
°820°0 + PL70
°S70'0 + 7S70
*7£0'0 + 00E0
°LZ0'0 + L970
°620°0 + SL70

 

' J-uonoosig

Sor

9€0°0 + 1S0°0

°£90'0 + 8010
£S0'0 + £700

TST‘0 + 7Ov'0

S600 + 866°0
°SS0'0 + IEe0
*cr0'0 + €LE0
£90'0 + 6L¢'0

£600 + 9180

*cEl0 + P90
“cI 10 + rr0'0
°F90'0 + 8SL°0
£600 + 1790
*LIT0 + 9LV'0
*£S0'0 + LV9'0
«L700 + ITE0
970°0 + 987°0

“£600 + TEE)
L700 + 887°0

*V70'0 + 60€°0

yno-d

OLY

«C100 + £00°0
°760'0 + 690°0
°000°0 + 000°0-
8710 + 8070
6r0'0 + 6SZ°0
«L600 + 90£'0
cv0'0 + 68£°0
790'0 + 16€°0
°7S0'0 + L080
“9CT'0 + ILV'0
*LZ1'0 + IS0'0
*1Z0°0 + 19L'0
°850'0 + 0790
*COT'0 + 8L7'0
«L600 + 8790
*£70'0 + SOE0
°970'0 + 6L7'0
°Sc0'0 + TEEO
*8Z0°0 + 9L7'0
*V70'0 + 80€°0

 

UM

que’

GOWAN OM TN OO OH OS
ee aN

7a NOM TN OR DW

dI

OSIOU %C YIM sjos veep uo Advndow ind Jo uostiedwioy gajqey

pringer

Y) S
2270 Machine Learning (2020) 109:2247-2281

  

< £

28 23

a 2 5 2

2 2

= = 6

o co

DE De

0 0

<5 MKIR @ 8.

SZ. EE p-cut 8 3

© 3 [il Bisection-F 1 © 3 [il Bisection-F 1

o %" [ij Grid-search o ® [i Grid-search

= [pg S2-search = [Eg S2-search
[}JBisection-PA [> yBisection-PA
[]+-interval (_]7-interval

0% noise 3% noise 5% noise 0% noise 3% noise 5% noise

(a) A (b) PA

Fig.5 Statistical comparison under different noise level. Each bar is the gap between the times of the sig-
nificant wins and the times of significant loses. The significant lose and win is defined according to Eq. (44)

7 Conclusion

With an increase in the complexity of the data, eliminating random consistency from learn-
ing algorithms has great potential to improve the generalization ability. In this paper, first,
we have shown that the PA is insensitive to the class distribution of classifiers in evaluation
and is more fairer than the A in learning classifiers through two vivid examples. Second,
we have given some novel bounds to show that learning by PA can approach to the optimal
A and have shown that the empirical risk minimization process of the PA is Bayes-risk
consistent. Based on these theoretical guarantees, we have proposed a plug-in rule model
that optimizes the PA. The experimental results have shown the fairness and effectiveness
of learning by PA. An interesting future work is to establish the other strategies to define
the random consistency. An analysis of the random consistency for each instance maybe a
promising direction.

Acknowledgements This work is supported by National Key R&D Program of China (No.
2018YFB1004300), the National Natural Science Foundation of China (Nos. 61672332, 61872226,
61976129), the Overseas Returnee Research Program of Shanxi Province (Nos. 2017023, 2018172), the
Natural Science Foundation of Shanxi Province (No. 201701D121052), and Program for the San Jin Young
Scholars of Shanxi.

Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-
mons licence, and indicate if changes were made. The images or other third party material in this article
are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the article’s Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.

Appendix: Proofs

Lemma 1 When the partitions in H™ are distributed uniforml , the expectation accurac
Pp y P y
of partitions in HW IS!

D) Springer
Machine Learning (2020) 109:2247-2281 2271

EyrenamA(h’) = pq(h) + (1 — p)(1 — qth). (45)
Proof Without loss of generality, we assume that g(h) < p. Assuming that the size of data
is N, we have
J CNah)-j
Np ~ N—Np

, (46)
Nq(h)
C Ny

J
P rena (rea = ‘) =

where j = 0,..., Ng(A) and C™ is the number of combinations of n items taken m at a time.
From (46), we know that N - TP(h’) follows the hypergeometric distribution with the size
of the population selected from be N, Np elements of the population belonging to one
group and N — Np belonging to the other group, and the number of samples drawn from
the population be Ngq(h). Thus,

E penal TP(h') = pqh). (47)
Then, according to TN(h’) = 1 — p-— (q(h) — TP(h' )), we have:

Ene qn Ah’) = EneHa 1 —p-—qh)+ 2TP(h’)

48
= 1 — p— ath) + 2pq(h). Oe)

oO

Example 2 Assume that two-class data are generated from two Gaussian distributions with
uncommon means /d,, f,, but a common covariance 2:

m(x|y = +1) = Mwy, 2), (49)

m(x|y = —1) = NM, 2) (50)

and the probability of the positive class is p = P(Y = +1). The label of the minority class
is corrupted by the instance-independent noise at the level s,;:; PY = —1|Y = +1) = s,. For
this learning task, the bias of hy 1S:

Bias(h;)
= |P(aX) < dl¥ = -1) — P(a%) > l¥ = +1)

- dy + A/2\_ dy - A/2
-|o( Va io VA )h 2)

where @(e) is the cumulative distribution function of the standard normal distribution,
A= (My — Hy)! 27 (Hy — Ma) and dy = In —

(51)

 

1-25,"

Proof According to Lemma 2, the corrupted conditional class probability PY = +1|X =x)
is needed. Based on the Bayes’ theorem:

P(Y = +1|X =x) (53)

Q) Springer
2272 Machine Learning (2020) 109:2247-2281

m(x|y = +1)P(Y = +1)

SE nee (54)
Dle{—141} m(x|y = DP(Y = 1)
Because P(Y = +1) = p(1 —s,), m(x|¥ = +1) = My, 2) and
m(xly = —1)
= Po mely=L¥=-DPW =/¥ =-1) (55)
le{+1,-1}
PY =4+1|Y =)P(Y =)
= » mxly = prs Fils DPus (56)
le{+1,-1} PY = -1)
(1 —p) PS)

(1 —p)+ps, Pa (1 —p)+ps, A (67)
where m(x|y = 1, = —1) = m(x|y = J) is satisfied because the label noise is independent
on instance: P(Y = -1|Y = 1,X =x) = P(Y = -1|Y = D, we have:

~ l—-s,
PY =4+1|X =x)= (58)

1 +exp(w!x + b)

with w? = (uy — m,)’ X~! and b= shy Sw —Inp- 5 Hy” S| y + In(1 — p).
Further, according to Lemma 2, the optimal classifier in the sense of accuracy is

mon={*h d(x) > do, (59)

—1, otherwise.

1
1-25,"

According to the additivity of the Gaussian distribution, we obtain the probability mass
function of d(x):

 

where d(x) = x’ X~ (fy — My) - 5(Hy + py)! X~ (fy — My) and dy = In +

m(d(x)|y = +1) = M(A/2, A), (60)

m(d(x)|y = —1) = N(—A/2, A) (61)
where A = (fy — Hy)’ X (My — Hy). Then
Bias(h*)
= P(dx) < dyl¥ = -1) — P(d(X) > dyl¥ = +1)|

dy + A/2 dy — A/2
= |(@| ——— ]}-1+4+#|( ——— ]]}, 63
( Va * ( Va ) )

where @(e) is the cumulative distribution function of the standard normal distribution.

(62)

O

Theorem 1 The classifier that maximizes the PA is

Q) Springer
Machine Learning (2020) 109:2247-2281 2273

hp, (x) = arg max PA(h) (64)

_ { +1, n(x) > (5 — p)PA* +p, 65)

—l, otherwise.

where PA* = PA(h,,) and p = P(Y = +1).

Proof The formulation of the pure accuracy measure is fractional, which hinders obtaining
the optimal classifier. Here, we resort to the cost-sensitive loss to obtain a non-closed-form
solution. We begin this proof with two existing definitions and two lemmas:

Definition 5 (Kotlowski and Dembczynski 2017) We refer to a measure as a linear-frac-
tional performance measure if it is non-increasing with FP, F'N and formalized as

ay +a, FP +a,FkN

W (FP, FN) = +> __,
( by + b| FP + b,FN

(66)
where dp, @), 47, 59, b,,b, € Rand by +b, FP +b,FN > C, > 0.

Definition 6 (Elkan 2001) The  cost-sensitive loss is defined as
L(h) = pFP(h) + (1 — p)FN(h), where p € (0, 1).

Lemma 7 (Kotlowski and Dembezynski 2017) The regret w.r.t the linear-fractional per-
formance measure '¥ (FP, FN) can be bounded by that w.r.t. L,(h) when p = Pina

Y* (b, +b,)—(a, +a,)
P* —P(h) < Ci(L,(h) - L)), (67)
where '¥* = max, ¥(h), L* = min, L,(h) and Cy = + (¥*(b; + by) — (a, + ay)).

Lemma 8 (Elkan 2001) The classifier that minimizes L, is

hi (®) = { +1, nG9) > p, (68)

1, otherwise.
where n(x) = P(Y = 1|X = x).

Because A = | — FN — FP, RA = 1 — p — q(h) + 2pq and g(h) = p + FP — FN, we have

_A-RA _ pU-p)-pFP-(1—p)FN

PA ; ,
1-RA  p(1 — p)+ (5 — p)(FP — FN)

(69)

According to Lemma 7, the regret of the PA can be bounded by that of L, with
p= (5 — p)PA* + p. Then by Lemma 8, we obtain the formulation. O

Lemma 3 For all distributions, the plug-in rule with p as the decision threshold

1
h,(x) _ { +1, yn(x)>p, where peEQ(0, 51. (70)

—1, otherwise,

satisfies:

Q) Springer
2274 Machine Learning (2020) 109:2247-2281

1-
Lh,) < re (71)
when p = 1/2, the equality holds.
Lh,)
(72)
= P(h,(X) = -1,¥ = +1 +P,(X) = 41 Y=-)
Proof
= Ex ncxy< pM + Ex:noxy>p(l — 1X) (73)
< Eye nxy<pA/e — Dn(X) + Ex: nay>,(1 -— nO) (74)
= Ey min{(1/p — 1)n(X), | — n(X)} (75)
< (1/p — DEy min{y(X), 1 — 4(X)}. (76)
L]

Loy 1 *
valves 4 For all distributions, suppose that p= P(Y = +1) < > the pure loss of h,
satisfies:

Fe
=0)-"=7)

Proof Let qi, = P(h’i, = +1), FP, = P(A = +1, Y = -1) and FNG = P(h, = -1, ¥ = +1).
By definition,

PL(h*) <

L*

PL(h’,) = ————_...

p+ (= 2a 78)
To obtain the upper bound of PL(h’,), we derive the lower bound of q7,. Because:
L*= Ey: xy<i/2MX) + Ex-naysiy2C — 1X) (79)
= Exn(X) — Ex snexys12@O + Ex -noysi 20 — 1X) (80)
= p — Ey max{2n(X) — 1,0}, (81)
and then, we have
g, = Exl{n(X) — 1/2 > 0} (82)
> Ey max{7(X) — 1/2, 0} (83)
1 *

=5P-L’), (84)

Q) Springer
Machine Learning (2020) 109:2247-2281 2275

where I{} is the indicator function. Putting the lower bound of q; into the formulation of
PL(h’,), we obtain the upper bound of PL(h;). oO

Theorem 3 For all distributions, suppose p < 7 the pure loss of h; satisfies:
PL(hp,) < PL(hy (85)
2(1 — p)
~ p(B — 2p) — L* — 2p)
Proof For any g(h), we have
1—-RA=p+(1—2p)qh) < 1—p, (87)

PL(h,,)- (86)

hence
L =(1-RA)PL < (1 —p)PL. (88)
Further amplifying the upper bound in Lemma 4:
L* < Ltt,) < 1 — p)PL(M,), (89)

we obtain the result. O

Lemma 5 For two random’ variables Z,,Z,€[0,1], any e¢€€(0,1], _ let
a = EZ,EZ,/(2EZ, + EZ,), we have
Z, EZ,

P/ — > :)
Z, EZ, (90)

< P(|Z, —EZ,| > ae ) + 3P(|Z, — EZ,| > ae).

 

 

Proof For f € [0, 1] and y > 0, we have
Z, EZ,

[Pp E
(Z-Elr4)

= — —— E

 

 

(91)

EZ, —Z,)EZ
21 _| 5 pe) +0(|_— ©

(Z, — EZ,)EZ, + (EZ,)2|~ (~p 1) (02)

 

 

< P(|Z, — EZ,| > BIEZ, — yele) + 2P(|Z, -— EZ,| > ye)

1 — B)EZ 93
+P (|Z, -EZ|> CORA EZ: ~ rele), @?)
EZ,
where the first inequality is obtained by
P(la + b| > €) < P(Jal > Be) + P([d| > A — Boe), (94)

and the second inequality is obtained by

Q) Springer
2276 Machine Learning (2020) 109:2247-2281
P(B,) = P(B,|B,)P(B,) + P(B,|B5)P(B5) (95)

< P(B,|B,) + P(B5), (96)

for any events B,,B,, where B) complementary set of B,. Here, we take the event
|Z, — EZ,| < ye as B, to divide the two terms of the first inequality.

Let
p =EZ,/(EZ, + EZ,), (97)
we have
BIEZ, — ye|
(99)
= (1 — P)EZ,|EZ, — ye|/IEZ,
= /. (100)
Finally, by the assumption € < 1, we have y > a and then get the result. oO

Theorem 4 Suppose the cardinality of H is finite: |H| < 00, then for every h € H, any
Ee € (0, 1], we have

2
P{ sup PLy(h) - PLN > e} < 8|H| exp {-29(« - a) \ (101)
heH

where @ = MiNpey pupal and Rc(H) is the Rademacher complexity of H.

 

 

Proof First, we process the superior limit in probability according to the union bound:
P { sup

heH

= Pf 3h EH: Pi yim) - PLN > e}

<> P| Piyim - PL(h) > e}

heH

 

PLy(h) - PLN > e}
(102)

< |H| sup P| Pry - PL(N) > c
heH

Second, we transform the gap in the sense of PL into that of L by Lemma 5. For every
h EH, let a = mine, —“~, we have:
° hEH Pr(h)+1" ,

P| Pi yim - a) > e}

<Pf

(103)

 

iy -1| > ce} +30 [Ray — Ral > ce}.

Q) Springer
Machine Learning (2020) 109:2247-2281 2277

Third, applying Theorem 8 in Bartlett and Mendelson (2003), for every h € H, with
probability at least 1 — 6/4, we obtain that:

 

 

 

A Rc(H) In(8/6)
Ly —L| < , 104
N | sy 2N (10)
Let 6 = 8 exp {-2N (ae — Rc(H)/ 2)°}, and then:
P{ ley - 2] > ae } < 5/4. (105)

For the second term in (103), by |H@™| = ce and the triangle inequality, we have:
ue

yi) - RA()| < a sa > (106)

Ly (hj)

 

      

According to Theorem 8 in Bartlett and Mendelson (2003), for every function
h, € H, with probability at least 1 -— 6/4, holds that:

Re(H*™) 1 In(8/6)

107
5 aN” (107)

ILy(h,) - Lth)| <

because H!” CH, we have Re(H") < Rc(H), and then for every function h,€ Hm,
with probability at least 1 — 6/4, holds that:

 

 

yc - RAC < << + ar (108)

Putting 6 into inequality (108), we obtain for every h € H:
P{ |RAy(h) — RA()| > ae} < 5/4. (109)
Thus, combining (102), (103), (105)and (109), we obtain the final result. O

Lemma 6 Let S\ = = {XY oe (Xiys Vy) be an independent and identically distributed
collection as Sy and PL,(h) is the corresponding empirical pure loss. Suppose
N > 5(6 + 4ae)aq7e7?, where a= E € (0, 1], then we have

P { sup
heH

< 2 sup sup

L
min —_
hEH opPr(h)+1°

>e}

PLy(h) — PLy(h)| > <I.

PLy(h) — PL(A)

 

 

(110)

 

Proof There exists at least one function hy) € H satisfies JPL (Io) — PL(I)| > €. For ho,
P| sup |PLy(h) PL yh) >£ \
heH 2
= |Sy \ ,

>E. II
= s, (sep 2

 

(111)

 

PLy(h) - PLN > :) P| PL) ~ PL(ho)| <

 

 

Q) Springer
2278 Machine Learning (2020) 109:2247-2281

Here, we omit the detail proof of this inequality because the technique is the same as
Lemma 2 in Vapnik and Chervonenkis (1971) on accuracy.
: _— mj L(h) .
According to Lemma 5, let a = miny,cy SPLAT” we have:

sy}

aE
> —

. sy} + apf RAC) — RA(y)

P| Pr) — PL(ho)

<Pf

For the first term of (112), according to the Bernstein’s inequality, we have
P| sy}

a’e?N
< 2exp4—

 

—
2

 

Al (Io) — A(Ig)

sy

 

 

 

 

QE
>—
2

 

(112)

aE
> —

Al (Io) — A(Ig) 5

 

 

 

4
2(Alo (1 — A(hg)) + = ) (113)

3a767N
S Zexp = \

) 1
c2{ 1+ 20e | <5.

where the second inequality is because for any p € [0, 1], it is satisfied that p(1 — p) < 1/4,
the third inequality is obtained by e* <(1+.x)7! for x >0 and the last inequality is
obtained by the assumption N > 5(6 + 4ae)a~*e~?.

For the second term of (112), by the definition of RA, (Ig); the only difference in the
proof of the two terms in (112) is the number of terms for summation. Under the assump-
tion on N, we have NCy > 5(6+ 4ae)a-*e~*, and then using the same technique as
(113), we have

 

 

aE
> —

P RA) - RA(h)| > 5

sy < = (114)

 

 

Combing the inequalities (112), (113), (114), we have

P| PLC) — PL(Iy)

 

a
2

 

1

sy} <x. (115)
2

Thus, according to (111) and (115), we obtain the final result. O

Theorem 5 As the same condition as Lemma 6 and suppose the VC dimension of H is
finite: d,.(H) < 00, we have
> e}

{sup
heH
2 _ ~ 2

< 4(N + Iyexp { (OE eee wh

PLy(h) — PL(A)

 

 

(116)

Proof By Lemma 6,

Q) Springer
Machine Learning (2020) 109:2247-2281 2279

 

P| sup PLy(A) — PLN > e}
heH
(117)
< 2p sup |PLy(h) — PL yh) >= \
heH 2

 

We divide the hypothesis space 71 into N + | subspaces according to the class distribu-
tion of hypothesis function: H = Us, eto, Lowy HN, where
Hi = {hs — Di, Wh(X)) = +1] = Gy. h € H}

. Thus, according to the definition of pure loss and the union bound, we have

 

 

{sup PLy(h) PLY) > <I
heH 2
= P< sup sup PLy(h) — PL yh) —
Gn heHan 2
(118)
<(N+1)supP4 sup |PLy(hA)— PLY) >=
an heHan 2

 

Ly(h) —L iI) > cee |

We employ Theorem 3.1 in Vapnik and Chervonenkis (1971) for the error terms.
Besides, for any Gy, it satisfies that 1 — RAy > > ae and d, (HA) <d,(H) for
H% CH. Then we have:

=(N+1)supP< sup
an heHin

 

 

vw A“, Ray)
supP< sup |Ly(h) — Ly (h)}| > —————
dn heHin
211 — RA.) = d. (H4®)[InQeN /d,. (Ha
al (cut tone aig
Gn 4 N
*(1 = [2p = HYP In(2
< exp { (SSE _ eet
16 N
Combining (117), (118) and (119), we obtain the final result. O
References

Agarwal, S., Graepel, T., Herbrich, R., Har-Peled, S., & Roth, D. (2005a). Generalization bounds for the
area under the ROC curve. Journal of Machine Learning Research, 6(2), 393-425.

Agarwal, S., Harpeled, S., & Roth, D. (2005b). A uniform convergence bound for the area under the ROC
curve. In Proceedings of the international conference on artificial intelligence and statistics (pp. 1-8).

Albatineh, A. N., & Niewiadomska-Bugaj, M. (2011). Correcting Jaccard and other similarity indices for
chance agreement in cluster analysis. Advances in Data Analysis and Classification, 5(3), 179-200.

Albatineh, A. N., Niewiadomskabugaj, M., & Mihalko, D. (2006). On similarity indices and correction for
chance agreement. Journal of Classification, 23(2), 301-313.

Q) Springer
2280 Machine Learning (2020) 109:2247-2281

Alcalafdez, J., Sanchez, L., Garcia, S., Jesus, M. J. D., Ventura, S., Garrell, J. M., et al. (2008). KEEL:
A software tool to assess evolutionary algorithms for data mining problems. Soft Computing, 13(3),
307-318.

Bartlett, P. L., & Mendelson, S. (2003). Rademacher and Gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(3), 463-482.

Bartlett, P. L., Jordan, M. I., & Mcauliffe, J. (2006). Convexity, classification, and risk bounds. Journal of
the American Statistical Association, 101(473), 138-156.

Blair, E., & Stanley, F. J. (2008). Interobserver agreement in the classification of cerebral palsy. Develop-
mental Medicine & Child Neurology, 27(5), 615-622.

Budescu, D., & Bar-Hillel, M. (1993). To guess or not to guess: A decision-theoretic view of formula scor-
ing. Journal of Educational Measurement, 30(4), 277-291.

Cameron, M. L., Briggs, K. K., & Steadman, J. R. (2003). Reproducibility and reliability of the outerbridge
classification for grading chondral lesions of the knee arthroscopically. American Journal of Sports
Medicine, 31(1), 83-86.

Chong, E. K. P., & Zak, S. H. (2011). An introduction to optimization (3rd ed.). New York: Wiley.

Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measure-
ment, 20(1), 37-46.

Devroye, L., Gy6rfi, L., & Lugosi, G. (1996). A probabilistic theory of pattern recognition. New York:
Springer.

Diamond, J., & Evans, W. (1973). The correction for guessing. Review of Educational Research, 43(2),
181-191.

Dua, D., & Graff, C. (2017). UCI machine learning repository. Retrieved May 26, 2018, from http://archi
ve.ics.uci.edu/ml.

Elkan, C. (2001). The foundations of cost-sensitive learning. Proceedings of the International Joint Confer-
ence on Artificial Intelligence, 17, 973-978.

Espinosa, M. P., & Gardeazabal, J. (2010). Optimal correction for guessing in multiple-choice tests. Journal
of Mathematical psychology, 54(5), 415-425.

Ferri, C., Hernandezorallo, J., & Modroiu, R. (2009). An experimental comparison of performance meas-
ures for classification. Pattern Recognition Letters, 30(1), 27-38.

Gao, W., Wang, L., Jin, R., Zhu, S., & Zhou, Z. (2016). One-pass AUC optimization. Artificial Intelligence,
236, 1-29.

Ghahramani, Z. (2015). Probabilistic machine learning and artificial intelligence. Nature, 521(7553),
452-459.

Goodman, L. A., & Kruskal, W. H. (1963). Measures of association for cross classifications. Publications of
the American Statistical Association, 49(268), 732-764.

Hazan, T., Keshet, J., & Mcallester, D. A. (2010). Direct loss minimization for structured prediction. In Pro-
ceedings of the advances in neural information processing systems (pp. 1594-1602).

He, H., & Garcia, E. (2009). Learning from imbalanced data. IEEE Transactions on Knowledge and Data
Engineering, 21(9), 1263-1284.

Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of Classification, 2(1), 193-218.

Ingo, S. (2005). Consistency of support vector machines and other regularized kernel classifiers. JEEE
Transactions on Information Theory, 51(1), 128-142.

Joachims, T. (2005). A support vector method for multivariate performance measures. In Proceedings of the
International Conference on Machine learning (pp. 377-384).

Kotlowski, W., & Dembczynski, K. (2017). Surrogate regret bounds for generalized classification perfor-
mance metrics. Machine Learning, 106(4), 549-572.

Koyejo, O.O., Natarajan, N., Ravikumar, P.K., & Dhillon, I. S. (2014). Consistent binary classification with
generalized performance metrics. In Proceedings of the advances in neural information processing
systems (pp. 2744-2752).

Kuncheva, L. I. (2013). A bound on kappa-error diagrams for analysis of classifier ensembles. JEEE Trans-
actions on Knowledge and Data Engineering, 25(3), 494-501.

Li, F., Qian, Y., Wang, J., & Liang, J. (2016). Multigranulation information fusion: A dempster-shafer evi-
dence theory-based clustering ensemble method. /nformation Sciences, 378(1), 58-63.

Li, F., Qian, Y., Wang, J., Dang, C., & Liu, B. (2018). Cluster’s quality evaluation and selective clustering
ensemble. ACM Transactions on Knowledge Discovery from Data, 12(5), 60.

Li, F., Qian, Y., Wang, J., Dang, C., & Jing, L. (2019). Clustering ensemble based on sample’s stability.
Artificial Intelligence, 273, 37-55.

Margineantu, D. D., & Dietterich, T. G. (1997). Pruning adaptive boosting. In Proceedings of the fourteenth
international conference on machine learning (pp. 211-218).

D) Springer
Machine Learning (2020) 109:2247-2281 2281

Martinezmunoz, G., & Suarez, A. (2006). Pruning in ordered bagging ensembles. In /nternational confer-
ence on machine learning (pp. 609-616).

Menon, A. K., Narasimhan, H., Agarwal, S., & Chawla, S. (2013). On the statistical consistency of algo-
rithms for binary classification under class imbalance. In Proceedings of the international conference
on machine learning (pp. 603-611).

Musicant, D. R., Kumar, V., & Ozgur, A. (2003). Optimizing F-measure with support vector machines. In
Proceedings of the Florida AI Research Society (pp. 356-360).

Narasimhan, H., & Agarwal, S. (2013). A new support vector method for optimizing partial AUC based on a
tight convex upper bound. In Proceedings of the conference on knowledge discovery and data mining.

Narasimhan, H., Vaish, R., & Agarwal, S. (2014). On the statistical consistency of plug-in classifiers for
non-decomposable performance measures. In Advances in neural information processing systems (pp.
1493-1501).

Narasimhan, H., Ramaswamy, H. G., Saha, A., & Agarwal, S. (2015). Consistent multiclass algorithms for
complex performance measures. In Proceedings of the international conference on machine learning
(pp. 2398-2407).

Qian, Y., Li, F., Liang, J., Liu, B., & Dang, C. (2016). Space structure and clustering of categorical data.
IEEE Transactions on Neural Networks and Learning Systems, 27(10), 2047-2059.

Sabers, D. L., & Feldt, L. S. (1968). An empirical study of the effect of the correction for chance success
on the reliability and validity of an aptitude test. Journal of Educational Measurement, 5(3), 251-258.

Sanyal, A., Kumar, P., Kar, P., Chawla, S., & Sebastiani, F. (2018). Optimizing non-decomposable measures
with deep networks. Machine Learning, 107, 1597-1620.

Scott, W. A. (1955). Reliability of content analysis: The case of nominal scale coding. Public Opinion
Quarterly, 19(3), 321-325.

Song, Y., Schwing, A. G., Zemel, R. S., & Urtasun, R. (2016). Training deep neural networks via direct loss
minimization. In Proceedings of the International Conference on Machine learning (pp. 2169-2177).

Valiant, L. G. (1984). A theory of the learnable. Communications of ACM, 27(11), 1134-1142.

Vapnik, V., & Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies of events to
their probabilities. Theory of Probability and Its Applications, 16(2), 264-280.

Vieira, S. M., Kaymak, U., & Sousa, J. (2010). Cohen’s kappa coefficient as a performance measure for fea-
ture selection. In Proceedings of the international conference on fuzzy systems (pp. 1-8).

Vinh, N.X., Epps, J., & Bailey, J. (2009). Information theoretic measures for clusterings comparison: is a
correction for chance necessary? In Proceedings of the international conference on machine learning
(pp. 1073-1080).

Vinh, N. X., Epps, J., & Bailey, J. (2010). Information theoretic measures for clusterings comparison: Vari-
ants, properties, normalization and correction for chance. Journal of Machine Learning Research, 11,
2837-2854.

Waegeman, W., Dembczynki, K., Jachnik, A., Cheng, W., & Hiillermeier, E. (2014). On the Bayes-optimal-
ity of F-measure maximizers. Journal of Machine Learning Research, 15(1), 3333-3388.

Wu, Q., Laet, T.D., & Janssen, R. (2017). Elimination scoring versus correction for guessing: A simulation
study. In Proceedings of the meeting of the psychometric society.

Zhang, T. (2003). Statistical behavior and consistency of classification methods based on convex risk mini-
mization. Annals of Statistics, 32(1), 56-134.

Zhao, M., Edakunni, N. U., Pocock, A. C., & Brown, G. (2013). Beyond Fano’s inequality: Bounds on the
optimal F-score, BER, and cost-sensitive risk and their implications. Journal of Machine Learning
Research, 14(1), 1033-1090.

Zhou, Z., Wu, J., & Tang, W. (2002). Ensembling neural networks: Many could be better than all. Artificial
Intelligence, 137, 239-263.

Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.

D) Springer
