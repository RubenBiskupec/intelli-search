Amer and Abdalla J Big Data (2020) 7:74 . .
https://doi.org/10.1 186/s40537-020-00344-3 oO Jou ral of Big Data

RESEARCH Oy oT-Ta waa -55 4

a ®
A set theory based similarity measure crea

for text clustering and classification

Ali A. Amer! ® and Hassan I. Abdalla?

 

*Correspondence:
aliaaa2004@yahoo.com Abstract

‘Computer Science Similarity measures have long been utilized in information retrieval and machine learn-

Department, Taiz University, . . . . . . . .

Taiz, Yemen ing domains for multi-purposes including text retrieval, text clustering, text summari-

Full list of author information zation, plagiarism detection, and several other text-processing applications. However,

avaiable at the end of the the problem with these measures is that, until recently, there has never been one
single measure recorded to be highly effective and efficient at the same time. Thus,
the quest for an efficient and effective similarity measure is still an open-ended chal-
lenge. This study, in consequence, introduces a new highly-effective and time-efficient
similarity measure for text clustering and classification. Furthermore, the study aims to
provide a comprehensive scrutinization for seven of the most widely used similarity
measures, mainly concerning their effectiveness and efficiency. Using the K-nearest
neighbor algorithm (KNN) for classification, the K-means algorithm for clustering, and
the bag of word (BoW) model for feature selection, all similarity measures are carefully
examined in detail. The experimental evaluation has been made on two of the most
popular datasets, namely, Reuters-21 and Web-kB. The obtained results confirm that
the proposed set theory-based similarity measure (STB-SM), as a pre-eminent measure,
outweighs all state-of-art measures significantly with regards to both effectiveness and
efficiency.

Keywords: Information retrieval, Text retrieval, Text classification, Similarity measures,
Empirical study

 

Introduction

In information retrieval and machine learning, a good number of techniques utilize the
similarity/distance measures to perform many different tasks [1]. Clustering and classi-
fication are the most widely-used techniques for the task of knowledge discovery within
the scientific fields [2-10]. On the other hand, text classification and clustering have
long been vital research areas of information retrieval (IR). While text classification is
the process of classifying the text/document into its actual class by utilizing a similar-
ity measure and a proper classifier. The clustering, on the other hand, is the process of
grouping similar texts into similar groups called clusters. As a matter of fact, with the
ever-piling amount of data and information on the internet, the necessity for a highly
effective classification algorithm is urgent. Nevertheless, the enhancement of classifica-
tion performance has still been the main task for researchers in the text mining field.

. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
GO) Springer O pen adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
— the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material
in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco

mmons.org/licenses/by/4.0/.
Amer and Abdalla J Big Data (2020) 7:74 Page 2 of 43

Given the fact that the similarity/distance measures are the core component of the clas-
sification and clustering algorithm, their efficiency and effectiveness directly impact
techniques’ performance in one way or another. Therefore, the selection of the best simi-
larity measure for the techniques in question is still an open-ended challenging task.

Even though there have been several proposed works in IR literature to compare the
similarity/distance measures for clustering and classification purposes [2, 3, 11-16],
these studies are still incapable of providing a comprehensive preview of the actual per-
formance of similarity measures. Besides, some of those works have presented an effi-
cient similarity measure while ignoring effectiveness [17, 21]. While the others have
presented only an effective similarity measure while ignoring efficiency [2—4]. Conse-
quently, this work comes to cover this critical limitation by introducing a compromised
(effective and time-efficient) similarity measure while the most widely used similar-
ity measures are elegantly investigated in a thorough pattern under numerous circum-
stances. Using the K-nearest neighbor classifier (K NN), K-means clustering algorithm,
and the bag of words (BoW) representation model [17-19] for feature selection, the
similarity measures are examined in details. The K values (in KNN) is varied from (1) to
(120) and the number of features is set to be in (50, 100, 300, 1000, 3000, 6000, and the
whole number of features of the considered dataset). In doing so, the superiority of STB-
SM measure is emphasized, and each measure is tested under several circumstances
so the desired effectiveness including accuracy is being obtained in certain K values
on several features. These measures are evaluated against low dimensional datasets (by
studying their performance on 50, 100, 200, and 350) and high dimensional datasets (by
studying their performance on 3000, 6000, and the number of all features of the dataset).
The measures’ behavior has been analyzed to determine which measure gives the best
results in certain K values on a specific number of features. Furthermore, for the cluster-
ing performance analysis, five evaluation metrics were employed with two of them are
internal and three are external. The key objective of this work is to present a new com-
petitive measure, compare and benchmark the similarity measures performance on the
targeted datasets on both the low and the high-dimensional datasets. Briefly, the main
contributions of this work are listed below:

1. Introducing a novel similarity measure for text retrieval that basically behaves based
on the set theory mechanism. This measure has been named a set theory based
similarity measure for text retrieval (STB-SM). In accordance with the experimental
results of both classification and clustering, STB-SM has been shown to be a promis-
ing measure with its being superior over the existing state-of-the-art measures.

2. Along with proposing the STB-SM, seven similarity measures, that are com-
monly applied for text retrieval and machine learning purposes, are thoroughly
investigated and evaluated to benchmark their impact on text retrieval. They are
comprehensively tested on two of the most publicly available datasets (namely,
web-KB and Reuters-21). Using BoW, a thorough comparative analysis for these
measures, in terms of their effectiveness and efficiency, are drawn. While the
classification effectiveness includes six evaluation factors, namely; accuracy, pre-
cision (PRE), recall (REC), F-Measure (FM), G-Measure (GM) and Average Pre-
cision Mean (AMP). The clustering effectiveness includes five evaluation met-
rics namely, Purity, Completeness and Rand Index as the external metrics, along
Amer and Abdalla J Big Data (2020) 7:74 Page 3 of 43

with Calinski-Harabasz index and Davies-Bouldin index as the internal metrics.
Moreover, for both classification and clustering efficiency, the run time, taken by
each measure to find the similarity degree, is rigorously observed.

3. The scope of this work concentrates on promoting the performance of text clus-
tering and classification through a new measure along with a detailed compara-
tive analysis for the proposed measure against the state-of-art BoW-based simi-
larity measures. The drawn analyses would provide an influential guide for the
selection of similarity measures in terms of considered datasets as well as helping
researchers in fully understanding the present and future challenges linked with

text retrieval.

The rest of this paper is structured as follows: the most relevant similarity measures
for this study are concisely presented in Sect. “Related work”. Section “The set theory”
briefly describes the basics and definitions of set theory in the context of text retrieval.
Section “The proposed similarity measure (STB-SM)” defines, formulates, and analyzes
the proposed similarity measure in the context of the set theory. The experimental setup
is drawn in Sect. “Experimental setup”. The results of the work are given in Sect. “Experi-
mental results” The discussion is profoundly detailed in Sect. “Discussion” Finally, con-
clusions and future work recommendations are presented in Sect. “Conclusions and

future work”

Related work

Vector Space Model (VSM) has long been used to represent document(s) when dealing
with text retrieval. In VSM, each document is drawn as an N-dimensional vector. Each
dimension represents a vocabulary term/feature. In information retrieval (IR) literature,
there are a good number of similarity measures to compute the pairwise document simi-
larity using VSM. While there have been some works that have been proposed in the
IR literature to perform the clustering along with the classification using the similarity/
distance measures [2—4, 11-16]. These works lack the comprehensive preview of the
actual performance of similarity measures. Moreover, some of them have proposed eff-
cient similarity measures irrespective of their effectiveness [21, 22]. Other works, how-
ever, have presented only effective similarity measures without consideration to their
efficiency [2-4].

Euclidean and Manhattan distances are among the most famous geometric measures
which have been utilized to find the distance between each vector pair [2, 20]. Similarly,
Cosine similarity finds similarity between each document pair using the angle between
their vectors [10]. The triangle distance is also looked at as the Cosine of a triangle
between vector pair [10]. The value of this measure range between 0 and 2. On the other
hand, for 0-1 vectors, the Hamming distance [4] is used to give the number of positions
at which the feature weights are not equal. Kullback—Leibler divergences [23, 24], KLD,
as a non-symmetric measure was used in [24] to compute the similarity between each
vector pair using the probability distribution that is associated with the both vectors.
In [4], a similarity measure for text processing, named SMTP, was found to calculate
the similarity between document pair. An Information-Theoretic measure (IT-Sim), was
proposed based on information theory in [18] for document Similarity purposes. In [3],
Amer and Abdalla J Big Data (2020) 7:74 Page 4 of 43

a new similarity measure called Improved Sqrt-Cosine (ISC) was proposed. Meanwhile,
Bhattacharya coefficient was invented in [21] to approximately calculate the overlap rate
between each statistical sample pair. Jaccard coefficient was developed in [25] to find
similarity using the ratio of the number of features existing in both documents to the
number of features existing in at least one of them. Subsequently in [2], a new similar-
ity measures named pairwise document similarity measure based on present term set
(PDSM), was presented based on the feature weights as well as the number of features
that existed in at least one of the considered documents.

Some of these measures have shown to be highly effective such as the PDSM [2], the
ISC [3], and the SMTP [4], yet unfortunately time-inefficient. In contrast, some meas-
ures are not effective yet highly efficient notably the Euclidean and Manhattan. Cosine,
on the other hand, has been seen as a compromised solution as an effective and highly
efficient measure. Furthermore, as reported in IR literature, almost all of these measures
were tested in the context of text classification and clustering. For example, PDSM was
compared in [2] with five similarity measures in terms of classification and near dupli-
cate application. Likewise, ISC [3] and SMTP [4] were evaluated against several similar-
ity measures concerning text classification and clustering. Similarly, our proposed paper
of this work has been evaluated against some of the most widely used similarity meas-
ures in machine learning and information retrieval literature, particularly with respect
to text classification and clustering. Finally, [7] assessing the clustering performance of
several measures on three collections of web documents. The experimental results of
their experiment revealed that Cosine similarity outweighs both the Jaccard coefficient

and the Euclidean distance.

The most relevant similarity measures

In this sub-section, the similarity measures that are considered to conduct this study are
presented. Seven similarity measures are introduced as the most widely used measures
for text clustering and classification [2, 20—24]. These similarity measures work by con-
sidering the terms’ presence and absence, or by evaluating the angle between each vector
pairs or by finding the distance. Assuming that we have two documents docl and doc2
that have two vectors d1 and d2, the aim is to find how much similarities are there when

using the intended similarity measure as follows;

Euclidean distance (ED)
Every document is drawn as a point in 2D space depending on the term frequency of N
terms that would represent the N dimension. ED finds the similarity between each point

pair in N-dimensional space using their coordinate based on the following equation:

Deyc(docl, doc2) = Ss” (doc\,—docj2)% + (docz,—doc22)2 +... (docy1—docy2)”
(1)

Manhattan
Manhattan distance (known as sum-norm) finds the sum of absolute differences between
the targeted coordinates of each document pair vectors as follows:
Amer and Abdalla J Big Data (2020) 7:74 Page 5 of 43

N
Manhattan — distance(docl, doc2) = Ss” \doclyi — doc2y2| (2)
i=1

Cosine similarity measure

The Cosine similarity calculates the pairwise similarity between the document pairs
using the dot product and the magnitude of both vectors of both documents. It is mostly
utilized within the scientific fields including the IR field [20], and is defined as follows:

So j-1 (doci1 * dociz)

(3)
\/ 1 doc;, * \/ 1 doc;

The union is used to normalize the inner product.

SiMCos(doc1, doc2) =

Jaccard similarity measure

This coefficient was invented in [25] to divide the intersection of the points by their
unions, and the value of coefficient ranges between 0 (there is no similarity between the
documents) and 1 (both documents are identical). The Jaccard similarity is given by the

next equation:

docl \ doc2

SiMjaccard (doc1, doc2) = docl\ Jdoc2 (4)

Bhattacharya coefficient

The Bhattacharyya coefficient is used to approximately calculate the overlap rate
between each statistical sample pair [21]. In our works, however, these samples are
thought of as documents. This coefficient is being utilized to find the approximate close-
ness of each document pair.

Simphatta(doc1, doc2) = 1 — log (S~ \ Vdocisxdocin) (5)

i=

Kullback-Leibler divergence

It is also known as a “relative entropy” [23, 24]. It is used to measure the difference
between probability distributions. Simply, when this measure reaches 0, it signals that
the intended distributions pair is identical, following that, its equation is then drawn as

 

follow;
- doc;
Sima och doc) = 9 Aden) (a) (6)
PDSM

This measure has been introduced in [2] to tackle the limitation of the-state-of-art meas-
ures which included a number of present terms into account. PDSM was seen effective
Amer and Abdalla J Big Data (2020) 7:74 Page 6 of 43

according to the experimental results of [2] as well as the experimental results of our
current work. The PDSM equation is formulated as follows

docj NM docj2 PF (doc; docj2)

D docl, doc2) = © AE (dae, doen) Ld
pds ( ) docj,Udocjg + M — AF (docj,docj2) +1

(7)

where

doc; M docj2 = min(doc;, doc;7)

doc; Udoc;, = max(doc;, doc;2)

where PF (doc; docj2) represents the number of present terms and AF (doc;\docj2) repre-

sents the number of absent terms and M is the total number of documents.

The set theory

Before introducing the proposed measure, some basics and definitions (upon which our
measure behaves) for the set theory in the context of text retrieval should be conceived.
So, in this section, the main objective is to introduce the relative set theory operations
upon which our proposed measure behaves.

Generally speaking, the set theory is a vital component of modern mathematics and
is widely used in all formal descriptions. The set can be a collection, a group, or even a
cluster of points that are named members of that set. For instance, a set of documents is
a collection of documents, or a set of people is a group of people, etc. For each point to
be a member of that set, its membership shall be defined clearly. However, sometimes,
due to the lack of information, membership definition is a difficult task and may even
be a vague. So, if the membership definition is vague for some collection, the collection
is then cannot be called a set. Simply put, if there has been a set S and its two members
X and Y, then it shall not be unknown whether X=Y or they are not. Strictly speaking,
the set can be either finite, infinite, or empty. In the following, some basic definitions
and key operations are introduced to further understand the basics upon which STB-SM

measure behaves.

Definition 1 If we have two sets S1 and 82, both sets are equal if and only if they
have the same points, and then every X € S1 & X € S2. For example, in the context of
text retrieval, if we have Docl {Ali, Jun, Sarah} and Doc2{Jun, Sarah, Ali}. Then, we can
say that Docl = Docz2, and they are both identical as every word belongs to Docl also
belongs to Doc2.

Definition 2 If we have two sets S1 and S82, S1 is “a proper” subset of $2 (S1 € S2) if
there has been X € S1 and also X € S2 as well. For example, in the context of text retrieval,
if we have Docl1{Ali, Hassan, Sarah} and Doc2{Hassan, Sarah, Ali, Mark, Farah}. Then,
we can say that Docl C Doc2, and Docl is a proper subset of S2 as every word belongs to

Docl also belongs to Doc?2.

Definition 3. he document doc is a collection of terms of vectors that holds these
terms, that is, any subset of C, when C is the document collection, (involving C itself).
Amer and Abdalla J Big Data (2020) 7:74 Page 7 of 43

Let doc be a document, a subset of C. We say that doc exists as a vector if the terms of
doc exist in the doc itself. First, let us define the key relationships between each docu-

ment pair docl and doc2 in the collection C, as follows;

doc\ C doc2 & T € docl = T € doc2(containment)

docl = doc2 doc C doc2anddoc2 Cc doc| (equality)

So, for the given document pair docl and doc2, the following set of operations are held

as follows;

Operation 1—union
The union of two sets $1 and S2 (S1 U 82), is the set that contains all the elements of both
sets S1 and S2 with the removal of duplication.

S1US2 = {X|X € Slorx e€ S2}

In the context of text retrieval, the Union operation of docl and doc2, docl1 U doc2, is the
group of terms {t,,..., t,} where n is the number of addressed terms in both documents, that
are involved in either docl, doc2 or both:

doc1 U doc2 = {t -t € doclort € doc2}.

Operation 2— intersection
The Intersection of two sets S1 and S2 (S11 S82), is the set that contains shared elements of
sets S1 and S2.

S11 S82 = {X|X € Slandx € $2}

In the context of text retrieval, the Intersection operation of docl and doc2, docl M doc2,
is the group of terms {t,,..., t,} where n is the number of addressed terms in both docu-
ments, that are involved in both documents docl and doc2 at the same time:

doc\ \ doc2 = {t :t € doclandt € doc2}.

Operation 3—negation
The negation operation of docl or doc2, doc1/doc2 or doc2/docl, is the group of terms that
are either belongs to doc2/docl or doc1/doc2:

doc1\doc2 = {t :t¢ doc2}.

doc2\docl = {t :t¢ docl}.
Amer and Abdalla J Big Data (2020) 7:74 Page 8 of 43

The proposed similarity measure (STB-SM)

The formulation of STB-SM similarity measure

Suppose we have a document pair doc 1 and doc2. Let docl=(w,,, Wy4),...) and
doc2 = (W1, Wo...) be the weighting vectors (using BoW model) of the term sets for
document 1 and document 2, respectively. Let T, {t,,, ty... ty,} and T, {tj to)... to,} be
the sets of items that are contained by docl and doc2, respectively. For the sake of sim-

plicity, the following is the proposed STB-SM equations:

X= Ss” Wi | * Ss” W; (8)

t€doc, doco t€doc, doco

Y= Ss” Wi | * Ss” Wy (9)

t €doc, \doc2 t€doc\doc

Z= Ss” Wij | * Ss” Wj (10)

tedoc, t€doco

xX Y

STB — SM(doc,, docz) = = * {1-— = (11)
Z Z

where the notations “N” and “\” denote the intersection and complement operators in

the set theory, and W;; is the weighting value. To further understand the mechanism of

this measure and briefly clarify some deficit of the state-of-the-art measures, we have

provided three examples as follows:

Example 1 Assuming we have docl (2, 5, 7, 8, 0, 9) and doc2 (9, 0, 0, 6. 5, 1), then STB-
SM will work as follows; (for simplicity, X is x1 and x2; Y is yl and y2, Z is z1 and z1, T;.w
suggests weighting of the term i)

 

T1.w T2.w T3.w T4.w T5.w T6.w
Doc!l 2 5 7 8 0 9
Doc2 9 0 0 6 5 ]

 

X1=2+8+9=19; X2=94+6+1=16; Z1=24+5+74+8+4+9=31; Z2=94+6+5+4+1=21;
Y1=5+7=12; Y2=5

While STB-SM yielded (0.47 * 0.91 =0.43) Cosine and Jaccard yielded (0.42) and (0.22)

respectively.

Example 2 Assuming we have docl (02, 1, 1, 0, 1) and doc2 (3, 1, 1, 1, 1, 0), then STB-

SM will work as follows;
Amer and Abdalla J Big Data (2020) 7:74 Page 9 of 43

 

T1.w T2.w T3.w T4.w T5.w T6.w
Doc] 0 2 ] ] 0 ]
Doc2 3 ] ] ] ] 0

 

X1=4 X2=3 Z1=5 Z2=7 Y1=1 Y2=4

While STB-SM yielded (0.34 * 0.89=0.30), Cosine and Jaccard yielded (0.42) and
(0.50) respectively.

Example 3 Assuming we have docl (1, 1, 3) and doc2 (1, 0, 2), then STB-SM will work

 

as follows;

T1.w T2.w T3.w
Doc!l ] ] 3
Doc2 ] 0 2

 

X1=4 X2=3 Z1=5 Z2=3 Y1=1 Y2=0

While STB-SM yielded (0.80), Cosine and Jaccard yielded (0.94) and (0.25)
respectively.

As seen from the drawn examples above, Cosine occasionally finds a good similarity
as indicated in example (1). However, the Cosine similarity gives the same value for both
examples (1 & 2) albeit the clear difference between both vectors, and to further exac-
erbate the issue the similarity value is highly exaggerated in example 3. It is worth indi-
cating that one novelty of STB-SM measure, is that the similarity value has never been
exaggerated as shown in example (3) for Cosine, or the more state-of-the-art measure.
STB-SM measure enables non-zero/non-shared features to have an explicit contribution
to the similarity computation. Therefore, STB-SM takes the presence and absence of all
features into consideration effectively.

On the other hand, Jaccard occasionally produces a good similarity as shown
in example (2), but more frequently the Jaccard similarity is poor, as indicated in
examples (1 & 3). Our proposed measure, therefore, comes to find a compromised
solution where the desired effect is being detected. Examples (1 & 3) show a better
and more accurate similarity found by STB-SM in comparison with the Cosine and

Jaccard.

STB-SM analysis
In this subsection, we concisely as well as informatively analyze the cases of the pro-
posed measure as follows;

The worst-case:

This case occurs when there is not even one shared feature between the document
vectors.

Example (worst case): Assuming we have docl (3, 0, 1) and doc2 (0, 2, 0). By apply-
ing the worst-case scenario, we find that X1=0, X2=0; Z1=4, z2=2, yl=4, y2=2;
Amer and Abdalla J Big Data (2020) 7:74 Page 10 of 43

because X = zero. Accordingly, STB-SM = zero, for both documents (1, 0, 1) and (0, 1,
0), which is logically true since there is no shared feature exist.

The average case:

This occurs when there has been at least one shared feature(s) as given in the drawn
above examples (1-3). In this case, STB-SM would have a value in the range [0-1].

The best case:

This occurs when both vectors are completely equivalent.

Example (best case): Assuming we have docl (4, 4, 4) and doc2 (4, 4, 4), or docl (1,
1, 1) and doc2 (1, 1, 1). By applying the best-case scenario, we find that x1=9, x2=9,
z1=9, z22=9, yl=0, y2=0. Accordingly, STB-SM =1 which is logically true as both

documents are identical.

The properties of similarity measures

According to [2, 4], six vital properties every similarity measure should have for the
relative measure to be considered an optimal measure. The following properties are
listed below;

Property 1: The existence or non- existence of the intended feature is more vital than
the difference between the values linked with the existing feature. According to the calcu-
lated-above examples, STB-SM explicitly takes the presence and absence of features into
consideration.

Property 2: The value of similarity should be grown as the difference between the val-
ues of non-zero features values decline. For instance, if we have f1 and f2 as two features
belong to docl and doc2 respectively. Then, for docl and doc2, the value of similarity
between fl=12 and f2=6 is higher than the similarity between f1=20 and f2=6. This
property is also clearly shown in example 3, along with the worst-case example.

Property 3: The value of similarity should be reduced as the number of existent or
non- existent features rises. This was showcased in both the worst and best case examples,
clearly indicating the applicability of his property.

Property 4: Any pair of documents is low similar to each other if there have been many
non-zero-valued features corresponding to many zero-valued features in the same pair.
For instance, if we have two vectors for two documents doc1(f1,f2)=(1,0) and doc2(f3,
f4)=(L 1). Then, doc1.f2 and doc2.f4 are the key cause for lowering the similarity between
both documents as f2 X f4=0 and, at the same time, f2+ f4>0. Example 2 supports the
applicability of this property.

Property 5: The similarity measure should possess asymmetrical features. For instance,
the similarity between both docl1 (1, 1, 0) and doc2 (1,1,1) must be the same when doc
2(1,1,1) and doc 1(1, 1, 0) are considered. According to the drawn above examples, STB-
SM enjoys this property completely.
Amer and Abdalla J Big Data (2020) 7:74 Page 11 of 43

Property 6: The distribution value should have a contribution to the similarity between
every two documents. That means features with higher spread (standard deviation) con-
tribute more in similarity than that of a lower spread.

Experimental setup

Text pre-processing

Some operations were carried out normally for the text to be transformed into text vectors
for processing. The text was converted from the lower case to upper case, numbers, punc-
tuations, and stop words (common words), in addition to that extra white space were all

removed, and some particular symbols (such as $, %)were converted into spaces.

Text representation
The bag of words (BoW) model [26, 27] was used to represent documents that were in
the vector space model (VSM). The BoW model represents each document as a word
collection disregarding the grammar and word order [28].

Given the fact that we have used a python to run the text pre-processing, the pre-
processing was performed using the Ntlk (Natural language toolkit) library of python as
follows;

¢ Tokenization: using the ntlk word tokenizer

¢ Converting all the words to lower case: using the lower() python string function
¢ Lemmatizing: using the ntlk stem WordNetLemmatizer

¢ Stopword Removal: using the ntlk stopwords

¢ Considering words with only 4 or more letters

The comparison mechanism of classification

After pre-processing, all of the documents were represented using the BoW model in
VSM in order for the classification process to start smoothly. Following that, the perfor-
mance of every similarity measured across the different kinds of documents was com-
pared and evaluated against each other. Six evaluation measures were used to evaluate,
namely, accuracy, precision, recall, F-measure, G-measure, and average mean precision.
For each criterion, the KNN algorithm runs from K=1 to K=120 over each number of
features of each dataset, and the averaged results were accumulated and drawn as given
in the Tables below (5, 6, 7, 8, 9, 10, 11, 12, 13). Number of features (NF) was varied from
NF=10, NF=50; NF=100, NF=200; NF =350, NF=3000, NF=6000 and NF=the
whole number of features (see Appendix samples). In consequence, we have eight runs
for the KNN algorithm over two datasets to test and examine six criteria using eight
similarity measures. The final number of implementations performed to have the results
below were (8 x 2 x 6 x 8=768) runs. If we also consider the sixty (60) values of K that
have been tested in each KNN cycle, the total runs would be 46080.
Amer and Abdalla J Big Data (2020) 7:74 Page 12 of 43

Term weighting
We adopted the most widely used Term Frequency (TF) technique of weighting which
simply gives the occurrence of each word in the relative document [29, 30].

K-nearest neighbor classifier

The K-nearest neighbor algorithm (k-NN) is most widely used, in the IR literature, to
perform document classification. Although it is a lazy algorithm [27], it is nonpara-
metric, simple, and believed to be amongst the top ten algorithms in data mining [31].
It works based on selecting the nearest points to the point at the question. The con-
cept of K-NN is that the points that exist in the same class are highly likely to be close
to one another depending on the used similarity measure. KNN assumes the next: (1)
Points in the feature space have a specific distance between each other and that dis-
tance is used as a metric to gauge closeness, (2) Each point in the training points has
its vector and class label. Later, a certain number “k” is determined to draw the neigh-
boring area of the point in question.

K-means clustering algorithm

Generally speaking, the clustering of a huge text dataset can be efficaciously made
through utilizing the algorithms of partitional clustering. One of the most-popular
partitional clustering algorithms is the K-means algorithm. It is widely known in the
literature to be the best-fit approach for handling huge volumes of datasets [8, 32].
Similarly to any clustering algorithm, K-means leverages a similarity measure that
finds the similarity between each document and the document representative of the
cluster (head of the cluster). The similarity measure represents the core of the clus-
tering process by which clustering algorithm performance is analyzed. However, the
most suitable similarity measure to effectively perform clustering is still an open-
ended challenge. In our work, for the clustering performance analysis, we ran the
K-means for each similarity measure, as well as the values of evaluation of metrics
(external metrics including purity, completeness and rand index, and internal met-
rics including the Calinski-Harabasz index and Davies-Bouldin index) were drawn
accordingly. We used the voting technique to determine the best similarity measure
that would best fit the K-means algorithm. The voting technique worked by enumer-
ating how many metrics each similarity measure had achieved its best values. The big-
ger number of metrics is the best fit which is the similarity measure. According to the
experimental results of the clustering process, our proposed measure (STB-SM) has
been seen as the best fit in most cases. It has achieved (11) out of the (20) points by
being the best in four metrics out of five. Unfortunately, in the K-means algorithm,
the number of clusters is still an ill-posed problem as stated in [32, 33]. Therefore, in
this study, we have picked numbers (4 and 8) to be the number of clusters just to ana-
lyze and emphasize the behavior of all the similarity measures. It is worth referring
that we are not arguing that (K =4 or K=8) is optimal or the best value for the num-
ber of clusters. It is just chosen as the number of actual classes in each dataset [34] to
draw the performance analysis of K-means using the considered similarity measures.

In the follow-up work, we plan to further examine the performance analysis with
Amer and Abdalla J Big Data (2020) 7:74 Page 13 of 43

Table 1 Machine and environment description

 

 

Task Tool Specification

Classification Language Python 3, development Software: Jupyter Notebook
OS Windows 7 (64 bit)
Memory RAM 4 GB
CPU Intel Core i5-3320 M (2.6 GHz)
Dataset Reuters and Web-KB

Clustering Language Python 3, Development Software: Jupyter Notebook
OS Windows 10 (64 bit)
Memory RAM 32 GB
CPU Intel (R) Core ™ (i9-8950 HK CPU 2.90 GHz/32/2TBSSD/4 GB)
Dataset Reuters and Web-KB

 

several K numbers of clusters, and at the same time, with other clustering algorithms,
like hierarchical clustering algorithms.

Machine description

Table 1 displays the machine and environment descriptions used to perform this work.

Dataset description

Reuters Dataset (Table 2): Reuters-R8 Dataset holds the eight most frequent classes of
the original ninety classes in Reuter’s dataset. After applying pre-processing, a total of
18308 features were extracted.

Web-kb dataset (Table 3): it consists of web pages of the computer science depart-
ment from the following universities: Cornell, Texas, Washington, and Wisconsin. It was
obtained from the World Wide Knowledge Base project of the CMU text learning group.
After applying the pre-processing, a total of 33,025 features were extracted. The data in
both datasets were divided into training and testing in ratio 2:1 (67%: 33%). To overcome
the over-fitting or under-fitting issue, instead of dividing the whole data randomly in
the training and testing data, each group was divided individually and then combined as
training and testing data. Both datasets are read directly from Python platform as they
are already integrated with python.

Table 2 Splitting of documents among eight classes in Reuters-R8 dataset

 

 

Class Samples
Cq 2292
Crude 374
Earn 3923
Grain 51
Interest 27)
Money-fix 293
Ship 144
Trade 326

 

Total 76/74

 
Amer and Abdalla J Big Data (2020) 7:74 Page 14 of 43

Table 3 Splitting of documents among four classes in Web-KB dataset

 

 

Class Samples
Project 504
Course 930
Student 1641
Faculty 1124
Total 4199

 

The classification evaluation criterions

This subsection holds the evaluation criterions used for classification as follows;

Accuracy (ACC)
ACC checks the total of samples that are correctly classified out of the whole sample col-
lection. ACC is defined in the next equation.

True Positive + True Negative

ACC (12)

~ True Positive + True Negative + False Positive + False Negative

Precision (PRE)
PRE checks the total number of items that are correctly identified as positive out of the
total items identified as positive.

True Positive

PRE (13)

~ True Positive + False Positive

Recall (REC)
REC checks the total number of items that are correctly identified as positive out of the

actual positive.

REC = True Positive
~ True Positive + False Negative (14)

F-measure or F-Score (FM)
FM is the harmonic mean of precision and recall. It is useful when classes are not dis-
tributed evenly.

Precision*Recall

EM = 2 % 15
Precision + Recall (15)

G-method or G-Score (GM)
GM is the geometric mean of precision and recall. It is also used when classes are not dis-
tributed evenly.

True Positive
GM =

 

(16)

(True Positive + False Positive) « (True Positive + False Negative)
Amer and Abdalla J Big Data (2020) 7:74 Page 15 of 43

Average mean precision (AMP)
AMP is the mean of the average precision of all classes. This is used to evaluate how pre-
cisely the classifier is performing.

AMP = Ss” (Ry — Rn-1)Pn

nN

(17)

where P,, and R, are the precision and recall at the nth threshold. Finally, True Positive,
True Negative, False Positive and False Negative are defined as follow;

True positive: the number of class1 testing documents that are correctly identified into
class1.

True negative: the number of instances of class2, class3,...., classN correctly identified
as class2, class3.. classN respectively.

False positive: the number of class1 testing documents that are incorrectly identified
into class2, class3,....., classN.

False negative: the number of class2, class3,....., classN testing documents that are
incorrectly identified into class1.

The clustering evaluation criterions

This subsection holds the evaluation criterions used for clustering. While the external
metrics require actual labels to assess the cluster quality (see Eqs. 18, 19, 20), the internal
metrics do not require actual labels to assess the cluster quality (see Eqs. 21, 22).

Accuracy (also known as Purity)
It is used to check the index to which a cluster is pure. Particularly, every cluster has only
one class and different clusters have different classes. In other words, this metric evalu-

ates the coherence of a cluster. It is defined by the following equation.
1 k
Purity = — max;\c; Ot;
Y N D J | 4 J | (18)

Where N is the number of objects(data points), k is the number of clusters, c, is a cluster

in C, and t, is the classification which has the max count for cluster ¢;.

Completeness

To check whether all members of a given class are assigned to the same cluster.

 

4 1 if H(K,C) = 0
~ jl ey else (19)
where
IC] |K|
ack ack
H(KIC) = —) > ee ae
c=1 k=1 k=1 ck
[K| IC| |C|
“ack Soo 1, ack
H(K) = — ) =*=*4— log == —
We = =) EEL og Ea

k=1
Amer and Abdalla J Big Data (2020) 7:74 Page 16 of 43

Rand index
It is used to check how many points are correctly predicted.

a at)
nC2

(20)

 

where n is the total number of samples, and (a+b) is the agreement between real and

the assigned cluster label.

Calinski-Harabasz index
It is used to measure the ratio between cluster dispersion and inter cluster dispersion.

tr(Bx) . ng —k

‘= inW)  k—-1 (21)

 

 

k
Br = S14 (cq — CE) (Cg — cr)
q=1

where Cq is the set of points in cluster q, cq is the center of cluster q, cp the center of E,

and n, is the number of points in cluster q.

Davies-Bouldin index
This index signifies the average ‘similarity’ between clusters, where the similarity is

a measure that compares the distance between clusters with the size of the clusters

themselves.
1 k
DB = k 2. max Ry (22)
where
Si + Sj
Ry = a

where s, is the average distance between each point of cluster i and the centroid of that
cluster, d; is the distance between cluster centroids i and j. Finally, the best and worst

values and the range of each measure are drawn in Table 4.
Amer and Abdalla J Big Data (2020) 7:74 Page 17 of 43

Table 4 The best and worst values and the range of each metric

 

 

Name Best value Worst value Range
Accuracy/purity 1 0 0-1
Completeness 1 0 0-1

Rand Index 1.0 — 1.0 — 1.0-1.0
Calinski-Harabasz index (Lower value is better) - -
Davies-Bouldin index 0 0-1

 

Experimental results

Classification results

This work investigated all considered measures comprehensively based on six criterions for
performance evaluation which is the first study of its type to do such investigation in the
information retrieval field with respect to text classification. The K values of KNN were var-
ied from (1) to (120) with an increment of value (2) in each cycle (see Appendix samples).
The number of features of each dataset was diversified (10, 50, 100, 200, 350, 3000, 6000, NF)
to clearly draw the best performance of each measure under several circumstances. Then, for
each measure, the results were averaged for all K values on each NF to yield the results drawn
in Tables (5, 6, 7, 8, 9, 10, 11, 12, 13). In other words, the following tables contain the results
of each similarity measure which were averaged on each Number of Features (NF) over all K
values in the range [1...120] as drawn in the appendix. In each table, the averaged results of
all K values for each performance criterion is displayed. Table 5 displays the averaged results
of all criterions when NF=10. For simplicity, we draw the averaged results of all measures
while analyzing briefly three criterions, namely, ACC, FM, and AMP.

As shown in Table 5, for the Reuters dataset, Euclidean, followed by STB-SM and Cosine,
met the highest accuracy. However, STB-SM, followed by Euclidean and kullback—Lei-
bler, outperformed all measures on both FM and AMP criterions. On the other hand, on
the Web-KB dataset, PDSM, followed by STB-SM and Cosine, outperformed all similarity
measures in ACC. In regard to FM and AMP, Cosine, followed by STB-SM and PDSM, out-
weighed all measures with STB-SM being superior to PDSM on FM and PDSM being supe-
rior to STB-SM on AMP. So, the best measures, when NF= 10, were Euclidean, STB-SM,
and Cosine on Reuters, and PDSM followed by STB-SM and Cosine on Web-KB.

Tables 6, 7, 8 9, 10, show that, for both Reuters and Web-KB, STB-SM, followed by PDSM,
and Cosine, achieved the highest ACC, FM and AMP respectively. However, two exceptions
are noted as follows; the first exception is when NF =350, Cosine outweighed PDSM in terms
of FM and AMP on both Reuters and Web-KB. The second exception is that when NF =3000,
Cosine outperformed PDSM in terms of FM and AMP on Reuters only. Nevertheless, the top
performer measures, when NF in the range [50-3000], were STB-SM, PDSM, and Cosine.

Reuters in Tables 11, 12, similarly to Table (10), STB-SM, followed by PDSM and
Cosine, had been superior with the highest ACC, FM, and AMP respectively. Moreover
Cosine outweighed PDSM in terms of FM and AMP. In contrast, on Web-KB, PDSM,
followed by STB-SM and Jaccard, had been superior in terms of ACC and AMP. How-
ever, Cosine had been superior to Jaccard in terms of FM only. So, the top performer
measures, when NF in the range [6000-All features], were STB-SM, PDSM, Cosine, and
Page 18 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

9Er'0 690 rb50 6£50 0650 6090 E70 7950 £0£0 E€E0 vEE0 6690 WS-9LS 8
8rr'0 990 6£50 E50 SS9'0 9790 9170 E50 L970 LOf0 6££0 LSo'0 WSdd /
Zev'0 L99°0 v7S0 S150 €79'0 S090 S0Z'0 9LS0 €/T0 870 /7E0 8790 Ue}eUUeY\ 9
9Er0 0290 9750 S750 S790 E190 8770 LSS0 7670 6Z£0 E80 689°0 43|q!97-eq || S
E/E0 S60 180 Sev0 S50 8sy0 vZl0 sev'0 0810 v07'0 ELL'0 7590 eAleyreneug b
LZE-0 0950 ecr'0 8er0 L190 vvS0 Z0Z'0 76r'0 ra) 870 6670 689°0 puender €
1Sr'0 1890 7950 8rS0 0190 1z90 8170 7vS'0 L870 LLEO 8ZE0 +690 SUISOD Z
6770 L990 v7S0 S150 090 S090 LITO L750 9870 £670 LIEO E120 ueapl|any
dWV WD W4 334 aud >2V dwv WD W4 334 dud DV UO9}149/A}Ue|ILWIS

gy-49M g-Saynay yaseyeq ON

 

(7+ /OZL-L=y) sHNsas pebesaae 2y}-OL = IN ‘UBM SaiNseaw |e Jo UOIZENjeAS BDURWOJad ¢ aIqGeL
Page 19 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

909°0 S6L'0 L1Z0 £690 Z6L'0 9920 EZr0 LSL0 9650 8850 0r9'0 £980 WS-9LS 8
8650 6ZL0 1690 0290 S780 LSL0 cbr 0 90 L950 1/50 rl90 L580 WSdd /
6/70 8/90 9950 8ES0 6820 7r9'0 Ssv'0 0€Z0 7850 LSs0 1590 E80 ueqeyuey\| 9
6/70 9/70 ETO 9670 Sv9'0 9770 SvT70 OLS0 L7E0 8870 L190 8790 43|q!97-eq || S
y8E0 6£9'°0 780 L6r'0 5990 76r'0 06£0 vZ90 g9r'0 7/70 9090 €08'0 eAseyreneug b
1870 1890 1550 9rS0 8080 9990 EvE0 LEO evr'0 170 0850 0620 puender €
550 €9/'0 1290 0590 SEL0 61Z0 L9v0 LrZ'0 2650 S950 9590 L080 dU!SOD Z
0670 LOZ'0 L6S'0 1950 OrZ0 799'0 0€70 8020 vSS0 0750 Ly9'0 L780 ueapl|nj
dWV WD W4 334 aud >2V dwv WD W4 334 dud DV UO9}149/A}Ue|ILWIS

gy-49M g-Saynay yaseyeq ON

 

(7+ /OZL-L=y) SHNsai pabesane 3y}-0¢ = JN ‘UBYM SaiNseaw je Jo UOIZENjeAS BDURWOJJadg 9 aIqGeL
Page 20 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

0c90 0080 SIZ0 669°0 6L80 LLL0 LVS 0 9620 8590 Sv9'0 O0L0 1060 WS-d1S 8
9090 v8lL0 9690 9L9°0 LC80 8920 S1S0 L820 CE90 LE9O 5990 C680 WSdd L
cev0 6c90 O06V 0 SZV0 9620 voS 0 970 6cL0 voS 0 S50 S890 O€8'0 UeWeYUR/\ 9
9SC0 evv 0 ZOLO 09C0 c6£0 96€ 0 ZOL0 cev0 LECO LLCO 9790 SSS°0 J9|Q!9]-YIeQ||M S
680 vv9 0 O6€ 0 6670 €v9'0 00S°0 Ov 0 OLZO O€S0 LcS0 6€9°0 CEB 0 eAjeyreieug v
9¢5°0 ZLL0 c6S 0 v8S0 LE80 COL0 eZeE0 L590 9870 OSv'0 €lL90 6L80 PJEDEL €
8£5 0 SZL0 6890 9990 6920 ZELO COS 0 LOL0 vo90 c09'0 8890 SZ80 9UISO") C
VLV 0 L890 v9S 0 LvS 0 6SZ0 €v9'0 9570 ecZL0 8850 evS 0 8890 ves oO Uespl|ony L
dWV W)D W4 D4 dud DIIV dWV W)D Wa D4 ddd DIV UOLI9}19/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ {OZ L-L =) SNsas paHessae 2y}-00 L = IN ‘USYM SasNseaw |je Jo UO!ZEN][eAS BDUCWOJJag / 21qGeL
Page 21 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

vv9 0 S180 lvZ0 LcL0 O€80 !6Z0 0650 LL8O 5890 699°0 LOL0 vl60 WS-d1S 8
8&90 908°0 CELO ZOL0 Svs'0 6820 8550 £080 E990 SS9°0 vCLO 5060 WSdd L
vo£O L6S°0 Scv0 8cv 0 SL80 9550 6ZE 0 0€9°0 S8v0 LCVO0 CLLO 6SZ0 UeWeYUR/\ 9
€ScO ZEVO0 CSL 0 vSCO OSE 0 L6e0 SSLO SZE0 6v LO LOLO C190 O0€S 0 J9|Q!9]-YIeQ||M S
ZOE 0 vS90 vOV 0 OLSO 8890 SLSO OSv'0 6SZ0 6850 06S5°0 LE90 £980 eAseyrenjeug v
£950 8vL0 Se90 Sc90 evs'0 9EL0 cev0 LOZ0 8750 ZOS0 8990 9”8'0 PJEDEL €
£090 L6oZ0 LLZ0 689°0 c6L0 LSL0 8550 8820 1990 Se90 6€L0 v680 9UISO") C
vOV 0 6990 evS 0 ScS0 VLLO 9€9°0 LSv'0 £690 eZS0 OLSO LVL0 vL8O Uespl|ony L
dWV W)D W4 D4 dud DIIV dWV W)D Wa D4 ddd DIV UOLI9}19/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ {OZ L-L =) S}Nsas pabessae 9y}-007Z = IN ‘USYM SasNseaw |je Jo UO!ZEN|eAS BDUCWOJJag § aIqGeL
Page 22 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

S590 €c80 0SZ0 LEZO Ovs'0 1080 6190 LC80 vlZ0 vo9 0 €6Z0 CC60 WS-21S 8
C590 LOLOL8O SvL0 S60LCL0 c60ES8'0 1080 C850 el80O 6890 €Z90 OLL0 C160 WSCdd L
O80 ZLS0 O00 LLVO 9780 evS 0 L6c0 SES0 S9E0 OcEO LEZO 6990 UeeYUe/ 9
CSCO Stv0 ZVLO CSCO 89€0 O6€ 0 evlO 8S€E0 ScLO ZVl0 LSS°0 OcS0 J9|Q!9 Je] |} S
S6£0 vS90 c0v0 OLSO v890 ZLS0 ScS0 LLLO eL90 8190 6LZ0 9/80 eAeyreieug v
S6S 0 CLLO OL9°0 8590 SS80 VOLO 6Sv0 6LZ0 eZS0 c€S0 8690 L980 PJEDEL €
vl90 8620 61Z0 869°0 0080 99L0 0650 vO8 0 v69 0 099°0 eZZ0 C060 9UISO") C
SSv0 €99°0 6c5 0 9LS0 SZL0 ce90 LCV 0 9990 vVSO0 CLV 0 LZZ0 8220 Uespl|ony L
dWV W)D W4 235 dud DIV dWV W)D W4 235 ddd IDV UOLI9}9/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ /OZL-L =}) SHNsai pabesaae 3yu}-0S¢ = IN ‘UBYM SaiNseaw je Jo UOIZEN|eAS BDURWIO}ad 6 BIGeL
Page 23 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

6090 £080 Z1Z0 COL 0 Svs'0 8820 6890 8580 S620 6vl0 9160 9160 WS-d1S 8
vv9 0 LL8O 0&Z0 CLLO LS8°0 6620 CE90 vEsO 8r7Z0 6020 L680 CL60 WSdd L
Z6C0 8870 9770 60€ 0 8vZ0 SvV 0 LOL‘O ZLEO CSL O 9910 8070 vESO UeWeYUR/\ 9
OSCO cev0 LvLO OSCO ZOLO 680 9EED 9EEO L60°0 6c10 LSL‘O 8050 J9|Q!9]-YIeQ||M S
9070 9990 cev0 ScS0 8890 ceS0 06S5°0 L180 £690 c89°0 L980 8880 eAjeyreieug v
Lc~90 0620 LOZ0 €890 8580 C8Z0 S6V0 SeZ0 OL9O0 LSS0 L080 8980 PJEDEL €
!C90 LOO ECLO COL0 VL8O 6920 v590 LE80 !ZZ0 ZLZ0 vO6 0 C060 9UISO") C
Ove 0 L090 9” 0 Lvv 0 LLLO OLS°0 £600 c€S0 9LE0 ZLEO evl0 8790 Uespl|ony L
dWV W)D W4 D4 dud DIIV dWV W)D Wa D4 ddd DIV UOLI9}19/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ ‘OZ L-L =) S}Nsas pabesane ay} -OO0E = IN ‘UBYM SaiNseaw |e JO UOI}EN]eAS BDURWIOJJad OL 21GeL
Page 24 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

ve90 L080 !@Z0 ZOL0 vv8 0 C6L0 0890 vS80 Z8L0 OSLO 9160 9160 WS-d1S 8
LV90 VL8O vELO ZLZ0 vS80 C080 6£90 vEsO vSZ0 6020 668°0 C160 WSdd L
06C0 6Z70 OECO OOE 0 8890 Stv0 O9L'O SZE0 8v 0 vOLO LOv'0 O€S°0 UeWeYUR/\ 9
SCCO LEvO 060°0 8Vc0 8200 8LC0 8cL0 See0 680°0 8cL0 €9L'0 €0S°0 J9|Q!9]-YIeQ||M S
9070 9990 cev0 ScS0 689°0 ceS0 06S5°0 8L80 £690 €89°0 L980 8880 eAleyreyjeyg v
190 L6oZ0 O0L0 v89 0 8580 v8ZL0 £6 0 vELO 609°0 SSS0 ElL8O L980 PJEDEL €
9L90 8620 Z1Z0 8690 OLS 0 9920 1590 LE80 6920 ZLZ0 C060 6680 9UISO") C
S80 06S°0 CCV 0 8cv 0 LZZ0 OSS°0 6£C0 6LS°0 9SE0 COE 0 vVCLO £790 Uespl|ony L
dWV W)D W4 D4 dud DIIV dWV W)D Wa D4 ddd DIV UOLI9}19/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ /OZL-L=y) SHNsai pabessaae 3y}-0009 = JN ‘USYM Sainseaw |e JO UOI}EN]eAS SDURWIOJad LL 2IGeL
Page 25 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

0&90 908°0 SIZ0 9020 Lv80 !6Z0 9290 LS8°0 E8Z0 6€L0 €L60 C160 WS-d1S 8
CV90 CL80 LCEL0 VLZLO vS80 1080 LE90 LC80 SvL0 O0L0 668°0 606 0 WSdd L
v8CO VLV 0 OCCO voc O 699°0 6cv 0 8510 eZeE0 vVlO COLO 9LE0 LCES0 UeWeYyue/\ 9
OSCO LEvO 060°0 8Vc0 6Z0°0 VELO 8cL0 See0 680°0 8cL0 VOLO €0S°0 J9|Q!9]-YIeQ||M S
9070 L990 vEVO 9¢5°0 689°0 vESO 06S5°0 8L80 £690 €89°0 L980 8880 eAleyreyjeyg v
6190 c6L0 vo9 0 v89 0 6S8°0 9820 8870 O€L0 LO9'0 OSS°0 ElL8O 593°0 PJEDEL €
vVL9O 6620 SLZ0 OL90 £080 9920 0590 9E80 L9L0 9LZ0 £060 L680 9UISO") C
VBE O L6oS°0 CCV 0 8cv 0 8920 OSS°0 LZ70 OLSO vVEO voc O 9LZ0 SL90 Uespl|ony L
dWV W)D W4 D4 dud DIIV dWV W)D Wa D4 ddd DIV UOLI9}19/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ {OZ L-L =) S}Nsas paHessaae ay}—(sa1n}ea} SZTOEE = G}-GeM ‘SOESL = S19}NAdY) OZIs BJOYM 3Y} = IN ‘USM Sainseaw je Jo UOIJEN]eAS BDURWIOJJadg ZL 2IGeL
Page 26 of 43

(2020) 7:74

Amer and Abdalla J Big Data

SoUJOW UOH}JEN|eAS HulpuodsasJod JO} panaiyde saunseaw do} Jeu} sanjea Jsaybiy ay} a}ed!Ipul sanjea 31/e}|

 

 

 

 

Z090 L6oZ0 COLO £890 O8Z0 v9L0 v9S0 0620 9990 9790 LVLO 0880 WS-d1S 8
6090 L820 0020 L89°0 LC80 8920 LES 0 CLL0 CE90 6L9°0 ScL0 698 0 WSdd L
VLEO CLS 0 88E0 6070 vVLO LESO v8cCO c€S0 eve 0 8ce0 ZESO 9990 UeWeYyue/\ 9
8270 OLV'0 c6l 0 c6C 0 8EE0 OLE 0 S6L0 SOv'0 VLLO 0610 LLVO SSS°0 J9|Q!9]-YIeQ||M S
S6£0 8790 8070 €0S0 vv9 0 OLS O SZV0 9¢L0 8550 LSS0 OL9°0 ZE80 eAjeyreieug v
OSS°0 ZELO €c90 eL90 L180 LcLO LLVO SZ90 SLSO 8270 L99°0 0620 PJEDEL €
1850 9/L0 8890 699°0 LOL0 8EL0 9E5 0 6920 SV90 S190 ZELO v98 0 9UISO") C
Stv0 Sv9'0 SO0S0 9670 9vL0 909°0 eSE0 OL9O0 eSv0 9070 8990 ceLO Uespl|ony L
dWV W)D W4 D4 dud DIIV dWV W)D Wa D4 ddd DIV UOLI9}19/APAe IWS

gy-qem siaynay yasejeg ON

 

(7+ {OZ L-L =) S}Nsa1 aHessAe—‘s}jNsai paHbesane jo abesaae dy} Udy} UBYM Sainseaw je JO UOIJEN]|LAS BULWOJI9g EL a/qeL
Amer and Abdalla J Big Data (2020) 7:74 Page 27 of 43

Jaccard. It is worth mentioning that from Table 6 , 7, 8, 9, 10, 11, 12, results have been
almost the same. In other words, results have been noted in stable condition.

Finally in Table 13, when the average of averaged results has been taken, it is clear
that, for both Reuters and Web-KB, STB-SM, PDSM, and Cosine have been the best
measures for all criterions. Thus, in conclusions, the top performer measures, when

the average results have been taken, are STB-SM, PDSM, and Cosine.

Clustering results

In this subsection, we have evaluated and compared the impact of all considered similar-
ity measures on the behavior of the K-means clustering algorithm. Fixing K on (4 and 8)
and using the presented-above evaluation metrics for clustering (see Table 4), the exper-
iments have been conducted on both datasets (Reuters and Web-KB) to experimen-
tally identify and distinguish which measure would be the best fit for K-means. Either
positively or negatively, the experiments could clearly signify the selection of similar-
ity measure impact on clustering quality. All features of both datasets were considered
when the clustering process has been running (Reuters = 18,308, Web-KB = 33,025 fea-
tures). As stated earlier, we have used two internal metrics, and three external metrics
for clustering evaluation of K-means under the umbrella of all considered similarity
measures. As for the stopping condition, the K-means was allowed to stop after running
(50) iterations to obtain the best performance, or alternatively, the algorithm reached
the stability situation for two consecutive cycles. The stability situation is the case in
which K-means clusters were recorded stable (unchanged) for two consecutive cycles.
Centroids of clusters were chosen randomly in each iteration. We have used the voting
technique (see Table 20) to decide the best fit similarity measure using which the perfor-
mance of K-means has been noted to be the highest. According to the results drawn in
Tables (14, 15, 16, 17, 18), STB-SM, followed by PDSM and Euclidean, has been the best
fit in this study. The bolded values in Tables (14, 15, 16, 17, 18) signify the best values
each measure had achieved on the corresponding metric.

Table 14 External Metric-Purity (mostly known as “Accuracy”)-K-means performance

 

 

 

 

Similarity K=4 K=8

measure/

metric Reuters-18308 Web-KB-33025 Reuters—18308 Web-KB-33025

features features features features

Euclidean 0.6745546742946301 0,4420100023815194 0.6651930828240801  0.5363181709930936

Cosine 0.6300871148095176 = 0.5651345558466302 0.5161877519178261 0.5710883543700881

Jaccard 0.5418021063580809 0.4060490592998333 0.5631257313743336 0.42462491069302216

Bhattacharya 0.6602522428812898 0.550845439390331 0.6573917565986218 —0.3908073350797809

kulloback-Lei-  0.5103367572487323 0.39104548702071923 0.5103367572487323 0.39175994284353416
bler

Manhattan 0.528799895982317 0.3912836389616575 0.5342608243401378 0.40128602048106693

PDSM 0.6628526849564426  04165277447011193 0.6329476010921856  0.40533460347701833

STB-SM 0.626706540111819  0.6110978804477256 0.6059030035105968 0.571802810192903

 

Italic values indicate the highest values that top measures achieved for corresponding evaluation metrics
Amer and Abdalla J Big Data

(2020) 7:74 Page 28 of 43

Table 15 External metric—completeness-K-means performance

 

 

 

 

Similarity K=4 K=8

measure/

Metric Reuters—18308 Web-KB-33025 Reuters—18308 Web-KB -33025
features features features features

Euclidean 0.199511647995826  0.0673834489462676 0.179814979894203 0.15452274 170402275

Cosine 0.176068659196536 0.2225985559938000 0.049343505745102 0.1665642553582776

Jaccard 0.055007933247357 0.0318290213093328  0.050929724394223 0.0309374891 1398607

Bhattacharya 0.161670093558948  0.3071187147856372 = 0.212872136220606 0.04952824 171446496

kullback—Leibler

0.138798782038987

0.0661952142325757

0.082386 15904440288

0.09274073186745294

Manhattan 0.140585609897670 0.0776936270894379 0.162146390404297 0.2204324456530784
PDSM 0.224510552205581 — 0.0377046283518796 —0.199091523319667 0.07172116609953971
STB-SM 0.267581280659119 = 0.2935685999112778 = 0.220934861375524 0.17402803377334777

 

Italic values indicate the highest values that top measures achieved for corresponding evaluation metrics

Table 16 External metric—Rand index-K-means performance

 

 

 

 

Similarity K=4 K=8

measure/

Metric Reuters—- 18308 Web-KB-33025 Reuters—18308 Web-KB-33025 features

features features features

Euclidean 0.18579386266996 0.07259267580718086 0.134417241084232 0.13997725 114650594

Cosine 0.1367979889 1326 0.19584148736739818 0.038271439259564 0.1388214135942175

Jaccard 0.03156196525375 = 0.03337323105349306 =: 0.033 25438109622 0.02868306344839796

Bhattacha- 0.10437193466823 0.21881970871431122 0.037753839600227 —0,.00837183862883132
rya

kullback— — 6,969300641e-05 —0.0003339640498897 —0.00089845850765 —0.00025681169419640
Leibler

Manhattan —0.0456795771624 0.05064565361647291 —0.02973126512374 0.003861173313610305

PDSM 0.10879494053512 0.00359089370980009 = 0.116230029219350 —0.00999313066207844

STB-SM 0.17377955245469 0.234594 10623220853 0.10889381193025 1 0.17012239790902425

 

Italic values indicate the highest values that top measures achieved for corresponding evaluation metrics

Table 17 Internal Metric-Calinski-Harabasz Index—K-means performance

 

 

 

 

Similarity K=4 K=8

measure/

metric Reuters-18308 Web-KB-33025 Reuters—18308 Web-KB-33025

features features features features

Euclidean 282.616242361247 733.8545918925699 175.69520099100788 376.55783811163883

Cosine 158.059649581219 41.1205304663345 36.67050378019272 22.764647326363253

Jaccard 10.76 166549332125 5 45223760438743 1 12590273044 103254 4.818955513623545

Bhattacharya  142.1019959257336 18.169867898545604  125.70926440159229 7.864068171394914

kullback—Lei- 0.6047659734858815 0.16973494233360648 0.3996948922364028 0.21630702269179736
bler

Manhattan 222.98925272777933 687.1305760147841 96.46528179432781 338.1631503564861

PDSM 132.7979522566384 6.947310633285274 87.3385616551679 7.2650953 16386779

STB-SM 127.69695325 126817 37.83093841884232 92.69694747153109  36.67724243309815

 

Italic values indicate the highest values that top measures achieved for corresponding evaluation metrics
Amer and Abdalla J Big Data

(2020) 7:74

Table 18 Internal metric—Davies-Bouldin Index-K-means performance

 

Similarity
measure/metric

K=4

 

Reuters—18308
features

Web-KB-33025
features

K=8

 

Reuters—18308

features

Web-KB-33025
features

 

Euclidean
Cosine

Jaccard
Bhattacharya
kullback—Leibler
Manhattan
PDSM

STB-SM

4.186722232186011
4974207600701 769
13.633300152033158
4597427791 367133
1.872529736499688
2.62753335535/464
3.78333773968 16443
5.764560759902967

4.4852705 17901569
5.18474833 1594617
15.865256973163703
4.893369736545789
2.8045629002126167
2.9998709693 117767
7.919681593999572
5.85720075727297

4.138785792328177
5.120474137629764
14.42610495247111

3.28045 1019807807
2.26682759544780 1

2.12372247499906 13
5.179788734900043
5.391258478217752

4.41464669835 1782
6.511148800037687
15.858516305005779
5.177469928065 736
2.1725394033652123
1.6587667141935847
5.386991 923494522
5.654144650340377

 

Italic values indicate the highest values that top measures achieved for corresponding evaluation metrics

Discussion

The discussion revolves around two key points. First, the measure performance sta-
bility over both datasets. Second, in which the number of features each measure has
performed the best in terms of accuracy (ACC), f-measure (FM), and average mean
precisions (AMP).

Classification- performance stability

Based on the results given in Tables (5, 6, 7, 8, 9, 10, 11, 12, 13), 19 observed that the
most stable measures on both datasets based on the points every measure has achieved
on each number of features. It is concluded that the more points each measure achieves,
the more stable it is. In the next Table, R and W indicate Reuters and Web-KB datasets
respectively.

Table 19 shows that the most stable measures were STB-SM PDSM and Cosine with
48, 45, and 45 points respectively. While PDSM was more stable on web-kb than Cosine,
Cosine was more stable on Reuters than PDSM. However, while Table 19 gives the stable
measures according to its giving one plus for each measure when a measure has been
superior in terms of specific criterions (out of three criterions, namely, ACC, FM, and
AMP) on each dataset, the real numbers in results drawn in Tables (5, 6, 7, 8, 9, 10, 11,
12, 13) also showed indisputably that the top performer measures are also STB-SM,
PDSM, Cosine, and Jaccard. It can also be deduced from the results that, unlike Reu-
ters upon which measures have unstable performance, all measures on web-kb have
almost stable performance, chiefly the top performers. On Reuters, the competition was
held between STB-SM, PDSM, and Cosine. Moreover, from Table 13, based on points
recorded on each NF, it can be concluded that STB-SM, PDSM, Cosine, and Jaccard can
be used effectively for low, middle and high dimensional datasets as these measures per-
formed well on each NF value. Euclidean and Manhattan also performed well on low
dimensional datasets (NF in [10—200] features). Bhattacharya was observed to behave
well on middle and high dimensional datasets (NF in [200-N] features).

Strictly speaking, the highest performance over both datasets was seen for both STB-
SM and PDSM as both measures showed almost stable and close performance on all val-
ues of NF when the poor performance was seen for kullback—Leibler, Manhattan and
Euclidean chiefly on high dimensional datasets.

Page 29 of 43
Amer and Abdalla J Big Data (2020) 7:74 Page 30 of 43

Table 19 Measure stability points

 

 

 

 

NF/ Euclidean Cosine Jaccard Bhattacharya kullback- Manhattan PSDM STB-SM
Measure Leibler
R Ww R W R W R W R Ww R W R Ww R W

10 2 2 3 2 1 3 3 3
50 3. 3 3 3 3 3
100 3 3 3 3 3 3
200 3 3 3 3 3 3
350 3. 3 3 3 3 3
3000 3. 3 3 3 3 3
6000 3] 2 3 3 3 3
The whole 3 2 3 3 3 3

features
Sum 2 23 22 4 2 1 21 24 24 24
Stable 2 45 4 3 45 48

points

 

Italic values indicate the most stable similarity measures

Classification-performance climax
Second, in which the number of features, the similarity measure performance was at its

climax in terms of accuracy, f-measure, and average mean precisions.

Performance analysis-reuters
The next Figs. (1, 2, 3) hold the map of the criterions movements (results were averaged)
for all measures over several NF values.

Figure l-a depicts that Manhattan followed by Euclidean and kullback—Leibler did
not have a stable accuracy and performed the worst as NF grew. In contrast, STB-SM,
PDSM, and Bhattacharya had the most stable performance. The Cosine and Jaccard
showed a punctuated accuracy while NF grew from 10 to 3000, and then started to be
slightly declined as NF grew. Figure 1b draws the top competitors which were STB-SM,
PDSM, and Cosine with STB-SM being superior.

Figure 2a shows that Euclidean and Manhattan had a higher FM when NF was in the
range of [10—100]. However, their performance started deteriorating as NF grew. Like
accuracy movement, STB-SM, PDSM, Cosine, Bhattacharya, and Jaccard yielded an
almost stable FM while NF grew from 10 to N, with STB-SM and PDSM being the best,
and Bhattacharya bettered Jaccard. Bhattacharya’s performance declined slightly for the
favor of STB-SM, PDSM, and Cosine, though. Finally, kullback—Leibler was shown to
perform the worst. Figure 2b draws the top competitors which were STB-SM, Cosine,
and PDSM with STB-SM being superior over all of them, and Cosine superior over
PDSM.

Finally, from Fig. 3a, it is noted that Euclidean followed by Manhattan and kullback—
Leibler had the worst performance in terms of AMP. On the other hand, STB-SM,
PDSM, Cosine, and Bhattacharya drew the best performance. The Jaccard measure was
seen to represent a middle ground between those of the highest AMP and those of low-
est AMP. It is worth noting that all measures of the highest performance were seen effec-
tive from NF = 10 till NF = 3000, and their effectiveness keeps improving on all features.
Amer and Abdalla J Big Data (2020) 7:74 Page 31 of 43

 

   

 

  

 

 

 

 

 

 

 

"a >
a Accuracy (Reuters) b Accuracy (Reuters)
mmm Euclidean — Cosine
Mmm Cosine 0.90 — ppsm
0.8 Mmm jaccard —— STB-SM
@@™ Bhattacharya
@@™ Kullback Leibler 0.85
o 0.6 @ Manhattan a
£ mmm DSM ® 0.80
3 oa mmm STB-SM 3
<x < 0.75
0.2 0.70
0 } | 0.65
‘ 10 50 100 200 350 3000 6000 18308 Avg 10 50 100 200 350 3000 6000 18308 Avg
No. of Features No. of Features
Fig. 1 a Accuracy over all Measures on all NF values— Average results (K= 1-120; + 2)—Reuters. b Accuracy
over competitive measures on all NF values— Average results (K= 1-120; +2)—Reuters
\ )
Oo >)
a F Measure (Reuters) b F Measure (Reuters)
Mm Euclidean 0.8 —— Cosine
MM Cosine —— PDSM
Mm jaccard 0.7 — STB-SM
@™m™ Bhattacharya
wo M@@™ Kullback Leibler wo
5 0. Manhattan 5 0.6
A mmm PDSM oe
g mmm STB-SM 2 0.5
wu Ww
0.4
0.3
10 50 100 200 350 3000 6000 18308 Avg 10 = 550-100-200 350 = 3000 6000 18308 Avg
No. of Features No. of Features

Fig. 2 a F-measure over all Measures on all NF values— Average results (K = 1-120; + 2)—Reuters. b
F-measure over competitive measures on all NF values—Average results (K = 1-120; +2)—Reuters

 

 

 

 

 

 

 

\ J
(- >
Average Precision Mean (Reuters) Average Precision Mean (Reuters)
0.7 :
Mm Euclidean ———~ Cosine

c a @™™ Cosine & b — PDSM
o mm jaccard Y 06 —— STB-SM
= @mm™ Bhattacharya =
c M@@M™ Kullback Leibler S
2 Manhattan H 0.5
oO mmm PDSM Oo
v mmm STB-SM v
a o 04
v v
2 z
© VY 03
Zz Z

| 0.2

10 5O 100 200 350 3000 6000 18308 Avg 10 50 100 200 350 3000 6000 18308 Avg
No. of Features No. of Features
Fig. 3 a AMP over all Measures on all NF values— Average results (K = 1-120; +2)—Reuters. b AMP over
competitive measures on all NF values— Average results (K = 1-120; +2)—Reuters

 

XX S

However, as NF surpassed 3000 to reach 6000 or bigger, their performance started to

lower slightly as shown in Fig. 3b.

Performance analysis-web-Kb
The next Figs. (4, 5, 6) hold the map of the criterions movements (results were averaged)
for all measures over several NF values.

From Fig. 4a, it can be seen that Manhattan followed by kullback—Leibler had got
an almost stable accuracy albeit the fact that they performed poorly as NF grew. STB-
SM, PDSM, Cosine, and Jaccard showed a clear stable higher accuracy while NF grew
from 10 to all features, with STB-SM and PDSM being highly superior. While STB-SM
outweighed PDSM when NF is in the range [50-3000], PDSM outweighed STB_SM
from 6000 to all features as shown in Fig. 4b both measures intersected at 3000 fea-
tures, though. In addition, on average, STB-SM was still taking the lead. Manhattan and
Euclidean had a close performance from each other when NF was in the range [10—200].
Amer and Abdalla J Big Data (2020) 7:74

However, as NF grew, Euclidean outperformed Manhattan, while seen more closely to

Bhattacharya when NF in [350—33,025].

From Fig. 5a, it is shown that Manhattan and kullback—Leibler had the worst perfor-
mance albeit the fact that kullback—Leibler was seen closer to Cosine when NF = 10. It
was a rare case, though. Similarly to Fig. 4b, PDSM, Fig. 5b exhibits that PDSM, STB-SM
and Cosine were the best performance with PDSM and STB-SM being fiercely rivals. On
the other hand, Jaccard, and Euclidean outperformed Manhattan and kullback—Leibler

and Bhattacharya as NF was in the range [50-6000]. However, as NF grew bigger, Bhat-

tacharya started to show a gradually-increasing performance over Euclidean.

Finally, from Fig. 6a, it is clear that Manhattan, Bhattacharya followed by kullback—
Leibler had the worst performance in terms of AMP albeit the fact that Manhattan had
higher AMP when NF was in [10-100]. When NF was in the range [50-200], Bhattacharya
followed by kullback—Leibler were seen to have the worst AMP values. As NF grew, Bhat-
tacharya started to have better performance over Manhattan, though. Similarly, Euclid-
ean outweighed Jaccard as NF was in the range [10—350]. However, as NF grew, Jaccard
behaved better than Euclidean. Similar to Fig. 5b, Fig. 6b exhibited that PDSM, STB-SM

and Cosine had the best performance with PDSM and STB-SM being highly rivals.

 

 

 

 

 

 

 

 

 

 

 

-
Accuracy (Webkb) Accuracy (Webkb)
Mm Euclidean 0.800 b —— Cosine
Mmm Cosine — PDSM
mm Jaccard 0.775 — STB-SM
@@™ Bhattacharya
> @@™ Kullback Leibler 0.750
& mE Manhattan & 0.725
c mmm POSM ©
3 mmm STB-SM oD 9.700
U u
< < 0.675
0.650
0.625
0.600
10 50 100 200 350 3000 6000 33025 Avg 10 50 100 =200 350 3000 6000 33025 Avg
No. of Features No. of Features
Fig. 4 a Accuracy over all Measures on all NF values— Average results (K = 1-120; + 2) - Web-KB. b Accuracy
over competitive measures on all NF values— Average results (K= 1-120; + 2)-Web-kKB
XX
(—
a F Measure (Webkb) b F Measure (Webkb)
Mm Euclidean 0.75 ———~ Cosine
Mm Cosine —— PDSM
Mmm jaccard — STB-SM
mmm Bhattacharya 0.70
w @@™ Kullback Leibler wv
5 @@— Manhattan 5
a mmm PDSM tg 0-65
vo mmm STB-SM o
= =
uw “ 0.60
0.55
10 50 100 200 350 3000 6000 33025 Avg 10 50 100 200 350 3000 6000 33025 Avg
No. of Features
No. of Features
Fig. 5 aF-measure over all Measures on all NF values—average results (K = 1-120; +2)-Web-KB. B F-measure
over competitive measures on all NF values— Average results (K = 1-120; +2)—Web-KBB F-measure over
competitive measures on all NF values— Average results (K= 1-120; +2)—Web-kKB

 

NS

 

Page 32 of 43
Amer and Abdalla J Big Data (2020) 7:74 Page 33 of 43

 

 

 

 

 

 

 

 

 

cr \
a Average Precision Mean (Webkb) b Average Precision Mean (Webkb)
Mm Euclidean 0.65 4 —— Cosine
c MMM Cosine c — PDSM
® Mmm jaccard ® — STB-SM
= @™™ Bhattacharya = 0.604
5 M@™™ Kullback Leibler §
a @™ Manhattan a
Oo Mmmm PDSM YU 0.554
2 mmm STB-SM e
Qa a
wo ov |
> S 0.50
oO o
Z a 0.45
10 50 100 200 350 3000 6000 33025 Avg 10 50-100 ‘io. ‘Fe ture. 6000 33025 Avg
No. of Features 0. oF Features
Fig.6 a AMP over all Measures on all NF values—average results (K= 1-120; +2)—Web-KB. B AMP over
competitive measures on all NF values—Average results (K= 1-120; +2)-Web-KB
we J)

 

Classification-execution time analysis

Finally, the time consumed by each measure on each dataset over each NF was accu-
mulated and averaged to show which one runs the fastest and which one runs the slow-
est. A certain measure could give higher accuracy and desired performance while it ran
slower compared with others and vice versa. The next Figures map the time taken by
each measure to produce the results. According to execution time drawn in Figs. 7, 8, it
is abundantly clear that all measures share one fact: the execution time is growing stead-
ily as NF increases, PDSM in particular. It is worth mentioning that time was calculated
as the similarity measure run on all evaluation metrics (six metrics) of classification.

Figure 7 clearly shows that Bhattacharyya and Manhattan were the fastest similar-
ity measures with Manhattan being much faster on all features. However, this came at
the expense of the drawn-above results by both measures as it occupied the second and
third-worst measures after kullback—Leibler. Euclidean had been observed to be the
middle ground in terms of speed between the first group (Bhattacharyya, Manhattan)
and the second group (PDSM, kullback—Leibler, Jaccard, Jaccard, Cosine, and STB-SM).
Surprisingly, when all features addressed, Manhattan was the fastest measure and PDSM
was the slowest measure as it took roughly 1493.85 min on Reuters when NF =all fea-
tures. On the other hand, closer to Bhattacharyya, Euclidean recorded worse results
when compared with Cosine, Jaccard, STB-SM, and PDSM. Jaccard, on the other hand,
was faster than Cosine and STB-SM, and Cosine was slower than STB-SM. In order,
PDSM, Jaccard followed by Cosine, and STB-SM were all observed to be the slower
measures comparing with the first group, though. In fact, PDSM has been seen to be the
slowest measure.

Similarly to Fig. 7, 8 clearly shows that Bhattacharyya and Manhattan were also the
fastest similarity measure with Manhattan being subtly faster on all features. However,
similar to Reuters, this came at the expense of the drawn-above results by both measures
as it occupied the second and third-worst measure after kullback—Leibler. Euclidean, on
the other hand, had been observed to be the fastest metric when all features considered,
and PDSM was seen to be the slowest ever as it took almost 1001.067 min on all fea-
tures—Web-KB. However, like Bhattacharyya, Euclidean recorded to have worse results
compared with Cosine, Jaccard, PDSM, and STB-SM. Meanwhile, Cosine was faster
than Jaccard and STB-SM in all NF cases except for the case in which all features were
addressed. In this case, Jaccard was faster than both Cosine and STB-SM. In general, in
order, PDSM, kullback—Leibler, Jaccard, STB-SM, and Cosine were the slowest measures
with PDSM being the slowest ever.
Amer and Abdalla J Big Data

 

 

 

 

 

(2020) 7:74
cr >)
Time (Reuters)
—— Euclidean
10° — Cosine
— Jaccard
—— Bhattacharya
F —— kullback Leibler
£ —— Manhattan
= 102 — PDSM
= — STB-SM
oO
£
10!
10 50 100 200 350 3000 6000 18308
No. of Features
Fig. 7 Execution Time-—Reuters
XM 7
cr >)
Time (Webkb)
10° —— Euclidean
— Cosine
— Jaccard
—— Bhattacharya
> — kullback Leibler
£ 10? — Manhattan
£ — PDSM
= — STB-SM
Vv
£
—
10?
10 50 100 200 350 3000 6000 33025
No. of Features
Fig. 8 Execution Time—Web-kB
NX

 

 

 

Clustering analysis
Based on the results drawn in Tables 14, 15, 16, 17, 18, the analysis is done briefly in
Table (20) across counting the points each similarity measure had achieved on each met-
ric. The point is counted for measure if it is being bolded as higher value, in Tables 14,
15, 16, 17, 18. The total number of points are 20 points as we have two datasets and
five metrics on two values of clustering variable (K=4, K=8). In each Table, there has
been four points each measure could achieve based on the drawn results. For example
Euclidean in Table 4 got 4 points as its results are spotted as top values for purity metric
on both datasets on both K (4 and 8). The next Table draws the points recorded for each
measure on each metric (Tables 14, 15, 16, 17, 18), and the points in total and rank as
well. The bolded values in Table 22 suggest the highest values in Table 20, which reflect
the optimality of each measures on the corresponding metric.

In general STB-SM behaves better than PDSM on web-kb clustering chiefly when
k grows. That means STB-SM could work optimally on big data, and STB-SM enjoys
the scalability properties. The scalability case is the case in which dataset grow larger

Page 34 of 43
Amer and Abdalla J Big Data (2020) 7:74 Page 35 of 43

and larger in terms of data, for example. Ironically, unlike classification, PDSM
works better than STB-SM on Reuters, though. Briefly, according to Tables (14, 15,
16, 17, 18), the order of top performer measures on Reuters was: Euclidean PDSM,
Cosine and STB-SM, On the other extreme, the order of top performer measures on
Web-KB was: STB-SM, Cosine, Euclidean and PDSM. Strictly speaking, the compe-
tition process is fiercely held between STB-SM, Euclidean, Cosine and PDSM with
STB-SM being maximally superior. In other words, according to the real numbers
drawn in results (Tables 14, 15, 16, 17, 18), STB_SM has better values than all meas-
ures in most cases. For example, in purity web-kb, completeness and Rand Index of
both datsets, the result values of STB-SM are much bigger than those values of other
measures. Thus, it can be confidently said that STB-SM outperformed all similarity

measures significantly in most cases of clustering evaluation metrics.

Table 20 Rank of similarity measures based on clustering results

 

 

Measure/table Purity Completeness Randindex  -Calinski- Davies- Point Total Rank
Harabasz Bouldin out of 20
Index Index
Euclidean 4 2 2 0 2 10 2
Cosine 3 2 3 1 0 9 4
Jaccard 0 0 0 4 0 4 7
Bhattacharya 1 3 1 0 1 6 6
kullback-Leibler 0 0 0 4 4 8 5
Manhattan 0 0 0 0 4 4 7
PDSM 2 2 2 2 1 9 3
STB-SM 2 4 4 ] 0 17 /

 

Italic values indicate the high-ranking similarity measure

Clustering—execution time analysis

Based on the time drawn in Tables (21, 22), PDSM was the slowest measure and Manhat-
tan was the fastest measure. As given in Tables (14, 15, 16, 17, 18; 21, 22), our proposed
STB-SM measure came as a compromised solution for both efficiency and effectiveness.
It is worth referring that the clustering time has been calculated while K-mean run on
nine evaluation metrics. However, in this work, we just used five metrics. So, this time
(drawn time in Tables 21, 22) would be shorter (either slightly or significantly) than the
expected time when the K-means is running on only these five metrics. That is because
adding each metric in clustering often takes extra time, and consequently increase clus-
tering time either slight or significantly. Nevertheless, this claim has not refuted or
contradicted the final conclusion drawn in this paper on the speed or slowness of each

similarity measure.

Table 21 Reuters-run time in (hour: minute: second)

 

 

K/measure Euclidean Cosine Jaccard Bhattacharya kullback- Manhattan PDSM_ STB-SM
Leibler
=4 0:38:18 0:9:06 0:8:04 0:07:56 0:13:13 0:7:04 2:26:14 0:10:18

K=8 0:15:03 0:16:37 0:15:00 0:15:05 0:25:04 0:12:59 4:44:16 0:41:58

 
Amer and Abdalla J Big Data (2020) 7:74 Page 36 of 43

Table 22 Web-KB - Run Time in (Hour: Minute: Second)

 

K/Measure Euclidean Cosine Jaccard Bhattacharya kullback-Leibler Manhattan PDSM STB-SM

 

K=4 0:6:36 0:7:26  0:7:01 6:42.98 0:11:14 0:5:55 2:33:09 0:8:54
K=8 0:12:09 0:13:50 0:12:40 0:12:22 0:21:08 0:10:41 6:21:26 0:17:10

 

The applicability of proposed measure (STB-SM) on big data environment

Since the advent of Internet, the size of textual information keeps growing because of
the continuous evolution of information technologies. These technologies have allowed
massive volumes of data to be exponentially increasing across the online contents like
all kinds of webpages (academic, scientific, news, medical, etc.), blogs, social network-
ing like Facebook and twitter, and Youtube. In daily basis, trillions of bytes of data are
generated that 90% of data in the world was thought to be existed in last couple of years
[34, 35]. Consequently, this fast growing of data volumes has led to a critical informa-
tion retrieval problems. Among these problems is how to get the relevant document(s)
of interest amid such gigantic volumes of textual data and information. To solve such
problem, the clustering as a data mining technique come for analyzing these massive
volumes of data which is called “Big Data’ Without the clustering and classification, it
is challenging to manage and discover the knowledge in the environment of big data.
However, there have been difficulties for implementing clustering algorithms to big data
as clustering algorithms accompanied with high computational costs and complexity. To
make it worse, recently, emanation of big data (with all its characteristics including vol-
umes, variety, velocity, variability and complexity) draws more difficulties to this issue
which pushes more studies and research to find every possible way to improve clustering
algorithms.

That lead us to the question of how to overcome this dilemma, and how to apply clus-
tering algorithms to big data while obtaining the results in a reasonable time. One pos-
sible solution to improve the performance of clustering to get results of higher accuracy
in reasonable time is to use the well-designed time-efficient similarity measure. In fact,
the performance of clustering and classification is maximally dependent on the similarity
measure in use as we have seen in this work in PDSM and STB-SM cases. Despite the fact
that both measures are effective, PDSM is seen time-inefficient chiefly when used to clus-
tering purpose. Unlike PDSM, STB-SM is time-efficient making it a promising measure for
scalability of clustering.

Presently, similarity measure have been sought to mainly promote the accuracy of
classification and clustering as well as the efficiency with the intended techniques like
KNN classifier and k-means clustering algorithm. Therefore, in this work, we proposed
a similarity measure which is thought to be capable of handling the big data analysis
effectively and efficiently. Based on the results drawn for both classification and cluster-
ing in particular, we believe that our proposed measure (STB-SM) is promising to be
an effective technique to process voluminous data in reasonable time with higher accu-
racy. When we applied STB-SM on all features of each dataset to perform clustering,
STB-SM drew highly competitive results in a reasonable time comparing with all state of
art. That means STB-SM enjoys it is being significantly effective and maximally efficient

and would add a valued-contribution to the field of information retrieval (which is vital
Amer and Abdalla J Big Data (2020) 7:74 Page 37 of 43

part of big data) in particular and machine learning in general. In fact, while designing
STB-SM, our focus has been on drawing the measure that would help scale up and expe-
dite clustering algorithm without sacrificing results quality. In doing so, the clustering
process will enjoy flexibility and provide faster response time at the same time. In other
words, with the proposed measure (STB-SM) being effective and efficient, the cluster-
ing for big data (including document clustering) can be efficaciously implemented to
enhance the speed of search, precision, recall, search engines, and so on.

Conclusions and future work

Using the BoW model, KNN classifier, and K-means algorithm, in the context of text
classification and clustering, this paper introduces a new similarity measure that is based
on the set theory mechanism and is named the STB-SM. Besides the STB-SM, a compar-
ative study has thoroughly been carried out on seven similarity measures using six clas-
sification criterions and five clustering metrics. The obtained results demonstrated that
STB-SM similarity measure achieved almost the best performance on all classification
and clustering criterions on both datasets (Reuters-21 or Web-KB). Moreover, to stress
proposed measure superiority, it was imperative to utilize more than one performance
criterion to effectively assess all similarity measures. In fact, it was difficult to deter-
mine which measure was the optimal one for any dataset and/evaluation criterion unless
they are all evaluated against each other comprehensively. Because of that, each data-
set displayed different characteristics when classification or clustering were performed
on them. Nonetheless, from the obtained results, it can be concluded that STB-SM,
PDSM, Cosine, and Jaccard showed superiority over other measures, and obtained the
most stable performance trends on both datasets for all K values, compared to Euclid-
ean, Manhattan, and kullback—Leibler measures with Manhattan and kullback—Leibler
being noted to have the worst results. On the other extreme, Euclidean and Bhattacharya
had a fluctuating performance which can be classified as a middle-ground between high
performance and poor performance measures.

Additionally, using the K-means clustering algorithm, all similarity measures were
involved in a fierce clustering competition. All similarity measures were individually
used to evaluate K-means performance with respect to five evaluation metrics from
which three metrics are external and the last two are internal metrics. The STB-SM,
PDSM, and Euclidean were observed to be the top performers in terms of cluster-
ing. The STB-SM has outperformed Euclidean and PDSM in most stages of evaluation
metrics. It worth mentioning that all these results of clustering were collected and
analyzed for the case in which the number of clusters K is taken as number of actual
classes in both datasets (4 and 8). Thus, in the follow-up work, to avoid and biased-
ness and get a deeper insight into clustering performance, an exhaustive analysis with
several K values on different clustering algorithms will be carried out.

All these measures were rigorously examined with regard to their execution time
when classification and clustering are run on either dataset. For classification, results
has shown that some measures met the highest speed but at the expense of their
overall performance, such as the Bhattacharyya, Manhattan, and Euclidean. On the
other hand, and to confirm the fact that the trade-off is un-escapable, PDSM had been

able to achieve better effectiveness results but again at the expense of its efficiency as
Amer and Abdalla J Big Data (2020) 7:74 Page 38 of 43

this measure in particular was the slowest measure. Nevertheless, the STB-SM, Jac-
card, and Cosine measures were a suitable compromised solution between the fast-
est measures (the Bhattacharyya, Manhattan, and Euclidean) and the slowest measure
(the PDSM). They were not only faster than the PDSM but they were also closer to the
speed of the fastest measures. On the other hand, for clustering, the PDSM was also
the slowest measure and Manhattan was the fastest measure. As a compromised solu-
tion for both effectiveness and efficiency on both the classification and the clustering,
our proposed measure the STB-SM has shown superiority with regard to clustering as
well as classification. Finally, this work briefly described the applicability of the STB-
SM to big data scenarios. In the future work, we plan to broaden the current work
to involve more state-of-the-art measures such as that described in [3, 4]. Moreover,
the behavior of all these measures will thoroughly be examined on different machine

learning tasks such as text summarization [36] and plagiarism detection.

Abbreviations
IR: Information Retrieval; NF: Number of features; ACC: Accuracy; PRE: Precision; REC: Recall; FM: F measure; GM: G Meas-
ure; AMP: Average mean precision.

Acknowledgements

The authors would like to sincerely express their thanks to both Prof Dr. Loc Nguyen (Loc Nguyen’s Academic Network)
and Prof Dr. Aodelmoneim Artoli (King Saud University) for their constructive suggestions while doing this work. The
authors would like to heartily extend their eternal thanks to both the Journal of Big Data Team (Editors, in particular)
along with those respected anonymous reviewers for their valuable comments and directions that otherwise this work
wouldn't have been improved.

Authors’ contributions

Both authors have been the key contributors in conception and design, implementing the approach and analyzing
results of all experiments, and the preparation, writing and revising the manuscript. Both authors read and approved the
final manuscript.

Authors’ information

Ali A. Amer is an assistant professor in Computer Science Department at Taiz University (YEMEN). He has been publish-
ing many research papers in highly-ranked and top-tier journals as well as refereed International conferences. He has
also acted as a reviewer for many top-venue platforms. Of these platforms, he has published in, and reviewed for: ACM
Computing Surveys, IEEE Access, Computer in Human Behavior, Journal of Big Data (Springer), International Journal on
Semantic Web and Information Systems, Universal Computer Science Journal, Journal of Supercomputing, Heliyon, and
Journal of Evolutionary Intelligence, to name a few. His primary interest of research falls into: Information Systems, Data-
base, Distributed and Parallel Database Systems, Data Integration, Data Mining, Network, and Information Retrieval.

Hassan Abdalla is an Associate Professor of Information Systems at the College of Technological Innovation since 2018.
He holds a PhD in Information Systems from London, UK. Prior to joining Zayed University, Dr. Abdalla who is an Oracle
Certified Professional has worked as an associate professor at the College of Computer and Information Sciences at King
Saud University (KSU), Riyadh, Saudi Arabia. He also served there as a head of Quality Unit. Dr. Abdalla has published his
research work in many reputable refereed international journals and conferences, he has also served as a reviewer for
many top-ranked Journals. Dr. Abdalla’s research interests include Distributed database systems, Information Retrieval,
Knowledge Management and Enterprise Computing.

Funding
This research has been supported by Research Incentive Fund (RIF) Grant Activity Code: R19093—Zayed University, UAE.

Availability of data and materials
Datasets used in this work is publically available, and code is being uploaded on GitHub (https://github.com/aliamer/
Information-Retrieval---A-Set-Theory-Based-Similarity-Measure-for-Text-Clustering-and-Classification).

Competing interests
The authors declare that they have no competing interests.

Author details
' Computer Science Department, Taiz University, Taiz, Yemen. * College of Technological Innovation, Zayed University, PO.
Box 144534, Abu Dhabi, UAE.

Appendix
Appendix 1
Amer and Abdalla J Big Data (2020) 7:74 Page 39 of 43

The code is now publically available on GitHub.

Appendix 2

In order for readers/researchers to absorb the idea of this work, we just provide a sam-
ple for accuracy Tables of STB-SM and PDSM similarity measures that would be simi-
larly produced for all similarity measures when the code of this work is used.

See Tables 23, 24

Table 23 STB-SM Accuracy when NF = 6000 on both datasets

 

No. of features Metric Measure k Reuters-8 Web-Kb

 

6000 STB_SM Accuracy 0.9255733448723497 0.7456418383518225

 

N WO Ww —

13
15
17
19
2]
23
25
27
29
31
33
35
37
39
4]
43
45
47
49
5]
53
55
57
59
6]
63
65
67
69
7]
73

0.9363911726525314
0.9333621808740805
0.9316313284292514
0.9294677628732151
0.927736910428386

0.9303331890956296
0.9264387710947641
0.9264387710947641
0.927736910428386

0.927736910428386

0.9255733448723497
0.9242752055387278
0.9273041973171787
0.9242752055387278
0.9255733448723497
0.9242752055387278
0.9251406317611424
0.9247079186499351
0.9225443530938987
0.9212462137602769
0.9225443530938987
0.9212462137602769
0.92081 35006490696
0.92081 35006490696
0.9186499350930333
0.9173517957594115
0.9177845088706188
0.9182172219818261
0.916053656425 7897
0.9151882302033751
0.9130246646473388
0.9138900908697534
0.9134573777585461
0.9134573777585461
0.9130246646473388
0.9121592384249243

0.777337559429477

0.786053882725832

0.7868462757527733
0.7828843 106180665
0.78843 10618066561
0.79397781 29952456
0.7995 245641838352
0.7995 245641838352
0.8042789223454834
0.805863708399366

0.8082408874801 902
0.7995 245641838352
0.803486529318542

0.8019017432646592
0.8066561014263075
0.8019017432646592
0.7995 245641838352
0.8019017432646592
0.8019017432646592
0.803486529318542

0.8019017432646592
0.802694 1362916006
0.801 1093502377179
0.7971473851030111
0.801 1093502377179
0.8003 169572107766
0.8003 169572107766
0.801 1093502377179
0.7971473851030111
0.7971473851030111
0.794770206022187

0.7955625990491 284
0.792393026941 3629
0.7908082408874801
0.7908082408874801
0.7876386687797148
 

 

Amer and Abdalla J Big Data (2020) 7:74
Table 23 (continued)
No. of features Metric Measure k Reuters-8 Web-Kb
75 0.9112938122025097 0.78843 10618066561
// 0.911726525313717 0.7908082408874801
79 0.911726525313717 0.792393026941 3629
81 0.9086975335352662 0.7900158478605388
83 0.9095629597576806 0.7892234548335975
85 0.9086975335352662 0.7892234548335975
87 0.9086975335352662 0.7876386687797148
89 0.90696668 1090437 0.7876386687797148
91 0.9078321073128516 0.786053882725832
93 0.9073993942016443 0.78446909667 19493
95 0.9065339679792298 0.786053882725832
97 0.90696668 1090437 0.783676703645008
99 0.9095629597576806 0.783676703645008
101 0.9065339679792298 0.78446909667 19493
103 0.9061012548680225 0.7852614896988906
105 0.9048031155344007 0.783676703645008
107 0.9030722630895716 0.78446909667 19493
109 0.9043704024231934 0.7852614896988906
111 0.9026395499783644 0.7876386687797148
113 0.902206836867 1571 0.78843 10618066561
115 0.902206836867 1571 0.7828843 106180665
117 0.900475984422328 0.7797147385103012
119 0.8991778450887062 0.7828843 106180665

Average

0.9163781912591951

0.792247754886423

 

Page 40 of 43
Amer and Abdalla J Big Data

(2020) 7:74

Table 24 PDSM Accuracy when NF = 6000 on both datasets

 

 

No. of features Metric Measure k Reuters-8 Web-Kb

6000 PSDM Accuracy 1 0.9381220250973604 0.7955625990491 284
3 0.938987451319775 0.8335974643423137
5 0.9394201644309823 0.824881 1410459588
7 0.9376893119861531 0.8335974643423137
9 0.9398528775421895 0.8280507131537242
11 0.9376893119861531 0.8312202852614897
13 0.9368238857637387 0.8280507131537242
15 0.9350930333189096 0.8240887480190174
17 0.93249675465 16659 0.8264659270998416
19 0.930333 1890956296 0.8193343898573693
21 0.9294677628732151 0.8225039619651348
23 0.92687 14842059714 0.8232963549920761
25 0.926006057983557 0.820919175911252
27 0.9268714842059714 0.8177496038034865
29 0.92600605 7983557 0.8169572107765452
31 0.92643877 10947641 0.8137876386687797
33 0.922977066205106 0.812202852614897
35 0.9199480744266552 0.8090332805071315
37 0.9186499350930333 0.8050713153724247
39 0.9169190826482042 0.8082408874801902
A] 0.9186499350930333 0.8066561014263075
43 0.9160536564257897 0.8042789223454834
45 0.9138900908697534 0.8050713153724247
47 0.9156209433145824 0.8019017432646592
49 0.9169190826482042 0.8019017432646592
51 0.9156209433145824 0.8026941 362916006
53 0.9125919515361316 0.7979397781 299524
55 0.9104283859800952 0.7987321711568938
57 0.9091302466464734 0.7987321711568938
59 0.9095629597576806 0.7979397781 299524
61 0.9099956728688879 0.7971473851030111
63 0.9086975335352662 0.7987321711568938
65 0.9086975335352662 0.7963549920760697
67 0.9086975335352662 0.7963549920760697
69 0.9073993942016443 0.7955625990491 284

Page 41 of 43
Amer and Abdalla J Big Data

(2020) 7:74

Table 24 (continued)

 

 

No. of features Metric Measure k Reuters-8 Web-Kb

7\ 0.9052358286456079 0.7955625990491 284
73 0.9039376893119861 0.794770206022187

75 0.9026395499783644 0.7979397781 299524
7/ 0.9026395499783644 0.794770206022187

79 0.9022068368671571 0.7931854 199683043
81 0.900475984422328 0.7955625990491 284
83 0.900475984422328 0.792393026941 3629
85 0.900475984422328 0.792393026941 3629
87 0.89961055819991 34 0.7916006339144216
89 0.9000432713111207 0.7916006339144216
91 0.8974469926438771 0.7876386687797148
93 0.8978797057550844 0.78843 10618066561
95 0.8970142795326699 0.7876386687797148
97 0.8961488533102553 0.7876386687797148
99 0.8965815664214626 0.7868462757527733
101 0.8948507139766335 0.78843 10618066561
103 0.8970142795326699 0.7900158478605388
105 0.8952834270878408 0.7868462757527733
107 0.8952834270878408 0.7876386687797148
109 0.8935525746430116 0.7852614896988906
111 0.892687 1484205972 0.786053882725832

113 0.8922544353093899 0.78446909667 19493
115 0.8909562959757681 0.78446909667 19493
117 0.889225443530939 0.783676703645008

119 0.8887927304197317 0.7797147385103012
Average 0.9120222126063754 0.8021526677231899

 

Received: 24 April 2020 Accepted: 14 August 2020
Published online: 14 September 2020

References

1,

Alvarez, J.E. and H. Bast, A review of word embedding and document similarity algorithms applied to academic text.

Bachelor thesis, 2017.

Oghbaie M, Zanjireh MM. Pairwise document similarity measure based on present term set. J Big Data. 2018;5(1):52.
Sohangir S, Wang D. Improved sqrt-Cosine similarity measurement. J Big Data. 2017;4(1):25.

Lin Y-S, Jiang J-Y, Lee S-J. A similarity measure for text classification and clustering. IEEE Trans Knowl Data Eng.

2013;26(7):1575-90.

Page 42 of 43
Amer and Abdalla J Big Data (2020) 7:74 Page 43 of 43

19,
20.

21.
22.
23.
24.
25,
26.
2/.
28.
29,
30.
31.
32.

33.
34.

35.
36.

Xu S. Bayesian Naive Bayes classifiers to text classification. J Inform Sci. 2018;44(1):48-59.

Sheydaei N, Saraee M, Shahgholian A. A novel feature selection method for text classification using association rules
and clustering. J Inform Sci. 2015;41(1):3-15.

Subhashini R, Kumar VJ. Evaluating the performance of similarity measures used in document clustering and
information retrieval. In: 1st Int Conf integrated intelligent computing, Bangalore, 2010, p. 27-31. https://doi.
org/10.1109/iciic.20https://doi.org/10.42.

Amer AA. On K-means clustering-based approach for DDBSs design. J Big Data. 2020;7(1):1-31.

Amer AA, Mohamed MH, Asri K. ASGOP: An aggregated similarity-based greedy-oriented approach for relational
DDBSs design. Heliyon. 2020;6(1):e03172.

Nguyen L, Amer AA. Advanced cosine measures for collaborative filtering. Adapt Personalization (ADP).
2019;1:21-41.

. Shahmirzadi O, Lugowski A, Younge K. Text similarity in vector soace models: a comparative study. In 2019 18th IEEE

International Conference On Machine Learning And Applications (ICMLA). 2019. IEEE.
Strehl A, Ghosh J, Mooney R. Impact of similarity measures on web-page clustering. In Workshop on artificial intel-
ligence for web search (AAAI 2000). 2000.

. White RW, Jose JM. A study of topic similarity measures. In Proceedings of the 27th annual international ACM SIGIR

conference on Research and development in information retrieval. 2004.

Huang A. Similarity measures for text document clustering. In Proceedings of the sixth New Zealand computer sci-
ence research student conference (NZCSRSC2008), Christchurch, New Zealand. 2008.

Forsyth RS, Sharoff S. Document dissimilarity within and across languages: a benchmarking study. Literary Linguistic
Comput. 2014;29(1):6-22.

Thompson VU, Panchev C, Oakes M. Performance evaluation of similarity measures on similar and dissimilar text
retrieval. In 2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowl-
edge Management (IC3K). IEEE. 2015.

Fahad A, et al. A survey of clustering algorithms for big data: taxonomy and empirical analysis. IEEE Trans Emerg Top-
ics Comput. 2014;2(3):267-79.

Aslam JA, Frost M. An information-theoretic measure for document similarity. In: Proc 26th SIGIR, Toronto. 2003. p.
449-50.

Zhao Y. R and data mining: examples and case studies. Cambridge: Academic Press; 2012.

Tata S, Patel JM. Estimating the selectivity of tf-idf based Cosine similarity predicates. ACM Sigmod Record.
2007;36(2):7-12.

Bhattacharyya A. On a measure of divergence between two statistical populations defined by their probability
distributions. Bull Calcutta Math Soc. 1943;35:99-109.

Schoenharl TW, Madey G. Evaluation of measurement techniques for the validation of agent-based simulations
against streaming data. In International Conference on Computational Science. 2008. Springer.

Kullback S, Leibler RA. On information and sufficiency. Ann Math Stat. 1951;22(1):79-86.

Kullback S. Information theory and statistics Wiley. New York, 1959.

Jaccard P. The distribution of the flora in the alpine zone. 1. New phytologist, 1912. 11(2): p. 37-50.

Turney PD, Pantel P. From frequency to meaning: vector space models of semantics. J Artificial Intell Res.
2010;37:141-88.

Al-Ghuribi SM, Alshomrani S. A simple study of webpage text classification algorithms for Arabic and English Lan-
guages. In 2013 International Conference on IT Convergence and Security (ICITCS). 2013. IEEE.

Patil DB, Dongre YV. A fuzzy approach for text mining. J Math Sci Comput. 2015;4:34-43.

Salton G, Buckley C. Term-weighting approaches in automatic text retrieval. Inf Process Manage. 1988;24(5):5 13-23.
Jabalameli, M., A. Arman, and M. Nematbakhsh, Improving the efficiency of term weighting in set of dynamic docu-
ments. 2015. International Journal of Modern Education and Computer Science, 7, 42-47,

Aggarwal CC, Zhai C. A survey of text classification algorithms, in mining text data. Boston: Springer; 2012. p.
163-222.

Lakshmi R, Baskar S. DIC-DOC-K-means: dissimilarity-based Initial Centroid selection for DOCument clustering using
k-means for improving the effectiveness of text document clustering. J Inform Sci. 2019;45(6):818-32.

Xu R, Wunsch D. Survey of clustering algorithms. IEEE Trans Neural Networks. 2005;16(3):645-78.

Khadija A. Almohsen, Huda Al-Jobori, “Recommender Systems in Light of Big Data’, International Journal of Electrical and
Computer Engineering (ECE), Vol. 5, No. 6, December 2015, pp. 1553-1563, 2015.

Hoad TC, Zobel J. Methods for identifying versioned and plagiarized documents. JASIST. 2003;54:203-15.

Nagwani NK. Summarizing large text collection using topic modelling and clustering based on MapReduce framework.
J Big Data. 2015;2:6.

 

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
