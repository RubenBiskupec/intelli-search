Journal of Systems Architecture 111 (2020) 101831

 

Contents lists available at ScienceDirect aaa
SOFTWARE

WS

    

Journal of Systems Architecture

 

journal homepage: www.elsevier.com/locate/sysarc

ELSEVIER

 

Research paper

ESSA: An energy-Aware bit-Serial streaming deep convolutional neural ®

Check for

network accelerator oa

 

Lien-Chih Hsu, Ching-Te Chiu*, Kuan-Ting Lin, Hsing-Huan Chou, Yen-Yu Pu

Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan

ARTICLE INFO ABSTRACT

Over the past decade, deep convolutional neural networks (CNN) have been widely embraced in various visual
recognition applications owing to their extraordinary accuracy. However, their high computational complexity
and excessive data storage present two challenges when designing CNN hardware. In this paper, we propose an
energy-aware bit-serial streaming deep CNN accelerator to tackle these challenges. Using ring streaming dataflow
and the output reuse strategy to decrease data access, the amount of external DRAM access for the convolutional
layers is reduced by 357.26x when compared with that of no output reuse case on AlexNet. We optimize the
hardware utilization and avoid unnecessary computations using the loop tiling technique and by mapping the
strides of the convolutional layers to unit-ones for computational performance enhancement. In addition, the
bit-serial processing element (PE) is designed to use fewer bits in weights, which can reduce both the amount
of computation and external memory access. We evaluate our design using the well-known roofline model. The
design space is explored to find the solution with the best computational performance and communication to
computation (CTC) ratio. We can reach 1.36x speed and reduce energy consumption by 41% for external memory
access compared with the design in [1]. The hardware implementation for our PE Array architecture design can
reach an operating frequency of 119 MHz and consumes 68 k gates with a power consumption of 10.08 mW using
TSMC 90-nm technology. Compared to the 15.4 MB external memory access for Eyeriss [2] on the convolutional
layers of AlexNet, our method only requires 4.36 MB of external memory access to dramatically reduce the

Keywords:

Convolutional neural networks (CNNs)
Hardware accelerator

Energy-aware

Precision

Bit-Serial PE

Streaming dataflow

costliest portion of power consumption.

1. Introduction

Over the past decade, deep convolutional neural networks (CNNs)
have been widely embraced in various applications, such as visual recog-
nition [3-7], object detection [8-12], and semantic segmentation [13-
15]. Furthermore, CNNs have achieved extraordinary accuracy in these
fields, surpassing that of human beings. However, the high computa-
tional complexity and excessive data storage required by CNNs present
important challenges for both computational performance and energy
efficiency. Although the existence of graphics processing units (GPU)
can remedy high computational complexity, they also operate with high
energy consumption.

This characteristic has pushed researchers toward dedicated CNN
accelerator designs, with many methods proposed for acceleration
[1,2,16-22]. Chen et al. [1] optimized both computation and commu-
nication and the roofline model [23] for evaluation. While Chen et al.
[20] make energy optimization, showing that using 16-bit fixed-point
representation only incurred a 0.26% greater error rate. In [24], Sakr
et al. demonstrated that precision can be further reduced under the lim-

* Corresponding author.
E-mail address: chiusms@cs.nthu.edu.tw (C.-T. Chiu).

https://doi.org/10.1016/j.sysarc.2020.101831

itation of 1% accuracy degradation for visual recognition, which can be
exploited in CNN accelerator designs.

On one hand, to address high computational complexity, the hard-
ware units of CNN accelerators must be fully utilized. In addition, they
must reduce latencies during computation cycles to avoid unnecessary
waste. Several approaches have been used in previous CNN accelerator
designs from different perspectives. In [16], Lin et al. optimized hard-
ware utilization using the runtime configurations of various kernel sizes.
In [1], Chen et al. used the loop tiling strategy for parallelism of data
processing to leverage the maximum number of hardware units.

On the other hand, the energy consumption of a CNN accelerator
includes both memory access and computation consumption. Memory
access includes multiple levels of memory hierarchies, such as external
DRAM access and SRAM access. Below, Fig. 1 shows the normalized
energy cost for 65-nm technology [25]. External DRAM access is the
most energy-consuming part of power consumption for CNN accelera-
tors. Thus, to achieve better energy efficiency, the amount of external
DRAM access must be reduced. In other words, data should be addressed
properly for improved data reuse efficiency.

Received 31 October 2018; Received in revised form 17 April 2020; Accepted 28 June 2020

Available online 3 July 2020

1383-7621/© 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.

(http://creativecommons.org/licenses/by-nc-nd/4.0/)
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

Normalized Energy Cost

   

ALU 1x (Reference)
6x
NT ALU ( 200~

Fig. 1. Normalized energy cost on 65-nm technology[25]..

To optimize both computational performance and energy efficiency,
we propose a CNN accelerator called ”ESSA”, which is an energy-aware
bit-serial streaming deep CNN accelerator with several features.

First, the bit reduction technique, inspired by Sakr et al. [24], and the
loop tiling approach are used for both computational performance en-
hancement and energy efficiency. In addition, the output reuse strategy
is exploited to reuse the data of the convolutional layers for increased
energy efficiency. Finally, mapping non-unit strides to unit strides is
leveraged to avoid unnecessary computation thereby enhancing com-
putational performance.

The main contributions of this paper are as follows:

1. We propose an energy-aware bit-serial streaming deep CNN acceler-
ator design to tackle issues related to computational complexity and
external memory access.

2. Using the roofline model for evaluation, we achieve 1.36x compu-
tational performance enhancement, reduce energy consumption by
41% for external memory access, and improve the CTC ratio for all
layers compared to Chen et al. [1] on AlexNet.

3. The implementation of the proposed PE Array architecture in 90-
nm technology reaches 178.24 GOPs with only 4.36 MB external
memory access on AlexNet.

The rest of this paper is organized as follows: in Section 2, we in-
troduce some prior studies relating to our work. In Section 3, we give
an overview of our proposed system, introducing our approaches for
both computational performance enhancement and energy efficiency.
In Section 4, we present the details of the proposed architecture and
dataflow. In Section 5, the theoretical evaluation and hardware imple-
mentation results with their comparison to other works are described.
In the final section, we present our conclusions.

2. Related works
2.1. CNN Basics

A typical CNN structure consists of three types of layers: the convo-
lutional layer, the pooling layer, and the fully connected layer. Fig. 2
shows a typical CNN structure.

Convolutional layer: The input feature map pixels convolve with the
kernel pixels to obtain the related output feature map pixels. As shown
in Table 1, we use R,, C;, and N to represent the numbers of the row,
column, and channel for the input feature map and R, C, and M to rep-
resent the numbers of the row, column and, channel for the output fea-

Table 1

Notations of the CNN.
Notation Definition

R, Row number of input feature map

C, Column number of input feature map

R Row number of output feature map

C Column number of output feature map

N Number of input feature map

M Number of output feature map

K Convolutional kernel size

S Stride for convolution

P Padding size

Journal of Systems Architecture 111 (2020) 101831

ture map. In addition, we use K to represent the numbers of the row
and column and N to represent the number of channels; M kernels were
included. Finally, we use S and P to represent the stride and padding of
convolution.

Pooling layer: The pooling layer is used for down-sampling. There are
two major types of pooling layers: average pooling and max-pooling.
Fig. 3(a) is an illustration of max-pooling.

Fully connected layer: The fully connected layer is generally used as
the classifier in the final stage of a CNN. The neurons in the former
layer are fully connected to the neurons in the latter layer, as shown in
Fig. 3(b).

2.2. Dataflow types

Modern CNN accelerators use different types of dataflow, which can
be divided into four main categories: pipeline-like dataflow [16,19,26],
Systolic-Array-like dataflow [27-29], DaDianNao-like dataflow [1,20-
22], and streaming-like dataflow [30-32]. Table 2 lists the summary
of four dataflow categories with their approaches for input, weights,
and partial sums; the benefits and limitations of each category are also
presented.

For each cycle of pipeline-like dataflow, the input pixels are broad-
cast to each PE and the weight pixels are fixed on each PE. Next, the
calculated partial sum is flown into the following PE. The advantage of
this dataflow type is that it is flexible for various kernel sizes; however,
it results in higher latency.

In each cycle of systolic-array-like dataflow, the input pixels and the
weight pixels flow sequentially into the PE. The calculated partial sums
are either added up by the adder tree or flow into vertical PE for ac-
cumulation. In this dataflow type, timing issues can be mitigated when
synthesizing a large design. However, it is not easy to find a feasible
mapping for CNNs to a systolic array.

In each cycle of DaDianNao-like dataflow, the input pixels are broad-
cast to each PE and the weight pixels are fixed on each PE. The calculated
partial sums are then added up by the adder tree. This dataflow type is
flexible for various kernel sizes; however, kernel level parallelization
(which can reduce power consumption) is not explored.

In each cycle of streaming-like dataflow, the input pixels flow into
the next PE and the weight pixels are fixed on each PE. The calculated
partial sums are added up by the adder tree. We can minimize the data
movement for this dataflow type, thereby achieving superior energy effi-
ciency. Some streaming-like dataflow [30,32] are not flexible for various
kernel size. However, Du, et al. [31] proposed a kernel decomposition
technique to solve the various kernel sized problem for streaming archi-
tecture.

Among the four dataflow types, we chose the streaming-like dataflow
owing to its lower external data movement, which produces better en-
ergy efficiency. To overcome the limitations of streaming-like dataflow,
we used kernel decomposition technique for kernel size flexibility. In
addition, we improved the dataflow to reduce latencies across the rows.

2.3. Bit reduction

Using fixed-point arithmetic units to compute CNN operations is
significantly more efficient than using floating point units. Sixteen-bit
fixed-point representations are widely used in modern CNN accelerators
[20-22,30,31]. Chen et al. [20] showed that using 16-bit fixed-point
rather than 32-bit floating point units only incurs a 0.26% greater error
rate. However, the bit-widths of the input and weight are uniform for
all layers.

In [24], the authors showed that less precision is required under the
constraints of 1% accuracy degradation. Moreover, per-layer precision
assignments can reach lower bit requirements than uniform precision
assignments (for all layers). In addition, a general trend of decreasing
precision requirements with layer depth was noted.
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

Convolution Pooling

   

Convolution

Journal of Systems Architecture 111 (2020) 101831

Pooling Fully Fully
Connected Connected

Output Predictions

dog (0.01)

thy (0.04)
boat (0.94)
bird (0.02)

Fig. 2. A typical CNN structure.

Former layer

 

 

 

 

(a) Max-pooling layer

Inspired by these experiments and the observations that precision
requirements vary across the layers of a CNN [24,33,34], Judd et al.
[35] implemented a hardware accelerator called Stripes, which exploits
the features of various precisions across CNN layers. In [36], the authors
not only make use of the properties of precision reduction, but also avoid
computations with zero product.

2.4. Approaches

FPGA Approaches: CNP [19] was the first CNN accelerator with the
FPGA design; however, it was difficult to scale up to higher performance
levels. NeuFlow [37] was motivated by the shortcomings of [24], reach-
ing higher scalability with its runtime reconfigurable dataflow. In [38],
nn-X extended the supported kernel size to 10 x 10, leading to lower
hardware utilization with smaller kernel sizes. In [1], the authors opti-
mized both computation and communication using the roofline model
and loop tiling strategy. In [30],a3 x 3 convolver design and dynamic
precision quantization technique were used to improve computational
performance.

Table 2

 

Fig. 3. The illustration of (a) max-pooling layer and
(b) fully connected layer.

Latter layer

(b) Fully Connected layer

ASIC Approaches: The DianNao series [20-22] leveraged the loop
tiling strategy and internal buffers for energy optimization. Among the
series, DaDianNao [21,22] used 36MB eDRAM to store all weights,
which can reduce off-chip communication traffic. In Eyeriss [2], the
authors proposed a row-stationary [39] dataflow and designed a spa-
tial array architecture for computational performance improvement and
energy consumption reduction. In [31], a streaming hardware accelera-
tor that avoided unnecessary data movement was proposed to improve
energy efficiency. In [16], the authors designed an end-to-end CNN ac-
celerator with runtime kernel size configuration to maximize hardware
utilization.

3. Proposed energy-aware system architecture and methods

In Section 3.1, we provide an overview of our proposed system ar-
chitecture. In Section 3.2, approaches for computational performance
enhancement are presented. In Section 3.3, techniques aimed at energy
efficiency are introduced. In Section 3.4, we analyze the kernel size dis-
tribution of popular CNNs and describe methods to support different
kernel sizes.

Summary of four dataflow categories with their approaches for input, weights and partial sums with the benefits and limitations of each category.

Category

Pipeline-like

Systolic-Array-like

DaDianNao-like

Streaming-like

Input

Broadcast the
pixel to each PE
Sequentially flow
into PE

Broadcast the
pixel to each PE
Flow into next PE

Weight

Fixed on each PE

Sequentially flow
into PE

Fixed on each PE

Fixed on each PE

Partial Sum

Flow into next PE
1. Added up by the adder
tree [27] 2. Flow into

vertical PE [28]
Added up by the adder tree

Added up by the adder tree

Benefits

Flexible for various kernel size
[16]

Mitigate timing issues in
synthesizing a large design

Flexible for various kernel size
1. Minimize data movement 2.

Achieve optimal energy
efficiency

Limitations

Higher latency

Not easy to find a feasible
mapping

Without kernel level
parallelization

Not flexible for various kernel
size
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

. CNN Accelerator

PE Array Units

Reuse
Strategy

 

* “Computational Performance Enhancement

Fig. 4. The system overview of the proposed CNN accelerator and methods.

3.1. System overview

Fig. 4 shows an overview of the proposed energy-aware system ar-
chitecture and the methods for optimization.

CNN accelerator optimization has two key features: computational
performance enhancement and energy efficiency. Computational per-
formance can be formulated as:

Computational Per f ormance (1)
=2x#of HW Units x HW Utilization x Frequency.

Each convolution operation includes a multiplication and an addi-
tion; thus, the factor 2 is required for computational performance. For a
fixed hardware architecture, # of HW units and the operating frequency
are constants, making the HW utilization key factor for computational

      
  
          
   

WSRAM
Weights (Kernels)

  

IFM SRAM

Poss

yo

    
  

-—-=

Kernel 2 2
I

Pe

K
K
Tn ¢
K

eee eee

Tm
K
Tn ¢
I

Kernel T,,

 

Journal of Systems Architecture 111 (2020) 101831

performance enhancement. As the result, we sought to maximize the
HW utilization.
Energy efficiency can be expressed as follows:

Energy Ef ficiency = Computational Per formance
Power (2)
__ Operations/Time __ Operations

Energy/Time Energy °

For a certain CNN, the number of computing operations is constant;
thus, Eq. 2 can be further expressed as:

 

Energy Ef ficiency « (3)

In addition, the most energy-consuming process is external DRAM
access, so we sought to eliminate DRAM access to significantly reduce
energy consumption.

1
Energy’

3.2. Computational performance enhancement

As discussed in Section 3.1, the key factor for improving computa-
tional performance is to maximize the HW utilization of the CNN accel-
erator. There are several methods to increase HW utilization, such as
parallel data processing and eliminating unnecessary computation. In
this paper, we introduce loop tiling and non-unit stride to unit stride
techniques to optimize the utilization of hardware units.

Another way to improve performance is to simply use fewer bits. By
designing dedicated hardware units to leverage the properties of using
fewer number of bits, we can further increase CNN computation speed.
We introduce concepts and strategies of bit reduction to exploit the fea-
ture of using less number of bits.

3.2.1. Loop tiling

The loop tiling technique is one type of data processing parallelism,
which aims to fully utilize hardware resources. Here, we propose two

Js

Output feature map

OFM SRAM
Output
feature map

  
  

Caled . "Cte : Cae e 2 :

  
  

R
Tal

Kernel T,,,

Fig. 5. Architecture of the proposed Overall Bit-Serial Streaming with PE Arrays Memory.
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

Per-Layer precision assignment

Gl Variable Precision emme Uniform Precision

10
: 9
7 7 7 7
: 6
° 5 5 5
_
4
3
2
1
0
L1 L2 L3 L4 LS L6 L7 L8

Fig. 6. The results of uniform precision and variable precision for AlexNet.
Here, L1-L5 represent convolutional layers, and L6-L8 represent fully connected
layers.

tiling factors, T,,, and T,,. T,, is the number of processed weights for each
occurrence, and T,, is the number of processed channels for input feature
maps and weights at each occurrence. As shown in Fig. 5, the colored
parts are the data in processing.

Different tiling factors will produce significantly different computa-
tional performance levels. A proper tiling factor can increase the effi-
ciency of data processing and data reuse; thus, the comprehensive ex-
ploration of different tiling factor pairs is needed; we present this design
space exploration in Section 5.1.

3.2.2. Bit reduction

As described in Section 2.3, according to Sakr et al. [24], satisfying
an accuracy degradation requirement of 1% in recognitions ensures that
most precision results are below the required uniform precision.

There are several effects of precision reduction. Computational per-
formance and energy efficiency can be improved owing to less external
data access; however, accuracy will decrease. Hence, there is a tradeoff
between computational performance, energy efficiency, and accuracy.
Where small accuracy losses can be tolerated, improvements for com-
putational performance and energy efficiency are available.

We ran our bit reduction experiment on AlexNet [3] with the ex-
tensive ImageNet [40] dataset using PyTorch. The input pixels were set
to 16-bit for all layers, and the weight pixels were variable for each
layer. Our strategy was to choose the lowest bit to prodcuce the preci-
sion degradation < 1% for each layer; we ran the experiment layer by
layer.

The proposed accelerator targets for inference applications. Indeed,
for inference, INT8 is usually enough and has been widely used in real

Kernel |Kernel| Kernel
(0,0) | (0,1) | (0,2

Kernel |Kernel|Ker
(1,

Kernel

ao | ea l|}Kernel |Kern

 

Journal of Systems Architecture 111 (2020) 101831

Table 3

The amount of external data access in AlexNet and VGG16. Baseline con-
tain Loop Tiling and Reuse strategy and Serial Dataflow is Baseline with
serial dataflow.

CNN type Method Precision External Data Access _— Ratio
AlexNet Baseline 16 Bits 4.36 MB 1x
AlexNet Baseline 8 Bits 1.84 MB 2.37X
AlexNet Serial Dataflow 8 Bits 1.75 MB 2.49x
VGG16 Baseline 16 Bits 121.58 MB 1x
VGG16 Baseline 8 Bits 76.29 MB 1.59x
VGG16 Serial Dataflow 8 Bits 58.2 MB 2.09x
Table 4

Top-1 Accuracy and decrease accuracy in AlexNet and
VGG16 in Serial Dataflow.

CNN type Precision Top-1 Accuracy Accuracy lose
AlexNet 16 Bits 55.58%

AlexNet 8 Bits 54.31% -1.27%
VGG16 16 Bits 70.15%

VGG16 8 Bits 69.96% —0.19%

world applications. Weight quantization is common adopted in CNN
quantization applications. The bit number for weights in each layers
are adjustable to utilize the bit-serial PE.

For our proposed bit reduction, the input pixels are set to 16-bit for
all layers, and the weight pixels are variable for each layer. The strategy
we use is to choose the lowest bit to make the degradation of precision
< 1% for each layer, and we do the experiment layer by layer. When
we adjust the weight bit number in a new layer, the total degradation of
precision < 1% including the new layer and all previous layers must be
< 1%. Therefore, the overall accuracy degradation of the whole CNN
model after bit reduction must be < 1%. Fig. 6 shows the results for
uniform and variable precision for each layer on AlexNet. These results
show that for most layers, the required precision levels were less than
the uniform precision. As shown in Fig. 6, the number of weight bits (B)
for each layer are 7, 9, 7, 6, and 5 in Alexnet. Most of them are less than 8
bits. In addition, precision requirements trended downward with layer
depth. Hence, this feature was useful for computational performance
enhancement and energy reduction.

We have already changed our design from 16-bit fixed point to 8-
bit fixed point. The proposed Process Element architecture is shown in
Fig. 12. In each Process element consist of 8 Sub-PE, and each Sub-PE is
responsible for the convolution of 1-bit weight and 8-bits input feature
map. With reduced weights and activation bit number, chip memory
cost and the data access between DRAM and internal memory are re-
duced as shown in Table 3 The accuracy is only reduced 0.19% and
1.27% in VGG16 and Alexnet as shown in Table 4.

Fig. 7. Kernel Decomposition of kernel size
5*5.

Kernel |Kernel

(3,1) | (3,2)

(3,4) Kernel |Kernel |Kernel
el| Kernel (4,0) | (4,1) | (4,2)

(4,0) | (4,1) | (42) | (4.3) | (44)
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

 

Ale
3|4
alee lee eae) leat le
Safe Flas | alsa
ae le ol als
a eal ea) ee anise lea
eee Pine) clesealolaio
Salsa sas as APITIRIEI
1j2/2/2)4/2/2)2)1/2 - Kemel (6x 6,K = 6)
3/a/3lal3lal3l4al3la
Input feature map (10 x 10, R, = C, = 10)

 

Journal of Systems Architecture 111 (2020) 101831

New IFM (5 x 5)

1 | New Kernel (3 x 3)
ay) ta |i2

1| [2[a[a]

1 [ie] eat Tea
1

NI NIN IN| N
NTN n|
NIN n |
NIN n|

}wlwl) wl wiw
Ww
Ww
Ww

 

[a|a|a
| 4 4] 4
[a | l4|4

Hl hh D/H

Fig. 8. The illustration of the process for mapping strides into unit-ones fora 10 x 10 input feature map anda6 x 6 kernel with stride = 2

3.2.3. Non-Unit stride to unit stride

In general, for stride (S) > 1, every Ss? cycles produce one needed
output pixel, which lowers the throughput. As the result, the non-unit
stride to unit stride approach is used to eliminate unnecessary compu-
tations. We adopt the approach in [16] for non-unit stride conversion.
Take a 6x6 kernel with stride 2 used in GoogleNet and ResNet for ex-
ample, we use zero padding for the kernels and have a new formula for
mapped kernel size as [K/S]|* |K/S]. Thus, for a 6x6 kernel with stride 2,
we will have four (3x3) kernels. For input feature maps and kernels, we
first decompose them into multiple sub-blocks, and then calculate these
sub-blocks individually. Last, we add up the results of the sub-blocks for
the final result, as shown in Fig. 8.

3.2.4. SRAM Access

Fig. 5 is the overall architecture of the proposed CNN accelerator sys-
tem. It consists of five parts T,,,*T,, PEArrays (PEA), T,, Weight SRAM
(WSRAM), T,,, Bias SRAM (WSRAM), T,, Input Feature Map SRAM (IFM
SRAM), and T,, Output Feature Map SRAM (OFM SRAM). T,, is the
number of tiling factor for output channels and T,, is the number of
tiling factor for input channels of a convolutional layer. Here we group
{PEA, ;,PEA);, ...,PEAr j} together into a Filter i, where i is from
1 to T,,. For each Filter i, the weight and the input feature map in
SRAM are loaded into the T,, PEArrays {PEA,;, PEA);,...,PEAr ;}
parallelly. The results of convolution from each PEArray are summed up
with the adder tree then are added with the bias value in each Filter I as
shown in the red dashed line boxes. The temporary convolutional results
are written into OFM SRAM and the value will looped back to added up
with the next convolutional results. Algorithm 1 shows how the convo-
lutional operation performed in the accelerator with Loop Tiling.

3.3. Energy efficiency

As discussed above, external DRAM access is the most energy-
consuming process for CNN accelerators. Hence, we sought to reduce ex-
ternal DRAM access. We used the loop tiling technique, the bit reduction
strategy, and the reuse strategy to accomplish this. The loop tiling tech-
nique and the bit reduction strategy are described in Section 3.2.1 and
3.2.2. Here, we describe the reuse strategy.

Algorithm 1 CNN accelerator with Loop Tiling.

Input:
T,, is the number of tiling factor for input channels;
T,, is the number of tiling factor for output channels;
N is channel number of input feature map;
M is channel number of output feature map;
Data Array = {Data[0], Data[1], Data[N-1]};
Weight Array = {Weight[0][0], Weight[0][1],
Weight[M] [N-1]};

Output:
oFMAP channel Array;

1: while Accelerate do

 

2 oFMAP channel [T,,] = 8b0;
3 fori =0;i< M;i=i+T,, do
4: for j =0; 7 < N;i=i+T, do
5 forh=0;h<T,;h=h+1do
6 fork=0;k<T,;k=k+1do
7 Partitial Sum[k] = PE_Array(Data[j +k],
Weight[i+ h] [j +k]);
8: end for
9: Filter Sum = Partitial Sum [0] +
Partitial Sum [1] + + Partitial_Sum [T,,-1];
10: if j! =N —T,, then
11: oOFMAP channel[h] = oFMAP channel[h] +  Fil-
ter_ Sum;
12: else
13: oOFMAP channel[h] = oFMAP channel[h] +  Fil-
ter Sum + Bias[i+h];
14: end if
15: end for
16: end for
17: return oFMAP channel;

18: end for
19: end while
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

Table 5
The ratio of computational complexity for different kernel
sizes on convolutional layers in state-of-the-art CNN models.

Ratio of Computational Complexity

Model OT
11x11 7X7 5x5 3x3
AlexNet [3] 6.91% 0% 58.76% 34.33%
ZF-Net [41] 0% 11.85% 50.34% 37.81%
VGG-11 [42] 0% 0% 0% 100%
VGG-16 [42] 0% 0% 0% 100%
VGG-19 [42] 0% 0% 0% 100%
ResNet-18 [43] 0% 6.00% 0% 94.00%
ResNet-34 [43] 0% 3.09% 0% 96.91%

3.3.1. Reuse strategy

With the help of on-chip SRAM, we can reduce the amount of exter-
nal DRAM access by reusing the data on SRAM. There are two types of
reuse strategies, that is, input reuse and output reuse.

Input reuse strategy: The partial input feature maps loaded from the
external DRAM will be fully reused on SRAM before overwritten by new
groups of partial input feature maps, while new groups of kernels are
updated to on-chip SRAM and new groups of calculated output feature
maps are written back to the external DRAM. Thus, the amount of ex-
ternal DRAM access was reducible by T,,,.

Output reuse strategy: The calculated output feature maps can be fully
utilized on SRAM before writing back the final results of the output
pixels to the external DRAM, while new groups of input feature maps
and kernels are updated to the on-chip SRAM. Therefore, the amount of
external DRAM access was reducible by T,,.

3.4. Kernel decomposition

Convolution kernel sizes in CNNs are variant. Early CNN designs
[3] [41] adopt larger kernel sizes like 11 x 11,7 x 7,or5 x 5.In
recent times, smaller kernel sizes like 3 x 3 have been used in modern
CNN models [42] [43]. The distribution of computational complexity
for different kernel sizes on convolutional layers in state-of-the-art CNN
models are listed in Table 5. We can see that convolution with smaller
kernel sizes (such as 3 x 3) are favored in new CNNs.

Since the 3 x 3 convolution kernel size is popular for new CNNs,
we adopted the 3 x 3 kernel size in our hardware design. For the CNNs
with larger kernel sizes like 11 x 11 and5 x 5 on AlexNet, we used
the kernel decomposition technique as shown in Fig. 7 to support them.

We adopt the kernel decomposition method in [31] in our approach.
To handle kernel sizes (K) bigger than 3, we decomposed each kernel
with a size of K x K into (f= 1) 3 x 3 sub-kernels. Following this, we
performed convolution operations for each 3 x 3 sub-kernel a total of
(F=1? times.

4. Proposed bit-Serial streaming PE array architecture and
dataflow

4.1. Architecture

In the overall system, T,,,xT,, PE Arrays (PEA) are included onto the
accelerator, as shown in Fig. 5, where PEA, ,, i=1,., to Ty and j=1,., to
T,,. There are T,, channels of input feature maps accessed from input fea-
ture maps IFM SRAM at each cycle. The T,, feature maps are distributed
to corresponding PEA from channel 1 to channel T,,. All the PEAs in the
same column have the same input feature map. There are T,,,xT,, chan-
nels of weights accessed from T,, weight SRAMs (WSRAM) before the
convolution of a new input feature map. Each cycle T,,, sets of weight are
sent to PEAs. It takes T,, cycles to load all the weights. After the distribu-
tion of input feature maps and kernels to PE Arrays, the PE Arrays start
computation following the ring streaming data flow. The introduction
of the proposed PE Array architecture will be presented in the top-down
manner.

Journal of Systems Architecture 111 (2020) 101831

 

PE_Row_Up

 

dai, Jappy

 

 

 

 

Fig. 9. The architecture of a PE Array.

4.1.1. PE Array

The proposed PE Array architecture comprises three PE Rows and
their inter-connections, as shown in Fig. 9. The input arrows for the PE
Array module indicates the eight possible access point for input pixels,
and we assumed the weights were fixed on each PE. The output ar-
row for the PE Array module represents the summation of nine partial
results from each PE using the adder tree, and will be returned to the
on-chip SRAM. All arrows inside the PE Array module, excluding the red
ones, indicate the inter-connections between PE Rows for the proposed
streaming dataflow.

4.1.2. PE Row

The proposed PE Row architecture is made up of 3 PEs, as shown
in Fig. 10. In other words, a PE Array is composed of 9 PEs. The input
pixels flow into neighboring PEs and PE Rows, while the weighting pix-
els are fixed onto each PE in a PE Array. There are three types of PE
rows: Up, Middle and Down. These rows differ only slightly in I/O and
connections.

4.1.3. Bit-Serial PE

In general, 16-bit fixed-point representations are widely used for
modern CNN accelerators. Hence, the bit-parallel PE (which consists of
a multiplier) is being used in their designs. In a 16-bit bit-parallel PE,
the 16-bit input pixels are fed into the multiplier parallelly; the 16-bit
weighting pixels are concurrently sent to the multiplier.

The bit-serial PE, which is designed to satisfy lower bit requirements,
uses AND gates to perform multiplication. The input pixels are fed into
the gates parallelly, while the weight pixels are sent into the gates seri-
ally.

However, by using the bit-serial PE to perform multiplication, more
cycles are required for the same bit-width computations. This is solved
by using multiple bit-serial PEs to compute the required bits concur-
rently and adding the individual results to produce the final calculation.

Considering the above analysis, we propose a bit-serial PE architec-
ture, as shown in Fig. 11(a). The proposed bit-serial PE is composed of
16 sub-PEs, which perform computations concurrently. The results of
each sub-PE are summed using an adder tree.

Using the bit reduction technique, weight pixels can be represented
using fewer bits. Regarding hardware, this results in the use of fewer
sub-PEs to perform multiplication operations. In this case, only n sub-
PEs are required; thus, the other 16 — n sub-PEs can be used to calculate
other input pixels and weight pixels to improve the efficiency of the data
parallelism.

Fig. 12 shows the architecture of a sub-PE from the proposed bit-
serial PE. It computes the multiplication results of a 16-bit input pixel
and 1-bit weight pixel using 16 AND gates and uses a barrel shifter for
bit shifting. For example, when we want to combine 16 proposed sub-
PEs to obtain the multiplication result of a 16-bit input pixel and a 16-bit
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al. Journal of Systems Architecture 111 (2020) 101831

 

 

Fig. 10. The architecture and inter-connections of PE Row_Up.

clk rst_b

Data_reg

Data sum_reg_valid

Data_valid

Weight_valid_reg
Weight_valid

Weight_reg

Weight[15]

 

(a) Architecture of proposed Bit-Serial PE for 16-bit weight pixel

clk rst_b

Data
Data_valid
Weight_valid

Weight

 

(b) Architecture of proposed Bit-Serial PE for n-bit weight pixel

Fig. 11. The architecture of the proposed Bit-Serial PE for (a) 16-bit weight pixel and (b) n-bit weight pixel.
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

clk rst_b

 

Journal of Systems Architecture 111 (2020) 101831

Fig. 12. The architecture of the proposed sub-
PE.

sum_reg_valid

 

 

 

 

Fig. 13. The illustration of (a) normal stream-
ing dataflow and (b) proposed ring streaming

 

 

  
 
   
     
 
    

  
 

; 0, O, 0, 0,4) | {0 10.6) T
|
aay [ea a) [aL op eer
SEED
EAT [TET TOT | Tena) [Lesh p oer

Kiem | eM | em | em | em
uM IFM_| Je
eS Cal] CA] Ca) | OT
IFM
(6,1)

  

  

    
  
 

    
 

  

IFM
(6,0)

 

COU AS St A Aa WNL)
ee iem | em | em | ew | em | ew

Fen CASIO] Es] Co]

diem | ie | ie | ie | em | ie

IFM | IFM | IFM
(6,4) | (6,5) | (6,6)

dataflow.

 

 

   
 

IFM
(7,0)

iFM
(7,1)

 

 

 

 

 

 

(a) Normal streaming dataflow

weight, then the accumulation bit is 32-bit. Depending the position of
the weights, we need to shift the AND sum according. There is no left
shifting of the AND sum for the first weight bit. There is n — 1 left shifting
of the AND sum for the nth weight bit. So we need a barrel shifter and
sign extend to obtain the corresponding 32 bits.

4.2. Dataflow

4.2.1. Proposed ring streaming dataflow for convolutional layer

Fig. 13 shows the difference between normal streaming dataflow and
our ring streaming dataflow.

For normal streaming dataflow, when the input pixel reaches the
end of the current row, it returns to the front of next row. As a result,
latencies appear across the rows.

On the other hand, the proposed ring streaming dataflow reduces
unnecessary computations to improve computational performance. As
the input pixel reaches to the end of current row, it moves down to
the next row and returns to the front of that row. Hence, no latency is
produced across the rows, and computational performance is improved.

To satisfy the design of our ring streaming dataflow, three directions
are introduced. Fig. 14 represents these directions based on the proposed
PE Array. The painted lines indicate the transmission paths between the
bit-serial PEs and the PE Rows.

The layout of the data in IFM SRAM is shown in Fig. 15. Compared
the required operational cycles of the proposed ring streaming dataflow
Fig. 13(a) with normal streaming dataflow Fig. 13(b), the proposed ring
streaming dataflow can reduce two cycles by using the up shifting oper-
ation. The reason why we can simplify two clock cycles because when

(b) Proposed ring streaming dataflow

operating up shifting in proposed ring steaming dataflow, we can reuse
data from PE row 2 and row 3 in Fig. 15(a). For the example shown
in Fig. 15, the last convolution is for the three rows of input data from
IFM SRAM including (0,6), (0,7), (0,8), (1,6), (1,7), (1,8), (2,6), (2,7),
(2,8). For the next convolution, the three rows of input data from IFM
SRAM including (1,6), (1,7), (1,8), (2,6), (2,7), (2,8), (3,6), (3,7), (3,8).
By using the upshifting connection in the PEArry, the (1,6), (1,7), (1,8),
(2,6), (2,7), (2,8) are moved up with those blue and purple wires. The
new row of data (3,6), (3,7), (3,8) are fed into the PEArry from the bot-
tom. Therefore, two cycles can be saved in the operation and overall
speed can be improved by this dataflow.

4.2.2. Dataflow for fully connected layer

The above dataflows are for the convolutional layers, which occupy
substantial amount of computation. For fully connected layers, we can-
not use the same dataflows. The convolutional layers can use the stream-
ing dataflow owing to their weight-sharing characteristics, which are not
exploited in the fully connected layers. However, we can still use the
same PE and PE Array architecture with the dataflow shown in Fig. 16.
In Fig. 16, the input pixels are fed into the leftmost PEs to perform cal-
culation and add the results with the adder tree. In this way, we can
calculate the fully connected layers with the same PEs and PE array
architecture.

4.2.3. Data flow and memory access

There are three sets of SRAM (IFM SRAM, WSRAM, and OFM SRAM)
in our proposed architecture. In order to satisfy the design of our ring
streaming dataflow, three directions are introduced. For the data access
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al. Journal of Systems Architecture 111 (2020) 101831

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

del, Jeppy
del, soppy
ae. sappy

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

(a) Right shifting on the perspective of PE Array. (b) Up shifting on the perspective of PE Array. (c) Left shifting on the perspective of PE Array.

Fig. 14. Three dataflows on the perspective of PE Array.

   
 
   
 

 
    
 

IFM SRAM

(0,0) (2,0) (2,0)
(0,2) (1,4) (2,4)
(0,2) (1,2) (2,2)

oo

(0,7) (1,7) (2,7)

(0,8) (1,8) (2,8)

(35) 6.4)(33)_

| .0,0,0 |

pt

  
 
   
   
  

 
  
 
 

IFM SRAM

(0,0) (1,0) (2,0)
(0,1) (1,1) (2,1)
(0,2) (1,2) (2,2)

| it

IFM | IFM | IFM | IFM | IFM | IFM ] IFM | IFM M
(0,0) } (0,1) | (0,2) | (0,3) | (0,4) | (0,5) | (0,6) | (0,7) /{0
IFM | FM | IFM | 1FM | IFM | IFM | IFM | IFM V
(2,0) | (2,2) | (2,2) | (2,3) | (1,4) | (2,5) | (2,6) | (1,7) [A
IFM | IFM | IFM | IFM | IFM | IFM {| IFM | IFM y
(2,0) } (2,1) | (2,2) | (2,3) | (2,4) | (2,5) | (2,6) | (2,7) |
IFM | IFM | IFM | 1FM | IFM | IFM | IFM | IFM H
(3,0) } (3,1) | (3,2) | (3,3) | (3.4) | (3,5) | (3.6) | (3,7) (0,7) (1,7) (2.7)
4,0) | (4,1) | (4,2) | (4,3) | (4,4) | (4,5) | (4,6) | (4,7

(4,0) | (4,1) | (4,2) | (4,3) | (4.4) | (4,5) | (4,6) | (4,7) as

 
   
 

 
  
 
 
   

Ee

  

IFM | IFM | IFM | 1FM | IFM | IFM | IFM | IFM | IFM
(0,0) | (0,1) | (0,2) | (0,3) | (0,4) | (0,5) | (0,6) | (0,7) | (0,8)
IFM | IFM | IFM | IFM | IFM | IFM | IFM | IFM | IFM
(1,0) | (1,2) | (1,2) | (2,3) | (1,4) | (1,5) | (2,6) | (1,7) | (1,8)
IFM | IFM | IFM | IFM | IFM | IFM | IFM | IFM | IFM
(2,0) | (2,1) | (2,2) | (2,3) | (2,4) | (2,5) | (2,6) | (2,7) | (2,8)
IFM | IFM | IFM | 1FM | IFM | 1FM JPM JEM | TEN
(3,0) | (3,4) | (3,2) | (3,3) | (3,4) | (3,5) ( (

IFM | IFM | IFM | IFM | IFM | IFM | IFM | IFM | IFM
(4,0) | (4,1) | (4,2) | (4,3) | (4,4) | (4,5) | (4,6) | (4,7) | (4,8)
IFM | IFM | 1FM | IFM | IFM | IFM | IFM | IFM | IFM
(5,0) | (5,2) } (5,2) | (5,3) | (5,4) | (5,5) | (5,6) | (5,7) | (5,8)

      
    
   
 
  

     

    
  

es

   
     
  
   
  

|
(

M
8)

FM
4,8)

    

F
3

   

        

(

    

  

  
   
 
  

iFM | iFM | iFM | iFM | iFM | iFM | iFM | FM | 1FM
e5|a/en)en] LeseNe

IFM | IFM | IFM | IFM IFM | IFM (3,3) (3,2) (3,2)

         
 

    
   

 

 

 

 

(6,0) | (6,1) | (6,2) | (6,3) | (6.4) | (6,5) | (6,6) | (6,7) |(68)} | (3,0),0,0 |
Hosnlor|es|ot| esl ee|en|en| poe
(7,0) | (7,4) | (7,2) | (7,3) | (7,4) | (7.5) | (7,6) | (7,7) | (7,8)
(4,3) (4,4) (4,5)
IFM | 1FM | IFM | IFM | IFM | FM | IFM | IFM ange
(8,0) | (8,1) | (8,2) | (8,3) | (8,4) | (8,5) | (8,6) | (8,7)

(a) Right shifting stage. (b) Up shifting stage.

    

     
  

Register

 

Reel IFM IFM IFM
egister (3,8) (3,7) (3,6)

   
   

 

  

Fig. 15. (a) During the right shifting stage. (b) During the up shifting stage.

Fig. 16. The dataflow for the fully connected
layers on the perspective of PE Array.

>
2.
Q
@
—
—
=
@
@

 
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

     

  
   

PEA(2,1)

      

PEA(2,2) 2
en U

PE | | PE) | Pe
PE PE | PE |
7

PEA(Tn,
U on
y | PE

         
 

 

PEA(Tn,Tm)
UY U.
Pe) YT pe) Y
PE | PE
ype "Te )" Te
PE | PE PE

U_> U
pe} > pe) YT ve
PE PE PE

     

)
Uy

   

Journal of Systems Architecture 111 (2020) 101831

IFM SRAM

Input feature map

(0,3) (1,3) (2,3)
(0,4) (1,4) (2,4)

(1,0) (2,0) (3,0)

 

(2,3) (3,3) (4,3)
(2,4) (3,4) (4,4) |Row=11

(b) (c)

 

 

 

 

 

 

 

 

(4,0) (4,1) (4,2)

 

 

Fig. 17. (a) Input Feature Map Data Access between PE Array and IFM SRAM, (b) Input Feature Map of size 5 x 5 and (c) Pixel Storage example is IFM SRAM.

between PEA and IFM SRAM, Fig. 17(b) shows the three directions on
the perspective of input pixels. For right shifting, the input pixel goes
right until it reaches the end of the row. At this moment as shown in
Fig. 17(b), the window moves down one row to go to the next row. After
that, left shifting begins that the input pixel goes left until it reaches to
the front.

The IFM SRAM stores the feature maps based on the ring streaming
dataflow as shown above. Each row of the IFM SRAM stores three pixels
of the feature maps. A PEA reads out three input pixels in one cycle.
Fig. 14 represents the three directions based on the proposed PE Array.
The painted lines indicates the transmission path between the bit-serial
PEs and between the PE Rows. We use a 5x5 feature map as an example
to show how to store the feature map into the IFM SRAM. For right
shifting, the input pixel goes right until it reaches the end of the row.
The first row of the IFM SRAM stores the first column of the feature
map. The second row of the IFM SRAM stores the second column of the
feature map and so on until it reaches the end of the row in Fig. 17(b).
Fig. 14(a) shows the dataflow in the PE Array during right shifting. Three
pixels from IFM SRAM are loaded into the left column of the PE array.
The pixels in the left column of the PE array are shifted to the middle
column of the PE array and the pixels in the middle column of the PE
array are shifted to the right column of the PE array.

For right shifting, the input pixel goes right until it reaches the end
of the row. Then as shown in Fig 17(b), the sliding window goes down
to the next row. Instead of storing column pixels, the IFM SRAM then
stores the next row of the feature map (32, 33, 34) into the sixth row of
the SRAM as shown in Fig. 17. Fig. 14(b) shows the dataflow in the PE
Array during up shifting. Three pixels from IFM SRAM are loaded into
the bottom row of the PE array. The pixels in the bottom row of the PE
array are shifted to the middle row of the PE array and the pixels in the
middle row of the PE array are shifted to the upper row of the PE array.

After that, left shifting begins that the input pixel goes left until it
reaches to the front. Instead of storing row pixels, the IFM SRAM then
stores the next column of the feature map (11, 21, 31) into the seventh
row of the IFM SRAM as shown in Fig. 17 and so on until it reaches the
front. Fig. 14(c) shows the dataflow in the PE Array during left shifting.
Three pixels from IFM SRAM are loaded into the right column of the PE
array. The pixels in the right column of the PE array are shifted to the
middle column of the PE array and the pixels in the middle column of
the PE array are shifted to the left column of the PE array.

The weight values are stored at the T,, WSRAM. The nine weight
values in a 3x3 kernel are stored in one row of WSRAM. In each of the

WSRAM, there are T,, sets of weights values for the corresponding T,, in-
put feature maps. The T,, WSRAM are connected to the T,,, rows of PEAs
as shown in Fig. 18. Before the convolution starts, the T,, sets of weights
values in each WSRAM are loaded into PEAs for the corresponding T,,
input feature maps in T,, cycles. The T,,xT,, weight values in WSRAM
are sent to the T,,xT,, PEAs in T,, cycles.

The calculation of output feature map values are done by adding
all of the T,, channels convolution results belonging to the same output
channel as shown in Fig. 19. Since we are based on the output reuse
strategy, the output feature maps are stored in OFM SRAM until all the
input pixels are finished convolution operations.

5. System evaluation

In 5.1, the theoretical evaluation of our proposed architecture us-
ing the roofline model [23] and the comparison with other work are
presented. In 5.2, we demonstrate the implementation results of our
proposed PE Array architecture and the comparison with other work.

5.1. Theoretical evaluation

5.1.1. Roofline model

We evaluated our design using the roofline model [23], which is an
efficient method of comparison with real hardware implementation. The
concept of the roofline model is to evaluate a design under certain hard-
ware constraints, which are represented by two bounds on the roofline
(as shown in Fig. 20). The computational roof and the I/O bandwidth
roof are the two bounds and represent the maximum attainable perfor-
mance and maximum I/O bandwidth speed for a certain hardware. In
addition, the vertical and horizontal axes of the roofline model represent
the attainable performance and the CTC ratio.

The attainable performance can be formulated as:
Total number of computational operations

Attainable Per formance = -
Number of execution cycles

(4)
and the CTC ratio can be expressed as

Total number of computational operations

CTC ratio = (5)

Total amount of external data access
For a certain CNN network, the total number of computational oper-

ations is constant. Thus, the key factors for the attainable performance
and the CTC ratio are the number of execution cycles and the total
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al. Journal of Systems Architecture 111 (2020) 101831

Weight
Wy,,.1 | Wr,.2 | Wr,,3
(0,0) | (0,1) | (0,3)
Wr,4 Wr, s | “7,6
Tet 0) “| (1,1) | (1,2)
Wr,,9
(2,2)

      
 
  

renin sn, =. re MSRAM
came [ve ) ome Ea in os J et Cet ae Wits Wiz, Wize Wigs Wiss Wier Wiz Wigs Wi
Pes) Pee eee

Perey [era] are

 

Wai, W22, W23, Waar, Was, W26, W27, Was, W29

 

Wr, Wr,2+ Wry3- Wryar Wryse Wry67 Wr,.77 Wr,8- Wr,9 Row =T

 

(c)

Fig. 18. (a) Weight SRAM Data Access between PE Array and WSRAM, (b) Weight Kernels of size 3 x 3 with input channels and (c) Weight storage example in

WSRAM.
_ fora | Tour
_ = eS 0) } (0,1) } (0,2)
(#)

 

    

       

PE PE OF ‘ae
a ore =

 

(b)

OFM SRAM

}*2-Lpe ) TPE

PE

a a al
PEA(1,Tm)

aa oe
GP ar ar
nee a

    
  

     
  

    
  

a
Swe
(a)

Fig. 19. (a) Output Feature Map Data Access between PE Array and OFM SRAM, (b) An example of output feature map of size 3 x 3 and (c) Pixel storage example
in OFM SRAM.

PEA(2,Tm)
not ret

re oY oe

    
  

OFM Tm
SRAM

 

where K’ indicates the new kernel size after mapping strides into the unit

Computational roof (GFLOPS) stride and can be calculated by K’ = [=]. B indicates the number of bits

to represent a weight pixel, and U indicates the number of sub-PEs to
process different bits of the same weight pixel.

Next, we formulated our proposed architecture and dataflow design,

 
  
  

|
|
a 4a: ; .;
1s described in Section 4 for the CTC ratio as:
=
S Ratio of
2 (FLOP)(DRAM byte CTC ratio = _ 2X RXCXMXNXKXK , (7)
| access) Bin X Qin + Bwent x Aweht + Bout X Aout

Attainable performance
(GFLOPS)

where Pins Pyent ANd fo, represent the trip count of the input feature
map, weight, and output feature map, and ain, Aon, ANd a, indicate
Fig. 20. The roofline model [23]. the external data amount per access of the input feature map, weight,
and output feature map.

The a and f# terms for the CTC ratio are related to the different reuse
strategies. Hence, we formulated the input reuse strategy for the denom-
inator term of the CTC ratio as:

Computation to communication ratio

amount of external data access. Both terms are related to the hardware
architecture design and the dataflow design.

5.1.2. Formulization Bin =T, X(SXR+K—-S)X(SXC+K-—S)
We formulated our proposed architecture and dataflow design, de- Qin = r.
scribed in Section 4 for attainable performance as: Buen = K XK XT XT,
MYON (8)
Attainable Per formance = PX RXCXMXNXKXKR Suge ~ Tn Th
(RxXC+2)x S?x [L] x f£] x [41x [21x P41 Bou = RX CXT,

— Ny M
(6) Gout = 2X FX Fs
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

120

     

/ Midees*®* 3% eB ee ote & ®

eece* Fee geoee® e006
ee

Journal of Systems Architecture 111 (2020) 101831

   
  
    

seecceceeee

a Ganuatondl sant Bandwidth roof upper bound (4.5 GB/s)
5. 100
oO A A A” AC
hil =e9 cameonen ° = SS eeece “** seeseeee
RH | eeeceseece eeeseee es eemesceaemeceses
© = * .
fa ! ee et eed | TTT T PTT ene geteeVG Slt sen  ceseeuenessessces °
Ps) | P ecccece ce ants” Seems 5 Ses ccteomenmmecccscece

onfins fo eeeees*? © @ eeeee seee seeeereeeee

60 eecoovees emameogerce ewes
: Smee Ea eT
seoees FFF 290 09, MMMM EERERSMSDOMeMELCOOCHe ee eee °

& x i. oe ge Sete tee ons ARIESERSE csesees
a.
=
mi «|

0.0, © te © OFF ewek®*® ce 8h BSESeccccesesocosoees
ceammgrneeeo POO? 44% 6 we 8 M8 oe ete? eesawercrece
eeeet*ee** © 6 F% wee coe *98e%%
zs coee 22 =; o + “a “ oe e wai. o
3% e oS S059 owe OSG eeeeee
& ‘i ‘ zt. n . “0° . sess ns sesssesecoe
a ; ‘. * ° . *: ‘s a : Cae - rimmemcezegoosooes Ratio of
si 0 i } i $$ See « pemeceees” (FLOP)/(DRAM byte access)
0 10 20 30 40 50 60
Computation to Communication Ratio
Fig. 21. Roofline model on AlexNet layer 5 using [1]’s approach.
The output reuse strategy for the denominator term of the CTC ratio factors can be expressed as:
can be expressed as:
0<T, xT, xU < 853
<
B, =T,X(SXR+K —S)X(SXC+K~—S) O< Tin SM (10)
MON 0<T,<N
Qin = 7 X T.
Bwon =KXK XT, xT,
Gen = xX (9) 5.1.4. Design space exploration
t
ee Tm Tn For design space exploration, we chose a design for each layer. Each
Bout = RXC XT» See 4. ; ; ; ;
M point in Fig. 21 indicates one pair of (T,,,, T,,) in the design. The points
Cour = T.

The differences between the input reuse and output reuse strategies
were the factors T,,, for a;, and T,, for a,;, while the factor 2 for a,,;
was used for both write and read access of output feature maps.

Table 6 shows the total amount of external DRAM access and re-
quired SRAM size for both the input and output reuse strategies on
AlexNet. These results show that the output reuse strategy reduced
357.26x more external DRAM access than the no reuse case and reached
almost half of the amount of DRAM access of the input reuse case. These
results owe to the fact that the output reuse strategy avoids the factor 2
for write and read access; thus, we choose the output reuse strategy for
our design.

5.1.3. Tiling factor constraints

We assume to use the same FPGA as [1], where 480 PEs are used
in their design. In our design, each PE consists of 16 sub-PEs, meaning
that there are 7680 sub-PEs in total. Each PE Array comprised nine PEs,
so there were 853 PE arrays in total. Thus, the constraints of the tiling

Table 6
The total amount of DRAM access and required SRAM size for input
reuse and output reuse on AlexNet.

Total amount of Ratio Needed SRAM
DRAM access (bit) size (KB)
No Reuse 13,075,923,312 1x -
Input Reuse 54,179,384 241.34x 401.43
Output 36600260 357.26x 389.20

Reuse

beyond the roof indicate that their computational performance was
bounded by the bandwidth or the computational roof. For example,
point A indicates the design was bounded by the bandwidth roof.

The strategy for design space exploration was to choose the points
with highest performance. If there were multiple points with highest per-
formance, we then chose the point with highest CTC ratio from among
points chosen in the first step.

5.1.5. Bit-serial PE

The proposed energy aware bit-serial PE can utilize the hardware
resources efficiently. The bit-serial streaming PE can be clustered dy-
namically to process different number of weight bits, and tiling num-
bers. The comparison of the bit-serial multiplier using the architecture
of the proposed sub-PE with a traditional design for 16x16 bits multi-
plication is shown in Table 7 using 90nm TSMC technology. Compared
with the transition parallel multiplier, the proposed bit-serial PE archi-
tecture saves about 34% area and 62% power. The clock rate is based
on 150MHz.

5.1.6. Dynamic configuration
We show various precision and how to dynamically configure the
precision of each multiplier and the consequent effect on performance

Table 7
Comparison of Bit-Serial multiplier and traditional
multiplier.
Bit-Serial Traditional Multiplier
Area(ym7 ) 70277.58 106066.83
Power(uw) 296.18 771.24
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al. Journal of Systems Architecture 111 (2020) 101831

and energy efficiency of the multiplier. We assume to use the same hard-

ware resources of the FPGA as [1], where 480 PEs are used in their de- o = 3 ie <3 os
sign and with Maximum operations of 100 GFLOPs. In our design, each Gym Tae
PE consists of 16 sub-PEs, meaning that there are 7680 sub-PEs in total. 9
Each PE Array comprised nine PEs, so there were 853 PE arrays in total. a
Thus, the constraints of the tiling factors can be expressed as: Eq. 10. 5/2529
Then we work on design space exploration on selecting T,,,, T,,, and BINGDAD
U. We chose a design point for each layer. The points beyond the roof in- 3
dicate that their computational performance was bounded by the band- ely B ¢ & S S SQ
width or the computational roof. The strategy for design space explo- silg/eeesage
: : : : = Oye NewrenRh
ration was to choose the points with highest performance. If there were <
multiple points with highest performance, we then chose the point with >/p}/Raner
highest CTC ratio from among points chosen in the first step. 4
With the help of the bit reduction technique, we can use less bits to g Flan lae
represent weight pixels. In the aspect of hardware, that means we can Sl elowena
use less sub-PEs to perform multiplication operations. Fig. 11(b) shows OlR | Tents
an example hardware architecture for using n-bit to represent weight nAnmat
pixel, where 1 < n < 16. In this case, we only need n sub-PEs, so the e S 6B a; 6 6
other n sub-PEs can be used to calculate other input pixels and weight
pixels to improve the efficiency of data parallelism. g
Based on the above approach, we obtain the parameters in layer 3 &
of Alexnet for the best operation speed with the least external memory SLM EARAM
access. We use this as an exmaple to show the architecture and dataflow. _|& BARAG
In this case, T,,, (number of channels generated for output feature maps), § cuoo lw
T,, (number of processed channels from input feature maps), B (the bit = e/Grt Fags
number of a weight pixel), U (the number of sub-PEs to process different ee S B N in = is un
bits of the same weight pixel) are 21, 12, 7 and 2 respectively. Since U ea
(number of sub-PEs to process different bits of the same weight pixel) 2 P/N aNTe
is set to two, so there are two sub-PEs(sPE) in Fig. 11(b). Each PE array 3 ; ~
consists of nine bit-serial PEs as shown in Fig. 9. The structure of the ALE | sn mee
bit-serial PE is shown in Fig. 11(b). The T,, arrays in each row are used el elowena
to store T,, input feature maps. The T,,, PEAs in each column are used to cre |S aa
load T,,, weights pixels of channel i (i from 1 to T,,) to generate output ANANLS5
feature maps. e G m Ci
By using bit-serial PEs and design space exploration, our proposed PE
supports various precision and can dynamically configure the precision g
of each multiplier and the consequent effect on performance and energy . 5
es . 4: oO B | iIn N © 0 ©
efficiency of the multiplier. zi] |€/na2ooo
Si/glalkeaag
<| #8
Y
5.1.7. Evaluation on alexnet 8 aly 2 S A 2 om
Table 8 shows the results of our approach with different methods on 3/3 2 eegyrsee
AlexNet. Because the loop tiling technique and the output reuse strategy ¢ 5 Op om we 9
were used in [1]’s design, we used them as our baseline. E 2 Klone+a
By using the non-unit stride to unit stride technique for computa- a ©
tional performance enhancement, the number of computational cycles 5 z i. mo ©
were dramatically reduced for AlexNet layer 1. In total, we reduced com- 2
putational cycle counts by 25.11% when compared with our baseline. 7 o/SStaS
: : °4: 2 FlOMoOoUNN
To improve other convolutional layers on AlexNet, we utilized the ° UVUl=ANNIN ©
bit reduction technique. Overall, results showed that we reduced com- 2 ©
putational cycle counts by 26.27% when compared with our baseline on mn 5
AlexNet. & 5 WR VRL
Finally, we used all aforementioned methods with our baseline ap- 4% S|*orccs
proach. Our results showed that performance improved for all layers on 3
AlexNet when using all methods with our baseline approach. Computa- E scooco §&
tional cycle counts were reduced by 38.65% when compared with our 5 o/Bourno is
; : vlwuoyryunee
baseline, and 26.4% when compared with [1]. 5 > > MOON AT
Table 9 shows the comparison of the power of external data access & 4
on AlexNet between [1] and our approach with all methods. The total 4s S8i[Fleantsa
power can be formulated as 3 | 2
Total Energy a 6 - ~ n
Total Power Tonal Time (11) g alnonon
and the access energy of external DRAM access was estimated as 70 & amt
pJ/bit [44]. These results show that, compared with [1], we reduced ° S SoD DUS
power consumption by 41% for external DRAM access, which is the a 2 PPP PRO
BR

most energy-consuming process in CNN accelerators.
Table 9

The comparison for the power of external data access on AlexNet between [1] and our approach.

Th
Layer 1 48
Layer 2 20
Layer 3 96
Layer 4 95
Layer 5 32
Total

Table 10

The comparison for the power of external data access on ResNet-50 between [1] and our approach.

Layer 1
Layer 2_1
Layer 2_2
Layer 2_3
Layer 2_1*
Layer 3_1
Layer 3_2
Layer 3_3
Layer 3_1*
Layer 4_1
Layer 4_2
Layer 4_3
Layer 4_1*
Layer 5_1
Layer 5_2
Layer 5_3
Layer 5_1*
Total

17
13
13
53
13
26
26
53
26
53
53
13

53
53
53

Sree ee RPE NH PWN HR A A W

Chen et al. [1]

Cycles

366,025
237,185
160,264
120,198
80,132

963,804

Performance
(GFLOPS)

28.80
94.42
93.30
93.30
93.30

Cycles

903,204
251,040
251,040
1,004,160
1,004,160
1,004,160
251,520
1,006,080
1,006,080
1,006,080
253,440
1,001,088

3 1,001,088

1,013,760
261,120
1,018,368

3 1,031,424

13,267,812

Chen et al. [1]

CTC

83.08
66.86
63.35
62.21
44.97

Performance

(GFLOPS)

26.13
10.23
92.1

10.23
10.23
5.12

91.93
10.21
10.21
5.11

91.23
10.26
10.26
5.07

88.55
10.09
9.96

Time
(sec)

0.0037
0.0024
0.0016
0.0012
0.0008
0.0097

CTC Time
(sec)
84 0.009
10.8 0.0025
89.1 0.0025
28.7 0.01
12.3 0.01
6.5 0.01
141.9 0.0025
35.8 0.0101
24 0.0101
12.9 0.0101
120.9 0.0025
11.6 0.01
3.9 0.01
11.6 0.0101
41.5 0.0026
24.3 0.0102
3.7 0.0103
0.1325

Energy
(mJ)

0.7105
1.8756
1.3216
1.0093
0.9309
5.8479

Energy
(mJ)

1.573
1.336
1.4532
2.0033
4.6697
4.419
0.9127
1.6087
2.3391
2.2222
1.0713
4.9450
14.7363
2.4877
3.1192
2.3726
15.2560
66.858

Total Power
(mW)

602.8763

Total Power

(mW)

504.5887

48
12
21
47
42

64
15
15

64
65
15
17

18
35
21
47
25
19
25

14
13
21

13
12

17
47
10
58
23

19
25
19

AaaAN ON

WOoDWDDMDADADADDADAHAHMWA MW CO CO WC CO

ma eB eSB NB WH WN DB DP WH RMR RB BR W WwW AP

Our approach with all methods

U

mere NHN O NI

Cycles

145,296
225,148
150,480
112,860
75,240

709,024

Our approach with all methods

Cycles

401,472
235,350
235,350
928,848
928,848
1,006,080
233,442
925,908
925,908
1,010,592
231,660
826,640
926,640
1,023,264
231,336
925,344
925,344
11,922,026

Performance

(GFLOPS)

72.55
99.47
99.36
99.36
99.36

Performance

(GFLOPS)

58.79
98.24
98.24
99.57
99.57
45.96
99.04
99.89
99.89
45.76
99.81
99.81
99.81
45.19
99.95
99.95
99.95

CTC

171.5
166.7
99.33
168.2
168.1

CTC

122.3
109.2
101.6
56.7
457.1
140.8
99.9
133.8
61.9
21.5
90.5
256.9
176
106.4
56.3
138.9
177.5

Time
(sec)

0.0015
0.0023
0.0015
0.0011
0.0008
0.0072

Time
(sec)

0.004

0.0024
0.0024
0.0093
0.0093
0.0101
0.0023
0.0093
0.0093
0.0101
0.0023
0.0093
0.0093
0.0102
0.0023
0.0093
0.0093
0.1205

Energy
(mJ)

0.3442
0.7524
0.8429
0.3733
0.2491
2.5619

Energy
(mJ)

1.0806
1.1862
1.2743
9.1292
1.1331
1.8387
1.296
3.8713
8.3699
12.0204
1.4304
2.0158
2.9433
2.4331
2.3006
3.7283
2.9171
58.9683

Total Power
(mW)

355.8194

Total Power

(mW)

489.3635

‘7D Ja UIT ‘L-"Y pub MYD ‘*L-'D NSH ‘D-"T

LE8IOL (OZOZ) III aamoanyosy suiajsXg fo joumor
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

Journal of Systems Architecture 111 (2020) 101831

Comparison of number of cycles between [1]’s approach and our baseline with all methods on

Ours (baseline + Unit Stride + Bit Reduction)

Table 11
VGG-16.
Chen et. al [1]
Layer BT, Tr, Cycles Tn
1-1 6 64 3 451,641 64
1-2 6 32 13 4,516,225 15
2-1 6 43 11 2,032,448 31
2-2 6 43 11 4,064,976 15
3-1 7 43 11 2,033,568 7
3-2 7 24 20 4,038,892 21
3-3 8 24 20 4,038,892 85
4-1 7 15 32 1,981,280 17
4-2 7 40 12 3,955,484 21
4-3 8 40 12 3,955,484 28
5-1 8 40 12 997,256 21
5-2 9 40 12 997,256 56
5-3 7 40 12 997,256 21
Total 34,060,698
Table 12

The Specification Table of the
Proposed PE Array Architec-

ture.
Technology TSMC 90 nm
Gate Count 68k
# PE 9
Frequency 119 MHz
Power 10.08mW

5.1.8. Evaluation on VGG-16

We also showed the comparison between [1] and our approach on
VGG-16, which is shown in Table 11. Performances improved for all
layers on VGG-16 using all aforementioned methods with our baseline
approach compared with [1]. In addition, 9.24% reduction in computa-
tional cycle counts was reached under the hardware constraints.

5.1.9. Evaluation on ResNet-50

For ResNet, we have used the 1x1 PEArray handle the convolution
with kernel size of 1x1. A multiplexer is used to choose different PEArray
with different size. The function used to evaluate the attainable perfor-
mance with extra 1x1 PEArray is shown below:

2XRXCXMXNXKXKXKXK
(RxXC+2)x S?x[£]xf£] x [41x f21x p41

(12)

Attainable Per formance =

Table 10 shows the results of the number of cycles and the power of ex-
ternal data access with our proposed method and the baseline approach
for ResNet-50. It shows that the total number of cycles of our proposed
method has reduced by 10.14% and the power of external data access
is reduced by 3%.

Table 13
Comparison with other works of PEArray.

T,, U Cycles Performance CTC

2 3. 200712 86.4 25.77
13. 2 = =3763350 98.3 107.7
7 2 1881900 98.29 182.5
12 2 3726162 99.28 116.7
8 7 1857696 99.57 55.45
20 1 3712254 99.65 160

7 1 3715392 99.57 501.1
14 2 1851816 99.89 117.7
19 1 3713850 99.61 146

17. 1 3703632 99.89 184.8
11 2 930600 99.38 103.3
10 1 926640 99.81 176.1
11 2 930600 99.38 106.8

30914604

5.2. Implementation results

The proposed PE Array architecture is synthesized under TSMC 90-
nm CMOS technology, and the specification table is listed in Table 12.
The design operates at 119 MHz while consuming 68 k gates with
10.08 mW power consumption.

Table 13 shows the comparison of our method with related works.
Here, we chose one study for each type of dataflow for comparison. For
gate counts, performance, and power, our method produced favorable
results among all dataflow types. Compared with [31] (though both be-
long to the streaming dataflow type), the ring streaming dataflow pro-
posed in this paper produced superior computational performance when
compared with normal streaming dataflow. In addition, compared with
the 15.4 MB external memory access for Eyeriss [2] on the convolutional
layers of AlexNet, we required 4.36 MB of external memory access. This
indicates a dramatic reduction in the most energy-consuming process of
power consumption.

We have implemented the full accelerator including PEA and SRAM.
We have synthesized our Accelerator in 40nm TSMC technology and
have reduced the input and weight bit number to 8 bits. The experi-
ment results are shown in Table 14. The hardware implementation un-
der TSMC 40-nm technology reached 3060 GOPs with only 1.75MB ex-
ternal memory access. Reach a good balance between performance and
energy efficiency.

The accelerator that we proposed is mainly for inference operation
and focuses on energy efficiency for portable devices. NVIDIA GPU
are usually used for training so high computational through is the tar-
get and usually consumes high energy. For the comparison shown in
Table 13 with inference based accelerators, our design achieves better
results on both the gate count and power.

Work [16] [2] [21] [31] Ours
Dataflow Type Pipeline Systolic-array DaDianNao Streaming Streaming
Technology 40 nm 65 nm 28nm 65 nm 90 nm
Frequency 454 250 606 500 119
Gate Count 1783k 74.29k 1852k 99.21k 34350k 75.47k 1300k 81.25k 68k

# MAC 216 9 168 9 4096 9 144 9 9
Performance 132.8 46.1 5580 152 178.24
Voltage (V) NA 1 0.9 1 0.9
Power (mW) NA NA 278 14.89 15,970 35.09 350 21.88 10.08
Bit-width (bits) 16 fixed 16 fixed 16 fixed 16 fixed dynamic
CNN type for ImageNet AlexNet AlexNet NA LeNet AlexNet
External Memory Access (MB) NA 15.4 NA NA 4.36
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

Table 14

Journal of Systems Architecture 111 (2020) 101831

Synthesis result of Accelerator and the comparison with other works.

vWork [16] [2] [21] [31] Ours
Dataflow Type Pipeline Systolic-array DaDianNao Streaming Streaming
Technology 40 nm 65 nm 28nm 65 nm 40 nm
Frequency 454 250 606 500 333
Gate Count 1783k 1852k 34350k 1300k 4272k
# MAC 216 168 4096 144 1024
Performance 132.8 46.1 5580 152 3060
Voltage (V) NA 1 0.9 1 1
Power (mW) NA 278 15970 350 392
Bit-width (bits) 16 fixed 16 fixed 16 fixed 16 fixed 8 fixed
CNN type for ImageNet AlexNet AlexNet NA LeNet AlexNet
External Memory Access (MB) NA 15.4 NA NA 1.75
6. Conclusion [13] V. Badrinarayanan, A. Kendall, R. Cipolla, Segnet: a deep convolutional encoder-
decoder architecture for image segmentation, arXiv preprint arXiv:1511.00561
. . . (2015).

In this paper, we proposed an energy-aware bit-serial deep CNN ac- [14] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic seg-
celerator design to address issues related to computational complexity mentation, in: Proceedings of the IEEE conference on computer vision and pattern
and external memory access. The proposed ring streaming dataflow and recognition, 2015, pp. 3431-3440.

[15] G. Lin, A. Milan, C. Shen, I.D. Reid, Refinenet: Multi-path refinement networks
output reuse strategy successfully reduced the external DRAM access for high-resoluti ; ; _
or high-resolution semantic segmentation, in: Cvpr, volume 1, 2017,
amount by 357.26x compared with the no output reuse cases. The loop pp. 5168-5177.
tiling technique and mapping strides to unit strides were used to in- [16] Y.-J. un T.S. Chang, Data and hardware efficient design for convolutional neural
: : : : network, IEEE Trans. Circu. Syst. I Regul. Pap. 65 (5) (2018) 1642-1651.

crease speed. The bit-serial PE architecture was designed to employ a [17] A. Ardakani, C. Condo, M. Ahmadi, W.J. Gross, An architecture to accelerate convo-

bit reduction technique to tackle of the above both challenges. lution in deep neural networks, IEEE Trans. Circu. Syst. I Regul. Pap. 65 (4) (2018)

Compared with [1] on AlexNet using the roofline model, we achieved 1349-1362.

a 1.36x speed increase and reduced energy for external DRAM access by [18] S. Sharify, A.D. Lascorz, K, Siu, P. Judd, A. Moshovos, Loom: Exploit:

0 _. . . ing weight and activation precisions to accelerate convolutional neural

41%. In addition, we improved the CTC ratio for all AlexNet layers. The networks, in: Proceedings of the 55th Annual Design Automation Conference,

hardware implementation of the PE Array architecture under the TSMC ACM, 2018, pp. 20:1-20:6.

90-nm technology reached 178.24 GOPs with only 4.36 MB external [19] C. Farabet, C. Poulet, J.Y. Han, Y. LeCun, Cnp: An fpga-based processor for convolu-

. . . . tional networks, in: Field Programmable Logic and Applications, 2009 International

memory access, which is the most energy-consuming process in a CNN Conference on, IEEE, 2009, pp. 32-37.

accelerator. [20] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, O. Temam, Diannao: a small-

-footprint high-throughput accelerator for ubiquitous machine-learning, ACM Sig-
plan Notices 49 (4) (2014) 269-284.
Declaration of Competing Interest [21] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, Z. Xu, N. Sun, et al.,
Dadiannao: A machine-learning supercomputer, in: Proceedings of the 47th Annual
. . . IEEE/ACM International Symposium on Microarchitecture, IEEE Computer Society,
No conflict of interest of this work. 2014, pp. 609-622.
[22] T. Luo, S. Liu, L. Li, Y. Wang, S. Zhang, T. Chen, Z. Xu, O. Temam, Y. Chen, Dadian-
nao: a neural network supercomputer, IEEE Trans. Comput. 66 (1) (2017) 73-88.
References [23] S. Williams, A. Waterman, D. Patterson, Roofline: an insightful visual performance
model for multicore architectures, Commun ACM 52 (4) (2009) 65-76.

[1] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, J. Cong, Optimizing fpga-based ac- [24] S. Charbel, S. Naresh, An analytical method to determine minimum per-layer preci-
celerator design for deep convolutional neural networks, in: Proceedings of the sion of deep neural networks, in: Acoustics, Speech and Signal Processing (ICASSP),
2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, 2018 IEEE International Conference on, IEEE, 2018.

ACM, 2015, pp. 161-170. [25] Eyeriss Dataflow Talk at ACM/IEEE ISCA, 2016. http://www.rle.mit.edu/

[2] Y.-H. Chen, T. Krishna, J.S. Emer, V. Sze, Eyeriss: an energy-efficient reconfigurable eems/wp-content/uploads/2016/06/eyeriss_isca_2016_slides. pdf
accelerator for deep convolutional neural networks, IEEE J. Solid-State Circu. 52 (1) [26] N. Li, S. Takaki, Y. Tomiokay, H. Kitazawa, A multistage dataflow implementation of
(2017) 127-138. a deep convolutional neural network based on FPGA for high-speed object recogni-

[3] A. Krizhevsky, I. Sutskever, G.E. Hinton, Imagenet classification with deep convo- tion, in: Image Analysis and Interpretation (SSIAI), 2016 IEEE Southwest Symposium
lutional neural networks, in: Advances in neural information processing systems, on, IEEE, 2016, pp. 165-168.

2012, pp. 1097-1105. [27] C. Zhang, Z. Fang, P. Zhou, P. Pan, J. Cong, Caffeine: towards uniformed repre-

[4] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Van- sentation and acceleration for deep convolutional neural networks, in: Proceed-
houcke, A. Rabinovich, Going deeper with convolutions, in: Proceedings of the IEEE ings of the 35th International Conference on Computer-Aided Design, ACM, 2016,
conference on computer vision and pattern recognition, 2015, pp. 1-9. pp. 12:1-12:8.

[5] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image [28] X. Wei, C.H. Yu, P. Zhang, Y. Chen, Y. Wang, H. Hu, Y. Liang, J. Cong, Automated
recognition, arXiv preprint arXiv:1409.1556 (2014). systolic array architecture synthesis for high throughput CNN inference on FPGAs,

[6] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: in: Proceedings of the 54th Annual Design Automation Conference, ACM, 2017,
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29:1-29:6.

2016, pp. 770-778. [29] N.P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bha-

[7] S. Ji, W. Xu, M. Yang, K. Yu, 3D convolutional neural networks for human action tia, N. Boden, A. Borchers, et al., In-datacenter performance analysis of a tensor
recognition, IEEE Trans. Pattern Anal. Mach. Intell. 35 (1) (2013) 221-231. processing unit, in: Computer Architecture (ISCA), 2017 ACM/IEEE 44th Annual

[8] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, Y. LeCun, Overfeat: inte- International Symposium on, IEEE, 2017, pp. 1-12.
grated recognition, localization and detection using convolutional networks, arXiv [30] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang, N. Xu, S. Song, et al., Go-
preprint arXiv:1312.6229 (2013). ing deeper with embedded fpga platform for convolutional neural network, in: Pro-

[9] R. Girshick, J. Donahue, T. Darrell, J. Malik, Rich feature hierarchies for accurate ceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable
object detection and semantic segmentation, in: Proceedings of the IEEE conference Gate Arrays, ACM, 2016, pp. 26-35.
on computer vision and pattern recognition, 2014, pp. 580-587. [31] L. Du, Y. Du, Y. Li, J. Su, Y.-C. Kuan, C.-C. Liu, M.-C.F. Chang, A reconfigurable

[10] T. He, W. Huang, Y. Qiao, J. Yao, Text-attentional convolutional neural network for streaming deep convolutional neural network accelerator for internet of things, IEEE
scene text detection, IEEE Trans. Image Process. 25 (6) (2016) 2529-2541. Trans. Circu. Syst. I Regul. Pap. 65 (1) (2018) 198-208.

[11] H. Li, Z. Lin, X. Shen, J. Brandt, G. Hua, A convolutional neural network cascade [32] K. Guo, L. Sui, J. Qiu, J. Yu, J. Wang, S. Yao, S. Han, Y. Wang, H. Yang, Angel-eye: a
for face detection, in: Proceedings of the IEEE Conference on Computer Vision and complete design flow for mapping CNN onto embedded FPGA, IEEE Trans. Comput.
Pattern Recognition, 2015, pp. 5325-5334. Aided Des. Integr. Circuits Syst. 37 (1) (2018) 35-47.

[12] D. Tomé, F. Monti, L. Baroffio, L. Bondi, M. Tagliasacchi, S. Tubaro, Deep convolu- [33] P. Judd, J. Albericio, T. Hetherington, T. Aamodt, N.E. Jerger, R. Urtasun, A.

tional neural networks for pedestrian detection, Signal Process. Image Commun. 47
(2016) 482-489.

Moshovos, Reduced-precision strategies for bounded memory in deep neural nets,
arXiv preprint arXiv:1511.05236 (2015).
L.-C. Hsu, C.-T. Chiu and K.-T. Lin et al.

[34]

[35]

[36]

[37]

[38]

S. Anwar, K. Hwang, W. Sung, Fixed point optimization of deep convolutional neu-
ral networks for object recognition, in: Acoustics, Speech and Signal Processing
(ICASSP), 2015 IEEE International Conference on, IEEE, 2015, pp. 1131-1135.

P. Judd, J. Albericio, T. Hetherington, T.M. Aamodt, A. Moshovos, Stripes: Bit-serial
deep neural network computing, in: Microarchitecture (MICRO), 2016 49th Annual
IEEE/ACM International Symposium on, IEEE, 2016, pp. 1-12.

J. Albericio, A. Delmas, P. Judd, S. Sharify, G. O’Leary, R. Genov, A. Moshovos,
Bit-pragmatic deep neural network computing, in: Proceedings of the 50th An-
nual IEEE/ACM International Symposium on Microarchitecture, ACM, 2017,
pp. 382-394.

C. Farabet, B. Martini, B. Corda, P. Akselrod, E. Culurciello, Y. LeCun, Neuflow:
A runtime reconfigurable dataflow processor for vision, in: Computer Vision and
Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference
on, IEEE, 2011, pp. 109-116.

V. Gokhale, J. Jin, A. Dundar, B. Martini, E. Culurciello, A 240 g-ops/s mobile copro-
cessor for deep neural networks, in: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops, 2014, pp. 682-687.

[39]

[40]

[41]
[42]

[43]

[44]

Journal of Systems Architecture 111 (2020) 101831

Y.-H. Chen, J. Emer, V. Sze, Eyeriss: A spatial architecture for energy-efficient
dataflow for convolutional neural networks, in: ACM SIGARCH Computer Archi-
tecture News, volume 44, IEEE Press, 2016, pp. 367-379.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hier-
archical image database, in: Computer Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, IEEE, 2009, pp. 248-255.

M.D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in:
European conference on computer vision, Springer, 2014, pp. 818-833.

K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image
recognition, arXiv preprint arXiv:1409.1556 (2014).

K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:
Proceedings of the IEEE conference on computer vision and pattern recognition,
2016, pp. 770-778.

K.T. Malladi, F.A. Nothaft, K. Periyathambi, B.C. Lee, C. Kozyrakis, M. Horowitz,
Towards energy-proportional datacenter memory with mobile DRAM, in: Computer
Architecture (ISCA), 2012 39th Annual International Symposium on, IEEE, 2012,
pp. 37-48.
