Gao and Chen Journal of Cloud Computing: Advances, Systems
and Applications (2020) 9:29

https://doi.org/10.1186/s13677-020-00173-y

RESEARCH Open Access

Approximation analysis of ontology
learning algorithm in linear combination

setting

Wei Gao!’t and Yaojun Chen??

Abstract

Journal of Cloud Computing:
Advances, Systems and Applications

Check for
updates

 

In the past ten years, researchers have always attached great importance to the application of ontology to its relevant
specific fields. At the same time, applying learning algorithms to many ontology algorithms is also a hot topic. For
example, ontology learning technology and knowledge are used in the field of semantic retrieval and machine
translation. The field of discovery and information systems can also be integrated with ontology learning techniques.
Among several ontology learning tricks, multi-dividing ontology learning is the most popular one which proved to be
in high efficiency for the similarity calculation of tree structure ontology. In this work, we study the multi-dividing

ontology learning algorithm from the mathematical point of view, and an approximation conclusion is presented
under the linear representation assumption. The theoretical result obtained here has constructive meaning for the
similarity calculation and concrete engineering applications of tree-shaped ontologies. Finally, linear combination
multi-dividing ontology learning is applied to university ontologies and mathematical ontologies, and the
experimental results imply that the higher efficiency of the proposed approach in actual applications.

Keywords: Ontology, Learning algorithm, Multi-dividing setting, Linear combination

Introduction

The term “ontology” was originally used in the field of
philosophy, meaning “the essence of things, itself’, and
the abstract nature of real things. Others describe ontol-
ogy as: “Ontology defines the relevant terms in the field,
associations and rules of domain lexical epitaxy”. Ontol-
ogy theory was first introduced into the field of computer
artificial intelligence. Later, the ontology was widely used
in the computer field, and more and more domain experts
defined the ontology. At present, the most extensive and
most popular ontology is defined as “the explicit formal
specification of sharing conceptualization’, whose mean-
ing includes four aspects:

 

*Correspondence: gaowei@ynnu.edu.cn

tWei Gao and Yaojun Chen contributed equally to this work.

'School of Information Science and Technology, Yunnan Normal University,
650500 Kunming, China

Full list of author information is available at the end of the article

o) Springer Open

 

e sharing, means people agree on the expression of
ontology;

e conceptualization, means that ontology is an abstract
expression of the real world;

e explicit, means concepts and conceptual
relationships are accurately and clearly defined;

e formal, means that concepts and relationships in
ontology are described as machine-recognizable
forms.

Domain ontology is an ontology type that is developed
through ontology research, and the ontology is classified
according to the degree of domain dependence. Specif-
ically, ontologies can be divided into four categories:
top-level ontology, domain ontology, task ontology, appli-
cation ontology. Since ontology can be regarded as a
structured collection of concepts, the inter-relationship
between concepts and the structural features of ontology

© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which
permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit
to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The

images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated
otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended
use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the
copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

are the essential problems of various applications of ontol-
ogy, and thus the semantic similarity calculation between
ontology concepts becomes the core of different ontology
algorithms.

Asa hot topic in the field of computer science and infor-
mation technology, ontology algorithms have always been
the key of data retrieval, image analysis, and applied to
many frontier fields such as big data, Internet of Things,
and deep learning. Subramaniyaswamy et al. [1] pro-
vided a personalized food recommender system in IoT-
based healthcare system in which ontologies are used to
bridge the gap between descriptions and heterogeneous
user profiles. Mohammadi et al. [2] examined the ontol-
ogy alignment systems using statistical inference where
some mathematical tricks like Wilcoxon signed-rank and
asymptotic tests are recommended based on their statis-
tical safety and robustness in different settings. Morente-
Molinera et al. [3] suggested a trick that uses sentiment
analysis procedures to automatically obtain fuzzy ontolo-
gies, and also multi-granular fuzzy linguistic modelling
are employed to choose the optimal representation mean
to store the information in fuzzy ontology. Sacha et al. [4]
raised an ontology VIS4ML to describe and understand
existing VA workflows used in machine learning. Kamsu-
Foguem et al. [5] pointed out that different flows and their
combinations can be dealt with by means of semantic Web
concepts and conceptual graph theories which permit
rules to be imbued to improve reasoning. Janowicz et al.
[6] focus on how to fully use SOSA, including integration
with the new release of the SSN ontology. Sulthana and
Ramasamy [7] proposed a neuro-fuzzy classification trick
in light of fuzzy rules, and ontology facilitates a systematic
and hierarchical methodology to manage the context. To
deal with syntactic evolution in the sources, Nadal et al.
[8] introduced a technology that the ontology upon new
releases are adapted semi-automatically. Scarpato et al. [9]
presented the reachability matrix ontology to describe the
networks and the cybersecurity domain, and then to com-
pute the reachability matrix. Karimi and Kamandi [10]
raised Inductive Logic Programming (ILP) based ontol-
ogy learning algorithm and used it to solve the ontology
mapping problem.

Various ontology algorithms are widely employed in
different engineering fields. Koehler et al. [11] intro-
duced the expansion of the Human Phenotype Ontology
(HPO). Chhim et al. [12] presented an efficacious prod-
uct design and manufacturing process based ontology
for manufacturing knowledge reuse. Ali et al. [13] mani-
fested a consensus-based Additive Manufacturing Ontol-
ogy (AMO) and presented how to use it for promoting re-
usability in dentistry product manufacturing. Neveu et al.
[14] proposed open-source Phenotyping Hybrid Informa-
tion System (PHIS) with its ontology-driven architecture
for building relationships between objects and enriching

(2020) 9:29 Page 2 of 10

datasets with knowledge and metadata. Kiefer and Hall
[15] updated gene ontology analysis for stimulate fur-
ther research and possible treatment. Jaervenpaeae et al.
[16] described the systematic development process of an
OWL-based manufacturing resource capability ontology
and its capabilities of manufacturing resources. Serra et
al. [17] demonstrated a proof of concept for leveraging the
built-in logical axioms of the ontology to classify patient
surface marker data into appropriate diagnostic classes.
Di Noia et al. [18] proposed to structure the knowl-
edge associated with NFRs in terms of fuzzy ontology for
tool-supported decision making in architectural design.
Ledvinka et al. [19] determined the implementation of
an ontology-based information system for aviation safety
data integration. Aameri et al. [20] raised an ontology
to specify shapes, parthood and connection in mechan-
ical assemblies such that the constraints of feasible con-
figurations can be logically expressed and used during
generative design.

With the proliferation of ontology processing concepts,
machine learning algorithms are applied to ontology sim-
ilarity calculations (some ontology learning tricks can be
referred to Gao et al. [21-23] and [24]). Among them,
the ontology learning algorithm in multi-dividing setting
has proved to be more efficient for the similarity calcula-
tion under the tree-shaped ontology structures (see Gao
et al. [25], Gao and Farahani [26], Wu et al. [27], Sangaiah
et al. [28] and Gao and Xu [29] for more details). Due to
the engineering accuracy of the multi-dividing ontology
learning algorithm has confirmed by different ontology
applications, this paper no longer gives the experimen-
tal results of the algorithm under special ontology data,
but from the statistical point of view. The approximation
property of the multi-dividing ontology learning algo-
rithm in a special expression setting is given.

In recent years, cloud computing has_ received
widespread attention, and the number and types of cloud
services it provides been increasing year by year (see
Bryce [30] and Song et al. [31]). Scholars are considering
how to quickly find the cloud services that users need
and effectively provide them to users (see Dimitri [32]).
The traditional cloud service is based on the search of
keywords, and the query results will contain irrelevant
information. At the same time, due to the defect of key-
word matching, it is easy to miss related services, which
is mainly because the traditional cloud query services
don't have keywords concept expand query function. In
order to solve the above-mentioned problems in existing
cloud services, cloud ontology-based semantic networks
can provide users with more accurate cloud services for
different needs information. Therefore, applying ontology
to cloud computing and cloud services is definitely worth
of looking forward to the future, which encourages us
to design specific ontology algorithms in cloud ontology
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

according to specific cloud service requirements (see
Sangaiah et al. [33] and [34]).

The rest of paper is organised as follows: first we intro-
duce the setting of ontology learning and in particular
multi-dividing ontology learning; then the main theoret-
ical result and its detailed proof are determined; finally,
we manifest two experiments on university and mathe-
matical ontology data to demonstrate the efficiency of the
algorithm.

Ontology learning problem

Throughout our paper, we use a graph to represent the
structure of the ontology. The vertices in the graph repre-
sent the concept of the ontology, while the edges between
the vertices express a direct subordinate relationship (or
inclusion relationship, affiliated relationship) between the
two concepts. In order to facilitate the mathematical rep-
resentation of ontology learning setting, we need to do
some processing and specification on the ontology data in
the begining stage, which has met the requirements of the
later mathematical expression.

First of all, we numerically denote the semantic informa-
tion, knowledge background, structure, instance, attribute
and classification information corresponding to a concept,
and then encapsulate it in a fixed-dimensional vector.
Through a certain technical means, we can unify the
dimensions of the vectors corresponding to the vertices of
all ontology concepts, and specify the same type of infor-
mation to be expressed of the corresponding sequence
number of components in the vectors. In this way, the
ontology information is represented by the corresponding
vector space, and thus the processing and calculation of
ontology data can be transformed into the processing and
calculation of multi-dimensional vectors. In what follows,
on the premise of not causing confusion, in order to facil-
itate the representation, we use v = (vj1,--- , Vp) (assume
v € R*) to simultaneously represent the ontology con-
cept, the vertex in the ontology graph corresponding to
the concept, and the vector corresponding to this vertex.

As a conceptual model, the main task of ontology is
to manage concepts and information mining. Therefore,
the similarity calculation between concepts is the core of
ontology application in various engineering fields. Specif-
ically, for the ontology vertices v; and v2, it is necessary
to characterize the measurement of sim(v1, v2). Since the
vertices are denoted by vectors, the similarity between
vertices can be regarded as the similarity between two
vectors in high-dimensional space.

A learning technique based on the dimension descent
method is to map each ontology vector into a real num-
ber, thereby mapping the entire ontology graph to the real
axis, and the similarity between the ontology vertices is
obtained by their one-dimensional distance on the real
axis. Specifically, let f : R’ — R be an ontology function

(2020) 9:29 Page 3 of 10

that maps ontology concept vectors into real numbers.
The similarity between the two ontology vertices v; and v2
is measured by |f(v1) — f(v2)|, and the larger the value is,
the smaller the similarity between v, and vz becomes, and
on the contrary, the smaller the value of |f(v1) —f(v2)|, the
larger the similarity between two vertices.

Therefore, in the standard ontology learning setting,
the ontology procedure can be described as follows. Let
V c R (p = 1) bean instance space (or called the input
space) for ontology graph, and the vertices in V are drawn
independently and randomly follow to a certain unknown
distribution D. The aim of ontology learning algorithms is
to deduce an optimal ontology function f : V — R using
the given ontology training set S = {v1,--- , v,} of size n.

Multi-dividing ontology setting

This framework of multi-dividing ontology learning algo-
rithm is based on the fact that most ontology graph
structures are tree structures (acyclic graphs). Forming
several branches of the tree below the topmost vertex,
if we classify all ontology concepts using a classification
algorithm, we find that the vertices in each branch corre-
spond exactly to a class of vertex classification. It means
that the similarity between the vertices of the same branch
is higher than the similarity between the vertices from
different branches. After mapping all ontology vertices to
the real axis, it can be observed that the real numbers
corresponding to the vertices of the same branch have
an aggregation effect on the one-dimensional axis (it can
be understood that the vertices of each branch form a
one-dimensional cluster on the axis). In light of this obser-
vation, we have reason to assume that the real numbers
corresponding to the same branch vertices are in the same
interval of the real axis. We can imagine cutting the entire
real axis into k-breaks (here k represents the number of
branches under the top vertex of the ontology graph), with
all the vertices of each branch happening within a certain
break. In the following contexts, we always assume k, a, b
are positive integers.

Now, we formally describe the multi-dividing ontology
algorithm. All the ontology vertices are divided into k
parts which is corresponding to k branches in the ontol-
ogy graph, and we endow rate number of these k parts of
vertices, denoted by 1, 2,--- ,k (note that the rate values of
all parts are determined by domain experts who have deep
domain knowledge related to ontology in certain engi-
neering application). Assume that f(v“) > f(v?) where f
is an ontology function, “ belongs to rate a vertices, v?
belongs to rate b vertices, and 1 < a < b < k. It reveals
that under the target ontology function, the value of high
rate vertex is bigger than the value of low rate vertex.

Correspondingly, the ontology training sample in
multi-dividing ontology setting is denoted as S =
(S1,S9,-++ , Sp) € V4 x V™ x +++ x V" which consists of
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

a sequence of training sample Sz = ({,--- ,v;,,) belongs
to rate a (here 1 < a < k). The ontology learner is given
such an ontology sample S and aim to learn a real-valued
ontology score function f : V > R (orf : R’ — R)
that the value of S, vertices are bigger than the value of
Sp vertices if 1 < a < b < k. Suppose that vertices in
each S, (here 1 < a < k) are drawn independently and
randomly according to certain unknown distribution D,
on the instance space V respectively. On the other hand,
b

since each vertex v/ or v; isa p-dimensional vector, we set

VE = (yeep) = (9), ooo), ) ith

i € {1,---,nmqg} andj € {1,--- , mp}.

Let [(-) be the binary truth function (it also named 0-1
function or 0-1 loss). Then, the ontology learning algo-
rithm in area under the receiver operating characteristic
curve criterion can be formulated by

 

k-1 k Nq Np
4¢,9= 0 > YY GW) = FO). W)
a=1 baa 92" j=1 j=l

Here we need to explain and state the following points:
(1) The optimal ontology function is obtained by maxi-
mizing A(f,S).

(2) The standard multi-dividing learning algorithm can be
stated as

 

k-1 k Na “bp
4,9 => YY [re = fopy
a=1b=at1 27> jt j=1

HUA) =Fopy

Clearly, our ontology framework omitted the sl fv) =

f (v?)) part in each of the accumulated items.
(3) The expected ontology model of (1) is denoted as

k-1 k
Af) => Yo Ev,~da,V5~D Lf (Va) > f (Vb). (2)
a=1 b=a+1

Remember that in the ontology sparse vector setting,
the ontology function can be concrete represented as

Pp

fv) =) Bi + Bo = vB" + Bo, (3)
t=1

where B = (f1,---, By) is ontology sparse vector such

that most of its components are supposed to be zero, and
Bo is an offset item. In many circumstances, we ignore fo
and consider f(v) = ye 1 Yt Pr. It’s general expand expres-
sion can be stated as f(v) = yet &+(Vz), where g; is some
kind of function (obviously, in the very special case of
ontology sparse vector setting, g:(v;) = vzP;).

(2020) 9:29 Page 4 of 10

Return to the standard framework with offset term, the
general ontology model (3) can be written by

P
fv) =) ge(vi) + Bo. (4)

t=1
In this paper, we consider the linear combination set-
ting where function g; can be formulated as g;(-) =
an Biqhq(-) where ¢,(-) are basis functions from (4).
Set

AAV) =f) — fH)

P
YS @((v)2) — ge((V})1))
t=1
p od
SoS Big Gat) — bq )2)) (5)

t=1 q=1

as the difference between value of two ontology functions
of v? and 4 . Thus, the expected version can be re-stated
as

k-1 k
A(f)= >> Y > P(ACVa, Vp) > 0), (6)

a=1 b=a+1

where V, ~ D, belongs to rate a and V;, ~ Dy, belongs to
rate b. Once a special combination of (a, b) is fixed, then
denote A”? (f) = P(A(Va, Vp) > 0).
Accordingly, the ontology empirical framework with
ontology sample set S = (S),S2,--- ,S%) € V" x *V™ x
- x Vk is re-formulated by

 

k-1 k Ng Np

A 1

AG,S)=)> > — YS MAGS, v7) > 0). (7)
a=\b=at+1 % 9 i=l j=l

It is not hard to verify that A (f, S) is an unbiased estimator
of A(f), that is to say, E[A(f,S)]= A(f). Therefore, the
ontology risk and ontology empirical risk are defined as
(5) — A) and (5) — A(f, S), respectively.

Our main result will be presented in next section which
characterizes the convergence property under condition
that each n; is a big number (here i € {1,--- ,k}) in the
multi-dividing ontology setting.

Main result and proof
In this section, we manifest our result and the detailed
proof is based on the statistical skills.

Hypothesis space is an important factor in statistical
learning theory. The ontology algorithm can't converge if
the space is too large, while the resulting optimal ontol-
ogy function does’t have excellent statistical properties if
the hypothesis space is too small. A crucial point in the
proof technique is to control the measure of the hypothe-
sis space to achieve a certain degree of balance. Here, we
set
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

Fa=f:f) =

Paty = = s > Bighg(vr)} (8)

t=1 q=1

as hypothesis space in our setting where ¢,(-) with q €
{1,--- ,d} are basis functions.

For each pair of (a4, b) with 1 < a < b < k,set on > ch,
Our main result is stated as follows.

Theorem 1 Assume c””
1<a<b<kandy-~

> 0 for each pair of (a, b) with

nee exp{—

n <1 "4--} < 00 for any

a € {l,- k —l}and ¢ > 0, ‘then
| sup A(f,S) — sup A(f)| > 0 (9)
fEFa feFa

holds almost everywhere.

Proof of Theorem 1. Our proof techniques depend
heavily on Hoeffding inequality, Borel-Cantelli lemma and
statistical property of shatter coefficient. For any com-
bination (a,b) with 1 < a < b < k, set Ae? =
1 LA, v7) > 0) where i € {1,---,n,}. Hence,
the ontology empirical version can be re-written as

A(f,S) =

1 > gee.

Ma ay

>

a=1 b=a+1

Notice that Ae? are all independent with Ar sb €[0,1] for
any combination (a, b) with fixed ve, ree, ve inp)? , where 1 <
a<b<k. Thus, in terms of Hoeffding Theorem, we infer

 

P(IA(f,S) — A(f)| > €)
k- k
_ -¥ S- (in Dat A®(f)| > ‘
a=1 b=a+1
k- k
- -> » (1) Scat — ae yp] > ret]
a=1 b=a+1 i=1
k-1 =k Na
-) > P ( SA? — BAP? V2, oy)
a=1 b=a+1 i=1

 

 

Na
+ dX (E (A? v8, vee Yeo) — A (f))| > nae)
k- k b ; ne
< x y- (1) (48 —E (at ivf, v5) ) 12 “a
a=1 b=a+1 i=l

Na
Ng€é
°( (E (AP? Wh, av) AMD) | > SE
L

‘—]

 

k-
“ry
a=1 b=a+1
=()+ dd).

(2020) 9:29 Page 5 of 10

For the first part (1), in view of Hoeffding inequality, we

deduce
7 ,D b b Ngé
—KE (47 [Vier vhy)) > **)

S (a7? — E (AP, 4,))

a,b
(4
i=1

 

 

 

 

 

a=1 b=a+1
i=l

k-1 k
“ry A
a=1 b=a+1
NgE, pf b
= ideo)

2
<) > 28 exe | "a | ot

a=1 b=a+1
a}

Using the same fashion, the second part (II) can be similar
bounded to

(II) = x “(i ( (Ari, Yrs)

 

 

 

b

 

a=1 b=a+1

 

 

a=1 b=a+1 i=1
—A*(f)) | > =)
2
k-1 k
< S S dexp |- at
a=1 b=a+1

By combining above two parts, we yield

P(IA(f,S) -A()| > «) < D+)

k-1 =k
<4 Yo ew{-™ 1} eo)

a=1 b=a+1

 

In terms of rn, 1 P(A(f, S) — A(f)| > €) < co and
Borel-Cantelli lemma, we know that IA(f, S) —A(f)| >
O holds almost everywhere. Since the VC dimension of

= ff = Bo + 02, T4 Boba) is dp + 1 and
there are )“7} i_, 41 NaNp U-statistic kind of observa-
tions, A (v4, v7) forl < a <b < k,i € (1,--:,nNg}
and j € {1,---,mp}, the shatter coefficient (the stan-
dard definition of shatter coefficient in multi-dividing
ontology setting can be stated as the same as it in the
k-partite ranking setting, for more details see Gao and
Wang [35]) of the linear ontology function space Fz can
be bounded by yet Ypeat1 (2 + 2(ngn, — 1)”} <
yt YW heatl 3(ngnp)”. In light of (10) and this upper
bound of shatter coefficient, we verity that

P (ss IA(f, S)

e€Fq

k-1 k 2
NgE
< 12 ) ) (ngnp)?4 exp {-"-| ;

a=1 b=a-+1

—A(f)| > ‘
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

According to the assumption in theorem that iA >

ch’ s+ Owith 1 < a < b < k, we see that for
each pair of (a,b), nq increases at the same rate of np.
It indicates that when the ontology vertex number is
increasing, the number of vertices in each branch will
grow relatively evenly (in graph theory, such structure
called nearly balanced tree). Thus, if iA > cH > 0,

k-1 =k ae”
then a=l ye b=at1 Sonam, (any r4 €xp \- * e

equivalent to yet peat yng ni? exp \- ne | < ©O
which is acted as known condition. Therefore, by means
of 72 > c% s 0 for each pair of (a,b) withl1 <a<b<k
and Borel-Cantelli Lemma, we confirm that

< © is

 

) P(sup |A(f, S) — A(f)| > €) < 00 (12)
feFa
and also derive
sup |A(f,S) — A(f)| > 0 (13)

SeFa

holds almost everywhere. .
Finally, we need to show that supyez,A(f,S) —
supfez, A(f) > 0 almost surely. Set

f* = arg max A(f, S)

SeFa
and
f* = sup A(f).
SeFa

Hence, the corresponding area under the receiver operat-
ing characteristic curve criterion are stated as

sup A(f,S) = A(f*, S)
feFa
and
sup A(f) = A(f*).
feFa
Combining all these facts, we get
| sup A(f,S) — sup A(f)|
feFa fEFa
= |A(f*,S) — A(f*)
< |AG*,S) -A™)| + lA) -AP YI.
By means of A(f*) < A(f*), the second term in (14) can be
decomposed into two terms and then dealt with as follows

IA(f*) —AG*)|

A(f*) — A(f*)

= A(f*) —A(f*,S) + AF*, S) — A(t")
A(f*) —A(f*,S) + AF*, S) — A)

2 sup |A(f,S) — AI.
feFa

(14)

lA

lA

(15)

(2020) 9:29 Page 6 of 10

From (13), (14) and (15), we obtain

| sup A(f,S) — sup A(f)|

fEFa feFa
< |A(f*,S) —AG*)| + IAG*) — Alf) |
< |A(Zf*,S) — A(f*)| +2 sup |A,S) — A/)|
feFg
< 3 sup |A(f,S) — A(f)| > 0
feFg
holds almost everywhere. L

Theorem 1 still establishes even if d and p are admit-
ted to increase relying on the ontology sample capacity.
In this case, the hypothesis space Fg increases as d and p
grows. However, the rates of d and p rely on the combi-
nation of (7 ,--- ,n,), which implies that the dimension
p has slower increasing rate than ontology sample capac-
ities. Theorem 1 reveals that if p is much larger than
ontology sample capacities, then the ontology empirical
risk minimization algorithm framework may not obtain
a desired performance. To our delight, in certain struc-
tural assumption settings (for instance, sparsity), we have
enough reasons to construct an optimal ontology rule if
some offset terms are eliminated when p is much larger
than ontology sample capacities.

Since 0-1 ontology loss is a non-derivable function, it
is difficult minimizing (5) — A(f , 5) in practice, and it is
natural to apply some approximation tricks based on the
smooth ontology function such as logistic ontology func-
tion A(x) = ns where Tt is a positive number
which is used to control how steep the logistic ontology
function is around zero. In light of A, (x), we consider the
®(f) approximation to (5) — A(f ,S) in the multi-dividing
ontology setting, i.e.,

 

k-1 k l Nq Nb
2) = ep LA (A084)
a=l baat 0"? j=1 j=l

b
1 la, exp {-A4, yr

 

k-1 k

dX fe, Mab iy j=1 1+ exp {-A4, vbr
for some positive number t. To Minimize ®(f) in Fg is
equivalent to that in terms of y € R?*%, and thus it can
be re-formulated as ®(f) = ®(y) where y = (1,--- 5 Yp)
and y% = (Y%1,--:, Yad) fort € {1,--- ,p}. Therefore, the
minimizer of ®(y) can be numerically identified since
®(y) is the smooth function of y. Theorem 1 indicates
that the optimization of 0-1 ontology loss is ensure to yield
the best character with certain restrictions on ontology
sample dimensions and capacities. But, generally speak-
ing, it is not practical to deduce the solution of minimizing
0-1 ontology loss, in particular in high dimensional set-
tings. In all, it’s a sensible and popular way to apply a
smooth ontology function to approximate 0-1 ontology
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

loss which is thought to be an intelligent approxima-
tion although its optimality of the approximation is still
unknown.

Now, we elaborately explain the weak points of linear
combination multi-dividing ontology learning algorithm,
that is, under what circumstances it is not suitable.

e It can be seen from the ontology learning models (1),
(2), (6), (7) that the technology to achieve
dimensionality reduction comes from the pairwise
comparison of the ontology sample vertices. The
weakness is that only two ontology vertices can be
extracted at one time for comparison, which results
in the number of vertex pairs to be compared in the
optimization model becoming very large as the
sample capacity increases.

e The depth of the vertex in the ontology graph is
defined as the distance between this vertex and the
top vertex, and the depth of the ontology graph is
generally defined as the depth of the deepest vertex.
As the depth increases, the concept of the ontology
will become more and more detailed, and the
similarity between the adjacent vertices of the upper
and lower layers will become greater. Conversely, the
smaller the number of layers is, the larger the span of
the ontology concept is, and the smaller the similarity
of the upper and lower concepts will become. This is
what we often say about the structure distribution of
the ontology graph. Again, back to look at our multi-
dividing ontology algorithm in linear combination
setting, it cannot reflect this structure characteristic
of the ontology graph since each pair of ontology
vertices comparison is from different rate of branches.

Experiment

In this section, we mainly focus on the effectiveness of
the algorithm in some specific fields from experimental
point of view. The ontology data we used here are all

(2020) 9:29 Page 7 of 10

tree-structures (or close to tree-structures) in order to
fit the multi-dividing setting, and we aim to investigate
the similarity-based ontology mapping between two dif-
ferent ontology trees in the same application field. The
entire execution process can be described as follows: first,
for two university ontology graphs or two mathematical
ontology graphs, domain experts determine the most sim-
ilar N vertices in another ontology of each vertex (here
N = 1,3,5), and they are marked as the target similar-
ity vertex set of each vertex; then, by means of our linear
combination multi-dividing ontology algorithm, we calcu-
late the real number corresponding to each vertex, and
record the most similar N vertices in corresponding ontol-
ogy; for each vertex, comparing the similarity set given
by the expert and obtained by the algorithm calculating,
and compute the matching rate; finally, the average match-
ing rate of the entire ontology graph is calculated in light
of the matching rate of all vertices in the two ontology
graphs.

Experiment on university data
University ontologies are very well-known ontologies,
which often appear in some explanations about ontology
introductory textbooks and examples, and the structure of
two university ontologies O; and O2 are depicted in Fig. 1.

In our multi-dividing linear combination setting, k =
3 and three branches correspond to “course’, “student”
and “staff” It is clear that |V(G)| = 28 where G is the
union of two subgraphs and the same concept in differ-
ent subgraphs can be regraded as different vertices. We
take 14 vertices as ontology sample set from the whole
vertex set. In order to compare it with other ontology
learning algorithms, we compare the experimental data
(some parts of these data have been already presented
in the previous articles), and part of the results are as
follows.

From the comparison of the data in Table 1, we can
see that our linear combination multi-dividing ontology

 

CS ontology A

 

 

Courses Student Academic Staff

Software Al vo.
Senior
Professor
Lecturer

Lecturer

 

 

Software

Engineering Data Mining

| ot |

Master PhD

 

Undergraduate

 

Fig. 1 University ontologies O; and O2

Software
engineering

CS ontology A

 

 

 

 

Courses people

 

 

 

Under graduate |
courses

Graduate

courses student faculty

Data mining

Lecturer professor

Associate
Professor

Undergraduate graduate

 
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

Table 1 Comparing result on university ontology when

 

 

N=1,3,5

P@idata P@3data P@5data
our linear combination ontology 0.5357 0.6785 0.800
learning algorithm
ontology learning algorithm in [22] 0.4643 0.5714 0.6642
ontology learning algorithm in [23] 0.5000 0.5962 0.6857
ontology learning algorithm in [36] 0.4286 0.5238 0.5929
ontology learning algorithm in [37] 0.2857 0.3929 0.5500
ontology learning algorithm in [38] 0.4643 0.5952 0.7429

 

learning algorithm is significantly better than the previ-
ous three algorithms for the average accuracy of university
ontology.

Experiment on mathematical data

Mathematical ontologies are constructed in mathematical
education and used to provide mathematical knowledge
for graduate students in the field of discrete mathematics,
and as the first experiment, our aim is to build a bridge
between the following two mathematical ontology graphs
based on similarity computing between ontology vertices.
The structure of two mathematical ontologies O3 and O4
are depicted in Fig. 2.

Although we found that the graph structures of O3 and
Og are not tree, their structures are very close to the tree-
shaped acyclic graph structure, and thus can be treated as
a tree structure after simple processing. After analysis we
take k = 4, and it is clear that |V(G)| = 26. We take half
of vertices as ontology sample, i.e., |S| = 13. Similarly, to
compare it with other ontology learning algorithms, we
directly use the experimental data which were presented
in the [24, 25] and [27]. Furthermore, we test the accuracy
of “confidence weighted ontology algorithm” presented in

(2020) 9:29 Page 8 of 10

[37] and “weak function based ontology learning algo-
rithm” manifested in [38], and compare to our ontology
learning algorithm. Part of the results are as follows.

From the comparison results manifested in Table 2, we
acquire that the linear combination multi-dividing ontol-
ogy learning algorithm proposed in our paper has higher
efficiency than the previous three algorithms for the aver-
age accuracy of mathematical ontology.

In the above two comparative experiments, we believe
that the reason why the data of the ontology learn-
ing algorithm in this paper is superior to the data of
other algorithms lies in that our algorithm is designed
for the tree structure, while focuses and goals achieved
in the engineering field of other ontology learning algo-
rithms are differently designed. The “University” ontology
is a pure tree structure, and although the “Mathemati-
cal” ontology is not strictly acyclic graph, it can also be
processed and divided according to the tree structure.
In contrast to other ontology learning algorithms com-
pared in experiments, some of them are not designed for
tree structures, and some of them use different angles to
design algorithms. For instances, (1) although the confi-
dence weighted ontology algorithm in [37] is also designed
under a multi-dividing framework, its purpose is to save
space complexity, and its core algorithm is a buffer update
strategy, not an iteration of ontology functions; (2) dise-
quilibrium ontology learning in [27] is also presented in
multi-dividing ontology learning setting, while it focuses
on the balance between the data rather than the struc-
ture of the ontology graph. In general, the efficiency of
the algorithm in this paper reflects its advantages over
tree-structured ontology graphs.

Conclusion

As a powerful auxiliary tool, the ontology has penetrated
into various research fields such as chemistry, genetics,
and pharmacy, providing technical support to scientists

 

Mathematics

a
4“ ¥ >
Operation Discrete mathematics Algebra
be a % 2
Mathematical logic | Combinatonics Group theory Matrices theory
ae fa cect
Graph theory Algebraic combinatonics
> 0 &

Topology graph theory Cayley graph theory Spectral graph theory

Fig. 2 Mathematical ontologies O3 and O4

 

b Mathematics

v »

4
Approximation Theory Discrete mathematics Algebra

-

be 4
; v
Rational approach Set theory Graph theory Group theory

/
Matrices theory

A
4 i ;
Fractional graph theory Algebra graph theory

a ay

Cayley graph theory Spectral graph theory

 

 
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

Table 2 Comparing result on mathematical ontology when

 

 

N=1,3,5

P@i data P@3data P@5data
our linear combination ontology 0.4615 0.6538 0.8077
learning algorithm
ontology learning algorithm in [24] 0.3846 0.5128 0.6923
PMDO Algorithm in [25] 0.4231 0.5128 0.7154
disequilibrium ontology learning in 0.3846 0.5000 0.6769
[27]
ontology learning algorithm in [37] 0.2692 0.3717 0.4923
ontology learning algorithm in [38] 0.4231 0.5513 0.6846

 

from all walks of life. In the process of ontology con-
struction, scholars found that most ontology uses tree
structure to represent the hierarchical relationship and
derivative relationship of concepts. It can be said that
tree structure is the most suitable structural representa-
tion of ontology concepts. Based on this fact, researchers
proposed several multi-dividing ontology learning algo-
rithms, which specifically divide the categories of vertices
for multiple branches of the ontology tree structure. The
existing experimental data can fully explain that the multi-
dividing ontology algorithms have higher efficiency for
some well-known application ontologies (such as “GO”,
“PO’; etc).

In this article, we only focus on the theoretical analysis
of ontology learning algorithm. The approximation prop-
erty of the multi-dividing ontology learning algorithm is
analyzed from the perspective of statistical learning the-
ory, and the result shows that the algorithm has very
good approximation properties in the linear combination
setting.

We list some open problems as the end of this paper:

e How to use the covering number to characterize the
properties of the hypothesis space, and then obtain
the theoretical boundary of the covering number
approximation in multi-dividing ontology learning
setting.

e What will happen if we assume the ontology tree is
not balanced (v; are not increasing at the same rate,
where i € {1,--- ,k})?

e Finda suitable assumption to ensure the ontology
function satisfy the “Uniform Glivenko-Cantelli”
characteristic in multi-dividing ontology learning
setting.

Acknowledgements
We thank the reviewers for their constructive comments in improving the
quality of this paper.

Authors’ contributions
The authors have worked equally when writing this paper. All authors read
and approved the final manuscript.

(2020) 9:29 Page 9 of 10

Funding
This work was supported in part by National Natural Science Foundation of
China (11761083).

Availability of data and materials
The datasets generated during the current study are available from the
corresponding author on reasonable request.

Competing interests
The authors declare no conflict of interest.

Author details

'School of Information Science and Technology, Yunnan Normal University,
650500 Kunming, China. Department of Mathematics, Nanjing University,
210093 Nanjing, China.

Received: 6 November 2019 Accepted: 29 April 2020
Published online: 04 June 2020

References

1. Subramaniyaswamy V, Manogaran G, Logesh R, et al. (2019) An
ontology-driven personalized food recommendation in loT-based
healthcare system. J Supercomput 75:3184-3216

2. Mohammadi M, Hofman W, Tan YH (2019) A comparative study of
ontology matching systems via inferential statistics. IEEE T Knowl Data En
31:615-628

3. Morente-Molinera JA, Kou G, Pang C, et al. (2019) An automatic
procedure to create fuzzy ontologies from users’ opinions using
sentiment analysis procedures and multi-granular fuzzy linguistic
modelling methods. Inform Sci 476:222-238

4. Sacha D, Kraus M, Keim DA, et al. (2019) VIS4ML: an ontology for visual
analytics assisted machine learning. IEEE Tran Vis Comput Gr 25:385-395

5. Kamsu-Foguem B, Abanda FH, Doumbouya MB, et al. (2019) Graph-based
ontology reasoning for formal verification of BREEAM rules. Cogn Syst Res
55:14-33

6. Janowicz K, Haller A, Cox SJD, et al. (2019) SOSA: A lightweight ontology
for sensors, observations, samples, and actuators. J Web Semant 56:1-10

7. Sulthana AR, Ramasamy S (2019) Ontology and context based
recommendation system using Neuro-Fuzzy Classification. Comput Electr
Eng 74:498-510

8. Nadal S, Romero O, Abello A, et al. (2019) An integration-oriented
ontology to govern evolution in Big Data ecosystems. Inform Syst 79:3-19

9. Scarpato N, Cilia ND, Romano M (2019) Reachability matrix ontology: a
cybersecurity ontology. Appl Artif Intell 33:643-655

10. Karimi H, Kamandi A (2019) A learning-based ontology alignment
approach using inductive logic programming. Expert Syst App!
125:412-424

11. Koehler S, Carmody L, Vasilevsky N, et al. (2019) Expansion of the Human
Phenotype Ontology (HPO) knowledge base and resources. Nucleic Acids
Res 47:D1018-D1027

12. Chhim P, Chinnam RB, Sadawi N (2019) Product design and
manufacturing process based ontology for manufacturing knowledge
reuse. J Intell Manuf 30:905-916

13. Ali MM, Rai R, Otte JN, et al. (2019) A product life cycle ontology for
additive manufacturing. Comput Ind 105:191-203

14. Neveu P, Tireau A, Hilgert N, et al. (2019) Dealing with multi-source and
multi-scale information in plant phenomics: the ontology-driven
Phenotyping Hybrid Information System. New Phytol 221:588-601

15. Kiefer J, Hall JG (2019) Gene ontology analysis of arthrogryposis (multiple
congenital contractures). Am J Med Genet C 181:310-326

16. Jaervenpaeae E, Siltala N, Hylli O, et al. (2019) The development of an
ontology for describing the capabilities of manufacturing resources. J
Intell Manuf 30:959-978

17. Serra LM, Duncan WD, Diehl AD (2019) An ontology for representing
hematologic malignancies: the cancer cell ontology. BMC Bioinformatics
20:231-236

18. Noia TD, Mongiello M, Nocera F, et al. (2019) A fuzzy ontology-based
approach for tool-supported decision making in architectural design.
Knowl Inf Syst 58:83-112
Gao and Chen Journal of Cloud Computing: Advances, Systems and Applications

19. Ledvinka M, Lalis A, Kremen P (2019) Toward data-driven safety: an
ontology-based information system. J Aerospace Inform Syst 16:22-36

20. Aameri B, Cheong H, Beck JC (2019) Towards an ontology for generative
design of mechanical assemblies. Appl Ontology 14:127-153

21. Gao W, Chen YJ, Baig AQ, et al. (2018) Ontology geometry distance
computation using deep learning technology. J Intell Fuzzy Syst
35:4517-4524

22. Gao W, Farahani MR, Aslam A, et al. (2017) Distance learning techniques
for ontology similarity measuring and ontology mapping. Cluster
Comput 20:959-968

23. Gao W, Zhu LL, Wang KY (2015) Ontology sparse vector learning
algorithm for ontology similarity measuring and ontology mapping via
ADAL technology. Int J Bifurcat Chaos 25. https://doi.org/10.1142/
$0218127415400349

24, Gao W, Zhang YO, Guirao JLG, et al. (2019) A discrete dynamics approach
to sparse calculation and applied in ontology science. J Differ Equ Appl
25(9-10):1239-1254

25. Gao W, Guirao JLG, Basavanagoud B, et al. (2018) Partial multi-dividing
ontology learning algorithm. Inform Sci 467:35-58

26. Gao W, Farahani MR (2017) Generalization bounds and uniform bounds
for multi-dividing ontology algorithms with convex ontology loss
function. Comput J 60:1289-1299

27. Wu JZ, Yu X, Gao W (2017) Disequilibrium multi dividing ontology
learning algorithm. Comm Stat Theory Methods 46:8925-8942

28. Sangaiah AK, Medhane DV, Han T, et al. (2019) Enforcing position-based
confidentiality with machine learning paradigm through mobile edge
computing in real-time industrial informatics. IEEE Tran Ind Inform
15(7):4189-41 96

29. Gao W, Xu TW (2013) Stability analysis of learning algorithms for ontology

similarity computation. Abstr Appl Anal. http://dx.doi.org/10.1155/2013/
174802

30. Bryce C (2019) Security governance as a service on the cloud. J Cloud
Comput 8:23. https://doi.org/10.1186/s13677-019-0148-5

31. SongR, Xiao Z, Lin J, et al. (2020) CIES: Cloud-based Intelligent Evaluation
Service for video homework using CNNLSTM network. J Cloud Comput
9:7. https://doi.org/10.1186/s13677-020-0156-5

32. Dimitri N (2020) Pricing cloud laaS computing services. J Cloud Comput
9:14. https://doi.org/10.1186/s13677-020-00161-2

33. Sangaiah AK, Medhane DV, Bian GB, et al. (2019) Energy-aware green
adversary model for cyber physical security in industrial system. IEEE Tran
Ind Inform. https://doi.org/10.1109/TII.2019.2953289

34. Sangaiah AK, Hosseinabadi AAR, Sadeghilalimi M, et al. (2019) Energy
consumption in point-coverage wireless sensor networks via bat
algorithm. IEEE Access. https://doi.org/10.1109/ACCESS.2019.295 2644

35. Gao W, Wang WF (2018) Analysis of k-partite ranking algorithm in area
under the receiver operating characteristic curve criterion. Int J Comput
Math 95:1527-1547

36. Gao W, Baig A. Q, Ali H, et al. (2017) Margin based ontology sparse vector
learning algorithm and applied in biology science. Saudi J Biol Sci
24:132-138

37. ZhuLl, Hua G, Zafar S, et al. (2018) Fundamental ideas and mathematical
basis of ontology learning algorithm. J Intell Fuzzy Syst 35:4503-45 16

38. Zhu LL, Hua G, Aslam A (2018) Ontology learning algorithm using weak
functions. Open Phys 16:910-916

Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional affiliations.

(2020) 9:29 Page 10 of 10

 

 

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

 

Submit your next manuscript at > springeropen.com

 

 
