Indonesian Journal of Electrical Engineering and Computer Science
Vol. 20, No. 3, December 2020, pp. 1584~1590
ISSN: 2502-4752, DOI: 10.1159 1/ijeecs.v20.13 .pp1584-1590 O 1584

Best neural simultaneous approximation

Hawraa Abbas Almurieb, Eman S. Bhaya
Department of Mathematics, College of Education for Pure Sciences, University of Babylon, Iraq

Article Info ABSTRACT

Article history: For many years, approximation concepts has been investigated in view of
neural networks for the several applications of the two topics. Researchers

Received Jan 14, 2020 studied simultaneous approximation in the 2-normed space and proved

Revised Apr 28, 2020 essential theorems concern with existence, uniqueness and degree of best

Accepted Jun 2, 2020 approximation. Here, we define a new 2-norm in Lp-space, with p < 1,

so we call it Lp quasi 2- normed space (Ly, 2). The set of approximations is a
space of feedforward neural networks that is constructed in this paper.
Keywords: Existence and uniqueness of best neural approximation for a function from
Ly,2 1s proved, describing the rate of best approximation in terms of modulus

2-normed space
of smoothness.

Approximation
Neural networks

Copyright © 2020 Institute of Advanced Engineering and Science.
All rights reserved.

Corresponding Author:

Hawraa Abbas Almurieb,

Department of Mathematics,

University of Babylon, Iraq.

Email: pure.hawraa.abbas @ uobabylon.edu.iq

1. INTRODUCTION

The first notes about simultaneous approximation was done by Dunham in [1]. He generated
the classical Chebyshev approximation by approximating two continuous functions f* and f-,
with f*(x) < f~ (x), for all x € [a, b], simultaneously. He also proved that his simultaneous approximation
is equivalent to the classical one function Chebyshev approximation when f* = f~.

For more specification, Diaz and Mclaughlin [2] proved that the above problem is equivalent to

the problem of approximating 5 |f* + f-|. Also approximating two appropriate functions simultaneously is
equivalent to approximating one function by elements of a certain set S. Moreover, they defined the best
simultaneous approximation Sto the set S in [3] as follow

inf supllf — sll = supllf — Sl,
ses fEF fEF

where F is a set of uniformly bounded functions on [a, b| and S is a set of functions on [a, b]. They proved
that S is equivalent to the best simultaneous approximation of two functions. The set F varies among
researchers, it is C[a, b| in [4] and [5], the space of uniformly bounded functions in [6], Banach space in [7],
weighted space [8], Lp spaces [4] or 2-normed space as in [9-16].

The 2-normed space was firstly defined by Gahler in his paper [9], and then generalized by Iseki in
his paper [17]. This space provides a tool to deal with 2-structures. For the same porpuse, others defined
quasi-normed , quasi-(2;p)-normed space and generalized each one (see [18, 19]). First, we define
the 2-normed space generally from [1]

Definition (1) A norm || +: || :X x X > R* is 2-norm on X if it satisfies the following conditions:
[C1] ||x,,x>|| = 0, if and only if x,,x, are linearly dependent from X. .
[C2] ||x,,x2|| = ||x2,x,, || , for all x,,x, from X.

Journal homepage: http://ijeecs.iaescore.com
Indonesian J Elec Eng & Comp Sci ISSN: 2502-4752 O 1585

[C3] |lax,,x2|| = lal||x,,x.|| ,for alla € R and x,,x, € X
[C4] ||x, + x2,Z]| < ||x1,2Z|| + |lx2,Z]| , for all x,,x2,z2 € X.

The space (X, || -, ||) is called 2-normed space. Later, Park [18] substitute [C4] with the following
condition
[C4*] ||x, + x2,2|] <Cf{I|x,,Z]| + |lx2,2z|| }, for all x,,x.,z EX.

In this paper, we deal with 2-normed space with a special definition that deals with
Lebesgue-integrable space

Lyla, b] = {f:S, If @) Pax < 0}

Through this paper, we refer L,, to the space L,,[a, b] X L,[a, b] in the following manner

b
Lyla, b] = tf: J, If xo (x) |Pdx < o, forevery py € Ly } (1)
Together with the non-neagative function ||-,-||,, over the vector space L,. as follow
b 1/p
If. dlp = (SUF @gG)lPax) (2)

for any function f and g from L, 2. The space L,, [a,b] is a 2-normed space since it satisfies the following
conditions

[C1] Ilf, gllp = 0, if and only if f and g are linearly dependent functions from L, 2. .

[C2] lf, gllp = Ilo. fll, . for all f and g from L, >.

[C3] llef, gll,p = lalllf, gll,, for alla € R and f,g € Ly».

[C4] If + 9, lly < CLF, Olly + Ilo, Gllp}, for all f, 9,9 € Ly».

The space L,, 5 [a, b] is a 2-normed space since it satisfies the conditions in Definition(1).
[C1] Let f, g be two linearly dependent functions from L,,., with f # g iff (f,g) = 0, iff ||f, I\lp = 9.
[C2] By (2), we have ||f, gllp = Ilo. Fllp-

[C3] Leta ER, then |laf, gll, = (Silat @gGol dx)? = |a| (if@gGor dx)? , = lalIf, Dl
[C4] Let f,g € L, 2, since 0 < p < 1, then there exists C > 0 satisfies

If + 9 9lly = (RIG + @)eC@)IPax)?

<C {(irco@corrar)’ +(f ax) 0(2) Pax)?

= C{Ilf, ¢llp + lla, oly}.

To continue our investigation for a neural best approximation, we need the following definitions that
are related to convergence sequences of functions from Ly >.
Definition(2) A sequence of functions {f, }n21 from L, is said to be Cauchy Sequence if and only
if
lim Ilfn Sm Prllp =0,
nm-0o
and
lim lf, ~~ Tm: Pally a 0,
nm-0o
for some independent functions @,, pz € Ly».
Definition(3) A sequence of functions {f,},>1 from L,, is said to be convergent to some f € Ly» if

and only if

nm--oo

Best neural simultaneous approximation (Hawraa Abbas Almurieb)
1586 O ISSN: 2502-4752

for all g € L,2. The following definitions give some useful properties to the space L, , that we need later in
the main results.

Definition(4) The space L,» is said to be complete if and only if every Cauchy sequence
(fnineifrom L, » converges to a function that belongs to L, 2. To measure the degree of best approximation,
we define the modulus of smoothness in L,, as follow

Definition(5) Let f € L, >, then the kth symmetric difference of f is given by

k _j kh, kh
AK(f,x,[ab)) = Ea(i)om f(x-Ftih), xt ela] (3)
; O.W.

So the kth modulus of smoothness of f is given by
of, 9, 5,14, bl)» = sup [ARG lL, (4)
0<hséd

for some 6 > 0.

2. CONSTRUCTION OF FNN WITH RELU ACTIVATION FUNCTION

We have to talk about the set of approximation. Choosing the target approximation space is as much
important as choosing the function space. It is related to the applicable properties and the accurate results to
each space. Moreover, sometimes it is preferred to replace a certain function by its approximation from some
vital space. Scientists approximate functions by polynomials, wavelets, splines and neural networks.
For the wide usage of neural networks and their ability to solve problems from different fields (see [21-38])
a set of functions from Lp space is approximated by neural networks in this work. Many papers contains this
topic widely, we mention some of them in the references below (see [39-47]).

Let the approximation neural operator

N = an c;R(w;x + Vi), (5)
where

0, x< ny

— + _ —
R(x) = xt = max(0,x) = Le x>0

Co)
is the Relu activation function. For its simplicity and efficiency, scientists use Relu function to activate
the neural network. In comparison with other activation functions, it gives faster and more acceptable results,
it solves the problem of vanishing gradient that most activation functions suffer from. In the field of function
approximation, [48-50] are some papers that dealt with neural approximation with Relu activation function.
Now, we are ready to discuss the essential point in this paper. Here is the definition of the best simultaneous
approximation of the set L,,. by elements of & under the norm (2).

Definition(6) The simultaneous best approximation of a subset F of L,, 2. is N* € & in the expression

inf }su —N, = su —N*, 7
inf {supli oll} supllf ll> (7)

In the next section, we construct our neural approximation of type (5) simultaneously to L,, >.

3. EXISTENCE THEOREM
Let f € L,, 2, then there exists a FNN of the form:
Nn = Liar GRWjx + 9;),

where & is the Relu activation function on [a, b] and the parameters c; , w, , and J; are chosen as follow:

Indonesian J Elec Eng & Comp Sci, Vol. 20, No. 3, December 2020: 1584 - 1590
Indonesian J Elec Eng & Comp Sci ISSN: 2502-4752 O 1587

hn
tL |b-al’

hn . b-a
0; — Tp-al (2a + (2i — 1) -*),

 

 

Co = f(a) — XL, GR(w,at 9;),
tuo, (k k-i kh,
Ci = sp eui=0 (") (-1)*"'f (x —Z Ft in),
where h = ~

Proof:
Since R(x) = xt, Vx € [a, b], then by (6), sup ROO! = bp.

Ls b
Let the partition a < xX, <x, << xX, * ‘ach that forall 1 <i<n,and let x; =ati—,

Choosing Cy = f(a) — 0, ¢;R(w,a + 0;), gives the guaranty that f(a) = N,,(a).
For all x € [a,b], thereis 7 EN, 0<j <n, such that x € [xj_1,%;], and that

q(x) = f(a) + Ta UK (7) Des (x — 2+ Uh) [Rox + 9,) — R(ma + 9)

ma

= f@ + TST (TF) Coes (x - S + ih) [Romx + 8) -— ROwia + 9]

i=1 9p

+2 0b (F) CDE (x4 —S + ih) Rox + 8) -— Ra + 9]

+The Ko() \pk's (x- + In) [Rox + 9) — ROwia + 9)

= f(a) +S, + S, + S3

To estimate |R(w;x + 0;) — R(w,a + 0;)|, we have two cases. For i > j, we have, x < x; < Xj-1,
so by monotonicity of R and our choices of the parameters w; , Y; and x;, we get
Case(1)

0< Rw; x+0;) —-Rwjat V;)

< R(wix; + 9;) —R(wja + 0;)

< Rw; x;_-1 + 0;) —R(wja + 9;)

= R(h) — R(2Zhi — h)

_ bt
= —2hi =

Case(2)
For i < j, we have, x; S x;_, < x, then
R(w,x + 9;) —-R(w,a + 9;)
> Rw; x; + 9;) — RW; X)-1 + Yj)

= R(—-h) — R(A)

Best neural simultaneous approximation (Hawraa Abbas Almurieb)
1588 O ISSN: 2502-4752

For the two cases, we conclude that
Rw x + 9) — Rwat9)| <h=—
Now, we are ready to estimate S,, S, and S3

j— k _ kh
Iss} <= yt Ko(7) (1) ¢ (x - + th)| IROw;x + 8) — Ra + 9,)|
< “AK CF, x, [a, b])

Sol < SDK (F) Ck" |f (45 — “+ th) | IROwix + 8) - ROma + 8)
< TAKS, x, [a, b)

IS3| < — ya Li=0 (*) (—1)*" If (x — = + in)| IR(w;,x + 9;) —R(wja + 9)
<7 AK(f,x, [a,b

Finally, let € L then

Dp,2 ?

[Nn (x) — FC), GIR < f'1Na CX) — FOOT? lx) Pax

< ASE Rx +8) - Rat PSE (*) Coe |¢(x- 2+ n)| leoolrax

2b

< = Wy (a a

4. UNIQUENESS THEOREM
The simultaneous best approximation N“ € & of a subset F of L,, 2 is unique.

Pr oof:

To prove that N* € XN is unique, suppose that N,,N. € & be two simultaneous approximations to F,
then by Definition(3)

lim IN; f,glly = 0,
and

lim |IN2-f, lly = 9,

So by condition [C4*] of Definition(1), there exists k => 1,

IN; — Nz, gly < KCN: — f,¢llp + IIN2 — fF, Gllp)

By taking limits to both sides as n tends to infinity, then for all p € L,,»

lim IIN, ~~ Np, ll> a 0,

So N, = No, and the best simultaneous approximation to F out of X is unique.

Indonesian J Elec Eng & Comp Sci, Vol. 20, No. 3, December 2020: 1584 - 1590
Indonesian J Elec Eng & Comp Sci ISSN: 2502-4752 O 1589

5. CONCLUSION

Simultaneous approximation in the L,,. space is defined in details in this paper. Construction of
neural networks with rectified activation function that approximates a subset of L,. simultaneously is
obtained too. In spite of its various applications, it gives accurate results that depends on modulus of
smoothness. It would be interesting to discuss vital applications in 2-structure spaces for the constructed
neural network.

REFERENCES

[1] C. B. Dunham, “Simultaneous Chebyshev Approximation of Functions on an Interval,” Proc. Am. Math. Soc.,
vol. 18, no. 3, p. 472, 1967.

[2] J.B. Diaz and H. W. Mclaughlin, “On Simultaneous Chebyshev Approximation and Chebyshev Approximation
with an Additive Weight,” J. Approx. Theory, vol. 71, no. 6, pp. 68-71, 1972.

[3] J. B. Diaz and H. W. Mclaughlin, “Simultaneous approximation of a set of bounded real functions,” Math.
Comput., vol. 23, no. 107, pp. 583-594, 1969.

[4] F. E. Levis, “Interpolation and best simultaneous approximation,” 2017.

[5] C. Li and G. A. Watson, “On nonlinear simultaneous Chebyshev approximation problems,” J. Math. Anal. Appl.,
vol. 288, no. 1, pp. 167-181, 2003.

[6] S$. Atacik, “Simultaneous approximation of a uniformly bounded set of real valued functions,” J. Approx. Theory,
vol. 45, no. 2, pp. 129-132, 1985.

[7] P. D. Milan, “On Best Simultaneous Approximation in Normed Linear Spaces,” J. Approx. Theory, vol. 238,
pp. 223-238, 1977.

[8] A. A. L. I. Hussein, “Best simultaneous approximation in weighted space Best simultaneous approximation in
weighted space,” 2019.

[9] S.Gahler, “Lineare 2-Normierte Raume,” Math. Nachrichten, vol. 28, pp. 1-43, 1964.

[10] S. Elumalai and R. Vijayaragavan, “Characterizations of best approximations in linear 2-normed spaces,” Gen.
Math., vol. 17, no. 3, pp. 141-160, 2009.

[11] A. Mehmet, “The best simultaneous approximation in linear 2-normed spaces,” Proc. Jangjeon Math. Soc., vol. 15,
no. 4, pp. 415-422, 2012.

[12] M. Acikgoz, “The best simultaneous approximation in linear 2-normed spaces,” 2014.

[13] M. Acikgoz, “Approximation in generalized 2-normed spaces,” 2014.

[14] S. Cobzas and R. Academy, “Extension of bilinear functionals and best approximation in 2-normed spaces,” 2017.

[15] M. Iranmanesh and F. Soleimany, “2-NORMED SPACES,” vol. 46, no. 1, pp. 207-215, 2016.

[16] A. Kundu and T. B. Sk, “On metrizability and normability of 2-normed spaces,” Math. Sci., vol. 13, no. 1,
pp. 69-77, 2019.

[17] K. Iseki, “Mathematics on 2-normed spaces,” Korean Math. Soc., vol. 13, pp. 127-136, 1976.

[18] C. Park, “Generalized quasi—banach spaces and quasi— (2;p)-Normed Spaces,” J. Chungcheong Math. Soc., vol. 19,
no. 2, pp. 197-206, 2006.

[19] K. Kikina, et al., “Quasi-2-Normed Spaces and Some Fixed Point Theorems,” vol. 474, no. 2, pp. 469-474, 2016.

[20] C. Park, “Generalized quasi-Banach spaces and quasi-(2, p)- normed spaces,” J. Chungcheong Math. Soc., vol. 19,
no. 2, pp. 197-206, 2006.

[21] L.E. Aik, et al., “An improved radial basis function networks based on quantum evolutionary algorithm for training
nonlinear datasets,” IAES Int. J. Artif. Intell., vol. 8, no. 2, pp. 120-131, 2019.

[22] A. S. Takialddin, et al., “Overview of model free adaptive (MFA) control technology,” IAES Int. J. Artif. Intell.,
vol. 7, no. 4, pp. 165-169, 2018.

[23] V. Kathirvel, et al., “Hybrid imperialistic competitive algorithm incorporated with hopfield neural network for
robust 3 satisfiability logic programming,” IAES Int. J. Artif. Intell., vol. 8, no. 2, pp. 144-155, 2019.

[24] M. Rhanoui, et al., “Forecasting financial budget time series: Arima random walk vs Istm neural network,” IAES
Int. J. Artif. Intell., vol. 8, no. 4, pp. 317-327, 2019.

[25] S. Ibrahim, et al., “Optimization of artificial neural network topology for membrane bioreactor filtration using
response surface methodology,” IAES Int. J. Artif. Intell., vol. 9, no. 1, pp. 117-125, 2020.

[26] S. I. Abdullahi, et al., “Intelligent flood disaster warning on the fly: Developing IoT-based management platform
and using 2-class neural network to predict flood status,” Bull. Electr. Eng. Informatics, vol. 8, no. 2, pp. 706-717,
2019.

[27] S. Verma, et al., “ANN based method for improving gold price forecasting accuracy through modified gradient
descent methods,” IAES Int. J. Artif. Intell., vol. 9, no. 1, pp. 46-57, 2020.

[28] I. Hachchane, et al., “Large-scale image-to-video face retrieval with convolutional neural network features,” [AES
Int. J. Artif. Intell., vol. 9, no. 1, pp. 40-45, 2020.

[29] P.R. Iyer, et al., “Adaptive real time traffic prediction using deep neural networks,” [AES Int. J. Artif. Intell., vol. 8,
no. 2, pp. 107-119, 2019.

[30] W.N. W. Md Adnan, et al., “Development of option c measurement and verification model using hybrid artificial
neural network-cross validation technique to quantify saving,” IAES Int. J. Artif. Intell., vol. 9, no. 1, pp. 25-32,
2020.

[31] S.B. Jadhav, et al., “Convolutional neural networks for leaf image-based plant disease classification,” [AES Int. J.
Artif. Intell., vol. 8, no. 4, pp. 328-341, 2019.

Best neural simultaneous approximation (Hawraa Abbas Almurieb)
1590

[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]

[40]

[41]
[42]
[43]
[44]
[45]
[46]

[47]
[48]

[49]

[50]

0 ISSN: 2502-4752

S. Barhmi and O. El Fatni, “Hourly wind speed forecasting based on support vector machine and artificial neural
networks,” IAES Int. J. Artif. Intell., vol. 8, no. 3, pp. 286-291, 2019.

N. Mahmod, et al., “Modelling and control of fouling in submerged membrane bioreactor using neural network
internal model control,” [AES Int. J. Artif. Intell., vol. 9, no. 1, pp. 100-108, 2020.

L. E. Aik, et al., “An improved radial basis function networks in networks weights adjustment for training real-
world nonlinear datasets,” [AES Int. J. Artif. Intell., vol. 8, no. 1, pp. 63-76, 2019.

H. A. Rahim, et al., “Exploration on digital marketing as business strategy model among malaysian entrepreneurs
via neurocomputing,” IAES Int. J. Artif. Intell., vol. 9, no. 1, pp. 18-24, 2020.

H. Ohmaid, et al., “Iris segmentation using a new unsupervised neural approach,” IAES Int. J. Artif. Intell., vol. 9,
no. 1, pp. 58-64, 2020.

A. Sarkar, “Multilayer neural network synchronized secured session key based encryption in wireless
communication,” [AES Int. J. Artif. Intell., vol. 8, no. 1, pp. 44-53, 2019.

M. S. Gaya, et al., “Estimation of water quality index using artificial intelligence approaches and multi-linear
regression,” IAES Int. J. Artif. Intell., vol. 9, no. 1, pp. 126-134, 2020.

G. Cybenko, “Continuous Valued Neural Networks: Approximation Theoretic Results,” in Computer Science and
Statistics: proceedings of the 20th Symposium on the Interface, pp. 174-183, 1988.

T. Chen and R. Chen, “Approximation Capability to Functions of Several Variables , Nonlinear Functionals and
Operators by Radial Basis Function Neural Networks,’ IEEE Trans. Neural Networks, vol. 6, no. 4, pp. 904-910,
1995.

Z. Zhang, et al., “The new approximation operators with sigmoidal functions,” J. Appl. Math. Comput, vol. 42,
no. 1-2, pp. 455-468, 2013.

E. S. Bhaya, “Neural Network Trigonometric Approximation,” no. 9, pp. 2395-2399, 2016.

H. A. Almurieb, “Simultaneous Approximation of order m by Artificial Neural Network,” vol. 56, no. 4, 2017.

E. S. Bhaya and H. A. Almurieb, “Neural Network Trigonometric Approximation,” J. Univ. Babylon, vol. 26, no. 1,
pp. 2395-2399, 2016.

E. S. Bhaya and M. A. Kareem, “Whitney Multiapproximation,” J. Univ. Babylon Pure Appl. Sci., vol. 24, no. 9,
pp. 385-403, 2018.

E. S. Bhaya and S. Z. Abdulmunim, “Approximation of Functions in L_P Spaces for p< 1, Using Radial Basis
Function Neural Networks,” J. Univ. Babylon Pure Appl. Sci., vol. 27, no. 3, pp. 400-405, 2019.

E. Bhaya, “lp approximation using radial basis neural network on ordered space,” 2019.

M. Hansson and C. Olsson, “Feedforward neural networks with ReLU activation functions are linear splines,”
2017.

H. Montanelli, et al., “Deep ReLU networks overcome the curse of dimensionality for bandlimited functions,”
2019.

E. J. S. Dittmer and P. Maass, “Singular Values for ReLU Layers,” IEEE Trans. Neural Networks Learn. Syst.,
no. 2, pp. 1-12, 2019.

BIOGRAPHIES OF AUTHORS

 

She received her MSc degree in science of mathematics from University of Babylon in 2010 and
sh is a PhD student of applied mathematics at University of Baghdad too in 2004. She has been a
lecturer at University of Kufa and then University of Babylon in Iraq. Her current research
interests include approximation theory, essential approximation, neural networks

She received her MSc degree in science of mathematics from University of Baghdad 1999 and
her PhD degree in science of mathematics from University of Baghdad too in 2004. Since then
she has been a professor at the University of Babylon in Iraq. She has tens of published papers in
research interests include approximation theory, interpolation, and neural networks.

Indonesian J Elec Eng & Comp Sci, Vol. 20, No. 3, December 2020: 1584 - 1590
