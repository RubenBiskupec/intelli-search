 

www.nature.com/scientificdata

SCIENTIFIC DATA:

OPEN :

DATA DESCRIPTOR |

® Check for updates

K-EmoCon, a multimodal sensor
dataset for continuous emotion
recognition in naturalistic
conversations

Cheul Young Park@®*™, Narae Cha’, Soowon Kang’, Auk Kim’, Ahsan Habib Khandoker?,
Leontios Hadjileontiadis”, Alice Oh*, Yong Jeong@®° & Uichin Lee*™

Recognizing emotions during social interactions has many potential applications with the
popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective
interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions
arising in the wild as they were collected in constrained environments. Therefore, studying emotions

in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal
dataset with comprehensive annotations of continuous emotions during naturalistic conversations.
The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral
physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute
long paired debates on a social issue. Distinct from previous datasets, it includes emotion annotations

: from all three available perspectives: self, debate partner, and external observers. Raters annotated

emotional displays at intervals of every 5 seconds while viewing the debate footage, in terms of
arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the first publicly
available emotion dataset accommodating the multiperspective assessment of emotions during social
interactions.

Background & Summary

Emotion recognition research seeks to enable computers to identify emotions. It is a foundation for creating
machines capable of understanding emotions, and possibly, even expressing one. Such a set of skills to recognize,
understand, and express emotions form emotional intelligence'”. It is suggested that emotional intelligence is
necessary for the navigation of oneself within a society, as it allows one to reason what is desirable and what is not,
and to regulate behaviors of self and others accordingly’.

Then why do machines need emotional skills? With advances in Machine Learning and Artificial Intelligence,
the transition from human to machine is noticeable in all areas of the society, including those requiring expertise
such as medical prognosis/diagnosis™® or automobile driving’. It seems inevitable that these narrow AI systems*®
supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s superior
performance in the game of Go over human champions”””.

Not all AI will compete with humans, albeit their superhuman ability. Instead, many AI systems will work
with us or for us. Emotional intelligence is critical for such human-computer interaction systems'!. Imagine a
smart speaker that delightfully greets users when they come home. How should a speaker greet when a user had
a rough day? A speaker neglectful of the user’s emotional states may aggravate the user, but a speaker aware of the
user's temper could remain silent to avoid the trouble. Similarly, emotional intelligence is critical for AI systems
designed for complex tasks. For example, on roads where autonomous and human-driven vehicles mix, accurate

‘Korea Advanced Institute of Science and Technology, Graduate School of Knowledge Service Engineering, Daejeon,
34141, South Korea. ?Khalifa University of Science and Technology, Department of Biomedical Engineering, Abu
Dhabi, 127788, United Arab Emirates. ?Aristotle University of Thessaloniki, Department of Electrical and Computer
Engineering, Thessaloniki, 54124, Greece. “Korea Advanced Institute of Science and Technology, School of
Computing, Daejeon, 34141, South Korea. °>Korea Advanced Institute of Science and Technology, Department of Bio
and Brain Engineering, Daejeon, 34141, South Korea. “e-mail: cheulyop@kaist.ac.kr; uclee@kaist.edu

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 1
www.nature.com/scientificdata/

 

recognition of human drivers’ emotions by autonomous vehicles would lead to more safety as autonomous vehi-
cles can better judge human drivers’ intentions’’.

Now for machines to become emotionally intelligent, they must first learn to recognize emotions, and the
prerequisite to learning is data. However, there lie several challenges in the acquisition of emotion data. While
emotions are prevalent, their accurate measurement is difficult. Most commonly, emotions are viewed as psycho-
logical states expressed through faces, with distinct categories’’, but research evidence claims the contrary. Rather
than distinct, facial expressions are compound”, relative'’, and misleading'®. A recent review of scientific evi-
dence also presses against the common view, suggesting that facial expressions lack reliability, specificity, and gen-
eralizability'’, together with past studies on contextual dependency'*”° and individual variability of emotions”).

Such inherent elusiveness of emotion renders many existing emotion datasets inapplicable for studying emo-
tions in the wild. The majority of emotion datasets consist of emotions induced with selected stimuli in a static
environment, i.e., a laboratory’?-*’. This method provides experimenters with full-control over data collection,
allowing assessment of specific emotional behaviors*”*! and acquiring fine-grained data with advanced tech-
niques like neuroimaging. Nevertheless, lab-generated data may generalize poorly to realistic scenarios as they
frequently contain intense expressions of prototypical emotions, which are rarely observed in the real world*””’,
acquired from only a subset of the population™.

An alternative approach utilizes media contents**-** and crowdsourcing’’, compensating for the shortcomings
of the conventional method. The abundance of contents available online, such as TV-shows and movies, allows
researchers to glean rich emotion data representative of various contexts efficiently. Crowdsourcing further sup-
ports inexpensive data annotation while serving as another data source***'. Datasets of this type have advantages
in sample size and the diversity of subjects, but generalizability remains an issue. Datasets based on media con-
tents often contain emotional displays produced by trained actors supposing fictitious situations. To what extent
such emotional portrayals resemble spontaneous emotional expressions is debatable*”-“*. They also provide no
access to physiological signals, which are known to carry information vital for detecting less visible changes in
emotional states**~°”.

To amend this lack of a dataset for recognition of emotions in their natural forms, we introduce K-EmoCon,
a multimodal dataset acquired from 32 subjects participating in 16 paired debates on a social issue. It consists of
physiological sensor data collected with three off-the-shelf wearable devices, audiovisual footage of participants
during the debate, and continuous emotion annotations. It contributes to the current literature of emotion recog-
nition, as according to our knowledge, it is the first dataset with emotion annotations from all possible perspec-
tives as the following: subject him/herself, debate partner, and external observers.

Methods

Dataset design. Intended usage. Inspired by previous works that set out to investigate emotions during
conversations**°!~°’, K-EmoCon was designed in consideration of a social interaction scenario involving two
people and wearable devices capable of unobtrusive tracking of physiological signals. The dataset aims to allow a
multi-perspective analysis of emotions with the following objectives:

35-38

1. Extend the research on how having multiple perspectives on emotional expressions may improve their
automatic recognition.

2. Provide a novel opportunity to investigate how emotions can be perceived differently from multiple per-
spectives, especially in the context of social interaction.

Previous research has shown that having multiple sources for emotion annotations can increase their rec-
ognition accuracy’*”’. However, no research in our awareness employs all three available perspectives in the
annotation of emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple
perspectives relates to the issue of establishing ground truth in emotion annotations. Emotions are inherently
internal phenomena, and their mechanism is unavailable for external scrutiny, even for oneself who is experi-
encing emotions. As a result, there may not be a ground truth for emotions. Should we consider what is most
agreed upon by external observers of emotions as the ground truth, or what the person who experiences emotions
reports to have felt the ground truth°®? Two views are likely to match if emotions are intense and pure, but as dis-
cussed, such emotions are rare. Instead, self-reported and observed emotions are likely to disagree for a variety of
reasons. People often conceal their true emotions; sometimes, people are not fully mindful of their internal states;
and some have difficulties interpreting or articulating emotions’””®.

With K-EmoCon, we intend to enable the comprehensive examination of such cases where perceptions of
emotions do not match, by bringing all three available perspectives into the annotation of emotions, in the context
of a social interaction involving three parties of:

1. The subject - is the source who experiences emotions firsthand and produces self annotations, particularly
the “felt sense” of the emotions.

2. The partner — is the person who interacts with the subject, experiencing the subject's emotions secondhand;
thus, he or she has a contextual knowledge of the interaction that induced the subject’s emotions and pro-
duces partner annotations based on that.

3. The external observers — are people who observe the subject's emotions without the exact contextual knowl-
edge of the interaction that induced the emotions, producing external observer annotations.

Notice, that while our definition of perspectives involved in emotion annotation is similar to definitions pre-
viously used by other researchers (self-reported vs. perceived’’/observed”’), we further segment observer anno-
tations based on whether the contextual information of the situation in which the emotion was generated is

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 2
www.nature.com/scientificdata/

 

available to an observer, as we wish to consider the role of contextual knowledge in emotion perception and
recognition.

Existing datasets of emotions in conversations provide a limited scope on this issue as they at most contain
emotion annotations from subjects and external observers”', leaving out annotations from other people who
engaged in the conversation (whom we call partners). Or, they either only consider a particular type of anno-
tations that is sufficient to serve their research goal” or their designs do not allow acquiring multi-perspective
annotations**»? (e.g., a dataset is constructed upon conversations from a TV-show, only allowing the collection
of external observer annotations). Refer to Table 1 to see how K-EmoCon is distinguished from existing emotion
datasets.

Context of data collection. In this regard, we chose a semi-structured, turn-taking debate on a social issue with
randomly assigned partners as the setting for data collection. This setting is appropriate for collecting emotions
that may naturally arise in a day, as it is similar to a social interaction that one could engage in a workplace.

Also, the setting is particularly suitable for studying the misperception of emotions. It is sufficiently formal
and spontaneous as it involves randomly assigned partners. We expect such formality and spontaneity of the
setting compelled participants to regulate their emotions in a socially appropriate manner, allowing us to observe
less pronounced emotions from participants, which were more likely to be misperceived by their partners”.

Data collection apparatus. Our choice of mobile, wearable, and low-cost devices to collect affective physiolog-
ical signals together with audiovisual recordings, while primarily aims to make findings based on our data more
reproducible and expandable, was also in consideration of our goal of investigating mismatches in perceptions
of emotions in the wild. Research has shown that fusing implicit and explicit affective information can result in
more accurate recognition of subtle emotional expressions from professional actors’. However, no work we are
aware of has shown that a similar result can be achieved for subtle emotions collected from in-the-wild social
interactions of individuals without professional training in acting. Therefore, our dataset provides an opportunity
to examine if emotions of lower intensity, produced from non-actors during communication, can be recognized
accurately.

It is also interesting to examine whether subtle emotions could signal instances where emotions are misper-
ceived during communication if their accurate detection is possible. In the same vein, to what extent the intensity
of emotions influences their decoding accuracy during a social interaction, where a broader array of contextual
information is present, is also worth exploring. K-EmoCon could enable an in-depth investigation of such issues.

Further, we considered the use case of mobile and wearable technologies for facilitating emotional communi-
cation. Researchers are actively exploring the potential for using expressive biosignals collected via wearables to
communicate one’s emotional and psychological states with others®~®°. Our dataset can contribute to the research
of biosignal-based assistive technologies to enable affective communication by providing insights on when are
apposite moments for communicating emotions.

Ethics statement. The construction of the K-EmoCon dataset was approved by the Korea Advanced
Institute of Science and Technology (KAIST) Institutional Review Board. KAIST IRB also reviewed and approved
the consent form, which contained information on the following: the purpose of data collection, data collection
procedure, types of data to be collected from participants, compensation to be provided for participation, and the
protocol for the protection of privacy-sensitive data.

Participants were given the same consent forms upon arriving at the data collection site and were asked to pro-
vide written consent after fully reading the form indicating that they are willing to participate in data collection.
Since K-EmoCon is to be open to public access, a separate consent was obtained for the disclosure of the data that
contains personally identifiable information (PII), which is the audiovisual footage of participants during debates,
including their faces and voices. Participants were also notified that their participation is voluntary, and they can
terminate the data collection at any point. The resulting K-EmoCon dataset includes the audiovisual recordings of
21 participants, out of 32, who agreed to disclose their personal information, excluding the 11 who did not agree.

Participant recruitment and preparation. 32 participants were recruited between January and March
of 2019. An announcement calling for participation in an experiment on “emotion-sensing during a debate” was
posted on an online bulletin board of a KAIST student community. The post stated that participants would have
a debate on the issue of accepting Yemeni refugees on Jeju Island of South Korea for 10 minutes. It also stated
that the debate must be in English, and participants should be capable of speaking competently in English, but
not necessarily at the level of a native speaker. Specifically, participants were required to have at least three years
of experience living in an English-speaking country, or have achieved a score above criteria in any one of stand-
ardized English speaking tests listed here: TOEIC speaking level 7, TOEFL speaking score 27, or IELTS speaking
level 7.

Once participants were assigned a date and time to participate in data collection, they were provided four
news articles on the topic of the Jeju Yemeni refugee crisis via email. The email included two articles with neutral
views on the issue®”*’, one in favor of refugees”, and one in opposition to refugees”’. We instructed the partici-
pants to read the articles beforehand to familiarize themselves with the debate topic.

All selected participants were students at KAIST, but their ages varied from 19 to 36 years old (mean = 23.8
years, stdev. = 3.3 years), as well as their gender and nationality. We randomly paired participants into 16 dyads
based on their available times. See Table 2 for the breakdown of participants’ gender, nationality, and age.

Data collection setup. All data collection sessions were conducted in two rooms with controlled temper-
ature and illumination. Two participants sat across a table facing each other with a distance in between for a

 

SCIENTIFIC DATA | (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 3
www.nature.com/scientificdata/

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Videos, face motion .
(oo) 10 ‘| capture, gesture, speech Both Botht rer dialog S,E Dyadic
(audio & transcribed)
SEMAINE Videos, FAUs, speech Trace-style ;
(2011)? 150 (audio & transcribed) Spon. Induced continuous E Dyadic
Videos (face and body),
MAHNOB-HCI eye gaze, audio, biosignals La. 1
(2011) 27 (EEG, GSR, ECG, respiration, Spon. Induced Per stimuli S Individual
skin temp.)
Pp
DEAP Face videos, biosignals
(2012)?! 32 (EEG, GSR, BVP, respiration, | Spon. Induced Per stimuli S Individual
skin temp., EMG & EOG)
DECAF NIR face videos, biosignals La. 1
(2015)28 30 (MEG, hEOG, ECG, tEMG) Spon. Induced Per stimuli S Individual
ASCERTAIN Facial motion units (EMO), Lg. a
(2016)2° 58 biosignals (ECG, GSR, EEG) Spon. Induced Per stimuli S Individual
MSP-IMPROV ; ; ; Per dialog ;
(2016) 12 Face videos, speech audio Both Both turn E Dyadic
oy” 23 Biosignals (EEG, ECG) Spon. Induced Per stimuli S Individual
AMIGOS Vidoes (face & body), er Individual,
(2018)8 40 biosignals (EEG, ECG, GSR) Spon. Induced Per stimuli SE Group
MELD Videos, speech ; Dyadic,
(2019)°*8 ? (audio & transcribed) Both Both Turn-based E Group
CASE Biosignals (ECG, respiration, Trace-style ws
(2019)? 30 BVP, GSR, skin temp., EMG) Spon. Induced continuous S Individual
(2620)! 64 Biosignals (© wo PPG, EDA), Spon. Induced Per stimuli/task | Predefined* | Individual
Videos (face, gesture),
K-EmoCon speech audio, accelerometer, Interval-based ;
(2020) 32 biosignals (EEG, ECG, BVP, Spon. Natural continuous SBE Dyadic
EDA, skin temp.)

 

 

Table 1. Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets.
Posed emotions are when a subject is instructed to enact a particular emotion while Spon. = spontaneous.
Similarly, induced emotions are when a set of selected stimuli is used for their elicitation. For annotation types,
S = self annotations, P = partner annotations, and E = external observer annotations. ‘A dataset was considered
to contain induced emotions if scripted interaction was involved in the data collection, even though no artificial
stimuli (such as an emotion inducing video clip) was used. *Predefined emotion categories of stimuli and
success rates of participants in a set of purposefully selected cognitive tasks were used as ground-truth labels.

eee e eer eer reece eee eer eecee eer eeseeeeee eee eerereeee see ereeeee eee eseeeseee eee eer eeeee ee eeeeeeee reese eeeeeereeeeeeeeeEeeeeeeeeeeeeeeseceeeeene

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Pl P2 M (25) M (23)
P3 P4 M (36) M (25)
P5 P6 M (22) M (23)
P7 P8 M (22) F (22)
P9 P10 M (21) M (22)
Pll P12 M (22) M (25)
P13 P14 M (22) F (21)
P15 P16 M (30) F (26)
P17 P18 M (21) M (20)
P19 P20 M (21) F (23)
P21 P22 M (25) F (25)
P23 P24 M (22) F (29)
P25 P26 F (26) M (25)
P27 P28 F (24) F (23)
P29 P30 F (23) F (24)
P31 P32 M (24) F (19)

 

 

 

 

 

 

Table 2. Participant pairs for debates.

eee eee reer cere cee errr eee ee reese secereeereeeeeeeee sees eeeeeseeeeseeeeseeeeeeeeeeeeereeeeeeeeeeeeeseeererereeerereeesreeeeererereeeeeeeeeeeee

comfortable communication (see Fig. 1). Two Samsung Galaxy S7 smartphones mounted on tripods were placed
in the middle of the table facing each participant, capturing facial expressions and movements in the upper body
from the 2nd-person point of view (POV) along with the speech audio, via the camera app.

 

SCIENTIFIC DATA |

(2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 4
www.nature.com/scientificdata/

 

Fig. 1 Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones
on tripods in the middle of the table (highlighted in red) recorded participants’ facial expressions and
movements in their upper body, as shown on the right in the sample screenshot of footage.

(1) LookNTell Head-Mounted Camera (2) NeuroSky MindWave Headset

    

(3) Polar H7 Bluetooth Heart Rate Sensor

During a debate, participants wore a suite of wearable sensors, as shown in Fig. 2, which includes:

1. Empatica E4 Wristband - captured photoplethysmography (PPG), 3-axis acceleration, body temperature,
and electrodermal activity (EDA). Heart rate and the inter-beat interval (IBI) were derived from Blood
Volume Pulse (BVP) measured by a PPG sensor.

2. Polar H7 Bluetooth Heart Rate Sensor — detected heart rates using an electrocardiogram (ECG) sensor and
was used to complement a PPG sensor in E4, which is susceptible to motion.

3. NeuroSky MindWave Headset - collected electroencephalogram (EEG) signals via two dry sensor elec-
trodes, one on the forehead (fp1 channel-10/20 system at the frontal lobe) and one on the left earlobe
(reference).

4. LookNTell Head-Mounted Camera — with a camera attached at one end of a plastic circlet, was worn on
participants’ heads to capture videos from a first-person POV.

All listed devices can operate in a mobile setting. Empatica E4 keeps the data on the device, and the collected
data is later uploaded to a computer. Polar H7 sensor and MindWave headset can communicate with a mobile
phone via Bluetooth Low Energy (BLE) to store data. Table 3 summarizes sampling rates and signal ranges of data
collected from each device.

Data collection procedure. Administration. All data collection sessions were conducted in four stages of
(1) onboarding, (2) baseline measurement, (3) debate, and (4) emotion annotation. Two experimenters admin-
istered each session (see Table 4 for the overview of a data collection procedure). One experimenter served as a
moderator during debates, notifying participants of the remaining time and intervening under any necessary
circumstances, such as when a debate gets too heated, or a participant exceeds an allotted time of 2 minutes in
his or her turn.

 

SCIENTIFICDATA| (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 5
www.nature.com/scientificdata/

 

   

 

 

 

 

 

 

 

 

 

 

 

 

3-axis acceleration 32Hz [—2g, 2g]
BVP (PPG) 64 Hz n/a
EDA 4Hz [0.01 pS, 100 nS]
Empatica E4 Wristband
Heart rate (from BVP) 1Hz n/a
IBI (from BVP) n/a n/a
Body temperature 4Hz [—40°C, 115°C]
Brainwave (fpl channel EEG) | 125 Hz n/a
NeuroSky MindWave Headset
Attention & Meditation 1Hz [0, 100]
Polar H7 Heart Rate Sensor HR (ECG) 1Hz n/a

 

 

Table 3. Data collected with each wearable device, with respective sampling rates and signal ranges.

Experimenters provided consent forms to participants, and
Read and sign consent forms | 10 min two written consents each for participation and the collection
of privacy-sensitive data were obtained.

 

Participants were assigned to either argue in favor of or

 

 

 

 

Choose sides and the order > min against accepting refugees and decided on the first speaker.
Prepare debate 15 min Participants were provided with supplementary materials to
prepare their arguments.
Eaui . Experimenters explained wearable devices to participants and
quip sensors 10 min : . . .
assisted them in wearing devices.
. . A baseline corresponding to a neutral state was measured for
Measure baseline 2 min oo
each participant.
. . The moderator explained the debate rules and notified
Overview debate 5 min

participants that they are allowed to intervene.

 

Participants could speak for two consecutive minutes during
Debate 10 min their turns and they were notified twice at 30 and 60 seconds
before the end of the debate.

 

Participants annotated emotions at intervals of every 5

Annotate emotions 60 min seconds, watching footage of themselves and their partners.

 

 

 

 

 

Table 4. Steps for a data collection session, each session lasted approximately two hours.

ee eee ere errr eee r eee eeecesrereeseeesreresee seers eeesreeeseeeeeeereesseeeeeeeEeeeeeeeeeEeEeeeeeeeerereeEeeeeeeeEeEEeeerererEeeeeeseeereeesesereseoee

Onboarding. Upon their arrival, participants were each provided a consent form asking for two written consents,
first for the participation in data collection that was mandatory, and second for the disclosure of privacy-sensitive
data collected during the session, which participants could opt-out without any disadvantage.

Once they agreed to participate in the research, participants decided whether they would argue for or against
admitting the Yemeni refugees in Jeju. Participants could either briefly discuss to settle on their preferred posi-
tions or toss a coin to decide at random. The same procedure was followed for deciding who goes first in the
debate.

Next, participants were given up to 15 minutes to prepare their arguments. Each participant was given a pen,
paper, and prints of the articles that they previously received via email. After they finished preparing, experiment-
ers equipped participants with wearable devices. Participants wore E4 wristbands on their non-dominant hand,
as arm movements may impede an accurate measurement of PPG. Experimenters assured that wristbands are
tightly fastened, and electrodes are in good contact with participants’ skin. Experimenters also assured the EEG
headsets and head-mounted cameras are well fitted on participants’ heads, and manually adjusted head-mounted
cameras lens to make sure the captured views are similar to participants’ subjective views. Participants wore Polar
H7 sensors attached to flexible bands underneath their clothes, so the electrodes are in contact with their skin and
placed the sensors above their solar plexus.

Baseline measurement. With all devices equipped, sensor measurements were taken from participants while
they watched a short clip. This step was to establish a baseline that constitutes a neutral state for each participant.
Establishing a neutral baseline is commonly used in the construction of emotion datasets to account for individ-
ual biases and reduce the effect of previous emotional states, especially when repeated measurements are taken.

A procedure for a baseline measurement varies across researchers and is often dependent on the purpose of
an experiment”’. In stimuli-based experiments, researchers take measurements as their subjects watch a stimulus
intended to induce a neutral emotional state*>** or measure resting-state activities between stimuli if they are tak-
ing multiple consecutive measurements’. Similarly, for K-EmoCon, participants watched Color Bars clip, which
was previously reported in the work of Gross et al. to induce a neutral emotion’*. Experimenters also ensured that
no devices were malfunctioning during the baseline measurement.

Debate. A debate began at the sign of the moderator and lasted approximately 10 minutes. Participants’ facial
expressions, movements in their upper body, and speeches were recorded throughout a debate. Participants were

 

SCIENTIFIC DATA | (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 6
www.nature.com/scientificdata/

 

 

 

Arousal/Valence Two affective dimensions from Russell’s 1: very low - 2: low - 3: neutral
circumplex model of affect’?! - 4: high - 5: very high
Cheerful/Happy/Angry/ 102 1: very low - 2: low - 3: high

Emotion states describing a subjective stress state

 

 

Nervous/Sad - 4: very high
Boredom/Confusion/Delight/ Commonly used Baker Rodrigo

Engaged concentration/ Ocumpaugh Monitoring Protocol (BROMP) Choose one
Frustration/Surprise/None educationally relevant affective categories'”
Confrustion/Contempt/Dejection/

Disgust/Eureka/Pride/ Less commonly used BROMP Choose one

. . . 103
Sorrow/None educationally relevant affective categories

 

 

 

 

 

Table 5. Collected emotion annotations.

Cece ere ere eee eee ees eee eeeeeesseeeeeseeeeeeeeeeeeeeseseeeeeeeesseeeeeeseeeeeeeeeeseeeeseeeeeeeeeseeeeeeeeeeeeeeeseereseseeeeereseseeeeeseoes

allowed to speak consecutively up to two minutes during their turns, with turns alternating between two partic-
ipants. However, participants were also notified that they could intervene during an opponent's turn, to allow
a more natural communication. The moderator notified participants 30 and 60 seconds before the end of their
turns and intervened if they exceeded two minutes. A debate stopped at the ten-minute mark with some flexibility
to allow the last speaker to finish his or her argument.

Emotion annotation. Participants took a 15-minute break upon finishing a debate. Participants then were each
assigned to a PC and annotated their own emotions and their partner’s emotions during the debate. Specifically,
each participant watched one audiovisual recording of him/herself and another recording of his/her partner
(both recordings from 2nd-person POV, including facial expressions, upper body movements, and speeches), to
annotate emotions at intervals of every 5 seconds from the beginning to the end of a debate. We chose 5 seconds
based on the report of Busso et al. that the average duration of the speaker turns in IEMOCAP was about 4.5 s”',
and findings from linguistics research also support this number’~”’.

This annotation method we employed, a retrospective affect judgment protocol, is widely used in affective com-
puting to collect self-reports of emotions, especially in studies where an uninterrupted engagement of subjects
during an emotion induction process is essential’*-”’. Likewise, we opted for this method as participants’ natural
interaction was necessary for acquiring quality emotion data.

Note that we did not provide 1st-person POV recordings captured from head-mounted cameras to partic-
ipants, and they only had 2nd-person POV recordings to annotate felt emotions. One may have a reasonable
concern regarding this choice, that participants watching their faces likely caused them to occupy a perspective
similar to an observer. Hence, this might have resulted in an unnatural measurement of felt emotions. Indeed, the
headcam footage could have been a more naturalistic instrument, as we intuitively take an embodied perspective
to recall how we felt at a specific moment in the past.

However, we found the extent of information captured by the headcam footage insufficient for accurate anno-
tation of felt emotions. Experimenters manually adjusted headcam lenses, so the recordings resembled partic-
ipants’ subjective views, but the headcam footage was missing fine-grained information such as participants’
gazes. Also, past research on memories for emotions has shown that they are prone to biases and distortion®”*’.
In that regard, it seemed headcam videos, which contain limited information compared to frontal face recordings,
would only result in an incorrect annotation of felt emotions, especially in retrospect. Further, we noted that it is
not uncommon for people to infer emotions from their faces, as they frequently do when looking in a mirror or
taking a selfie.

As a result, participants were given 2nd-person recordings of themselves for the retrospective annotation
of felt emotions. In total, participants annotated emotions with 20 unique categories, as shown in Table 5.
Experimenters assisted participants throughout the annotation procedure. Before participants began annotating,
experimenters explained individual emotion categories to participants, so they correctly understood a meaning
and a specific annotation procedure for each item. Experimenters also explicitly instructed participants to report
felt emotions, not perceived emotions on their faces. Lastly, experimenters ensured that the start time and end
time for two participants matched to obtain synchronized annotations.

External emotion annotation. Additionally, we recruited five external raters to annotate participants’ emotions
during debates (see Table 6). We applied the same criteria we used for recruiting participants in data collection
to recruit the raters. The raters were provided the 2nd-person POV recordings of participants during debates and
annotated emotions following the same procedure our participants followed. External raters performed their
tasks independently, and the experimenters communicated remotely with the raters. Once a rater finished anno-
tating, an experimenter checked completed annotations for incorrect entries and requested a rater to review
annotations if there were any missing values or misplaced entries.

Data Records

Dataset summary. The resulting K-EmoCon dataset contains multimodal data from 16 paired-debates on
a social issue, which sum to 172.92 minutes of dyadic interaction. It includes physiological signals measured with
three wearable devices, audiovisual recordings of debates, and continuous annotations of emotions from three
distinct perspectives of the subject, the partner, and the external observers. Table 7 summarizes data collection
results and dataset contents.

 

SCIENTIFIC DATA | (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 7
www.nature.com/scientificdata/

 

 

 

 

 

 

RI M (27)
R2 M (25)
R3 F (22)
R4 M (24)
R5 F (28)

 

 

 

 

Table 6. Gender and age of external raters.

 

 

Number of participants 32 (20 males and 12 females)
Participants age 19 to 36 (mean = 23.8 years, stdev. = 3.3 years)
Session duration Total 172.92 min, (mean = 10.8 min, stdev. = 1.04 min)

 

1-5: Arousal, Valence

 

Emotion annotations categories 1-4: Cheerful, Happy, Angry, Nervous, Sad

 

Choose one: Common BROMP affective categories +
less common BROMP affective categories

3-axis Acc. (32 Hz), BVP (64 Hz), EDA (4 Hz), heart rate
Measured physiological signals (1 Hz), IBI (n/a), body temperature (4 Hz), EEG (8 band,
32 Hz), ECG (1 Hz)

 

 

 

 

Debate audios 172.92 min (from 16 debate sessions)

Debate footage 223.35 min (from 21 participants)

Physiological signals Refer to Dataset contents subsection
Self: 4,159

Emotion annotations

(# of 5-second intervals annotated) Partner: 4,159

5 external observers: 20,803

 

 

 

 

Table 7. Summary of data collection results and the dataset.

Preprocessing. For the time-wise synchronization across data, we converted all timestamps from Korea Standard
Time (UTC +9) to UTC +0 and clipped raw data such that only parts of data corresponding to debates and
baseline measurements are included. For debate audios and the footage, subclips corresponding to debates were
extracted from the raw footage. Audio tracks containing participants’ speeches were copied and saved separately
as WAV files. Physiological signals were clipped from the respective beginnings of data collection sessions to
the respective ends of debates, as the initial 1.5 to 2 minutes immediately after a session begins corresponds to
a baseline measurement for a neutral state. Parts in between baseline measurements and debates correspond
to debate preparations, which may be excluded from the analysis. Note that we do not provide unedited audio/
video recordings and raw log-level data, nor codes for preprocessing this data, as they contain privacy-sensitive
information outside the boundary of information we have been permitted to share. See Code Availability section
for further detail.

Dataset contents. The K-EmoCon dataset*? is available upon request on Zenodo (https://doi.org/10.5281/
zenodo.3931963). In the following, we describe directories and files in the dataset and their contents.

metadata.tar.gz. includes files with auxiliary information about the dataset. Included files are:

1. subjects.csv - each row contains a participant ID (pid) and three timestamps in UTC +0. Three
timestamps respectively mark the beginning of a data collection (initTime), the start of a debate
(startTime), and the end of a debate (endTime).

2. data_availability.csv - shows files available for each participant. For each participant (row), ifa
data file (column) is available, the corresponding cell is marked TRUE, otherwise FALSE.

data_quality_tables.tar.gz. includes seven CSV tables with information regarding the quality of physiological
signals in the dataset. With participant IDs (pid) in rows and file types (ACC, BVP, EDA, HR, IBI, and TEMP
for E4 data, and Attention, BrainWave, Meditation,and Polar HR for NeuroSky + Polar H7 data) in
columns, included files are as follows:

1. e4 durations.csv - contains the duration of each file in seconds, where duration = (last timestamp -
first timestamp) / 1000.

2. neuro polar durations.csv-sameas above.

3. e4 zeros.csv-contains the number of zero values in each file. ACC and BVP were excluded as zero
crossings are to be expected during their measurement.

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 8
www.nature.com/scientificdata/

 

4. neuro polar zeros.csv -sameas above. Note that zero values for NeuroSky data (Attention,
BrainWave, Mediation) indicate the inability of a device at a given moment to obtain a sufficiently
reliable measurement due to various reasons.

5. e4 outliers.csv - contains the number of outliers in each file. Chauvenet’s criterion was used for
outlier detection (refer to Code Availability section for its implementation in Python).

6. e4 completeness.csv - contains the completeness of each file as a ratio in the range of [0.0, 1.0]. 1.0 indi-
cates a file without any missing value or an outlier. The completeness ratio was calculated as completeness =
(total number of values — (number of outliers + number of zeros))/total number of values.

7. neuro polar completeness.csv-sameas above, with completeness calculated as completeness
= (total number of values — number of zeros)/total number of values.

debate_audios.tar.gz. contains 16 audio recordings of debates in the WAV file format. The name of each file fol-
lows the convention of p<X>. p<Y>.wav, where <X> and <Y> stand for IDs of two participants appearing in the
audio. The start and the end of each recording correspond to start Time and endTime values in the subjects.
csv file, respectively.

debate_recordings.tar.gz. contains 2nd-person POV video recordings of 21 participants during debates in the
MP4 file format. The name ofa filep<X>_<T>.mp4 indicates that the file is the recording of participant <X> that
is <T> seconds long.

neurosky_polar_data.tar.gz. includes subdirectories for each participant, from P1 to P32, which may contain up
to four files as the following:

l. Attention.csv - contains eSense Attention ranging from 1 to 100, representing how attentive a user
was at a given moment. Attention values can be interpreted as the following: 1 to 20 - “strongly lowered’,
20 to 40 - “reduced”, 40 to 60 - “neutral”, 60 to 80 — “slightly elevated”, and 80 to 100 - “elevated”. 0 indicates
that the device was unable to calculate a sufficiently reliable value, possibly due to a signal contamination
with noises.

2. BrainWave.csv - records the relative power of brainwave in the following 8 bands of EEG: delta
(0.5-2.75 Hz), theta (3.5-6.75 Hz), low-alpha (7.5-9.25 Hz), high-alpha (10-11.75 Hz), low-beta (13-
16.75 Hz), high-beta (18-29.75 Hz), low-gamma (31-39.75 Hz), and middle-gamma (41-49.75 Hz). The
values are without a unit and are only meant for inferring the fluctuation in the power of a certain band or
comparing the relative strengths of bands with each other.

3. Meditation.csv -contains eSense Meditation ranging from 0 to 100, measuring the relaxedness of a
user. For their interpretation, use the same ranges and the meanings as those for the attention values.

4. Polar HR.csv - contains heart rates measured with ECG sensors during debates.

e4_data.tar.gz. contains subdirectories for each participant (except P2, P3, P6, and P7), which may contain up
to six files as the following:

1. E4 ACC.csv - measurements from a 3-axis accelerometer sampled at 32Hz in the range [—2g, 2g] under
columns x, y, and z. Multiply raw numbers by 1/64 to convert them into units of g (i-e., a raw value of 64 is
equivalent to 1g).

2. E4 BVP.csv-PPG measurements sampled at 64Hz.

E4 EDA.csv - EDA sensor readings in units of 1S, sampled at 4Hz.

4. E4_ HR.csv - the average heart rates calculated in 10-second windows. The values are derived from the
BVP measurements, and the values are entered at the frequency of 1Hz. The first 10 seconds of data after
the beginning of a recording is not included as the derivation algorithm requires the initial 10 seconds of
data to produce the first value.

5. E4 IBI.csv-IBI measurements in milliseconds computed from the BVP. From a second row onwards,
one row is separated from the previous row with an amount equal to a distance between two peaks (i.e.,
t,,,; — t;=IBI,). Note that HR in terms of BPM can be derived from IBI by taking 60/JBI* 1000).

6. E4_ TEMP.csv -a body temperature measured in the Celsius scale at the frequency of 4Hz.

»

Note that E4 data entries for P29, P30, P31, and P32 are entered with each row designated with either one
of two unique device serial values. It is necessary that the dataset users only use rows corresponding to
asingle device serial. We further recommend using rows with the following device serial values:

e P29, P31-A013E1 for all files, except A01525 for IBI.
e P30, P32 -A01A3A for all files.

emotion_annotations.tar.gz. includes four subdirectories as listed below, which each contain annotations for
participant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:

1. self annotations - annotations of participant emotions by participants themselves.

2. partner annotations - annotations of participant emotions by respective debate partners.

3, external annotations - annotations of participant emotions by five external raters. Files follow the
naming convention of P<X>.R<Z>.csv, where <X> is a participant ID, and <Z> is a rater number.

 

SCIENTIFIC DATA | (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 9
www.nature.com/scientificdata/

 

Mam Self (N=4159) Mm Partner (N=4159) Mm External (agg.) (N=4187)

Arousal Valence Cheerfu ay

   
  

    
 

 

6% 6%
‘Oo

1 3 5 1 3 5 1 4
(very low) (neutral) (very high) (very low) (neutral) (very high) (very low) (very high)
Happy Angry Nervous,

 
 
 

   

99%

   

96% : 1.69, 0. 80%
sd. . 84% 84% eae 80%
60% a

40%
31% 32%

10% 10%

 

1 4 1 4

(very low) (very high) (very low) (very high)

Common BROMP affective categories Less common BROMP affective categories
4000

  

1
(very low)

Sad

95% 94%

80% 100% 4000 100%

  

60% 3000 3000

 

 

40% 2000 2000
20% 1000 1000
oy, 5% 4 9 9
0% , 2% . 2% “ee 0.6% 5 0 . 0+ Sie = . 1
Oo get so go _ ae > 30 a 30 gy a Gad® co’ 2
(very low) (very high) go cos oe? oo os on™ yor" Cot Gse™ om ew” gw oo wor"
a9 ce

Fig. 3 Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and
external raters (E), with external annotations aggregated by majority voting. Annotations were summed across
32 subjects for each emotion and affective categories. Means and standard deviations measured respectively
from three perspectives are shown on the upper right corner of figures if available.

Cee e eee reese eee eee ese ees eeeresseeeeeseeseseeeseseseseese sere eeeeeseeeeeeeeeeesesesesereseeeeeeeeseseeeeeeseeeeeeeeeeseeeEeeeeeeeeeseeseneeeeseses

4. aggregated external annotations - contains external rater annotations aggregated across five
raters via majority voting. Refer to Code Availability section for the Python code implementing the majori-
ty vote aggregation.

The first row in a valid file has annotations for the first five seconds, and rows coming afterward contain anno-
tations for the next consecutive five-second intervals, non-overlapping. Also, each row in a valid file contains 10
non-empty values (eight numeric values, including seconds column, and two x’s). Note that annotation files
for a participant may not have an equal number of rows (e.g., there may be more self-annotations than partner/
external annotations for some participants). In that case, longer files should be truncated from the start such that
they have the same number of rows as shorter files since the extra annotations at the beginning are possibly from
participants mistakenly annotating emotions during baseline measurements.

Technical Validation

Emotion annotations. Distribution and frequency of emotions. The distributions and the frequencies of
emotion annotations are as shown in Fig. 3. Overall, annotations for emotions measured on Likert scales (arousal,
valence, cheerful, happy, angry, nervous, and sad) are biased towards a neutral with only a minuscule fraction of
annotations for non-neutral states. Categorical emotion annotations (common and less common BROMP affec-
tive categories) are similarly biased, with a predominant portion of annotations falling under only two categories
of concentration and none. This imbalance in annotations is as expected as emotion data is commonly imbal-
anced by its nature in the wild (i.e., people are more often neutral than angry or sad)**"**.

Inter-rater reliability. As individual-level information is missing in aggregated data, we used Krippendorff’s
alpha®’, which is a generalized statistic of agreement applicable to any number of raters, to measure the inter-rater
reliability (IRR) of emotion annotations from different perspectives for each participant. Figure 4 shows heatmaps
of alpha coefficients computed for seven emotions measured on ordinal scales (arousal, valence, cheerful, happy,
angry, nervous, and sad).

All annotation values were interpreted as rank-ordered (ordinal scaled) for the IRR computation. Likert scales
we used are not intervals or ratios with meaningful distances in-between. While participants and raters were
provided numeric scales labeled with semantic meanings (see Table 5), the individual interpretations of scales
were likely disparate.

Given that, before the computation, annotation values were scaled relative to a neutral, by estimating modes
of columns as neutrals and deducting them from respective column values (i.e., if the mode of a cheerful col-
umn for a particular participant was one, then one was subtracted from all values in that cheerful column). This
mode-subtraction step was necessary to prevent the underestimation of IRRs.

Annotations in our dataset for scaled emotions are highly biased, as shown in Fig. 3. However, while arousal
and valence are explicitly centered at zero (which corresponds to 3 = neutral), five emotions measured in the
scale of 1 = very low to 4 = very high (cheerful, happy, angry, nervous, and sad) are systematically biased without

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 10
www.nature.com/scientificdata/

 

Arousal
M%
> ¢ 5S 6 > 6 9g % % 2 LW Ly Le Xe Lp Xe Lg 2 2% 2% 23 2¢ 2% LG 2> 2 29 3 32 3% SH

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

i 2
SP- 0.002 oat 0.21 -0.17 -0.29 -0.29 -0.18 -0.11 -0.30 -0.11 04s, 0.15 0.02 -0.04 0.03 -0.16 -0.19 os -0.29 -0.05 -0.21 -0.13 -0.15 -0.17 0.004 EA a= -0.04 ‘008 048 -0.33 -0.13 0.75
za
' 0.50 =
SE- 0.13 0.08 0.04 0.08 -0.27 0.02 0.06 0.03 0.12 -0.06 0.03 -0.14 -0.06 0.17 -0.05 0.04 (¢ 0.06 -0.12 -0.03 -0.18 -0.25 0.40 -0.08 -0.26 0.19 0.02 Ss
___} l ! 0.25 8
a
PE- 0.05 oat, 0.23 -0.41 -0.18 -0.04 -0.003 -0.02 0.006 0.28 -0.29 -0.15 0.05 -0.26 -0.02 -0.07 -0.25 (057 002 0.02 0.006 0.27 -0.09 -0.04 -0.32 9 ost, -0.11 0.03 -0.17 =0 0.00 5,
-0.25 4
All- 0.08 0.16 -0.12 -0.12 -0.18 -0.001 0.01 -0.02 0.13 -0.16 0.07 0.03 -0.07 0.11 -0.05 -0.07 -0.24 0.08 0.21 -0.13 0.02 0.06 -0.09 0.09 -0.10 -0.09 -0.08 -0.09 -0.10 -0.10 0.002 zs
—0.50 =
Diff. [SE-SP]- 0.12 0.08 -0.13 0.21 036, 0.02 0.20 0.17 om 0.24 00, -0.12 -0.15 -0.02 0.14 0.11 0.24 FEY om: 025 022 050 0.10 -0.03 046 -0.25 049, 0.09 -0.04 -0.25 8 0.18 0.14 -0.75
—1.00
Valence
Ni
2 2 2 g Ss 6 > 8 9 0 24> lg Lg le Lp le Lg [9 2 2 2 g 25 LE 2> 8 29 FO 3 Sap 100
SP- -0.05 -0.05 052 030, -0.23 -0.12 0.002 -0.04 -0.30 -0.09 -0. 0.006 -0.07 -0.37 -0.08 07 -0.02 -0.29 059 -0.04 -0.16 -0.38 0.23 -0.01 0.19 (050, -0.04 =0 0.19 -0.23 -0.12 0.75
za
=.
SE- -0.03 -0.03 003 -0.05 -0.02 0.008 -0.09 -0.13 -0.14 -0.17 =O -0.03 0.003 004/083) 0.10 -0.06 -0.32 -0.22 -0.15 -0.14 -0.07 -0.003 n/a 0.16 0.19 0.05 -0.03 0.16 -0.02 -0.26 -0.02 -0.08 0.50 3S
0.25 8
a
PE- 0.10 0.003 -0.15 -0.07 -0.32 0.08 -0.07 -0.04 -0.006 -0.04 -0.06 -0.14 -0.23 -0.05 -0.07 -0.33 0 -0.03 om -0.03 -0.21 -0.008 -0.08 -0.38 0.13 -0.25 0.09 -0.19 -0.07 0.02 -0.21 -0.14 -0.08 0.00 5,
-0.25%
All--0.008 -0.03 -0.28 0.12 -0.14 -0.02 0.002 -0.08 -0.10 -0.07 -0.04 -0.20 -0.02 -0.03 -0.32 -0.30 -0.05 -0.26 0.09 -0.12 -0.26 -0.03 -0.09 -0.23 0.18 0.04 0.13 0.19 0.03 0.02 -0.009 -0.12 -0.06 0.502
-0.509
Diff. [SE-SP]- 0.02 0.03 0.08 -0.35 0.21 0.13 -0.10 -0.09 0.16 -0.08 0.07 040 0003 0.06 -0.26 0.02 0.15 -0.20 0.14 038, -0.03 0.16 n/a -0.07 0.20 -0.14 054 0.20 -0.02 04s, 0.21 0.05 -0.75
—1.00
Cheerful
My
2 2 2 ¢ $ 6 > 86 9 % 2 Lo 2% Ye Wy Xe Lp Le lg 2% ye 2% 23 My 25 GH_—2>_ 2B 2Q_ SQ BY_D._ “SD 100
SP- 0.03 -0.12 -0.13 (072) 050 -0.17 -0.18 0.20 -0.07 0 -0.13 -0.004 -0.12 -0.07 -0.27 -0.06 0.28 083, -0.10 -0.13 -0.31 -0.11 n/a n/a -0.08 -0.26 -0.23 0.16 0.10 -0.11 0.75
; za
7 0.50 =
SE- -0.11 -0.16 -0.02 -0.01 -0.27 -0.12 -0.07 -0.03 -0.12 n/a -0.09 -0.01 0.04 -0.07 -0.15 -0.07 -0.27 MRR -0.14 n/a 0 -0.05 -0.06 0.15 -0.04 -0.08 -0.09 0.26 -0.05 3
| 0.25 8
a
PE: 030 -0.003 -0.09 0.002 -0.21 0.19 0.008 -0.008 0.20 0 -0.03 -0.004 -0.004 n/a -0.29 -0.28 -0.13 -0.26 -0.09 -0.004 0.11 0.05 n/a 0 on -0.01 -0.19 -0.13 oat 0.08 0.10 0.006 0.001 0.00 S,
| -0.254
All- 0.13 -0.08 -0.04 -0.37 -0.24 -0.01 -0.04 0.08 0.06 0 -0.07 -0.008 -0.003 -0.04 -0.25 -0.14 0.10 -0.32 -0.09 -0.13 -0.09 n/a 0 0.18 -0.06 -0.09 0.07 0.11 0.04 0.27 0.03 -0.02 =
—0.50 =
Diff. [SE - SP]- -0.14 -0.03 0.10 on 0.23 0.05 0.11 -0.23 -0.05 n/a 0.04 -0.009 0.16 0 -0.23 -0.08 -0.34 | -0.01 -0.11 -0.17 n/a n/a 0.04 0.20 038 -0.20 -0.18 -0.19 0.06 08 0.06 -0.75
—1.00
Happy ly
2 2 3 ¢ $ 6 > 86 9 % 2% 2 2% Xe Ws W Lp Ye 2% 2% % 25 26 22 28 29 3M Z_—- SQ. “Na
SP- 0.28 =0 -0.20 -0.06 0.13 038, n/a -0.06 -0.14 -0.008 -0.06 -0.20 -0.004 -0.17 0.004 -0.20 -0.22 -0.07 -0.02 -0.13 -0.28 -0.26 -0.28 -0.15 -0.17 0.008 0.09 =O -0.08 -0.08 0.75
za
0.50 zs
SE- -0.02 =0 -0.03 -0.30 -0.29 -0.32 -0.02 -0.06 -0.10 0 -0.05 -0.16 -0.17 -0.20 -0.30 -0.21 -0.15 n/a -0.13 -0.03 -0.10 0 -0.02 0.18 0.003 -0.04 -0.02 -0.06 Ss
0.25 §
a
PE- -0.02 0 -0.03 -0.19 -0.18 -0.33 -0.02 n/a -0.01 -0.01 -0.02 -0.004 0 n/a -0.18 -0.31 0 -0.03 -0.02 n/a -011 n/a n/a n/ja -0.24 coat, -0.01 -0.25 -0.09 0.13 -0.04 -0.06 -0.07 0.00 5
-0.25 9
All- 0.07 =0 -0.07 -0.10 -0.04 0.03 -0.01 -0.04 -0.07 -0.008 -0.05 -0.11 -0.11 -0.12 -0.16 -0.14 -0.06 -0.02 -0.08 -0.10 -0.25 -0.06 n/a -0.08 0.007 -0.08 -0.10 0.06 0.07 -0.010 -0.05 -0.04 s
l —0.50 z
Diff. [SE-SP]--0.29 ~=0 0.17 -0.24 -0.42 089, n/a 0 0.04 0.008 0.009 0,04 087 0 -0.21 -0.10 0.006 -0.08 n/a 0 0.25 0 0 n/a (ORS 0.17 0.15 0.15 0.17 -0.09 -0.04 0.07 0.02 —-0.75
—1.00
Angry
Ni
2 2 2 ¢ § 6 F 8 9 2% yy L % yg Xe We Lp Xe YXg 29 2% 2 2% ye 2% GF 27 2% 2% 3 By 3 SH 1.00
SP- n/a -0.003 -0.13 -0.22 -0.26 -0.08 n/a -0.02 -0.008 0.24 n/a -0.25 -0.04 -0.18 -0.21 -0.18 -0.07 -0.03 052, -0.10 n/a -0.35 -0.002 -0.20 035, n/a n/a n/a -0.09 0.003 n/a -0.19 -0.11 0.75
0.50 =
SE- n/a -0.003 -0.33 -0.03 -0.14 -0.07 n/a -0.02 n/a -0.09 n/a -0.02 n/a -0.18 -0.21 -0.01 -0.07 n/a -0.12 -0.02 n/a -0.35 -0.15 n/a -0.05 n/a nia n/a n/a 0,003 n/a -0.004 -0.10 . 3
0.25 8
a
PE- n/a n/a -0.03 -0.28 -0.23 -0.004 n/a n/a -0.008 -0.01 n/a -0.15 -0.04 n/a n/a =0 n/a -0.03 -0.008 -0.01 n/a n/a -0.06 -0.20 -0.12 n/a n/a n/a -0.09 n/a n/a -0.23 -0.09 0.00 5,
-0.25 4
All- n/a -0.002 -0.11 -0.15 -0.11 -0.05 n/a -0.01 -0.006 0.10 n/a -0.11 -0.02 -0.11 -0.13 -0.03 -0.04 -0.02 -0.17 -0.03 n/a -0.21 =0 -0.12 0.14 = n/a n/a n/a -0.06 0.002 n/a -0.13 -0.06 s
—0.50 =
Diff. [SE-SP]- n/a 0 -0.21 0.19 0.13 0.010 n/a 0 n/a -0.34 n/a 0.23 n/a 0 0 O17: 0 na 00 0.07 n/a 0 -0.15 n/a -040 n/a nla nia nla 0 ma 0.19 0.007 -0.75
—1.00
Nervous
NM
Zz 2 > g Ss 6 > 8 9 % 2 lo 2 Lg Le le Lp le lg [I 2% 2 2 ¢ 2% GS 2 2% 2% YZ 2 BW Sa 00
SP- -0.28 -0.06 0.07 -0.10 052 0.15 -0.36 0.10 0.06 -0.14 -0.13 0.24 0.13 -0.03 -0.02 -0.04 -0.36 -0.34 -0.07 -0.29 -0.19 -0.31 -0.11 -0.22 on 0.03 0.21 (050, 0.010 0.02 0 -0.10 -0.05 0.75
0.50 =
SE- -0.03 -0.05 -0.23 -0.41 04s, 0.13 -0.10 -0.02 -0.20 -0.13 -0.02 -0.02 om -0.17 -0.02 -0.07 -0.30 -0.02 0.003 -0.29 -0.24 -0.33 -0.15 0 =O -0.11 0.12 oat -0.13 0.07 -0.23 -0.30 -0.09 . 3
L 0.25 8
a
PE- -0.23 0.20 -0.04 -0.05 -0.14 -0.14 -0.21 -0.01 -0.14 0.02 -0.16 0.17 0.20 0.20 -0.04 -0.29 -0.008 -0.36 0.01 n/a -0.03 0 -0.03 -0.18 -0.26 -0.007 -0.07 06, -0.01 -0.005 -0.19 -0.31 -0.05 0.00 S,
-0.254%
All- -0.17 -0.02 -0.01 -0.10 -0.29 0.10 -0.21 0.05 -0.04 -0.04 -0.10 0.13 0.25 0.010 -0.02 -0.05 -0.19 -0.21 0.001 -0.18 -0.15 -0.20 -0.04 -0.12 0.15 0.009 0.13 0.003 0.08 -0.12 -0.16 -0.03 =
—0.50 z
Diff. [SE - SP]- 0.25 0.003 -0.30 -0.32 0.06 -0.02 0.26 -0.13 -0.26 0.01 0.11 -0.26 0.20 -0.14 -0.003 -0.03 0.06 ox 0.07 0 -0.05 -0.02 -0.05 0.22 -0.42 -0.14 -0.09 -0.05 -0.14 0.05 -0.23 -0.20 -0.04 —0.75
—1.00
Sad
My
2 2 3 ¢ S$ 6 > € 9 % Wz % Wy Xe Wy YW LW Xe %yq 2% 2% 2% 2 % % BG > % 2% 3% 2 3% SH 00
SP- n/a -0.003 n/a 0.03 -0.25 -0.06 n/a -0.05 -0.01 n/a 0 0.09 n/a -0.02 n/a n/a n/a 0 n/a n/a n/a n/a -0.004 -0.17 n/a n/a n/a n/a -0.26 -0.30 n/a -0.02 -0.08 0.75
za
0.50 zs
SE- n/a -0.003 n/a -0.08 -0.25 -0.004 n/a -0.05 -0.01 n/a n/a -0.008 n/a -0.02 n/a n/a n/a 0 n/a n/a n/a n/a -0.17_ n/a n/a n/a n/a n/a n/a -0.30 n/a -0.02 -0.08 so
0.25 8
a
PE- n/a n/a n/a -0.11 n/a -0.05 n/a n/a n/a n/a 0 -0.08 n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a -0.12 -0.17 n/a n/a n/a n/a -0.26 n/a n/a n/a -0.11 0.00 S
-0.25%
All- n/a -0.002 n/a -0.02 -0.16 -0.04 n/a -0.03 -0.008 n/a 0 -0.06 n/a -0.01 n/a n/a n/a 0 n/a n/a n/a n/a -0.002 -0.11 n/a n/a n/a n/a -0.16 -0.19 n/a -0.01 -0.05 =
—0.50 z
Diff. [SE - SP]- n/a 0 nia 012 O 0.06 n/a 0 0 nia nla 0.08 n/a 0 na ona a 0 nia onla nla ofa -0.17 n/a nla nla nla naa 0 n/a 0 0.003 -0.75
-1.00

Fig. 4 Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were
aggregated by majority voting. The first 4 rows of each heatmap show alpha coefficients across four different
combinations of annotation perspectives: (1) SP = self vs. partner, (2) SE = self vs. external, (3) PE = partner
vs. external, and (4) All = self vs. partner vs. external, while the last row (Diff [SE - SP]) shows the difference
between self vs. external agreement and self vs. partner agreement. The columns show those for each
participant.

a zero neutral. All of their values indicate that some emotion is present, and this absence of zero results in a widely
varying interpretation of scale values by our participants and raters.

Consider the following scenario further elaborating this issue: a subject rates that she was cheerful as much as
1 for the first half of a debate, then 2 for the rest, but her debate partner rates that she was cheerful as much as 3
for the first half then 4 for the rest. In this example, self and partner annotations both imply that the subject was
less cheerful for the first half of the debate. However, an IRR of two sets of annotations is close to zero without

 

SCIENTIFICDATA| (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 11
www.nature.com/scientificdata/

 

subtracting modes. Indeed, it is possible that the partner perceived the subject as more cheerful overall, compared
to the subject herself. In that case, a low IRR correctly measures the difference between emotion perceptions of
the subject and partner. Nevertheless, this assumption cannot be confirmed, as there is no neutral baseline.

Therefore, we applied the proposed mode-subtraction to emotion annotations such that alpha coefficients
measure raters agreement on relative changes in emotions rather than their absolute agreement with each other.
This adjustment mitigates spuriously low alpha coefficient values obtained from raw annotations (refer to Code
Availability section for the code implementing the mode-subtraction and plotting of heatmaps).

These fixed alpha coefficients are low in general. In particular, a noticeable pattern emerges when comparing
alpha coefficients of self-partner (SP) annotations and self-external (SE) annotations. As shown in the last rows of
heatmaps (Diff. [SE - SP]) in Fig. 4, the differences between the IRRs of SE annotations and SP annotations tend
to be above zero (for 22 out of 32 participants for arousal: mean = 0.145, stdev. = 0.279). This pattern possibly
indicates that there exists a meaningful difference in the perception of emotions from different perspectives, while
further study is required to validate its significance.

Physiological signals. Data quality. The quality of physiological signal measurements in the dataset has
been thoroughly examined. The examination results are included as a part of the dataset in the data_qual-
ity tables.tar.gz archive file.

Missing data. E4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during
data collection. While physiological signals in the dataset are mostly error-free with most of the files complete
above 95%, a portion of data is missing due to issues inherent to devices or a human error:

e IBI - data from P26 is missing as the internal algorithm of E4 that derives IBI from BVP automatically dis-
cards an obtained value if its reliability is below a certain threshold.

e EDA - data from P17 and P20 is missing, possibly due to poor contact between the device and a participant's
skin.

e NeuroSky (Attention, Meditation) - measurements from P1 and P20 are missing due to a poorly equipped
device. A portion of data is missing for P19 (~32%), P22 (~59%) and P23 (~36%) for the same reason. No
BrainWave data was lost.

e Polar HR - data from seven participants (P3, P12, P18, P20, P21, P29, and P30) are missing due to a device
error during data collection. Parts of data are missing from P4 (~38%) and P22 (~38%) due to poor contact.

Usage Notes

Potential applications. In addition to the intended usage of the dataset discussed above, there are uncer-
tainties as to how physiological markers of an individual’s capacity for flexible physiological reactivity relate to
experiences of positive and negative emotions. Our dataset could potentially be useful to examine the role of
physiological signal based markers in assessing an individual’s use of emotion regulation strategies, such as cog-
nitive appraisal.

Additionally, various data mining and machine learning techniques could be applied to set up models for an
individual's emotional profile based on sensor-based physiological and behavioral recordings. This could further
be transferred to various positive computing use-cases*®®, such as helping children with autism in their social
communication®*”, helping people who are blind to read facial expressions and get the emotion information of
their peers”’, finding opportune moments for conversational user interactions””’, assisting social anxiety disor-
der patients to overcome their conditions”, allowing robots to interact more intelligently with people””®, and
monitoring signs of frustration and emotional saturation that affect attention while driving, to enhance driver
safety’””®.

Limitations. Data collection apparatus. Contact-base EEG sensors are known to be susceptible to noises,
for example, frowning or eyes-movement might have caused peaks in the data. Other devices may also have been
subject to similar systematic errors.

Data collection context. ‘The context of the turn-taking debate may have caused participants to regulate or even
suppress their emotional expressions, as an unrestrained display of emotions is often regarded undesirable during
a debate. This may have contributed to a deflated level of agreement between self-reports and partner/external
perceptions of emotions, which may not be a case for more natural interactions in the wild.

Retrospective emotion annotation with 2nd-person footage. We used retrospective affect judgment protocol
where our participants annotated emotions they felt during debates watching the 2nd-person footage of them-
selves. This approach may have introduced unintended effects to self-ratings of emotions, which pertain to the
interaction between interoception”’, emotional reasoning, and self-perception. Nonetheless, we clearly illustrate
our rationale for choosing this annotation method on page 5, under Emotion annotation. Further, our dataset
includes annotations of participant emotions from debate partners and external raters who watched the same
footage. Therefore, rather than being flawed, our dataset opens a window for investigating the effects mentioned
above while altogether enables a comprehensive study of emotions by comparing their perceptions across multi-
ple perspectives.

Mode-subtraction in IRR computation. With the mode-subtraction, inter-rater reliability values represent
the agreement of raters on relative emotion changes rather than perceived emotions in an absolute sense (see

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 12
www.nature.com/scientificdata/

 

page 8, Inter-rater reliability). Therefore, the dataset users should keep this in mind and determine whether
mode-subtraction is optimal for their particular use-cases.

Demographics. The participant demographics likely have introduced bias in the data. All of our participants and
raters are young (their ages were between 19 to 36) and highly-educated, and the majority of them are individu-
als of Asian ethnicity. Therefore, our data may not generalize well to individuals of different ethnic groups or of
younger or older age groups.

Unaccounted variables. Many variables unaccounted during data collection, such as the level of rapport between
debating pairs, a participant’s competence in spoken English, and a participant's familiarity with the debate topic,
may also have contributed to a variance in the level of mismatch between the perceptions of emotions across
different perspectives.

Code availability
Python codes implementing outlier detection using Chauvenet’s criterion, majority voting, mode-subtraction,
and other utility functions, including the generation of heatmap plots, are available on https://github.com/Kaist-
ICLab/K-EmoCon_SupplementaryCodes. The Krippendorff package (https://github.com/pIn-fing-udelar/fast-
krippendorff) was used for the computation of Krippendorff’s alpha. Python version 3.6.9 was used throughout.
Codes for preprocessing the raw log-level data in SQL databases to CSV files were implemented in Python with
the SQLAIchemy package. However, these codes and the raw log-level data are not made available as they include
privacy-sensitive information outside the agreed boundary for public sharing of the dataset, which was collected
only for logistic reasons. Nevertheless, we welcome the dataset users to contact the corresponding authors if they
need any further assistance or information regarding the raw data, and it’s preprocessing.

Received: 19 May 2020; Accepted: 30 July 2020;
Published online: 08 September 2020

References
1. Salovey, P. & Mayer, J. D. Emotional intelligence. Imagination, Cogn. Pers. 9, 185-211 (1990).
2. Mayer, J. D., Caruso, D. R. & Salovey, P. Emotional intelligence meets traditional standards for an intelligence. Intell. 27, 267-298
(1999).
. Salovey, P. E. & Sluyter, D. J. Emotional development and emotional intelligence: educational implications. (Basic Books, 1997).
. Lopes, P. N. et al. Emotional intelligence and social interaction. Pers. Soc. Psychol. Bull. 30, 1018-1034 (2004).
. Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural networks. Nat. 542, 115-118 (2017).
. Mastoras, R.-E. et al. Touchscreen typing pattern analysis for remote detection of the depressive tendency. Sci. Reports 9, 1-12
(2019).
7. Yurtsever, E., Lambert, J., Carballo, A. & Takeda, K. A survey of autonomous driving: common practices and emerging
technologies. IEEE Access 8, 58443-58469 (2020).
8. Pennachin, C. & Goertzel, B. Contemporary approaches to artificial general intelligence. In Artificial General Intelligence, 1-30
(Springer, 2007).
9. Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nat. 529, 484 (2016).
10. Silver, D. et al. Mastering the game of go without human knowledge. Nat. 550, 354-359 (2017).
11. Reeves, B. & Nass, C. I. The media equation: how people treat computers, television, and new media like real people and places.
(Cambridge University Press, 1996).
12. Turpen, A. Mit wants self-driving cars to traffic in human emotion. New Atlas, https://newatlas.com/automotive/mit-self-driving-
cars-human-emotion/ (2019).
13. Barrett, L. F How emotions are made: the secret life of the brain (Houghton Mifflin Harcourt, 2017).
14. Du, S., Tao, Y. & Martinez, A. M. Compound facial expressions of emotion. Proc. Natl. Acad. Sci. 111, E1454-E1462 (2014).
15. Yannakakis, G. N., Cowie, R. & Busso, C. The ordinal nature of emotions. In 2017 Seventh International Conference on Affective
Computing and Intelligent Interaction (ACII), 248-255 (IEEE, 2017).
16. Frank, M. G. & Svetieva, E. Microexpressions and deception. In Understanding Facial Expressions in Communication, 227-242
(Springer, 2015).
17. Barrett, L. FE, Adolphs, R., Marsella, S., Martinez, A. M. & Pollak, S. D. Emotional expressions reconsidered: challenges to inferring
emotion from human facial movements. Psychol. Sci. Public Interest 20, 1-68 (2019).
18. Carroll, J. M. & Russell, J. A. Do facial expressions signal specific emotions? judging emotion from the face in context. J. Pers. Soc.
Psychol. 70, 205 (1996).
19. Cauldwell, R. T. Where did the anger go? the role of context in interpreting emotion in speech. In ISCA Tutorial and Research
Workshop (ITRW) on Speech and Emotion (2000).
20. Barrett, L.E, Mesquita, B. & Gendron, M. Context in emotion perception. Curr. Dir. Psychol. Sci. 20, 286-290 (2011).
21. Larsen, R. J. & Diener, E. Affect intensity as an individual difference characteristic: a review. J. Res. Pers. 21, 1-39 (1987).
22. Gross, J. J. & John, O. P. Individual differences in two emotion regulation processes: implications for affect, relationships, and well-
being. J. Pers. Soc. Psychol. 85, 348 (2003).
23. Soleymani, M., Lichtenauer, J., Pun, T. & Pantic, M. A multimodal database for affect recognition and implicit tagging. IEEE
Transactions on Affect. Comput. 3, 42-55 (2011).
24. Koelstra, S. et al. Deap: a database for emotion analysis; using physiological signals. IEEE Transactions on Affect. Comput. 3, 18-31
(2011).
25. Abadi, M. K. et al. Decaf: meg-based multimodal database for decoding affective physiological responses. IEEE Transactions on
Affect. Comput. 6, 209-222 (2015).
26. Subramanian, R. et al. Ascertain: emotion and personality recognition using commercial sensors. IEEE Transactions on Affect.
Comput. 9, 147-160 (2016).
27. Katsigiannis, S. & Ramzan, N. Dreamer: a database for emotion recognition through eeg and ecg signals from wireless low-cost
off-the-shelf devices. IEEE J. Biomed. Heal. Informatics 22, 98-107 (2017).
28. Correa, J. A. M., Abadi, M. K., Sebe, N. & Patras, I. Amigos: a dataset for affect, personality and mood research on individuals and
groups. IEEE Transactions on Affect. Comput., https://doi.org/10.1109/TAFFC.2018.2884461 (2018).

Nm W

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 13
www.nature.com/scientificdata/

29.

30.

31.

32.
33.

34.
35.

36.

37.

38.

39.

40.

41.

42.

43.

44,

45.
46.

47.

48.

49.

50.

51.

52.

53.

54.

55.

56.

57.

58.

59.

60.

61.

62.

63.

64.

65.

66.

67.

68.

69.

70.

71.

 

Sharma, K., Castellini, C., van den Broek, E. L., Albu-Schaeffer, A. & Schwenker, F. A dataset of continuous affect annotations and
physiological signals for emotion analysis. Sci. Data 6, 1-13 (2019).

Yan,W.-J.,Wu, Q., Liu, Y.-J.,Wang, S.-J. & Fu, X. Casme database: a dataset of spontaneous micro-expressions collected from
neutralized faces. In 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), 1-7
(IEEE, 2013).

Schmidt, P., Reiss, A., Duerichen, R., Marberger, C. & Van Laerhoven, K. Introducing wesad, a multimodal dataset for wearable
stress and affect detection. In Proceedings of the 20th ACM International Conference on Multimodal Interaction, 400-408 (2018).
Watson, D. Mood and temperament (Guilford Press, 2000).

Batliner, A., Fischer, K., Huber, R., Spilker, J. & Noth, E. How to find trouble in communication. Speech Commun. 40, 117-143
(2003).

Henrich, J., Heine, S. J. & Norenzayan, A. The weirdest people in the world? Behav. Brain Sci. 33, 61-83 (2010).

Dhall, A., Goecke, R., Lucey, S. & Gedeon, T. Collecting large, richly annotated facial-expression databases from movies. IEEE
Multimed. 34-41 (2012).

Mollahosseini, A., Hasani, B. & Mahoor, M. H. Affectnet: a database for facial expression, valence, and arousal computing in the
wild. IEEE Transactions on Affect. Comput. 10, 18-31 (2017).

McDuff, D., Amr, M. & El Kaliouby, R. Am-fed+: an extended dataset of naturalistic facial expressions collected in everyday
settings. IEEE Transactions on Affect. Comput. 10, 7-17 (2018).

Poria, S. et al. Meld: a multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, 527-536 (2019).

McDuff, D., El Kaliouby, R. & Picard, R. W. Crowdsourcing facial responses to online videos. IEEE Transactions on Affect. Comput.
3, 456-468 (2012).

Morris, R., McDuff, D. & Calvo, R. Crowdsourcing techniques for affective computing. In The Oxford Handbook of Affective
Computing, 384-394 (Oxford Univ. Press, 2014).

Korovina, O., Baez, M. & Casati, FE. Reliability of crowdsourcing as a method for collecting emotions labels on pictures. BMC Res.
Notes 12, 1-6 (2019).

Motley, M. T. & Camden, C. T. Facial expression of emotion: a comparison of posed expressions versus spontaneous expressions in
an interpersonal communication setting. West. J. Speech Commun. 52, 1-22 (1988).

Juirgens, R., Grass, A., Drolet, M. & Fischer, J. Effect of acting experience on emotion expression and recognition in voice: non-
actors provide better stimuli than expected. J. Nonverbal Behav. 39, 195-214 (2015).

Juslin, P. N., Laukka, P. & Banziger, T. The mirror to our soul? comparisons of spontaneous and posed vocal expression of emotion.
J. Nonverbal Behav. 42, 1-40 (2018).

Cacioppo, J. T. et al. The psychophysiology of emotion. Handb. Emot. 2, 173-191 (2000).

Picard, R. W., Vyzas, E. & Healey, J. Toward machine emotional intelligence: analysis of affective physiological state. IEEE
Transactions on Pattern Analysis Mach. Intell. 23, 1175-1191 (2001).

Lisetti, C. L. & Nasoz, F. Using noninvasive wearable computers to recognize human emotions from physiological signals.
EURASIP J. on Adv. Signal Process. 2004, 929414 (2004).

Rainville, P., Bechara, A., Naqvi, N. & Damasio, A. R. Basic emotions are associated with distinct patterns of cardiorespiratory
activity. Int. J. Psychophysiol. 61, 5-18 (2006).

Nummenmaa, L., Glerean, E., Hari, R. & Hietanen, J. K. Bodily maps of emotions. Proc. Natl. Acad. Sci. 111, 646-651 (2014).
Pace-Schott, E. F. et al. Physiological feelings. Neurosci. & Biobehav. Rev. 103, 267-304 (2019).

Busso, C. et al. lemocap: interactive emotional dyadic motion capture database. Lang. Resour. Eval. 42, 335 (2008).

McKeown, G., Valstar, M., Cowie, R., Pantic, M. & Schroder, M. The semaine database: annotated multimodal records of
emotionally colored conversations between a person and a limited agent. IEEE Transactions on Affect. Comput. 3, 5-17 (2011).
Busso, C. et al. Msp-improv: an acted corpus of dyadic interactions to study emotion perception. IEEE Transactions on Affect.
Comput. 8, 67-80 (2016).

Healey, J. Recording affect in the field: towards methods and metrics for improving ground truth labels. In Affective Computing and
Intelligent Interaction, 107-116 (Springer, 2011).

Zhang, B., Essl, G. & Mower Provost, E. Automatic recognition of self-reported and perceived emotion: does joint modeling help?
In Proceedings of the 18th ACM International Conference on Multimodal Interaction, 217-224 (2016).

Truong, K. P., van Leeuwen, D. A. & Neerincx, M. A. Unobtrusive multimodal emotion detection in adaptive interfaces: speech and
facial expressions. In International Conference on Foundations of Augmented Cognition, 354-363 (Springer, 2007).

Grossman, J. B., Klin, A., Carter, A. S. & Volkmar, FE. R. Verbal bias in recognition of facial emotions in children with asperger
syndrome. The J. Child Psychol. Psychiatry Allied Discip. 41, 369-379 (2000).

Dickson, H., Calkins, M. E., Kohler, C. G., Hodgins, S. & Laurens, K. R. Misperceptions of facial emotions among youth aged 9-14
years who present multiple antecedents of schizophrenia. Schizophr. Bull. 40, 460-468 (2014).

Truong, K. P., Van Leeuwen, D. A. & De Jong, F. M. Speech-based recognition of self-reported and observed emotion in a
dimensional space. Speech Commun. 54, 1049-1063 (2012).

Hess, U., Blairy, S. & Kleck, R. E. The intensity of emotional facial expressions and decoding accuracy. J. Nonverbal Behav. 21,
241-257 (1997).

Ranganathan, H., Chakraborty, S. & Panchanathan, S. Multimodal emotion recognition using deep learning architectures. In 2016
IEEE Winter Conference on Applications of Computer Vision (WACV), 1-9 (IEEE, 2016).

Min, H. C. & Nam, T.-J. Biosignal sharing for affective connectedness. In CHI ’14 Extended Abstracts on Human Factors in
Computing Systems, 2191-2196 (2014).

Hassib, M., Buschek, D., Wozniak, P. W. & Alt, F Heartchat: heart rate augmented mobile chat to support empathy and awareness.
In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 2239-2251 (2017).

Liu, F., Dabbish, L. & Kaufman, G. Supporting social interactions with an expressive heart rate sharing application. Proc. ACM
Interact. Mob. Wearable Ubiquitous Technol. 1, 1-26 (2017).

Liu, F. et al. Animo: sharing biosignals on a smartwatch for lightweight social connection. Proc. ACM Interact. Mob. Wearable
Ubiquitous Technol. 3, 1-19 (2019).

Liu, E, Kaufman, G. & Dabbish, L. The effect of expressive biosignals on empathy and closeness for a stigmatized group member.
Proc. ACM on Human-Computer Interact. 3, 1-17 (2019).

Kim, S. South korea’s refugee debate eclipses a deeper, more fundamental question. The Hill, https://thehill.com/opinion/
international/395977-south-koreas-refugee-debate-eclipses-a-deeper-more-fundamental-question (2018).

Kang, J.-k. Yemeni refugees become a major issue on jeju. Korea JoongAng Daily, http://koreajoongangdaily.joins.com/news/article/
article.aspx?aid=3049562 (2018).

Park, N. South korea is going crazy over a handful of refugees. Foreign Policy, https://foreignpolicy.com/2018/08/06/south-korea-
is-going-crazy-over-a-handful-of-refugees/ (2018).

Seo, B. In south korea, opposition to yemeni refugees is a cry for help. CNN, https://edition.cnn.com/2018/09/13/opinions/south-
korea-jeju-yemenis-intl/index.html (2018).

Diers, K., Weber, F., Brocke, B., Strobel, A. & Schénfeld, S. Instructions matter: a comparison of baseline conditions for cognitive
emotion regulation paradigms. Front. Psychol. 5, 347 (2014).

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 14
www.nature.com/scientificdata/

 

72. Gross, J. J. & Levenson, R. W. Emotion elicitation using films. Cogn. Emot. 9, 87-108 (1995).

73. Kemper, S. & Sumner, A. The structure of verbal abilities in young and older adults. Psychol. Aging 16, 312 (2001).

74. Yuan, J., Liberman, M. & Cieri, C. Towards an integrated understanding of speaking rate in conversation. In Ninth International
Conference on Spoken Language Processing (2006).

75. Gabig, C. S. Mean length of utterance (mlu). Encycl. Autism Spectr. Disord. 1813-1814 (2013).

76. Graesser, A. & Chipman, P. Detection of emotions during learning with autotutor. In Proceedings of the 28th Annual Meetings of the
Cognitive Science Society, 285-290 (Erlbaum, 2006).

77. Afzal, S. & Robinson, P. Natural affect data - collection annotation in a learning context. In 2009 3rd International Conference on
Affective Computing and Intelligent Interaction and Workshops, 1-7 (IEEE, 2009).

78. D’Mello, S. K., Lehman, B. & Person, N. Monitoring affect states during effortful problem solving activities. Int. J. Artif: Intell. Educ.
20, 361-389 (2010).

79. D’Mello, S. K. On the influence of an iterative affect annotation approach on inter-observer and self-observer reliability. IEEE
Transactions on Affect. Comput. 7, 136-149 (2015).

80. Levine, L. J. & Safer, M. A. Sources of bias in memory for emotions. Curr. Dir. Psychol. Sci. 11, 169-173 (2002).

81. Safer, M. A., Levine, L. J. & Drapalski, A. L. Distortion in memory for emotions: the contributions of personality and post-event
knowledge. Pers. Soc. Psychol. Bull. 28, 1495-1507 (2002).

82. Lench, H. C. & Levine, L. J. Motivational biases in memory for emotions. Cogn. Emot. 24, 401-418 (2010).

83. Park, C. Y. et al. K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations. Zenodo
https://doi.org/10.5281/zenodo.3931963 (2020).

84. Calix, R. A., Mallepudi, S. A., Chen, B. & Knapp, G. M. Emotion recognition in text for 3-d facial expression rendering. [EEE
Transactions on Multimed. 12, 544-551 (2010).

85. Wang, W., Chen, L., Thirunarayan, K. & Sheth, A. P. Harnessing twitter “big data” for automatic emotion identification. In 2012
International Conference on Privacy, Security, Risk and Trust and 2012 International Conferenece on Social Computing, 587-592
(IEEE, 2012).

86. Xu, R. et al. Word embedding composition for data imbalances in sentiment and emotion classification. Cogn. Comput. 7, 226-240
(2015).

87. Krippendorff, K. Computing krippendorff’s alpha-reliability. Retrieved from, https://repository.upenn.edu/asc_papers/43 (2011).

88. Lee, U. et al. Intelligent positive computing with mobile, wearable, and iot devices: literature review and research directions. Ad Hoc
Networks 83, 8-24 (2019).

89. Picard, R. W. Future affective technology for autism and emotion communication. Philos. Transactions Royal Soc. B: Biol. Sci. 364,
3575-3584 (2009).

90. Washington, P. et al. Superpowerglass: a wearable aid for the at-home therapy of children with autism. Proc. ACM Interact. Mob.
Wearable Ubiquitous Technol. 1, 1-22 (2017).

91. Buimer, H. P. et al. Conveying facial expressions to blind and visually impaired persons through a wearable vibrotactile device. Plos
One 13 (2018).

92. Cha, N. et al. “Hello there! is now a good time to talk?”: understanding opportune moments for proactive conversational
interaction with smart speakers. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4 (2020).

93. Kim, A., Park, J.-M. & Lee, U. Interruptibility for in-vehicle multitasking: influence of voice task demands and adaptive behaviors.
Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 1-22 (2020).

94. Warnock-Parkes, E. et al. Seeing is believing: using video feedback in cognitive therapy for social anxiety disorder. Cogn. Behav.
Pract. 24, 245-255 (2017).

95. Breazeal, C. Emotion and sociable humanoid robots. Int. J. Human-Computer Stud. 59, 119-155 (2003).

96. Kwon, D.-S. et al. Emotion interaction system for a service robot. In RO-MAN 2007 - The 16th IEEE International Symposium on
Robot and Human Interactive Communication, 351-356 (IEEE, 2007).

97. Nass, C. et al. Improving automotive safety by pairing driver emotion and car voice emotion. In CHI ’05 Extended Abstracts on
Human Factors in Computing Systems, 1973-1976 (2005).

98. Eyben, EF et al. Emotion on the road—necessity, acceptance, and feasibility of affective computing in the car. Adv. Human-Computer
Interact. 2010 (2010).

99. Craig, A. D. How do you feel? interoception: the sense of the physiological condition of the body. Nat. Rev. Neurosci. 3, 655-666
(2002).

100. Markova, V., Ganchev, T. & Kalinkov, K. Clas: a database for cognitive load, affect and stress recognition. In 2019 International
Conference on Biomedical Innovations and Applications (BIA), 1-4 (IEEE, 2019).

101. Russell, J. A. A circumplex model of affect. J. Pers. Soc. Psychol. 39, 1161 (1980).

102. Plarre, K. et al. Continuous inference of psychological stress from sensory measurements collected in the natural environment. In
Proceedings of the 10th ACM/IEEE International Conference on Information Processing in Sensor Networks, 97-108 (IEEE, 2011).

103. Ocumpaugh, J. Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 technical and training manual. New York, NY Manila,
Philipp. Teach. Coll. Columbia Univ. Ateneo Lab. for Learn. Sci. 60 (2015).

Acknowledgements

The 2019 KK-JRC Smart Project and the Next-Generation Information Computing Development Program
through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT supported
this research (NRF-2017M3C4A7065960). The authors cordially thank all participants for contributing their data
for the development of the dataset.

Author contributions

C.Y.P. designed, prepared, and conducted the data collection, constructed and pre-processed the collected dataset,
performed the technical validation, and wrote the manuscript. N.C. prepared and conducted the data collection.
S.K. designed and assisted in the data collection. A.K. assisted in the data collection, constructed the dataset, and
revised the manuscript. A.H.K. and L.H. advised, revised, and verified the manuscript. A.O. and YJ. revised and
verified the manuscript. U.L. supervised the dataset design and the data collection, advised the overall project,
and revised the manuscript.

Competing interests
The authors declare no competing interests.

 

SCIENTIFICDATA| — (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 15
www.nature.com/scientificdata/

 

Additional information
Correspondence and requests for materials should be addressed to C.Y.P. or U.L.

Reprints and permissions information is available at www.nature.com/reprints.

Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.

Open Access This article is licensed under a Creative Commons Attribution 4.0 International

License, which permits use, sharing, adaptation, distribution and reproduction in any medium or
format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-
ative Commons license, and indicate if changes were made. The images or other third party material in this
article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the article’s Creative Commons license and your intended use is not per-
mitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the
copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.

The Creative Commons Public Domain Dedication waiver http://creativecommons.org/publicdomain/zero/1.0/
applies to the metadata files associated with this article.

© The Author(s) 2020

 

SCIENTIFICDATA| (2020) 7:293 | https://doi.org/10.1038/s41597-020-00630-y 16
