Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 EURASIP Jo urna | on Ima ge
https://doi.org/10.1186/s13640-020-00533-3 . .
and Video Processing

RESEARCH Oy else =e

Image completion via transformation and ®
structural constraints

updates
Qiaochuan Chen', Guangyao Li’, Qingguo Xiao’, Li Xie’ and Mang Xiao*

 

 

* Correspondence: lgy423@126.com
"College of Electronics and

Information Engineering, Tongji . . . . . . .
University, Shanghai, China Image completion is an approach to fill a damaged region (hole) in an image. In this

Full list of author information is study, we adopt a novel method which can repair a target region with structural

available at the end of the article constraints in an architectural scene. An objective function that consists of three
terms is proposed to solve the image completion problem. In color term, we
compute a parameterized transformation model using detected plane parameters
and measure the distance between the target patch and transformed source patch.
This model helps to extend the patch search space and find an optimal solution. To
improve the patch matching accuracy, we add a guide term that includes structure
term and consistency term. The structure term encourages sampling patches along
the structural direction, and the consistency term is used to maintain the texture
consistency. Considering the color deviation between patches, we add a gradient
term into a framework that can solve more challenging problems. Compared with
previous methods, the proposed method has good performance in preserving global
structure and reasonably estimating perspective distortions. Moreover, we obtain
acceptable results in natural scenes. The experimental results illustrate that this novel
method is a potential tool for image completion.

Abstract

Keywords: Image completion, Image inpainting, Structural constraints,
Transformation, Objective function

 

1 Introduction

Image completion methods aim to repair the defects of digital images with plausibly
synthesized content to make images look more natural. This task is applied to many
image editing applications ranging from object removal to movie clip and image un-
derstanding [1-3]. In general, there are two main types of image completion methods:
diffusion-based methods and exemplar-based methods.

A diffusion-based method completes the target region using partial differential equa-
tions which propagate image information from surrounding areas into an unknown re-
gion. Bertalmio et al. [4] first proposed a method in which the information was
propagated through the edge of a contour line in the occlusion area. Furthermore,
these methods have two types: Euler’s model [5] and total variation model [6]. They
perform well in the images with thin cracks and scratches; however, they are not suit-
able for large damaged regions.

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which
GQ) Springer Open permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the
— original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or
other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit
line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by
statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a
copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 2 of 18

Exemplar-based methods sample the pixels from a known region of the image
and copy them to a damaged region. Efros and Leung [7] proposed a non-
parametric method for texture synthesis. The texture synthesis process grows a
new image outward from an initial seed, one pixel at a time. The method in Ref.
[8] processed an image in a greedy way to research the best matching patches.
Due to the greedy strategy, this method resulted in an inconsecutive texture. Sun
et al. [9] developed a method that first allowed users to draw lines in a target re-
gion. Then, the target region was completed along the lines. Meanwhile, some ap-
proaches [10, 11] improve Criminisi’s effect and pose the completion task as a
global optimization problem with a well-defined objective function and propose an
algorithm to optimize it. However, the cost of the objective function is usually ex-
orbitant. According to this case, a fast PatchMatch method [12] solves this prob-
lem to a considerable extent by propagating the neighborhood information using
neighbor patches. Furthermore, this method has been adopted by Adobe Photo-
shop. As for simple patch translation, it is difficult to find the most suitable patch
without extending the search space. In fact, many methods [13-17] have addressed
this issue via geometric transformation as well as photometric transformation. Xiao
et al. [18] filled in the target region using a sample image. This method adopted
an image with a similar texture and structure to enrich the search space. Le Meur
et al. [19] used a coarse version of the input image to generate multiple inpainted
images with different parameter settings and then recovered the full resolution of
the final result. Using a Markov Random Field (MRF) model to build the energy
function, many methods [20, 21] optimize the energy function for its efficiency in
realizing global image consistency. He and Sun [22] calculate the statistical offset
to obtain regular structure information. This method demonstrates an excellent re-
sult in images with a large amount of duplicated information; however, it is still a
problem when images have perspective shape deformation. The methods in Refs.
[23, 24] using a convolutional neural network (CNN) to generate the contents ac-
cording to its surroundings. This method provided a great solution for filling in a
large region and keeping the image semantically correct. However, it could not
handle images with perspective distortion.

In this study, we propose a novel method for inpainting damaged regions in structural
scenes. We also extend the method to different natural scenes. It has been discovered that
most of the texture in various scenes have structured features (regular or linear). There-
fore, we detect these parameters and make use of them to find a transformational relation
between source patch and target patch. Moreover, we propose an objective function with
two constraints to guide the texture synthesis. Different from previous methods, these
two constraints can provide effective guidance when searching for the best matching
patches. Finally, we apply a gradient term that is conducive to a gradual adjustment of the
colors of our objective function to maintain the texture details.

The three main contributions of the proposed method can be described as follows.
First, we adopted a parameterized transformation model to guide the image completion
process. Second, we proposed an objective function with two constraints which to-
gether guide texture synthesis. Third, we combine the effect of these constraints and
gradients into a framework that solves more challenging problems.
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 3 of 18

2 Method overview

Given an input image J with a damaged region (hole), we aim to fill in the damaged re-
gion using pixels from the known region. In practice, it is a challenge to fill in a dam-
aged region and obtain satisfying results, especially in architecture scenes. In many real
scenes, the shape can change dramatically because of perspective distortions. For each
target patch P in the damaged region, we calculate a transformation matrix T; that cor-
relate target patch to the best matching patch Q. To estimate the parameters of T;, we
adopt detected plane parameters [25] to generate the transformation model, instead of
searching for the best matching patches by simply translation (details given in Section
3.1). When searching for the similar source patches, unconstrained process usually
causes poor results. We constrain the patch sampling locations using texture direction
and texture consistency (Section 3.3). Furthermore, we add gradient into our frame-
work to obtain a smooth transition of color.

To obtain plausible texture in the hole region, the problem is translated into an
optimization scheme. We define an objective function consists of color term, guide
term, and gradient term. The color term explains how the source patches should be
transformed. The guide term provides constraint, i.e., how the searching process should
be limited. The gradient term gives an adjustment that leads to a smooth transition of
color. Combining these three terms, we show that the proposed method can effectively
improve the completion results in visual consistency. The flowchart of the proposed is
shown in Fig. 1.

3 Objective function

To achieve a high-quality result, we develop an objective function for image comple-
tion. The objective function is a measured distance function that includes three terms.
Here, we develop a transformation parametrized by 6; for each patch P.

We denote the improved energy minimization function as follows:
—_ = 5
E YS qEcolor (s; li, 0;) + Eeguide (si, t;) + Fgradient (s;, ti, 0:), (1)

where t; = (t*,f2)’ is the center position of a target patch in Q and s; = (s%,s”)" is the

 

 

Objective function

 

 

 

 

 

Gradient term

4

Smooth color

Color term

 

 

 

 

Transformation

Structure term
model

 

[
I ly
\ I]
I a}
I ly
\ ly
I I
I I
I a}
I ly
I ly
I ly .
| |
l \
; I A ly a . oe, | ;
Input image > | 1 Consistency -—> Optimization Completed image }
| \
_ ! i : _
I ly
| ly
I a}
I a}
I um}
I a}
I a}
I 1
I ly
I a}
I ly
I I

planar term
parameters

 

Constraints

Calculating the

 

 

difference Adjusting the
between the Constraining the color and h
target patch and searching process maintain the
texture details
the transformed
atch
1 Loi Loi

 

Fig. 1 Flowchart of proposed method

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 4 of 18

center position of the corresponding source patch in Q. Here, Q and Q are the labels
of known pixels and unknown pixels, respectively. We define 0; as a set of parameters
for generating a transformation matrix T;. The three terms Egoior, Egradient) ANd Eguide
are the color term, gradient term and guide term, which together form the function.
These terms will be explained in detail in the following sections.

3.1 Color term
The color matching term is similar to Ref. [8]:

E color (tis 6;) = | P(e, t”) ~~ Q(s* s’0;)| 7 (2)

1?

 

where P(t*,t}) is the target patch centered at ¢, and Q(t*,t7,0;) denote the matched
source patch using the transformation matrix T; with the parameter 6;. Here, the color
term represents the distance between the target patch and the transformed patch. We
use sum of squared distance in the RGB space to calculate the distance. In Refs. [13-
15], many geometric transformations were applied, e.g., rotation, scale, and flip. On the
contrary, we use a homograph matrix to transform the patches into an affine correction
space.

We now illuminate how we generate the transformation matrix T; based on the par-
ameter 0;. In many real scenes, the shape can change dramatically because of a perspec-
tive distortion. It is difficult to fill in a damaged region if only simple patch
transformation is taken into consideration. Xiao et al. [25] solved this problem by de-
tecting planes and making use of them to generate a projective transformation matrix.

In Fig. 2, we show the plane detection and posterior probability map.

 

(g) (h) (i)

 

Fig. 2 The process of generating the posterior probability map. a—-c Detected line segments in three
directions. d-f The density map in three channels. g-i Posterior probability maps

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 5 of 18

In our paper, we use the detected planar parameters to parametrize T; by 6; = (f, k,),
where k; is the index of plane and f; = (f7, f3,f;,f Si; ft ) is the six-dimensional af-

fine parameter. We define a transformation matrix as follow:
Ti (Uh, ti, f;,9:) = Hp (US, ti fi ki) Hr (7 Hs (fj) He (F1, $f) Hy (f;, fi): (3)

where H, indicates the projective transformation between source patch and target
patch. The matrix H, has the form:

1 0 0

H,=|0 1 0 (4)
k k k
ly ls ls

here, /X = (Jf, 5, IS) is vanishing line which has two degrees. The matrix H,

0 cos@6 = sin@ 0
H, = a) | = |- sinOd coséd O}, (5)
0 0 0 1

indicates a rotation transformation by a 2 x 2 rotation transformation M(s’). We define
the matrix H, as follow:

. 0 0
a, =| NY) 0s ol, (6)
0 0 O 1

where H, indicates the scale transformation by a 2 x2 scaling transformation N(s‘).
The matrix H,

1 f% 0
H.=|f? 1 Of, (7)
0 0 1

indicates the shear transformation. The matrix H;
1 0 f*
H,=|0 1 fi}, (8)
00 1

indicates the translation transformation by translation parameters f? and f}. The trans-
formation model is similar to the decomposition of projective transformation matrix
[26]. This formula effectively shows the transformation relation between source patch
and target patch.

3.2 Guide term

Owing to the difficulty of acquiring excellent inpainting results just using color and
gradient, we apply a guide term to constrain the patch search. Our guide term includes
two constraints:

E guide (s;, t;) = NE structure (si, t;) + C consistency (si, ti), (9)

where J is the weight of the structure term. These two constraints can together guide
the completion process.
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 6 of 18

3.2.1 Structure term

Many approaches [12, 27, 28] have demonstrated that limiting the search space by la-
beling the texture region could improve the completion result. Hence, we adopt a
method using Gray-level co-occurrence matrix (GLCM) to detect the dominant texture
direction and then automatically generate a structure guidance map that serves as a
position constraint. The detail about this method can refer to Ref. [28]. In this study,
we improve this method by further analyzing the optimal direction angle.

Based on Ref. [28], the greater the GLCM contrast, the smaller the similarity between
two pixels. We also obtain the relation between offset value (d) and the number of dir-
ection angle: the greater the offset value, the more is the directional angles. Zarif et al.
[28] analyzed the texture direction using eight direction angles (d = 2). In this study, we
compute the minimum of contrast to detect the current direction angle (also called
minimum direction angle), as shown in Fig. 3b. We detect more directions to deter-
mine the optimal direction. Note that big value of offset may reduce the sensitivity to
the texture direction. Thus, we set the maximum of offset d,,,, = 20. The distribution
of minimum direction angle is illustrated in Fig. 3c. We adopt the average value of all
the minimum direction angle to determine the optimal texture direction.

Given an original image, the content along a direction usually has a similar structure
and texture. To develop this property, we use the detected optimal direction to repre-
sent the content changes. Rather than limiting the search space using a non-gradient
color, a gradient color is adopted, as shown in Fig. 3d. Here, the structure guidance
map is regarded as a soft constraint in the completion process. In the structure guid-
ance map, the location of the same color usually has the same texture. The structure
guidance map encourages searching similar patches along the same direction.

The structure term Egtrcuture Makes use of the guidance map to constrain the position
where the source patches are drawn from (Gpos). The structure term is defined as
follows:

E strcuture (si, ti) — L(|Gpos (s7, a) ~ Gpos (t7, t}) 1) ’ (10)

where L(-) is the €-insensitive loss function L(x) = max(0,« - €). We denote Gyos as the
position information of the source patch and target patch in structure guidance map.
Gpos indicates the pixel values at the center of sampling patches. According to GLCM,
locations that have the same color in structure guidance map usually have similar tex-
tures. This means that locations with similar pixel values (G,,;) in structure guidance

map often have similar textures. Thus, the source patch and target patch have different

 

 

Value of contrast
° ara ee
Minimum Angle(°)
om - -

° 2 © x
is » @& & - iy a @& = Nn on
° :

S$

3}

B}

   

2 4 60 80 00 4 60 80
Angle(°)

(a) (b)
Fig. 3 Direction analysis. a Input. b Distribution of contrast. ¢ Distribution of minimum direction angle. d
Structure guidance map

 

(d)

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 7 of 18

values in Gyo; should be penalized by this term. Sampling along the texture direction is

encouraged to minimize the energy function.

3.2.2 Consistency term

Inspired by Ref. [12, 20], we add a consistency term into the completion process to
sample patches in adjacent regions. Given a target patch ¢;, if we can find a matching
patch s,, their neighboring patches ¢? and s’ are very likely to be the most similar
patches. We assume that every patch has neighbors in four directions. We define £7’,

Ny

ti’, t°, and t* as the neighboring patches of t; and s;

, ;?, 8°, and s/* as the neighbor-
ing patches of s;, respectively. If the difference between neighboring patches exceeds a
threshold, we add a consistency constraint to encourage sampling patches from neigh-

boring areas. The consistency term has the form:
cE consistency (si, ti) = > ja1234 ICs! (s;, s;) - on (¢7, ti) | > e| ’ (11)

where C;,’ and C;/ represent the current position of the neighboring patches. C;, indi-
cates the distance between a target patch and its neighboring patch. Similarly, C,! indi-
cates the distance between a source patch (corresponding to the target patch) and its
neighboring patch. If the argument is false, the indicator function [-] is 0; otherwise, it
is 1. Cs’ and C;’ have large difference value should be penalized. If the difference value
| C2! (s?,39) - C(t, 2) [> e, the argument is true and [|C*!(s3,s?) - C2"(#,8)| >
= 1. Similarly, if the difference value | Cs, (s¥,s)) - C;/(t%, 7) | <e, the argument is false
and [|C;/(s%,s)) — C;/(t*,t?)| > e] =0. Here, we set e=1 to encourage sampling near

the source patch. It helps to maintain the texture consistency.

3.3 Gradient term

To improve the results of completion, finding correct patches is necessary. Barnes et al.
[12] adopted Ly patch distance to compute the similarity between two patches. How-
ever, PatchMatch [12] may discover patches incorrectly when the texture is compli-
cated, as shown in Fig. 4d. The method of Barnes fails to find the correct texture
because it does not consider gradient. Adding a gradient term is helpful for gradually
adjusting the colors. We define the gradient term as follows:

Egradient — || VQ(si, Li, 0;) ~ VP(ti)||5, (12)

where VQ(s;, ¢;,0;) and VP(t;) denote the gradient of the patches centered at s; and t,,
respectively. The gradient term is used to adjust the local color of patch. It can lead to
a globally smooth transition of intensity and color [14]—a property that is lacking in
patch-based methods. Here, we also use sum of squared distance in the RGB space to
calculate the distance. This term can play to our strengths and search for the best simi-
lar patch for higher consistency.

4 Optimization

Given a large search space, it is intractable to discover a globally optimal completion.
Wexler et al. [11] proposed an iterative algorithm which includes two steps named
search and voting to optimize an objective function.
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 8 of 18

(a) (b) (c)
(d) (e)

Fig. 4 Gradient for image completion. a Damaged image. b Result of PatchMatch. ¢ Our result. d Finding
the most similar patches without gradient. d Finding the most similar patches with gradient

 

       

 

In the search step, we adopt PatchMatch [12] method to accelerate our algorithm.
When searching for the best matching patches, the position of a matching patch is found
first. Then, we search for a transformed matching patch. The nearest neighbor patches
are searched in the source region for every target patch to minimize the function.

Unlike previous methods, we reject unlikely patch transformation in scale when finding the
similar patches, ie., scale; < Sgcate(T;, $5 £;) < scalen, where S,-a1-(T;) indicates the scale estima-
tion. Large range of scale cannot provide effective constrain when finding source patches. Too
small range of scale can lead to narrow patch searching space. We set scale, = 0.7 and scalez =
1.3 as the acceptable range in our experiments and obtain valid results. The approximated
scale can be estimated using the first-order Taylor expansion [29].

In the voting step, the overlapping patches containing p have correspondence patches
in the source region. Wexler et al. [11] adopted a weighted voting program to fill a tar-
get region. Similarly, we take the median of all the votes as the pixel to reduce the blur
of pixel colors.

When calculating the patch distance, following HaCohen et al. [30], bias and gain are
added to obtain the best matching patches. In this study, we set bias to [— 50,50] and gain
to [0.5,1.5]. They are used to reject source patches whose gain or bias deviates the range.

This can also help to extend the patch searching space and match wide color difference.

5 Experimental results and discussion

5.1 Implementation details

Our algorithm was implemented with MATLAB and C++. The PatchMatch iteration
was [20, 30]. A large hole region required more iterations. The time of the proposed
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 9 of 18

image completion method can be categorized into two cases. The first case is to gener-
ate several guidance maps, which requires several seconds. The second case, which de-
termines the running time, depends on the image size, hole region, and the texture
complexity. For instance, given a 400 x 600 image with 120 x 140 damaged region, the

inpainting process may require 2—3 min.

5.2 Comparison results

To demonstrate the results of the proposed method, we compare our method with sev-
eral existing image completion algorithms, including Criminisi [8], image melding [14]
and He and Sun [22]. We run these methods on six test images, as shown in Fig. 5.

In the first two rows, the buildings contain more than one plane. We can see that the
proposed method can deal well with structural scenes. The other methods could not
maintain structural consistency if only using patch translation. In the third row, we
show buildings with projective distortions. Criminisi’s method obviously propagated
error information into a damaged region because of the flaw of priority in special cases.
Image melding, while taking into account multiple patch transformations, failed to
complete the original structure. He and Sun filled in the damaged region based on the

 

 

Fig. 5 Comparison results. a Input. b Criminisi’s result. ¢ Image melding. d He’s result. e Our result
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 10 of 18

offset statistics. However, it could not find the solution in a perspective space. The re-
sults in the fourth and fifth rows show that our method can recover structural
consistency. We transform sampled patch in source region into target region using
transformation model with a scale variation. The last row illustrates that our algorithm

demonstrates outstanding performance in maintaining textural consistency.

5.3 Qualitative evaluation

To find a satisfactory completion for the user is the real purpose of image completion.
One important test is visual inspection and another one is obtaining quantitative results
using peak signal to noise ratio (PSNR). The PSNR comparison of six images in Fig. 5
is shown in Table 1.

We observe that the PSNR value of the proposed algorithm is slightly higher overall
than the value of other algorithms. It is easy to know that the images completed by our
method are better than the other methods in image consistency and coherence for hu-
man eyes. Figure 6 shows the comparisons.

5.4 Other results

Object removal is also one of the application occasions of image completion. In order
to demonstrate the robustness of our method, we compare our method with current
methods in the natural scenes. In these scenes, we cannot acquire a set of plane param-
eters. Our method can also maintain the consistency of textural structure, and the re-
sults satisfy human visual coherence. Figure 7 shows the comparisons with methods
from Criminisi [8], Komodakis [10], He [22], and Le Meur [19]. In Table 2, we give the
quality scores of the inpainted images, as determined by the technique reported in Ref.
[31]. The lower the scores, the better the quality of the image. We can see from the
contrast result that Criminisi’s method introduces texture in a wrong location. Methods
of Komodakis’s and Le Meur’s can hardly guarantee the structure continuity. He’s
method achieves more satisfactory inpainting result, while small flaw still exists. Com-
pared with those methods, our method achieves better texture coherence and structure
continuity.

In Fig. 8, we compare the proposed algorithm with the method using a deep learning
model [23]. The input images are 128 x 128. We show the results in structural scenes
and natural scenes. Compared with deep learning models, our method has better per-
formance in maintaining the structural integrity and the global consistency of texture.
The deep learning model repairs the damaged region using a “generate” way. The qual-
ity of results relies on numerous training data and excellent network structure. On the

contrary, we estimate perspective distortions using a transformation model and

Table 1 Image completion performance measured in PSNR

 

 

Examples 1 2 3 4 5 6 Mean
Criminisi 19.96 21.89 20.83 23.58 23.57 18.91 21.32
Image melding 19.67 22.06 20.37 24.08 24.81 18.68 21.61
He 19.94 22.35 215] 23.94 23.94 19.51 21.87

Ours 20.21 22.43 21.80 24.26 25.05 19.95 22.28

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 11 of 18

 

26

   
    
    
  

—®— Criminisis's

25 —#*— Image melding
—t— He's

—— Ours

 

24

    
     

PSNR(dB)

NO
=

 

Fig. 6 PSNR comparison

 

constrain the completion process using the guidance map. The PSNR (dB) value is

shown in Table 3.

Figure 9 shows the comparison of results by the proposed method and Huang’s
method [16]. Form the first row, we can see that our algorithm has better performance
when inpainting large damaged region. The second row shows the comparison of re-
sults in a perspective scene. Due to the lack of search space and scale constraints, the
structure was distortions at the end of the building in Huang’s result. In the third row,

we show the comparison in keeping texture continuity. Huang’s method failed to find

 

(a) (b) (c) (d) (e) (f)

Fig. 7 Comparison of results in different scenes. a Input. b Criminisi’s results. ¢ Komodakis’s results. d He’s
result. e Le Meur's results. f Our results

 
Chen et al. EURASIP Journal on Image and Video Processing

Table 2 The quality scores for Fig. 7

(2020) 2020:44

 

 

Examples 1 2 3 4 Mean
Criminisi 44 4.78 5.95 5.85 5.25
Komodakis 4.14 475 5.97 5.5 5.09
He 4.05 4.75 5.88 5.6 5.07
Le Meur 4.18 4.74 6.08 5.6 5.15
Our 3.96 471 5.89 5.62 5.05

 

the demarcation between two kinds of texture. The fourth row demonstrates that our
method has a plausible performance in maintaining global texture consistency. We
apply a gradient term and a consistency term into our objective function to maintain
texture details and encourage sampling patch in adjacent areas. Therefore, the pro-
posed method performs better in both continuity and visual effect. The PSNR (dB)

value is shown in Table 4.

(b)

Fig. 8 Comparison of results. a Input. b Results in Ref. [23]. ¢ Our results

 

 

Page 12 of 18
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 13 of 18

Table 3 The comparisons measured in PSNR for Fig. 8

 

 

Examples 1 2 3 4 Mean
Ref. [23] 18.83 22.67 23.61 25.27 22.60
Ours 21.73 23.7] 23.95 26.94 24.08

 

5.5 Effect of patch size

Figure 10 shows the impact of patch size on the completion results. Our algorithm led
to poor performance when using too small patch. Small patch cannot capture enough
texture. Similarly, redundant texture was copied when using a too large patch. We

apply different patch sizes on an example, as shown in Fig. 10.

5.6 Effect of structure guidance

Figures 11 and 12 show the effect of the guidance map and the parameter 1. The guid-
ance map offers significant guidance for the patch searching process. Here, we show
the results of our method with different parameter values and comparisons. Figure 11
shows that the structure guidance map can help preserve structure integrity. In Fig. 12,
we show the result and effect of parameter 1. We can see that the structure line of the
house cannot be repaired reasonably if the value of A is too small. On the contrary, the
structure texture is discontinuous if the value of 1 is too large. In our experiments, A

was set to 2.5 and the performance is receivable.

 

 

 

Fig. 9 Comparison with Huang’s work. a Input. b Huang’s results. ¢ Our results
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 14 of 18

Table 4 The comparisons measured in PSNR for Fig. 9

 

 

Examples 1 2 3 4 Mean
Huang 18.21 18.34 17.45 18.35 18.09
Ours 19.32 18.82 17.53 19.00 18.67

 

5.7 Effect of gradient term and consistency term

To get some intuition on the importance of the gradient term and consistency term,
we illustrate four cases of information usage in Fig. 13. In Fig. 13b, we show the com-
pletion result without any guidance. The result is blurry and the structure is wrong. In
Fig. 13c, we only use the gradient term in the optimization process. Since the structure
information is insufficient, we obtain broken structures. In Fig. 13d, we only use the
consistency term. While the completed region has structure information, the texture
synthesis has an error in detail. The best result is acquired using both gradient and
consistency, as shown in Fig. 13e.

5.8 Limitations
It is difficult for our method to handle the texture details if the opposite sides of the
hole have textures with very different dominant directions. We fail to complete the

structure lines, as shown in Fig. 14. The results may be improved using more

 

Fig. 10 Effect of patch size. a Input. b 5 x 5 patch size. ¢ 7 x 7 patch size. d 9x9 patch size.e 11x 11
patch size. f 13 x 13 patch size

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 15 of 18

 

 

 

(g) (h) (i) (3)

Fig. 12 Effect of parameter A. a Input. b Criminisi’s result. ¢ Wexler’'s result. d Aurélie’s result. e He's result. f
Our result (A= 0). g Our result (A= 0.5). h Our result (A = 1.5). § Our result (A = 2.5). j Our result (A = 3.5)

 

 

oe aaa sc
| ae Soe

 

Fig. 13 Effect of gradient and consistency. a Input. b Unguided completion. ¢ Gradient guide only. d Consistency
guide only. e Gradient and consistency guides

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 16 of 18

 

     

 

(a) (b)

Fig. 14 Failure. a Input. b Our result

 

sophisticated computer vision methods, which we leave to future work. Running time

is also a limitation. Our approach is just a prototype. The time cost of our approach
can be further improved using more efficient algorithms.

6 Conclusion

We have proposed an improved image completion method using structural constraints.
First, we adopted a parameterized transformation model with detected plane parame-
ters to extend the patch search space. Furthermore, we proposed an objective function
with two constraints to guide the completion process. These two constraints provided
effective guidance when searching for the best matching patches. Finally, we combined
the constraints and gradient into a framework that could solve more challenging prob-
lems. We implemented our method in many images with various scenes and acquired

promising results of visual consistency.

Abbreviations
MRF: Markov random field; CNN: Convolutional neural network; GLCM: Gray-level co-occurrence matrix; PSNR: Peak
signal to noise ratio

Acknowledgements
Not applicable.

Authors’ contributions

QC and GL conceived and designed the study. QC, GL, QX, and MX performed the experiments. OX and LX offered useful
suggestions and helped to modify the manuscript. QC, GL, QX, LX, and MX reviewed and edited the manuscript. All
authors read and approved the manuscript.

Authors’ information

Qiaochuan Chen received his BS degree in information security from Hefei University of Technology, China, in 2013,
and his MS degree in computer technology from Yunnan University, China, in 2015. Since 2015, he has been working
toward his PhD in the College of Electronics and Information Engineering, Tongji University. His current research
interests include pattern recognition, image inpainting, and image encryption.

Guangyao Li received his BS and MS degrees from Nanjing University of Aeronautics and Astronautics, China, in 1986
and 1989, respectively. He received his PhD from Nanjing University of Aeronautics and Astronautics in 1997. Now, he
is a professor and doctoral supervisor in the College of Electrical and Information Engineering, Tongji University. His
primary research interests include image processing and virtual reality.

Qingguo Xiao received his BS degree in communication engineering from Zhejiang Normal University, China, in 2012,
and his MS degree in electronics and communication engineering from Ningbo University, China, in 2015. Since 2016,
he has been working toward his PhD in the College of Electronics and Information Engineering, Tongji University. His
current research interests include pattern recognition, artificial intelligence, and data analysis.
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 17 of 18

Li Xie received his BS degree in computer science from Zhejiang University of Technology, China, in 2012. Since 2014,
he has been working toward his PhD in the College of Electronics and Information Engineering, Tongji University,
China. His current research interests include machine learning, pattern recognition, and remote sensing classification.
Mang Xiao received his BS degree from Jiangxi Normal University in 2005 and his MS degree in computer science
from Nanchang University in 2010. He received his PhD from Tongji University in 2016. Now, he is a lecturer in the
School of Computer Science and Information Engineering, Shanghai Institute of Technology. His research interests
include image processing and machine learning.

Funding
This work was supported by the National Natural Science Foundation of China under grant number 61771346.

Availability of data and materials

Data will not be shared; the reason for not sharing the data and materials is that the work submitted for review is not
completed. The research is still ongoing, and those data and materials are still required by the author and coauthors
for further investigations.

Competing interests
The authors declare that they have no competing interests.

Author details
"College of Electronics and Information Engineering, Tongji University, Shanghai, China. “School of Computer Science
and Information Engineering, Shanghai Institute of Technology, Shanghai, China.

Received: 4 December 2018 Accepted: 22 September 2020
Published online: 08 October 2020

References

1. C Yan, H. Xie, J. Chen, et al., A fast Uyghur text detector for complex background images. IEEE Transactions on
Multimedia. 20, 3389 (2018)

2. C Yan, L. Li, C. Zhang, et al., Cross-modality bridging and knowledge transferring for image understanding. IEEE
Transactions on Multimedia (2019)

3. C. Yan, Y. Tu, X. Wang, et al., STAT: spatial-temporal attention mechanism for video captioning. IEEE Transactions on
Multimedia (2019)

4. M. Bertalmio, G. Sapiro, V. Caselles et al. Image inpainting, in Proceedings of the 27th Annual Conference on Computer
Graphics and Interactive Techniques (ACM Press/Addison-Wesley Publishing Co., 2000), pp. 417.

5.  F. Bornemann, T. Marz, Fast image inpainting based on coherence transport. Journal of Mathematical Imaging and

Vision. 28, 259 (2007)

6. D.Tschumperlé, Fast anisotropic smoothing of multi-valued images using curvature-preserving PDE's. Int. J. Comput. Vis.
68, 65 (2006)

7. A.A. Efros and T. K. Leung, Texture synthesis by non-parametric sampling, in Computer Vision, 1999. The Proceedings of
the Seventh IEEE International Conference on (IEEE, 1999), pp. 1033.

8. A. Criminisi, P. Pérez, K. Toyama, Region filling and object removal by exemplar-based image inpainting. IEEE Trans.
Image Process. 13, 1200 (2004)

9. J. Sun, L. Yuan, J. Jia et al., Image completion with structure propagation, in ACM Trans. Graph. (ACM, 2005), pp. 861.

10. N. Komodakis, G. Tziritas, Image completion using efficient belief propagation via priority scheduling and dynamic

pruning. IEEE Trans. Image Process. 16, 2649 (2007)

11. Y. Wexler, E. Shechtman, and M. Irani, Space-time completion of video. IEEE Trans. Pattern Anal. Mach. Intell. 29 (2007).

12. C. Barnes, E. Shechtman, A. Finkelstein, et al PatchMatch: a randomized correspondence algorithm for structural image

editing. ACM Trans. Graph. 28, 24 (2009)

13. A. Mansfield, M. Prasad, C. Rother et al. Transforming image completion, in BMVC 2011), pp. 1.

14. S. Darabi, E. Shechtman, C. Barnes et al., Image melding: combining inconsistent images using patch-based synthesis.

ACM Trans. Graph. 31, 82:1 (2012).

15. J.-B. Huang, J. Kopf, N. Ahuja et al, Transformation guided image completion, in Computational Photography (ICCP),

2013 IEEE International Conference on (IEEE, 2013), pp. 1.

16. J.-B. Huang, S.B. Kang, N. Ahuja, et al, Image completion using planar structure guidance. ACM Trans. Graph. 33, 129

(2014)

17. J.-B. Huang, A. Singh, and N. Ahuja, Single image super-resolution from transformed self-exemplars, in Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition 2015), pp. 5197.

18. M. Xiao, G. Li, L. Peng, et al., Completion of images of historical artifacts based on salient shapes. Optik-International

Journal for Light and Electron Optics. 127, 396 (2016)

19. O. Le Meur, M. Ebdelli, C. Guillemot, Hierarchical super-resolution-based inpainting. IEEE Trans. Image Process. 22, 3779
(2013)

20. J. Kopf, W. Kienzle, S. Drucker, et al., Quality prediction for image completion. ACM Trans. Graph. 31, 131 (2012)

21. Y. Pritch, E. Kav-Venaki, and S. Peleg, Shift-map image editing, in Computer Vision, 2009 IEEE 12th International
Conference on (IEEE, 2009), pp. 151.

22. K.He and J. Sun, Statistics of patch offsets for image completion, in Computer Vision-ECCV 2012 (Springer, 2012), pp. 16.

23. D. Pathak, P. Krahenbuhl, J. Donahue et al. Context encoders: Feature learning by inpainting, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition 2016), pp. 2536.

24. S. lizuka, E. Simo-Serra, H. Ishikawa, Globally and locally consistent image completion. ACM Trans. Graph. 36, 107 (2017)

25. M. Xiao, G. Li, Y. Jiang et al., Image completion using belief propagation based on planar priorities. KSII Transactions on
Internet & Information Systems. 10 (2016).

 

 

 

 
Chen et al. EURASIP Journal on Image and Video Processing (2020) 2020:44 Page 18 of 18

26. R. Hartley, A. Zisserman, Multiple view geometry in computer vision Cambridge University Press (2003)

27. A. Hertzmann, C. E. Jacobs, N. Oliver et al. Image analogies, in Proceedings of the 28th Annual Conference on Computer
graphics and interactive techniques (ACM, 2001), pp. 327.

28. S. Zarif, |. Faye, D. Rohaya, Image completion based on statistical texture analysis. Journal of Electronic Imaging. 24,
013032 (2015)

29. O.Chum and J. Matas, Planar affine rectification from change of scale, in Asian Conference on Computer Vision
(Springer, 2010), pp. 347.

30. Y. HaCohen, E. Shechtman, D.B. Goldman, et al., Non-rigid dense correspondence with applications for image
enhancement. ACM Trans. Graph. 30, 70 (2011)

31. A. Mittal, R. Soundararajan, A.C. Bovik, Making a “completely blind” image quality analyzer. IEEE Signal Processing Letters.
20, 209 (2013)

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen”®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

 

Submit your next manuscript at > springeropen.com

 

 
