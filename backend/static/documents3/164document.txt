Idakwo et al. J Cheminform (2020) 12:66

https://doi.org/10.1186/s13321-020-00468-x Journa | of Cheminformatics

RESEARCH ARTICLE Open Access

- ®
Structure-activity relationship-based cn

chemical classification of highly imbalanced
Tox21 datasets

Gabriel Idakwo!, Sundar Thangapandian?, Joseph Luttrell!, Yan Li?, Nan Wang4, Zhaoxian Zhou!,
Huixiao Hong?, Bei Yang®, Chaoyang Zhang! ® and Ping Gong?

Abstract

The specificity of toxicant-target biomolecule interactions lends to the very imbalanced nature of many toxicity
datasets, causing poor performance in Structure—Activity Relationship (SAR)-based chemical classification. Undersam-
pling and oversampling are representative techniques for handling such an imbalance challenge. However, remov-
ing inactive chemical compound instances from the majority class using an undersampling technique can result in
information loss, whereas increasing active toxicant instances in the minority class by interpolation tends to introduce
artificial minority instances that often cross into the majority class space, giving rise to class overlapping and a higher
false prediction rate. In this study, in order to improve the prediction accuracy of imbalanced learning, we employed
SMOTEENN, a combination of Synthetic Minority Over-sampling Technique (SMOTE) and Edited Nearest Neighbor
(ENN) algorithms, to oversample the minority class by creating synthetic samples, followed by cleaning the misla-
beled instances. We chose the highly imbalanced Tox21 dataset, which consisted of 12 in vitro bioassays for > 10,000
chemicals that were distributed unevenly between binary classes. With Random Forest (RF) as the base classifier and
bagging as the ensemble strategy, we applied four hybrid learning methods, i.e., RF without imbalance handling (RF),
RF with Random Undersampling (RUS), RF with SMOTE (SMO), and RF with SMOTEENN (SMN). The performance of
the four learning methods was compared using nine evaluation metrics, among which F, score, Matthews correlation
coefficient and Brier score provided a more consistent assessment of the overall performance across the 12 datasets.
The Friedman's aligned ranks test and the subsequent Bergmann-Hommel post hoc test showed that SMN signifi-
cantly outperformed the other three methods. We also found that a strong negative correlation existed between

the prediction accuracy and the imbalance ratio (IR), which is defined as the number of inactive compounds divided
by the number of active compounds. SMN became less effective when IR exceeded a certain threshold (e.g., > 28).
The ability to separate the few active compounds from the vast amounts of inactive ones is of great importance in
computational toxicology. This work demonstrates that the performance of SAR-based, imbalanced chemical toxicity
classification can be significantly improved through the use of data rebalancing.

Keywords: Structure—activity relationship (SAR), Chemical classification, Molecular fingerprints, Random forest (RF),
Ensemble learning, Bootstrap aggregation (bagging), Class distribution imbalance, Resampling, Synthetic minority
over-sampling technique (SMOTE), Edited nearest neighbor (ENN), Random undersampling (RUS)

 

 

*Correspondence: chaoyang.zhang@usm.edu; Ping.Gong@usace.army.mil Introduction

' School of Computing Sciences and Computer Engineering, University Structure—activity relationship (SAR) has been frequently
of Southern Mississippi, Hattiesburg, MS 39406, USA . . . sage .

2 Environmental Laboratory, U.S, Army Engineer Research used to predict the biological activities of chemicals from
and Development Center, Vicksburg, MS 39180, USA their molecular structures. One of the major challenges
Full list of author information is available at the end of the article in SAR-based chemical classification or drug discovery

© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and
the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material

in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material
is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the
permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativeco
mmons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/
zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.

 

 
Idakwo et al. J Cheminform (2020) 12:66

is the extreme imbalance between active and inactive
chemicals [1]. Despite the existence of as many as 10’
commercially available molecules [2], there is almost
always a skew in the distribution of molecules across the
bioactivity landscape or toxicity classes. Biomacromol-
ecules such as proteins are often highly selective in their
binding to small molecular ligands. Regardless of the
huge chemical space, only a few compounds are likely to
interact with a target biomacromolecule causing biologi-
cal effects and are consequently labelled as active com-
pounds, whereas the remaining majority are labelled as
inactive compounds. This gives rise to a common prob-
lem of class imbalance for SAR-based predictive mod-
eling, particularly in chemical classification and activity
quantification using machine learning approaches [3-5].

In machine learning, classifiers are built on data statis-
tics and require a balanced data distribution to achieve
optimal performance. Classifiers trained from imbal-
anced data tend to have a bias towards the majority class.
This leads to low sensitivity and precision for the minor-
ity class [6], even though the minority class is usually of
greater importance than the majority class [7, 8]. In fields
such as toxicology and disease diagnosis, bias towards
the majority class may result in a higher rate of false neg-
ative predictions [1].

The problem of data imbalance has been studied
in the context of machine learning-based SAR mod-
eling for more than two decades [7, 9, 10]. As a result,
a plethora of methods have been proposed to alleviate
the skewness of class distribution. These methods can
be grouped into three categories: data-level, algorithm-
level, and hybrid [7, 11]. Data-level methods aim to
rebalance the training dataset’s class distribution either
by undersampling the majority class or oversampling
the minority class [12, 13]. They also include meth-
ods that clean overlapping samples and remove noisy
samples that may negatively affect classifiers [13, 14].
Algorithm-level methods attempt to alter a given learn-
ing algorithm by inducing cost sensitivity that biases a
model towards the minority class. For example, this may
be achieved by imposing a high misclassification cost
for the minority class [7, 11]. Recently, Mondrian con-
formal prediction (MCP) has been applied to improve
the performance of machine learning from imbalanced
datasets by computing nonconformity scores to model
the reliability of predictions. This allows for identifying
reliable predications at user-defined significance and
confidence levels [15-19]. The MCP approach does not
require data rebalancing. Hybrid methods combine the
use of resampling strategies with special-purpose learn-
ing algorithms [11]. Ensemble approaches (e.g., bagging
and boosting), known to increase the accuracy of single

Page 2 of 19

classifiers, have also been hybridized with resampling
strategies [6].

The selection of appropriate metrics plays a key role
in evaluating the performance of imbalanced learning
algorithms [11, 20]. In consideration of user preference
(e.g., identifying rare active chemicals) and data distribu-
tion, a number of metrics have been proposed, including
precision, recall, Area Under the Precision-Recall Curve
(AUPRC) [21], Area Under the Receiver Operating Char-
acteristics (AUROC) [22], F-measure, geometric mean
(G-mean), balanced accuracy, etc. [23-26]. For instance,
precision is not affected by a large number of negative
samples because it measures the number of true positives
out of the samples predicted as positives (i.e., true posi-
tive + false positive). A high AUPRC represents both high
recall and high precision. High precision relates to a low
false positive rate, and high recall relates to a low false
negative rate [21, 27].

The present study was motivated by the scarcity of
reported efforts in the application of the above-men-
tioned methods to the SAR-based chemical classifica-
tion domain. We conducted a literature survey which
only identified a few studies in this domain where cost-
sensitive learning [28, 29], resampling [29, 30], confor-
mal prediction [18] and extreme entropy machines [1,
31] were employed to specifically deal with data imbal-
ance. Although predictive modeling was improved for
certain datasets, a consistent performance enhancement
was not observed as a result of resampling and algorithm
modification. Apparently, more studies are warranted
to further examine such questions as: (1) Does imbal-
ance ratio (IR), i.e., inactive-to-active sample ratio, affect
the effectiveness of data-level methods (particularly
resampling methods)? (2) Would different data rebal-
ancing techniques affect the performance of a classifier
differentially, and does the combination of undersam-
pling and oversampling techniques, such as SMOTEENN
(SMOTE+ ENN) [32], outperform an undersampling or
oversampling technique alone? (3) What metrics can bet-
ter evaluate the results of imbalanced learning in SAR-
based chemical classification? This study attempted to
address all three of these questions.

To address the first question, we selected twelve binary
datasets of 10 K compounds with varying degrees of
imbalance, which were generated within the Toxicology
in the 21st century (Tox21) program [33] and used for the
Tox21 Data Challenge 2014 [34, 35] (https://tripod.nih.
gov/tox21/challenge/about.jsp). To address the other two
questions, we chose nine evaluation metrics, compared
three resampling algorithms integrated with the base
classifier (random forest—RF), and performed statistical
analysis to rank the metrics.
Idakwo et al. J Cheminform (2020) 12:66

In this work, we selected RF as the base classifier
and bagging as the ensemble learning algorithm to
improve the stability and accuracy of model predic-
tions. Then, we applied three representative resam-
pling methods for data imbalance handling, i.e., random
under-sampling (RUS), the synthetic minority over-
sampling technique (SMOTE) and SMOTEENN (i.e., a
combination of SMOTE and Edited Nearest Neighbor
(ENN) algorithms). Consequently, four hybrid learn-
ing methods, i.e., RF without imbalance handling (RF),
RF with RUS (RUS), RF with SMOTE (SMO), and RF
with SMOTEENN (SMN) were tested. Here, we did
not intend to conduct a comprehensive or exhaus-
tive comparative investigation of all existing imbalance
handling methods, but rather to use this case study to
demonstrate that appropriate handling of imbalanced
data and the choice of appropriate evaluation metrics
could improve SAR-based classification modelling.
We also investigated the performance of these existing
approaches and highlighted their limitations regard-
ing imbalance ratio. The rest of the paper is organized
as follows: “Materials and methods” section covers the
study design, data curation and preprocessing steps,
imbalance handling methods, and performance metrics.
“Results and discussion” section presents our classifica-
tion performance results, statistical analysis, and a com-
parison with published results for the Tox21 datasets.
Lastly, “Conclusions” section briefly summarizes the
major findings from this study and concludes with some
remarks on future research needs.

Materials and methods

Study design

The workflow of our study design is outlined in Fig. 1.
It consists of data preprocessing, feature generation and

Page 3 of 19

selection, resampling, model training (ensemble learn-
ing), model testing and performance evaluation. The data
preprocessing and feature generation steps were applied
to a total of 12,707 compounds in the raw dataset of 12
assays. However, feature selection, resampling and train-
ing of classifiers were conducted separately for each indi-
vidual assay. For each assay, the preprocessed compounds
in the training set were split into N stratified bootstrap
samples with replacement (i.e., samples were randomly
selected but retained the same imbalance ratio). This was
followed by ensemble learning either without resampling
(RF) or with the application of a resampling technique
(RUS, SMOTE, or SMOTEENN). Optimal parameters
for each base learner were obtained via grid search with
fivefold cross validation. Optimized base learners were
combined to form the final ensemble learner. Evalua-
tion metrics were calculated using the prediction results
of RF, RUS, SMO and SMN to statistically compare their
performance. Details of the workflow are presented
below.

Chemical in vitro toxicity data curation

The Tox21 Data Challenge dataset used in this study
consisted of 12 quantitative high throughput screening
(qHTS) assays for a collection of over 10 K compounds
(with redundancy within and across assays). The 12
in vitro assays included a nuclear receptor (NR) signal-
ing panel and a stress response (SR) panel. The NR panel
comprised 7 qHTS assays for identifying compounds
that either inhibited aromatase or activated androgen
receptor (AR), aryl hydrocarbon receptor (AhR), estro-
gen receptor (ER), or peroxisome proliferator-activated
receptor y (PPAR-y). The SR panel contained 5 qHTS
assays for detecting agonists of antioxidant response
element (ARE), heat shock factor response element

 

Data Feature Generation
Preprocessing and Selection

Tox21 Challenge ECFP
Data

MACCS FP
RDKit Descriptors

  
        
 
       
      
 

    
  
   

 

Training
Data Set

 
 

RDKit

1a ¢ e
Standardization Remove features

W th zero
variance

 

and duplicates
Test Data
Set

 

XN

Resampling

Generate WN stratified
Bootstrap samples

[Rus]
SMOTE
SMOTEENN |

Fig. 1 Workflow of structure—activity relationship (SAR)-based chemical classification with imbalanced data processing designed for this study

Performance
Evaluation

Training Testing
Parameter search with
cross vancaton
Metrics
F. Score
MCC
AUROC
AUPRC
Balanced Acc
Precision
Recall
Brier Score

Ensemble Learning

 

Classfier 1
Classifier 2

ClassifierN

 

 

Statistical
Analysis

 
Idakwo et al. J Cheminform (2020) 12:66

(HSE) or p53 signaling pathways, disruptors of the
mitochondrial membrane potential (MMP), or genotox-
icity inducers in human embryonic kidney cells express-
ing luciferase-tagged ATAD5. There were three sets of
chemicals: a training set of 11,764 chemicals, a leader-
board set of 296 chemicals and a test set of 647 chemi-
cals [35]. For this study, we merged the leaderboard
set with the original training set to form our “training
set” and retained the original test set as our “test set”
The Tox21 dataset was downloaded in SDF format at
https://tripod.nih.gov/tox21/challenge/data.jsp. There
were four possible assay outcomes for each compound:
active, inactive, inconclusive or not tested. Only those
chemicals labeled as either active (1) or inactive (0)
were retained for this study.

Compound preprocessing and chemical descriptor
(feature) generation

Chemical structures were also downloaded at https://
tripod.nih.gov/tox21/challenge/data.jsp as SMILES
files. Data standardization/cleaning was carried out
using MolVS [36], a publicly available tool built on
RDKit [37]. Standardization involved a fragmentation
step as described in [25] where compounds possessing
distinct structures not linked by covalent bonds were
split into separate “compound fragments” Then, sol-
vent fragments, salts and problematic molecules with
inconsistent resonance structures and tautomers [38],
which should not contribute to the biological effect
of a compound [39], were removed. The resulting
SMILES entries were canonicalized by standardizing
chemotypes such as nitro groups and aromatic rings,
and the largest uncharged fragments of the compound
were retained. After standardization, the resulting
fragments were merged based on their reported activ-
ity to exclude replicates and conflicting instances. Spe-
cifically, only one instance of a set of duplicates was
retained with the most frequent activity label, while
duplicates with ambiguous activity labels (i-e., an equal
number of active and inactive outcomes for the same
chemical) were removed. Three types of molecular fea-
tures (> 2000 in total), i.e., RDKit descriptors, MACCS
(Molecular ACCess System) keys and Extended-Con-
nectivity Fingerprints (ECFPs) [40] with a radius of 2
and a fixed bit length of 1024, were generated using
RDKit [37] to characterize the final set of compounds.
All features with zero variance were dropped.

Sampling and classification methods

Here we briefly describe the three resampling techniques
(i.e, RUS, SMOTE and SMOTEENN) that we used for
handling imbalanced data with RF chosen as the base
classifier.

Page 4 of 19

RUS

RUS is a widely used undersampling technique which
randomly removes samples from the majority class. In
our study, RUS was used to randomly remove inactive
compounds. While RUS alleviates imbalance in the data-
set, it may potentially discard useful or important sam-
ples and increase the variance of the classifier. Recent
studies have shown that the integration of RUS with
ensemble learning can achieve better results [6, 41]. To
overcome its drawbacks, we combined RUS with bagging
(an ensemble learning algorithm) for SAR-based chemi-
cal classification.

SMOTE

SMOTE is an oversampling technique that creates
synthetic samples based on feature space similarities
between existing examples in the minority class [12]. It
has shown a great deal of success in various applications
[20]. To create a synthetic data sample, we first took a
sample from the dataset of the minority class and consid-
ered its kK-nearest neighbors based on Euclidian distance
to form a vector between the current data point and one
of those k neighbors. The new synthetic data sample was
obtained by multiplying this vector by a random number
a between 0 and 1 and adding the product to the cur-
rent data point. More technical details on how to create
synthetic samples are described in the Additional file 1:
Figure S1 and in [12, 20]. Applying SMOTE to the minor-
ity class instances can balance class distributions [12] and
augment the original dataset in a manner that generally
significantly improves learning [20].

SMOTEENN

Despite many promising benefits, the SMOTE algo-
rithm also has its drawbacks, including over generaliza-
tion and variance [20]. In many cases, class boundaries
are not well defined since some synthetic minority class
instances may cross over to appear in the majority class
space, especially for nonlinear data with a large feature
space [42]. As a result, some new synthetic samples in the
minority class may be mislabeled and attempting to learn
from such datasets often results in a higher false predic-
tion rate [43]. To remove the mislabeled samples created
by the SMOTE technique, we applied SMOTEENN [32],
a combination of SMOTE and the Edited Nearest Neigh-
bor (ENN) [44] algorithm, to clean the synthetic data
samples.

In the ENN algorithm, the label of every synthetic
instance is compared with the vote of its k-nearest neigh-
bors. The instance is removed if it is inconsistent with its
k-nearest neighbors; otherwise, it remains in the data-
set. The process of removing mislabeled samples and
retaining the valid synthetic instances is illustrated in
Idakwo et al. J Cheminform (2020) 12:66

the Additional file 1: Figure Sic. A higher k value in the
edited nearest neighbors algorithm leads to a more strin-
gent cleaning rule that allows more synthetic instances to
be eliminated. Applying SMOTEENN to an imbalanced
dataset does not automatically result in a perfectly bal-
anced set after resampling, but it creates more meaning-
ful synthetic samples in the minority class and reduces
the imbalance ratio to a more manageable level.

RF and ensemble learning RF is a robust supervised
learning algorithm that has been widely used for clas-
sification in many applications in data science [45]. An
RF model consists of many individual decision trees that
operate as an ensemble. The individual decision trees are
generated using a random selection of features at each
node to determine the split. During classification, each
tree votes and the class with most votes becomes the
model’s prediction.

RF can be built [46] and improved [47] using bagging
(short for bootstrap aggregation). Bagging is a common
ensemble method that uses bootstrap sampling in which
several base classifiers are combined (usually by averag-
ing) to form a more stable aggregate classifier [48]. Each
base classifier (RF in this study) in the ensemble is trained
on a different subset of the training dataset obtained by
random selection with replacement, thus introducing
some level of diversity and robustness. It is well known
that the bagging classifier is more robust in overcom-
ing the effects of noisy data and overfitting, and it often
has greater accuracy than a single classifier because the
ensemble model reduces the effect of the variance of
individual classifiers [6, 48, 49].

Page 5 of 19

In our case, the Tox21 dataset was both highly dimen-
sional and highly imbalanced [6, 50]. For datasets with
such a large feature space and a small number of minority
class samples, classification often suffers from overfitting.
Because bagging is less susceptible to model overfitting,
we chose it as the ensemble method. Combining the
base classifier RF with three sampling techniques (RUS,
SMO and SMOTEENN) and bagging, we assembled four
hybrid classification methods: (1) RF without resampling,
(2) RF+RUS, (3) RF+SMO, and (4) RF+SMOTEENN.
For more convenient result analysis, the four meth-
ods were simply denoted as RF, RUS, SMO and SMN,
respectively.

Here we use SMN as an example to illustrate the algo-
rithm that integrates resampling with ensemble learn-
ing (see Algorithm 1 and Fig. 1). First, a subset, S;, was
obtained by taking a stratified bootstrap sampling from
the training set, X. This sampling process was repeated
N times, where i=1 to N, with N ranging between 5 and
100 in steps of 5. Stratification was employed to ensure
that each bootstrap had the same class distribution as
the entire training set. Each subset is used to train a clas-
sifier in the ensemble, hence N is also equivalent to the
number of classifiers. Then, the SMOTEENN algorithm
was applied to S; to oversample the minority class and
obtain an augmented training subset S/, which was used
to train a random forest classifier f(x). The parameters
for each classifier in the ensemble were selected using a
grid search with a fivefold cross-validation. This would
give every individual classifier a chance to attain its best
performance and contribute optimally to the ensemble.
The final ensemble model was a bagged classifier that
would count the votes of the WN classifiers and assign the
Idakwo et al. J Cheminform (2020) 12:66

class with the most votes to a chemical in the test data-
set. The other three methods RF, RUS and SMO also
employed Algorithm 1 with the only difference being
the resampling technique, i.e., no resampling, RUS and
SMOTE, respectively. All classifiers were implemented
using the Scikit-learn package [51] and Imbalanced-learn
in a Python toolbox [52].

Algorithm 1: N = Number of classifiers, X = Training set

For i from 1 to N (number of classifiers):

Page 6 of 19

correctly predicted as active by the model; (2) false posi-
tive (FP) as the number of true inactive chemicals incor-
rectly predicted as active; (3) true negative (TN) as the
number of true inactive chemicals correctly predicted
as inactive; and (4) false negative (FN) as the number of
true active chemicals incorrectly predicted as inactive.

(1) Take a stratified bootstrap sample, 5;, from training set, X

(2) Apply SMOTEENN to S; in order to obtain S,’

(3) Build a classifier f;(x) using S;’ as the training set and 5-fold cross validation with a grid parameter

search

Obtain the ensemble model, F(x), a collection of the classifiers given as (f;(x)|i = 1, ..., N)

Prediction of F(x) = majority votes of all N classifiers for a test instance

Performance evaluation metrics

The output of a binary classification model can be pri-
marily represented by four terms: (1) true positive (TP)
defined as the number of true active chemicals that are

Most evaluation metrics are derived from these four
terms. True positive rate (TPR), also referred to as sen-
sitivity or recall, represents the fraction of correctly pre-
dicted active chemicals. In SAR modeling, recall is also

Table 1 Class distribution and imbalance ratio (IR) of the preprocessed training and test chemical datasets from Tox21

Data Challenge

 

 

 

 

 

In vitro qHTS assay ID Total number Training set Test set
of chemicals
Inactive Active IR Inactive Active IR
R-AR 6436 5698 166 34.3 560 12 46,7
R-AR-LBD 5931 5223 143 36.5 557 8 69.6
R-AhR 5596 4445 561 7.9 520 70 74
R-Aromatase 4901 4193 193 21.7 478 37 12.9
R-ER 517] 4167 500 8.3 455 49 9.3
R-ER-LBD 6043 5239 221 23./ 563 20 28.2
R-PPAR-y 5712 5005 120 41.7 558 29 19.2
SR-ARE 4808 3669 603 6.1 448 88 5.1
SR-ATAD5 6320 5515 203 27.2 568 34 16.7
SR-HSE 5529 4733 206 23.0 573 17 33.7
SR-MMP 4955 3763 666 5.7 472 54 8.7
SR-p53 6009 5110 303 16.9 558 38 14.7

 

The highest and lowest IRs for the training and test sets are in bold
Idakwo et al. J Cheminform (2020) 12:66

considered as a measure of the accuracy of the active
(minority) class. True negative rate (TNR) or specific-
ity provides a similar measure (accuracy) for the inactive
(majority) class. Precision estimates the probability of a
model to make a correct active class prediction. F, score
is the harmonic mean of precision and recall. Similarly,
balanced accuracy (BA) is the average of correct predic-
tions for both classes. Matthews correlation coefficient
(MCC) offers a good index for the performance of imbal-
anced classification tasks as it incorporates all the compo-
nents of the confusion matrix [53]. MCC has been widely
used to evaluate the performance of SAR-based chemical
classification [34, 54]. The MCC value varies in the range
of [— 1, 1] with — 1 implying disagreement, 1 complete
agreement and 0 no correlation between the prediction
and the known truth. The Brier score is a measure of the
average squared difference between the predicted prob-
abilities and the known value for a class, and it assesses
the overall accuracy of a probability model. The formulas
of these evaluation metrics are given as follows:

Page 7 of 19

Precision x Recall
F, score = 2 xX ——________
Precision + Recall

Sensitivity + Specificit
Balanced accuracy (BA) = =ensttiniy + Specsiety

TP x TN—FP x FN
/ (TP + FP)(TP + FN)(TN + FP)(TN + FN)

1 N
N it (Pi — 0;

where N is the total number of chemicals in a data-
set, p; (€ [0,1]) is the predicted probability, and 0; is the
ground truth for the ith chemical (equal to 1 for active
and 0 for inactive).

In addition, the two widely used metrics AUROC and
AUPRC were also calculated using Scikit-learn [51] to
evaluate and compare the overall performance of a clas-
sifier against another. Finally, sensitivity—specificity gap
(SSG), calculated as the absolute value of the difference
between sensitivity and specificity, was introduced as a
metric to evaluate how balanced a classifier was in terms
of its performance on these two metrics [13].

We performed statistical analysis to assess if there
existed significant differences among the four investi-
gated classification methods in terms of their perfor-
mance metrics across the twelve bioassays (Table 1).
We adopted a nonparametric test for multiple com-
parisons as described in Garcia et al. [55]. Using the

MCC =

 

Brier score =

 

Les TP
Recall = Sensitivity = ———
TP + FN
a TN
Specificity = —————
IN + FP
a TP
Precision = ——————
TP + FP
0.6
—=— RF —e KNN
— —-
0.5

Mean F, score
o 2
Wo 4

©
N

 

as evaluated using F, score
XX

CART —&—-NB —6—SVM —&—MLP

Y-
\\

0.1 = —
0
Q 2) @ @ j 92 @ <, % K, Q 2
a PP FM OP NS OF
rT or £ SS SSF SX KO Fw ¥
ee © ge & $ 9
Ss x Ss Q
S R
S X
SS
Assays

Fig. 2 A spot check of six popular machine learning algorithms: performance of classifiers trained using the preprocessed Tox21 training datasets

 
Idakwo et al. J Cheminform (2020) 12:66

Statistical Comparison of Multiple Algorithms in Multi-
ple Problems (scmamp) library in R [56], we conducted
a Friedman’s aligned-rank test [57]. The Friedman test
was chosen over other statistical tests such as ANOVA
because it does not require the assumption of data nor-
mality. The Bergmann-Hommel post-hoc test was car-
ried out for pairwise comparisons between SMN and the
other three methods (RF, RUS and SMO) [54).

Results and discussion

In this section, we present (1) a summary of the curated
and preprocessed Tox21 dataset, (2) the preliminary
comparative results to justify the selection of RF as the
base classifier, (3) parameter optimization for RF and
ENN algorithms, (4) performance metrics of four classi-
fication methods for the twelve imbalanced Tox21 data-
sets, (5) the impact of IR and classification methods on
prediction performance, and (6) a comparison between
this study and published Tox21 studies.

Data curation and preprocessing

A summary of the preprocessed training and test datasets
of chemicals and their activities measured by 12 qHTS
in vitro assays is presented in Table 1. Although the origi-
nal raw Tox21 datasets contained more than 12 K chemi-
cals, approximately 50% of them or fewer were retained
for each assay after preprocessing. This was primarily due

Page 8 of 19

to duplication and the absence of testing data for individ-
ual assays. The imbalanced ratio (IR), defined as the ratio
of the number of the majority class (inactive compounds)
to that of the minority class (active compounds) [42], var-
ied widely between assays and between the training and
the test sets. Such large disparities offered a great oppor-
tunity to investigate the performance of different ensem-
ble-resampling approaches as a function of IR (see below
for detailed results). In the training datasets, the high-
est IR of 41.7 appeared in the dataset of the NR-PPAR-y
assay, whereas the lowest IR of 5.7 was observed with
the SR-MMP assay. The test datasets generally had IRs
larger than or equivalent to those of their correspond-
ing training datasets, e.g., measuring as high as~ 70 for
NR-AR-LBD (except for NR-Aromatase, NR-PPAR-y, and
SR-ATAD5).

Selecting RF as the base classifier

A comparison of six popular machine learning algo-
rithms, ie, RF, K-nearest neighbors (KNN), decision
trees (CART), Naive Bayes (NB), support vector machine
(SVM) and multilayer perceptron (MLP), was performed
using the training datasets of all twelve assays and a strat-
ified fivefold cross validation. These algorithms were all
implemented in Scikit-learn [51] with default parameter
settings. The purpose of this preliminary study was to
select a base classifier from these algorithms. F, score

 

 

XN

1
0.8
O
a
5 0.6
<x
op 0.4
Z
0.2 — | :
0
5 10 20 30 50 100
Number of Classifiers
1

—®—RF —®-—RUS SMO —*—SMN

0
Oo
S 0.6
9d 0.4
<x ~ Wa a __wN

5 10 20 30 50 100

Number of Classifiers

1
w 0.8
O
2 0.6
uo 0
yp Ot i $$
2 or er
<x 0.2 ao" ~ Ww
0
5 10 20 30 50 100
Number of Classifiers
1
O 0.9
2
5 0.8 - ry
oD 0.7
>
ft 06
0.5
5 10 20 30 50 100
Number of Classifiers
Fig. 3 The relationship between model performance and the number of classifiers in the RF base classifier

 
Idakwo et al. J Cheminform (2020) 12:66

 

0.9

08 ee acc

0.7
0.6

0.5

Performance metrics

0.1 —¢#— Fi-score —@— AUPRC —t— BA —— MCC

1 2 3 4 5
Number of nearest neighbors (K)
Fig. 4 Performance metrics of SMN models measured as the number
of nearest neighbors (k) varied in the ENN

 

NS 7

was calculated and used as the metric to evaluate clas-
sification performance. As shown in Fig. 2, RF was the
frontrunner for four of the 12 assay datasets, including
NR-AR-LBD, SR-ARE, SR-HSE, and SR-MMP. RF was
the second best performer for another five assays (i.e.,
NR-AR, NR-ER, NR-ER-LBD, NR-PPAR-y, and SR-p53).
The average F, score of RF for all 12 assays was the high-
est (0.2783) among all six algorithms, and the runner-up
was MLP with an average F, score of 0.2487. Clearly, RF
outperformed the other five algorithms on the Tox21
dataset, which informed our decision to proceed with
choosing RF as the base classifier and to focus our study
on imbalance handling methods.

Furthermore, the RF classifier was widely used by the
participating teams in the Tox21 Data Challenge [28,
48]. Two of the winning teams developed RF models
that achieved the best performance in predicting com-
pound activities against AR, aromatase, and p53 [58] as
well as ER-LBD [59]. Using the same RF classifier and the
same dataset made it convenient to compare our results
with those from the participating teams and allowed us
to better investigate the impact of resampling methods
on improving imbalanced learning and, consequently,
improving classification performance (see “Comparison
with Tox21 Data Challenge winners” section below for
more info).

Parameter optimization for the RF classifier

It is generally accepted that the accuracy of a classifier
ensemble is positively correlated with ensemble diver-
sity [60]. Here, we adjusted the ensemble diversity by
randomly selecting data instances to create the boot-
strap samples (see Fig. 1) and by increasing the number
of classifiers included in the ensemble. Figure 3 shows
that the performance of classifier ensembles measured by
the average F, score, AUPRC, AUROC and MCC for all

 

Page 9 of 19

four methods changes with the varying number of clas-
sifiers in the ensemble. A plateau was encountered when
the number of classifiers reached 30, which may have
been the optimal number of classifiers in this situation.
After this point, there was little improvement in per-
formance as the number of classifiers increased. Even if
minor improvements were noticed using 100 classifiers
for some metrics (e.g., MCC), this dramatically increased
the computational time and resources needed to train the
model. The relationship between performance and the
number of classifiers may be explained by the importance
of diversity in ensemble learning. With every bootstrap
sample being different from another in terms of chemi-
cal composition and fingerprint features, diversity in the
bagging ensemble was inherent. However, as the number
of classifiers increased, the number of times (frequency)
that a sample was selected from the same population also
increased. This would result in a decline in the variance
between such bootstrap samples or a flat line in ensem-
ble diversity. Consequently, a flat line was observed in
performance metrics as the number of classifiers in an
ensemble increased from 30 to 100 (Fig. 3). In the subse-
quent experiments, we adopted the optimal number of 30
classifiers for ensemble learning.

Optimal number of nearest neighbors (k) in the ENN
algorithm of SMN models

Another parameter we optimized was the k value in the
ENN algorithm. The choice of a synthetic instance to
be removed from the training set is determined by the
voting of its k neighbors. As shown in Fig. 4, we varied
the number of nearest neighbors k from 1 to 5, and 3
appeared to be the optimal k value for most of the five
measured performance metrics. F, score and AUPRC
peaked at k=3, BA plateaued when k=3 or 4, whereas
MCC peaked earlier at k=2. AUROC was the only met-
ric not affected by the change in k value. Thus, the k value
was set at 3 for SMN in this study.

By setting k at this optimal value, ENN may help
increase the classifier’s generalizability by removing
noisy (mislabeled) synthetic instances introduced in the
SMOTE step. By reducing the amount of noise in the
dataset while reducing imbalance, it is expected that the
class boundaries between active and inactive compounds
can be better defined. A reduction in noisy instances can
also reduce the chance of over-fitting. This is essentially
where the power of SMN lies. However, further incre-
ments in the k value beyond the optimum led to a decline
in classifier performance.

Performance evaluation metrics
Table 2 reports nine performance metrics and their
average values for four classification methods (RF, RUS,
Page 10 of 19

(2020) 12:66

Idakwo et al. J Cheminform

 

sl CLEEO VLOEO LZECTO L6VEO LEve'O LOLCTO cOSEO OZ9€°0 VELEO LCEEO €8S7°0 9SLV0 cOEv0 NWS
CC ~(O0LVO =—VL8E0 LOVED 9VLVO vcoeO 188e0 8elv0 L90V0 SOC¢V0 6ZEEO  =1L8cE0 6€Z9°0 c9cv0 OWS
EL voseO 960 coseO LZ8v0 6c8e0 SEZ O vOCE'O 667v0 €6ZE0 veLeO vOlEO wvZ8E°0 Lovv0 SNd
Vi L96€0 OL8E0 SLCEO LSCV0 LV6EO SCLEO L96¢0 c9lvO €88E0 L66€0 vOved SCVS0 LL8€°0 dd (Sq) 8109s Jag
tv S96V0 LVOED ©96Z0 67SE0 90/70 S6c8°0 c6OZE 0 O00v0 LCE90 S98V0 = LZSZ0 OSLO E€EE0 NWS
vS 87c9l0 682400 £9670 Y9LLL0 LZvlLO ©SvOTO VeOLO 000CO0 6r”r7c0 L8SOL0 ZS8c0 0000'0 L99L'0 OWS
ve €Z£590 LLC60 8rL8'0 LZV9‘O Lv9L0 ZLCLL0 ZLSS°0 00SS°0 6€69°0 89520 €vlL6é0 00SZC°0 00SC0 SMa
SL LL9L0 68200 vwrvO 8 S9L1L0 LZvL0O O0SC0 Sve00 OOSL'O ZE8LO LSELO =98CE0 0000'0 c€800 dd AUAUSURS JO ||2994Y
vy veEle0 SVrsv0 voovVO—- 8 L810 ©9670 8ZS°0 vLOV 0 cEEEO 67850 v6ecsSoO = SLZLV'0 000 1°0 6ZEL 0 NWS
9€ LvSS0O 000970 vilZS0 000S°0 eVlZO0 evlSO 0009°0 00050 OOSZ‘0 0008'0 = L909°0 0000'0 000S°0 OWS
v9 60610 v9r7l'0 e8Sv0 92500 cOlLlLO 69870 O0CEO LLLLO vO9L'O cOeLO ©1660 OSLO 69Z0°0 SMa
8v vo9so0 000S0 0009°0 98770 €€Esvo Y9L1S0 00SC0 000990 76cS0 €€E80 68E9°0 0000'0 0000°L dd UOIS|DAq
cl 86LZ0 C1890 CSV80 =6cS9°0 8lL0Z0 StvS8°0 €SZ9°0 85890 Cc6L0 S9TLO0 = 87é78°0 vvSS 0 €vv9'0 NWS
£ 192450 LZESO vSE90 LZSS0 8lLZSO €e8S0 6675 0 v96S0  =L8L90 O€SSO vO0e90 C867 0 SL8S°0 OWS
LL S80Z0 ZLLLZZ0 €cS8°0 S999°0 €ELZO L690 vSvZ0 896990 ©1590 8c3s90 §=6©6cl80.—s HTL O"O 6c6S 0 SNd
OL 9LZS50 8950 eSOZ0 = Lv8S0 9CLZS0 91090 9VLS‘0 CELSO = OE8SO S99S0~—s- 8 LS9'0 L660 L1VS0 dd (Vg) AdeundDe paoueleg
09 96€EO0 PSCEO VECSO VELLO CCvVCO0 =Ebvb9"0 9ELE0 8lL0cO 889S9°0 Sv8E0 09950 6€90°0 58900 NWS
9€ ELCEO E1670 8c6v0 €66L°0 OSEeO csorv0 90870 89970 S68t0 vOSEO [9050 L@80°0 06C7E0 OWS
6S C9LTO S6CCO ZECSO =CC90'0 ecveO0 = OvlvO £90S°0 SVSLO OVO evOCO 9870 890L°0 vvvl 0 SNd
LS €e6c0 188l0 ce9sg'd 80910 L88TO =vccv0 OCLL‘O L88lL0 €0CE0 SC8TO =978S°0 S9S0°0 LcSE0 dd Judnv
8 v8sEso S8Z8'0 76260 €8178°0 ©6080 OL68°0 elLZZ0 €€c8'0 + 8z98'0 00S8°0 9616°0 69620 01890 NWS
9 69080 OLS80 c6880 €86Z0 LSL8°0) = vLZZ0 CL8L0 CLO80 96CL0 CLI8O —-9EHB"N 60520 O8ZZ0 OWS
OL 6c6Z0 89180 S6c60 = =S90Z'0 L6ZZ0 ©8690 LE6L0 61920 vZlZL0 L£C9L0 = CS880-— EE L6'O 58290 Sd
OL €l18Z0 evrZ0 volod OL9Z0 LéE8Z0 =—L98L0 0v99'0 ©9690 10920 9SEZ0 = 9060 €96Z0 CECB 4d JOUNV
Cv S880 CL8E°0 COvS0 9C7CTO L9ceO LLLIO LE9E0 vove’d EVv9s'°0 LLZv'O §=6cvES'O S600 98810 NWS
6€ 62970 61070 C99ED = LCETO L60€0 86770 SSETO 8lOEO O066E0 C6LZTO ~=—- 6990 LZ000—- S08c0 OWS
eS 89SC0 69270 LESSO 06110 6v0TO0 05670 OL8E°0 80610 9L8l0 vVl6lLO 60¢v0 cO9L*O 9SOL'0 SNd
6y Lv9TO  LO8L0O LOZvO 6L97°0 LLEEO OLLTO L9L00 L68C0 9CLCT0O cOcEO = LOL OSO00—- 6&82°0 dd DOW
cv LLtvO Sccr'o OS8S0 O0V”C0 9€9€°0 1629°0 6C6E 0 9€9€°0 8209°0 040S°0 998S°0 LLLLO LS6L‘O NWS
Ly leveO S6El0 CO6EO SO0610 6€vc0 LC6C0 S9ZL0 £5870 C69E0 SO6L0 =EB8ED 0000'0 00SZz°0 OWS
eS Sl8cO L¢éSCTO L985°0 85010 c90C0 = =S8lv0 LSOv'O 6v8l0 S09C0 ccCCO = LOSV'O——s LY LO 9ZLL0 SNd
09 L6ETO VIELO 90150 O00Sc°0 O0STO O6SEEO 9090'0 O0vT0 = LCLCTO 9CETO = OVE 0000'0 8ESl0 dd 1095 '4
(%)eAD Ue®aW €Sd-YS dWW-YS JASH-YS SGVIV-US JYV-US A-YWdd-YN G@1-Y3-YN YF-YN ase}eWOJY-YN YYV-YN CETHV-UN YV-YN Jaylsse/D SILO

 

sjyasezep Aesse ST Hb LZxO] aAjaM} YUM (NINS PUE OWS ‘SNY ‘4y) SPOYJEW UO!}EdYISSe]> NOY Jo S2URWIOJJad BY} HuljeNjerd 10j SJJEW SUIN Z aIqeL
Idakwo et al. J Cheminform (2020) 12:66 Page 11 of 19

Table 3 Correlation coefficients (CCs) between log,IR

 

 

 

 

 

 

 

gS $
< . : and six performance metrics plus the average of nine
or B® ARRAS 8 2 metrics in Table 2 for all four classification algorithms
J) To
- £ . .
c Fm Sxetesgeis % Metrics Algorithms
© — \o NM = ¥ wo
YVlmH NG JRA SME =
ZF ce Ss SSO OS Og S RF RUS SMO SMN
g >
on N T,) > Oo
N mM OR’ m wo] ow
a2 eo > 70 FH GIG r= F, score — 0.7217 — 0.7394 — 0.6941 — 0.9817
—_ —_ ~N WwW OO oO ]
enV nner Mai AZ D
M1 S OFC COCO AOC Os v MCC — 0.5778 — 0.6180 — 0.6419 — 0.9761
a w
a v o BA — 0.6539 — 0.6274 — 0.6227 — 0.9461
SnrnQ@=naxtron SIS o
Srn n © RXNKKSowOi ez a AUPRC — 0.7034 — 0.7148 — 0.8418 — 0.9628
xe on eOow eaonxtxat®ig o
wpCo FP fF FFF eC Ole S AUROC — 0.277 — 0.1589 — 0.3713 — 0.7417
U WY
Mio Ba2onPn wo F as SSG 0.7158 0.7072 0.7006 0.9195
< — ~ °o n
er on oH SH 9 ~ SF 2 Average — 0.6536 — 0.8421 — 0.7725 — 0.9822
Ln 8 = 7 Insignificant CCs are highlighted in bold and are those whose absolute values
0 N N gS S 3 are smaller than 0.5760, the critical value at a=0.05 significance level for the
Fic an & ASN SE YG wv degree of freedom df= 10 (i.e., n—2, where n= 12 assays)
se S82fgSRRas see =£
” So oO OD oO CO oO CO oO | eg oa
73 & S
Wu ° tos g
Cian &xvL 6g TATA = 5 . Lo .
<isfGnrurasSal se 8 SMO and SMN) for 12 bioactivity assays, with the best
ein - Rn ONAN HNh BL 5
. . . . oOo ° ° ° °
MPO SO POOF OES 5 performer highlighted in bold for each evaluation met-
ov . . . .
= oO g ric and assay. The derived specificity results are reported
= 52 g
Q@l- FsooopGunalse > alone with sensitivity and SSG results in Additional
* SS SARRRS BE e
cao 9 enon gi se u file 1: Table S1. For each assay, the training dataset was
£3 5 ;
a 3 § 2 employed to train a classifier using four different algo-
q 63 ¥ rithms, and then the trained classifier was applied to
! N nmi =a . .
mo Om RF LCHRSH Ga v the test dataset to determine performance metrics as
ec bAN SRLARLA EB v »
—_— —_— ° ° a . °
Z cao 6@ SocooOSG 6 #e 2 described in the “Materials and methods” section (also
v s ,; ; ;
enn m nora 2% o see Fig. 1). The reported values varied greatly depending
Lu ©O \O CON —- OO Fa e . . .
xe ©< SXF pnetnkgs lke ° on metrics, assays and algorithms. For instance, AUROC
Zz co GC COD CGO CG CO Oo | ve v .
5 = has the highest values averaged at 0.8049, whereas MCC
wy os $
o y 2 Y has the lowest mean value of 0.2945. This is not surpris-
% £2 e
€ ae a ing as different metrics measure different aspects of
° -/|o8 oO . . :
zi & % &R SSSRQ ES a learning algorithm performance and trained model qual-
\O ao © OR SO 2 f .
SSeS FLISS 53 8 ity [61].
oc + o fo Ss We excluded accuracy (the ratio of correct predictions
Sit © Mm mRo- Oo o w . ip
q<egu gmt 5 a5 o to the total number of chemicals) and specificity from the
ef© oT OO BEM TAT Be . ‘
ZS cS SGOSCoOS 8s, SF metrics panel presented in Table 2 because accuracy may
v5 se 3 . . . .
a oO ~ feR be misleading in evaluating model performance for highly
N ty OWN mM O;RS & a . . . : ‘
rT Sa VST MBO e aN t imbalanced classification [22]. Specifically, a high accu-
XN HWA DOR C Cl1SE 5 . . “1.
eo 6 SC SSSSS RSS SF racy does not translate into a high capability of the pre-
| | 5 Ss oe
< gee of diction model to correctly predict the rare class, whereas
—- BS a => ‘ ‘ ‘ : :
ais & & VH Qo ag 2 52 specificity is less relevant since we are more interested
rox eS Nn S2ev effec + . wy: ; i,
SGie2ee2 82eeusg 8 £ 5 By in the positive class (active minority). However, the nine
> c . . . .
y nas 8 > chosen metrics in the panel are not necessarily the ideal
= of ele ones for evaluating the performance of classification with
v n OZ OZ\e2 5 8Y ir er .
Si. 5 5 Le SS mre & a skewed class distribution. For instance, both AUROC
U Cc THA HEH H YH & e | 9 t
o 28s 8 and AUPRC can provide a model-wide evaluation of
= 7 QOL ;
3 3) £5 go ll binary classifiers [27]. Although AUROC, proposed as an
> 3s5& 5S v9 ;
c G ov ege2e alternative to accuracy [22], is unaffected by data skew-
- = ve 6 > Y . . . 1 oe ge .
€ go o 6 22F ness [62], it may provide an excessively optimistic view
oO. o = . . .
Y ia 3 x eR sy of an algorithm’s performance on highly imbalanced
N & U = Oo = rio] wo °
a 2 2% % 5 2 i 2 data [21]. AUPRC, on the other hand, is affected by data
2 6) 28 © Fe en ss imbalance [62], but it is a more informative and more
© Oo > coVUMN IG
F 2=/ un <x FQe.o2 . 2
Idakwo et al. J Cheminform (2020) 12:66

realistic measure than AUROC for imbalanced classifica-
tion [27]. Another example is precision and recall, both
of which depend on a threshold selected to determine if a
chemical compound is active or inactive. A higher recall
may be obtained by setting a lower threshold (increasing
the number of TP predictions and decreasing the num-
ber of FN predictions), which results in a lower precision
(more FP predictions). On the other hand, raising the
threshold for labeling active chemicals may benefit preci-
sion but hurt recall. Optimizing both precision and recall
occurs with a tradeoff, especially with imbalanced data.
F, score appears to be a balanced trade-off between pre-
cision and recall. Nevertheless, like AUPRC, F, score is
also attenuated by data skewness [62]. SSG, a good indi-
cator of balance between sensitivity and specificity [13],
may become an inefficient performance metric when
both sensitivity and specificity are low. For such appli-
cations as predictive toxicology and drug discovery, one
may be more interested in improving sensitivity instead
of reducing SSG due to the rarity of positive instances.
Given the pros and cons of these metrics, it is neces-
sary to use a suite of metrics for performance evaluation.
Hence, we calculated the “average” of the nine metrics
(Table 2) which may serve as a comprehensive indicator
of model performance. However, its formula (e.g., mem-
bership composition, weight of each component metric,
and normalization method) and applicability still require
further investigation.

Impact of imbalance ratio on performance metrics

The variation in the same performance metrics between
different assay datasets is as high as 87% CV (Table 2),
suggesting that dataset properties (IR in particular) have
a significant impact. Nevertheless, systematic assess-
ment of the impact of IR on prediction accuracy remains
a challenging problem. The IRs in our assay datasets
varied from 5 to 70 (Table 1). We calculated correlation
coefficients (CCs) between log,(IR) and the score of five
evaluation metrics (Table 3). Except for the CCs between
AUROC and RF/RUS/SMO, there exists a significant
negative correlation between IR (of the test datasets) and
the performance evaluation metrics F,; score, MCC, BA,
AUPRC, AUROC, and the average of all 9 chosen met-
rics. This is consistent with earlier reports on the adverse
effects of IR on these metrics [62]. The statistically sig-
nificant positive correlation between IR and SSG sug-
gests that higher IRs would increase SSG, which is also
undesirable.

To investigate how IR affects the extent of performance
improvement obtained by different resampling tech-
niques, the scores of four metrics (F, score, MCC, SSG
and the average of 9 metrics) of all twelve assays are plot-
ted against their log,IR (see Fig. 5). For MCC, F, score

Page 12 of 19

and the average of 9 metrics, the trend line of SMN is well
above those of SMO, RUS and RF, indicating that SMN
performed better than other classifiers. The trend lines
of SMO and RUS intertwine with that of RF, suggesting
that both SMO and RUS did not consistently improve
the performance metrics over the base classifier RF. In
addition, the SMN trend line intercepts with the other
three at about log,IR=4.8 (for average), 5.5 (for MCC)
or 6.1 (for F, score), suggesting that a metric-specific IR
between 28 and 70 is likely the threshold at which SMN
can outperform other classifiers. The lower the IR value
is, the more improvements SMN can achieve, compared
to the RF, RUS and SMO classifiers. When IR approaches
the threshold, the improvements are insignificant. These
results demonstrate the limitation of data rebalancing
techniques and also provide useful feedback for data
acquisition. If evaluated by the SSG metric (the smaller,
the better), RUS outperformed SMN and the other two
algorithms, suggesting that SMN had limited power in
narrowing the gap between sensitivity and specificity.
Whenever possible, we should increase the number of
active compounds to reduce the imbalance ratio in order
to obtain more accurate predictions in SAR-based chemi-
cal classification.

Impact of resampling techniques on classifier performance
The effect of algorithm choice is partially reflected by a
change of 0.1263 in the average metrics score from RF
(0.1854) to SMN (0.3116) (Table 2). We also calculated
the average Friedman ranking of each classifier [55] by
ranking the four algorithms from 1 to 4 based on their
performance on each assay dataset. The best classifiers
were assigned a rank of 1 and the worst classifiers were
assigned a rank of 4. The algorithm with the lowest aver-
age rank is considered the best for a specific metric. As
shown in Fig. 6, SMN outperformed the other algorithms
(RF, RUS and SMO) in terms of four metrics (F, score,
AUPRC, AUROC and MCC) and was only slightly sur-
passed by the frontrunner RUS for the BA metric. Taking
F, score as an example, SMN performed better in seven
of the 12 assay datasets, followed by RUS which was the
best performer for three assays (Table 2). More interest-
ingly, the magnitude of improvement offered by SMN
from the next best method ranged from approximately
8% for the NR-ER-LBD dataset to as much as 27% for the
SR-ARE and NR-Aromatase datasets. Understandably,
the baseline classifier RF had the worst average perfor-
mance even though its parameters were also optimized.
SMN demonstrated a better F, score in most cases
because of its ability to improve recall without excessively
lowering precision. A moderately higher recall value with
comparable precision positively impacts the F, score.
Idakwo et al. J Cheminform (2020) 12:66 Page 13 of 19

 

@~_R? = 0.9637
a

8
8
= OS

 

Log,IR

@RF @RUS @SMO @SMN

 

__ |R? =0.9528
eo

SS
= se
es”

o™
s

se ae =
o eR’ = 0.412 Le
—_— em
Sse
R? = 0.3338

R? = 0.3818

6.0

 

 

Log,IR

@RF @RUS @SMO @SMN

 

R? =0.5124

 

Log,IR

@RF @RUS @SMO @SMN

 

-_
oF -0:9648

Average of 9 metrics

©  R*=0.7091
R=0.4273 eS

—

 

 

Log, IR

@RF @RUS @SMO @SMN

 

Fig. 5 The relationship between imbalance ratio (Log IR) and prediction performance metrics calculated for four classification methods (SMN, SMO,
RUS and RF): a F, score, b MCC, ¢ SSG, and d the average of 9 metrics

 

X
Idakwo et al. J Cheminform (2020) 12:66

Page 14 of 19

 

3.5

2.

uw

Rank
NO

1.

Fe WM

0.

uw

 

F1-Score AUPRC

 

X

AUROC

  

BA

MCC

Perfrormance Metrics

m@RF @RUS BSMO mSMN

Fig. 6 Average Friedman ranks of the four classification methods (RF, RUS, SMO and SMN) with respect to five metrics (F, score, AUPRC, AUROC,
MCC and BA). Error bars represent standard errors. See Table 4 for statistical significance in the difference between classifiers

 

 

Table 4 Friedman's aligned rank test and Bergmann-Hommel post hoc analysis results showing corrected p-values
for multiple and pair-wise comparisons between SMN and the other three classifiers, respectively

 

 

Comparisons F, score AUPRC AUROC MCC BA Precision Recall Brier score SSG

All four classifiers 0.0005 0.1322 0.0462 0.0111 5.4e—06 9.0e—05 1.8e—06 0.0017 2.0e—06
SMN vs RF 0.0003 0.5253 0.0168 0.0088 0.0001 0.0278 0.0013 0.0009 0.0010
SMN vs RUS 0.0051 0.1008 0.0504 0.0062 1.0000 0.0948 0.2307 0.0022 0.0274
SMN vs SMO 0.0003 0.7818 0.3320 0.0088 0.0001 0.0278 0.013 0.0007 84e—04

 

Insignificant statistics (p > 0.05) are highlighted in bold

The Friedman's Aligned Rank Test for Multiple Com-
parisons [55] was performed to further examine the
statistical significance of the algorithmic effects of resa-
mpling techniques. Our null hypothesis was that all four
algorithms had similar capability in classification meas-
ured by nine metrics for 12 datasets. Results shown in
Table 4 suggest that all metrics except AUPRC were sig-
nificantly affected by the resampling algorithm (p <0.05).
The Bergmann-Hommel post hoc analysis was applied to
compare pairwise performance metrics of SMN against
the other three classifiers. SMN differed more from RF
than from SMO and RUS because one, two, and five met-
rics were insignificantly different (yp > 0.05) between SMN
and RF, SMN and SMO, and SMN and RUS, respectively.
F, score, MCC and Brier score showed significant dif-
ference among the four classifiers in both multiple and
pair-wise comparisons. For instance, SMN had the lowest
average Brier score of 0.3312+0.0509 (average + stand-
ard error) in comparison with SMO (0.4109 +0.0627),
RUS (0.3894+0.0361), and the baseline classifier RF
(0.3967 + 0.0395). A lower Brier score indicates that the
predictions of a classifier are more accurate because they

are closer to the ground truth. MCC, a metric widely
used to evaluate the performance of SAR-based chemi-
cal classification [63, 64], embodies all the components of
the confusion matrix and hence presents a reliable sum-
mary of the performance of models trained on imbal-
anced data.

On the contrary, AUPRC was the sole metric that did
not differ significantly in any of the comparisons. AUPRC
computes the area under the precision-recall curve that is
obtained by using the output of the precision function at
different recall levels to assess the overall performance of
a prediction model [51]. SMN showed improved AUPRC
scores compared to the other algorithms. However, this
improvement was not very substantial. Unlike F, score,
which benefits from a varied classification threshold,
minor improvements in the probabilities for each class
do not translate to a marked improvement in the AUPRC
score. This is because, being a threshold-independent
metric, AUPRC computes the entire area under the
curve for the plot of precision versus recall at all possi-
ble thresholds. Nevertheless, SMN still showed the best
Idakwo et al. J Cheminform (2020) 12:66

performance in 33% (4/12) of cases tested, RF and SMO
in 25% (3/12) each, and RUS in 16% (2/12).

The above results suggest that AUPRC is not sensitive
to algorithmic effects, whereas F, score, MCC and Brier
score are sensitive metrics that can distinguish among
the classifiers by their performance. These results also
indicate that SMN was the best performer, followed by
RUS, while SMO and RF had the poorest performance
with the Tox21 datasets. When looking at the average
of all 9 metrics (Table 2), SMN and RUS ranked the best
for 6 and 5 assays, separately, whereas RF only had the
best performance with the NR-AR assay and SMO always
underperformed across all 12 assays. These results led
us to speculate that the activity landscape of the major-
ity class (inactive compounds) may be more continuous
and smooth than that of the minority class (active com-
pounds) [65]. Consequently, removing some instances
from the majority class would not affect class boundaries.
On the contrary, adding synthetic instances to the minor-
ity class (SMOTE) may introduce noise along the border-
lines, leading to the loss of activity cliffs and mislabeling
of the synthetic instances [66]. The ENN algorithm may
effectively remove those synthetic outliers and restore the
activity cliffs and class boundaries, leading to enhanced
prediction performance for SMOTEENN (SMN) [67]

Comparison with Tox21 Data Challenge winners

In this section, we compared the prediction performance
of the four classifiers in this study with those developed
by the winning teams for each of the assays in the Tox21
Data Challenge [34]. The winning team for each sub-
challenge was judged by AUROC (and BA if there was a
tie in AUROC [35]). The AUROC and BA scores of the
top ten ranked teams are posted at (https://tripod.nih.
gov/tox21/challenge/leaderboard.jsp). The 12 assay sub-
challenges were won by four teams: Bioinf@JKU, Amaziz,
Dmlab and Microsomes. Bioinf@JKU developed Deep-
Tox models using deep learning [25] and won six out
of the 12 assay sub-challenges (NR-AhR, NR-AR-LBD,
NR-ER, NR-PPAR-y, SR-ARE, and SR-HSE) in addition to
the Grand Challenge and two additional sub-challenges
for the Nuclear Receptor Panel and the Stress Response
Panel. Amaziz [68] employed associative neural networks
to develop winning models for SR-ATAD5 and SR-MMP
assays, and had the best overall BA score. Dmlab [58]
used multi-tree ensemble methods, such as Random
Forests and Extra Trees, to produce winning models for
three assays (i.e., NR-AR, NR-aromatase and SR-p53).
Microsomes [59] chose Random Forest for descriptor
selection and model generation, and produced the best
performing NR-ER-LBD model. For the purpose of com-
parison, we selected Dmlab and Microsomes because
they used Random Forest. We also compared our best

Page 15 of 19

classifier with the winner of each assay sub-challenge.
Given the over-optimistic nature of AUROC, the BA
metric provides a more realistic and reliable measure for
performance comparison. The titles of the best BA scores
were shared by five teams: Kibutz (1 assay), Bioinf@JKU
(2), Amaziz (2), T (3), and StructuralBioinformatics@
Charite (4). The AUROC and BA scores of the winning
teams are shown in Table 5 side by side with those of our
best performing classifiers because they are the only met-
rics available for the Tox21 Data Challenge.

Although the AUROC and BA metrics are not ideal for
evaluating imbalanced classification, we made the com-
parison to demonstrate that the improvement obtained
from imbalance pre-processing enabled our classifiers to
perform equally well or outperform the winning models
of the Tox21 Data Challenge. This is primarily reflected
by the following observations: (1) our best classifiers
outperformed Dmlab and Microsomes in terms of both
AUROC and BA by large margins with only four excep-
tions (NR-AR, NR-PPAR-y, SR-ATAD5 and SR-MMP),
where Dmlab exceeded our best classifiers in AUROC
by less than 4%; (2) our best classifiers had the same or
higher AUROC and a higher BA than challenge win-
ners for six and three assays, respectively, with less than
8% (AUROC) or 17% (BA) difference for the remaining
assays; and (3) on average, our best classifiers performed
almost equally as well as the challenge winners as a whole
(Table 5). The last two columns in Table 5 report the
comparison between our best classifier and the winner
of Tox21 Challenge in terms of BA and AUROC ratios,
with a value greater than 1 indicating that our model per-
formed better than the Challenge winning model. These
results (particularly the BA scores) not only establish
the validity, credibility and scientific soundness of the
approach, methodology and algorithms implemented in
this study, but also demonstrate that the excellence of our
work reached levels comparable to that of the Tox21 Data
Challenge winners.

It is also worth noting that Banerjee et al. [13] per-
formed similar work on three Tox21 datasets (AhR, ER-
LDB, and HSE). They employed RF as the base classifier
(without ensemble learning) and applied eight different
undersampling or oversampling techniques (including
random undersampling and SMOTE). Similar to this
study, their work also demonstrated that dataset and
resampling techniques had significant impacts on classi-
fication outcome and that such impacts varied from one
metric to another with sensitivity and F-measure being
more sensitive than AUROC and accuracy.

Another study worth mentioning described how
Norinder and Boyer [16] achieved balanced prediction
performance with sensitivity and specificity (for the
external test dataset) both attaining 0.70 — 0.75 when they
Page 16 of 19

(2020) 12:66

Idakwo et al. J Cheminform

[pe] swes} bunediiyed abuajeyd eyeq LZXO] ay} Huowe jsaq ay} ae UOJ Pjog Ul SaNnjeA ay} seasaymM (abuayjeyD e}eq LZXOL pue Apnjs siy} Yy}OQ) ssayisse|> ay} jje HBuowe JsayHiy ay} ae se} Ul SAaNjeA aUL

 

 

 

 

€/60 CO0'L v9L0 685'0 85°0 CvL0 L98'0 OL8'0 O€8'0 C980 abelary
ZL0'L 866'0 S9Z°0 EcS'0 85°0 SNY 8ZL0 088'0 978'0 088°0 NWS 6280 EGC-YS
Cv6'0 8260 706'0 Y/N 690 SNY CS8'0 0S60 Y/N 0S60 SNY 0€6'0 dWIW-YS
SEs‘ 086'0 6620 Y/N 095'0 SNY 2990 S980 Y/N 0980 NWS 8780 ASH-US
€96'0 860 LyZ0 6€5°0 OL9'0 SNY ELLO 8780 CL8‘0 0080 OWS SL8‘0 SCVLV-YS
EZL'L L90'L 6¢L0 S09°0 0¢5'0 NWS 5580 Ovs'0 708'0 OLL0 NWS 1680 JYV-US
6760 CC6'0 S8Z°0 Y/N 0SS'0 SNY SvL0 L98'0 8LZ0 O€8'0 sale v6L0 A-YVdd-Y
S60 S660 SLZO 0SS'0 065'0 Snd £690 L780 L730 OLL0 NWS E80 Gdds-d
ZSO0'L S90'L 67L0 L790 099'0 NWS C6L0 OL8'0 €8Z0 OLL0 NWS £980 Y4-y
986'0 LOL ZELO Y/N 095°0 NWS LCLO 8E8'0 Y/N 0vs'0 NWS 0580 ase]eWOly-Y
S960 L660 €S8°0 869'0 095°0 NWS €c8'0 860 LO6'0 0820 NWS 0260 YUV-Y
Cv6'0 6€0'L 0S9'0 Y/N 06r'0 SNY C190 6280 Y/N 0c8'0 SNY £160 Gd uv-d
SZ8°0 660 9€L0 Y/N OL9'0 NWS vv9'0 8780 Y/N ocs'0 du Ec80 YV-d
vd DOYUNV owen anjeA oweN anjeA
JOUUIM (Apnjs JOUUIM (Apnjs
abuayjeyd [6S] SowososDI/W [gs] qejwq = Sl4}) JaYIsse]D ysag ebualjeyd [6S] SOWOSOIDI/\ [ss] qejwq = sI4}) JayIsse|> ysag
JaUuUIM abuayjey>
/ AayISSe|D Jsag (vq) Adeund3e paduejeg ,0YNNV q| Aessy

 

A>dean>>2e paduejeq pue DOYUNY S24}eW SUeWOJJad UOILIYISSE]D JU} JO SU} UI SADUUIM aHuayljeyD e}eg LZXOL pue Apnjs siy} UdaMjyeq UOSIIedWOD ¢ ajqeL
Idakwo et al. J Cheminform (2020) 12:66

applied MCP to the similar ToxCast and Tox21 datasets
of estrogen receptor assays and used SVM as the classi-
fier. These results are far superior to those obtained using
SVM or RF alone without resampling or MCP [16, 69],
but they are only slightly better than the performance of
RUS with sensitivity at 0.69 or 0.55 (Table 2) and specific-
ity at 0.61 or 0.84 (Additional file 1: Table S1) obtained in
our study. Therefore, it warrants further in-depth inves-
tigations to compare side-by-side resampling with MCP
and MCP + resampling using the same machine learning
algorithms, the same raw datasets, and the same preproc-
essing procedure.

Conclusions
Due to the specificity of toxicant-target biomolecule
interactions, SAR-based chemical classification stud-
ies are often impeded by the imbalanced nature of many
toxicity datasets. Furthermore, class boundaries are often
blurred since active toxicants often appear in the minor-
ity class. In order to address these issues, common resa-
mpling techniques can be applied. However, removing
majority class instances using an undersampling tech-
nique can result in information loss, whereas increas-
ing minority instances by interpolation tends to further
obfuscate the majority class space, giving rise to over-fit-
ting. In order to improve the prediction accuracy attained
from imbalanced learning, SMOTEENN, a combination
of SMOTE and ENN algorithms, is often employed to
oversample the minority class by creating synthetic sam-
ples, followed by cleaning the mislabeled instances. Here,
we integrated an ensemble approach (bagging) with a
base classifier (RF) and various resampling techniques to
form four learning algorithms (RF, RUS, SMO and SMN).
Then, we applied them to the binary classification of 12
highly imbalanced Tox21 in vitro qHTS bioassay datasets.
We generated multiple sets of chemical descriptors or
fingerprints and down-selected small groups of features
for use in class prediction model generation. After data
preprocessing, parameters were optimized for both resa-
mpling and classifier training. The performance of the
four learning methods was compared using nine evalua-
tion metrics, among which F, score, MCC and Brier score
provided more consistent assessment of the overall per-
formance across the 12 datasets. The Friedman’s aligned
ranks test and the subsequent Bergmann-Hommel post
hoc test showed that SMN significantly outperformed the
other three methods. It was also found that there was a
strong negative correlation between prediction accuracy
and IR. We observed that SMN became less effective
when IR exceeded a certain threshold (e.g.,>28). There-
fore, SAR-based imbalanced learning can be affected by
the degree of dataset skewness, resampling algorithms,
and evaluation metrics. We recommend assembling a

Page 17 of 19

panel of representative, diversified and imbalance-sensi-
tive metrics, developing a comprehensive index from this
panel, and using the index to evaluate the performance of
classifiers for imbalanced datasets.

The ability to separate the small number of active
compounds from the vast amounts of inactive ones is
of great importance in computational toxicology. This
work demonstrates that the performance of SAR-based,
imbalanced chemical toxicity classification can be signifi-
cantly improved through imbalance handling. Although
the best classifiers of this study achieved the same level
of performance as the winners of the Tox21 Data Chal-
lenge as a whole, we believe that there is still plenty of
room for further improvement. Given the exceptionally
outstanding performance of DeepTox [25] and our own
experience with deep learning-based chemical toxicity
classification [70], our future plan is to replace RF with
a deep learning algorithm like deep neural networks as
the base classifier and combine it with class rebalancing
techniques to build novel deep learning models for SAR-
based chemical toxicity prediction. We are also interested
in pursuing a novel approach by integrating MCP, resa-
mpling and ensemble strategies to further improve the
robustness and performance of imbalanced learning.

Supplementary information

Supplementary information accompanies this paper at https://doi.
org/10.1186/s13321-020-00468-x.

 

(— >)

Additional file 1: Text $1. SMOTEENN algorithm. Figure $1. Illustra-

tion of SMOTE and ENN techniques. (a) The original imbalanced data; (b)
Synthetic samples are generated for the minority class using SMOTE. (c)
Using ENN, those mislabeled synthetic samples were removed from the
minority class. (d) The rebalanced data after the application of SMOTEENN.
Table $1. Evaluation metrics derived for four classification methods (RF,

RUS, SMO and SMN) with twelve Tox21 qHTS assay datasets. Specificity

and two other metrics (sensitivity and SSG, both appearing in Table 2) are
shown.

Ne /

 

 

Abbreviations

ANOVA: Analysis of variance; AUPRC: Area Under the Precision-Recall Curve;
AUROC: Area Under the Receiver Operating Characteristics; BA: Balanced
accuracy; CART: Classification and Regression Trees (Decision trees); CC: Cor-
relation coefficient; CV: Coefficient of variation; df: Degree of freedom; ENN:
Edited Nearest Neighbor (ENN) algorithm; FN: False negative; FP: False posi-
tive; IR: Imbalance ratio; KNN: K-nearest neighbors; MCC: Matthews correlation
coefficient; MLP: Multilayer perceptron; NB: Naive Bayes; RF: Random Forest
classification method; RUS: Random Undersampling; SAR: Structure—Activity
Relationship; SMN: RF classification method with SMOTEENN technique; SMO:
RF classification method with SMOTE technique; SMOTE: Synthetic Minority
Over-sampling Technique (SMOTE); SMOTEENN: Combined the SMOTE tech-
nique with the ENN algorithm; SVM: Support vector machine; TN: True nega-
tive; TNR: True negative rate; Tox21: Toxicology in the 21st Century program;
TP: True positive; TPR: True positive rate.

Acknowledgements

Permission was granted by the Chief of Engineers, U.S. Army Corps of
Engineers to publish this paper. This research was supported in part by an
appointment to the Research Participation Program at the U.S. Army Corps
of Engineers-Engineer Research and Development Center-Environmental
Idakwo et al. J Cheminform (2020) 12:66

Laboratory (USACE-ERDC-EL), administered by the Oak Ridge Institute for Sci-
ence and Education (ORISE) through an interagency agreement between the
U.S. Department of Energy and USACE-ERDC-EL.

Disclaimer

The content is solely the responsibility of the authors and does not necessarily
represent the official views of U.S. Army Corps of Engineers or U.S. Food and
Drug Administration.

Authors’ contributions

PG and CZ conceived and supervised the study. GI, CZ and PG designed the
study plan. Gl was also responsible for implementing the algorithms. ST and
YL undertook chemical descriptors generation and bioassay data curation.
NW, ZZ, HH and BY provided useful insights for machine learning implementa-
tion. Gl, PG and CZ drafted the manuscript. All authors revised the manuscript.
All authors read and approved the final manuscript.

Funding
PG, ST and YL were supported by the U.S. Army Environmental Quality and
Installations Research Program.

Availability of data and material

The dataset supporting the conclusions of this article is available at https://
tripod.nih.gov/tox21/challenge/datajsp in sdf and smi formats. The source
code of this article is available at https://github.com/Idakwo/SAR_Imbal
ance_SMOTEENN.

Competing interests

The authors declare that the research was conducted in the absence of any
commercial or financial relationships that could be construed as a potential
competing interest.

Author details

' School of Computing Sciences and Computer Engineering, University

of Southern Mississippi, Hattiesburg, MS 39406, USA. * Environmental Labora-
tory, U.S. Army Engineer Research and Development Center, Vicksburg, MS
39180, USA. ? Bennett Aerospace Inc, Cary, NC 27518, USA. * Department

of Computer Science, New Jersey City University, Jersey City, NJ 07305, USA.

> Division of Bioinformatics and Biostatistics, National Centre for Toxicologi-
cal Research, U.S. Food and Drug Administration, Jefferson, AR 72079, USA.

© School of Information & Engineering, Zhengzhou University, Zheng-

zhou 450000, China.

Received: 13 December 2019 Accepted: 13 October 2020
Published online: 27 October 2020

References

1. Czarnecki WM, Rataj K (2015) Compounds activity prediction in large
imbalanced datasets with substructural relations fingerprint and EEM.
2015 IEEE Trustcom/BigDataSE/ISPA. IEEE, Helsinki, pp 192-192

2. — Irwin JJ, Sterling T, Mysinger MM et al (2012) ZINC: a free tool to discover
chemistry for biology. J ChemInf Model 52:1757-1768. https://doi.
org/10.1021/ci3001277

3. Dahl GE, Jaitly N, Salakhutdinov R (2014) Multi-task neural networks for
QSAR predictions. https://arxiv.org/abs/1406.1231. Accessed 6 Oct 2017

4. Darnag R, Mostapha Mazouz EL, Schmitzer A et al (2010) Support vec-
tor machines: development of QSAR models for predicting anti-HIV-1
activity of TIBO derivatives. Eur J Med Chem 45:1590-1597. https://doi.
org/10.1016/j.ejmech.2010.01.002

5. Polishchuk PG, Muratov EN, Artemenko AG et al (2009) Application of
random forest approach to QSAR prediction of aquatic toxicity. J ChemInf
Model 49:2481-2488. https://doi.org/10.1021/ci900203n

6. Galar M, Fernandez A, Barrenechea E et al (2012) A review on ensembles
for the class imbalance problem: bagging-, boosting-, and hybrid-based
approaches. IEEE Trans Syst Man Cybern Part C 42:463-484. https://doi.
org/10.1109/TSMCC.2011.2161285

7. Krawczyk B, Krawczyk BB (2016) Learning from imbalanced data: open
challenges and future directions. Prog Artif Intell 5:221-232. https://doi.
org/10.1007/s13748-016-0094-0

20.

21,

22.

23.

24,

25.

26.

2/.

28.

29.

Page 18 of 19

Hido S, Kashima H, Takahashi Y (2009) Roughly balanced bagging for
imbalanced data. Stat Anal Data Min 2:412-426. https://doi.org/10.1002/
sam.10061

Chawla NV (2005) Data mining for imbalanced datasets: an overview.

In: Maimon O, Rokach L (eds) Data Mining and Knowledge Discovery
Handbook. Springer-Verlag, New York, pp 853-867

He H, Ma Y (2013) Imbalanced learning: foundations, algorithms, and
applications. John Wiley & Sons Inc, New York

. Branco P Torgo L, Ribeiro R (2015) A survey of predictive modelling under

imbalanced distributions. https://arxiv.org/abs/1505.01658. Accessed 8
Aug 2017

. Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002) SMOTE: Synthetic

Minority Over-sampling Technique. J Artif Intell Res 16:321-357. https://
doi.org/10.1613/jair.953

. Banerjee P, Dehnbostel FO, Preissner R (2018) Prediction is a balancing

act: importance of sampling methods to balance sensitivity and specific-
ity of predictive models based on imbalanced chemical data sets. Front
Chem 6:362. https://doi.org/10.3389/fchem.2018.00362

Stefanowski J (2016) Dealing with Data Difficulty Factors While Learning
from Imbalanced Data. Challenges in computational statistics and data
mining. Springer, Cham, Switzerland, pp 333-363

Bosc N, Atkinson F, Felix E et al (2019) Large scale comparison of QSAR
and conformal prediction methods and their applications in drug discov-
ery. J Cheminform 11:4. https://doi.org/10.1186/s13321-018-0325-4
Norinder U, Boyer S (2016) Conformal Prediction Classification of a

Large Data Set of EnRvironmental Chemicals from ToxCast and Tox21
Estrogen Receptor Assays. Chem Res Toxicol 29:1003-1010. https://doi.
org/10.1021/acs.chemrestox.6600037

Sun J, Carlsson L, Ahlberg E et al (2017) Applying mondrian cross-con-
formal prediction to estimate prediction confidence on large imbal-
anced bioactivity data sets. J ChemInf Model 57:1591-1598. https://doi.
org/10.1021/acs.jcim.7b00159

. Cortés-Ciriano |, Bender A (2019) Concepts and applications of conformal

prediction in computational drug discovery

Norinder U, Boyer S (2017) Binary classification of imbalanced datasets
using conformal prediction. J Mol Graph Model 72:256-265. https://doi.
org/10.1016/j,jmgm.2017.01.008

He H, Garcia EA (2009) Learning from Imbalanced Data. IEEE Trans Knowl
Data Eng 21:1263-1284. https://doi.org/10.1109/TKDE.2008.239

Davis J, Goadrich M (2006) The relationship between precision-recall and
ROC curves. In: Proceedings of the 23rd International Conference on
Machine Learning. ACM, Pittsburgh, pp 233-240

Provost F, Fawcett T, Kohavi R (1998) The case against accuracy estimation
for comparing induction algorithms. In: Proceedings of the Fifteenth
International Conference on Machine Learning. Morgan Kaufmann
Publishers Inc, San Francisco, pp 445-453

Capuzzi SJ, Politi R, Isayev O et al (2016) QSAR modeling of Tox21 chal-
lenge stress response and nuclear receptor signaling toxicity assays. Front
Environ Sci 4:3. https://doi.org/10.3389/fenvs.2016.00003

Ribay K, Kim MT, Wang W et al (2016) Predictive modeling of estrogen
receptor binding agents using advanced cheminformatics tools and
massive public data. Front Environ Sci 4:12. https://doi.org/10.3389/fenvs
.2016.00012

Mayr A, Klambauer G, Unterthiner T et al (2016) DeepTox: toxicity
prediction using deep learning. Front Environ Sci 3:1-15. https://doi.
org/10.3389/fenvs.2015.00080

Drwal MN, Siramshetty VB, Banerjee P et al (2015) Molecular similarity-
based predictions of the Tox21 screening outcome. Front Environ Sci
3:54. https://doi.org/10.3389/fenvs.201 5.00054

Saito T, Renmsmeier M, Hood L et al (2015) The precision-recall plot is
more informative than the ROC plot when evaluating binary classifiers
on imbalanced datasets. PLoS ONE 10:e0118432. https://doi.org/10.1371/
journal.pone.01 18432

Chen J, Tang YY, Fang B, Guo C (2012) In silico prediction of toxic

action mechanisms of phenols for imbalanced data with Random

Forest learner. J Mol Graph Model 35:21-27. https://doi.org/10.1016/J.
JMGM.2012.01.002

Pham-The H, Casanola-Martin G, Garrigues T et al (2016) Exploring differ-
ent strategies for imbalanced ADME data problem: case study on Caco-2
permeability modeling. Mol Divers 20:93-109. https://doi.org/10.1007/
$11030-015-9649-4
Idakwo et al. J Cheminform

30.

31.

32.

33.

34.

35.

36.

37.
38.

39.

 

42.

43.

Ad,

 

49.

50.

51.

52.

(2020) 12:66

Lei T, Sun H, Kang Y et al (2017) ADMET evaluation in drug discovery. 18.
Reliable prediction of chemical-induced urinary tract toxicity by boosting
machine learning approaches. Mol Pharm 14:3935-3953. https://doi.
org/10.1021/acs.molpharmaceut.7b00631

Czarnecki WM, Tabor J (2017) Extreme entropy machines: robust informa-
tion theoretic classification. Pattern Anal Appl 20:383-400. https://doi.
org/10.1007/s10044-015-0497-8

Batista GEAPA, Prati RC, Monard MC (2004) A study of the behavior of
several methods for balancing machine learning training data. ACM
SIGKDDExplorNews!l 6:20-29. https://doi.org/10.1145/1007730.1007735
NCATS Toxicology in the 21st Century (Tox21). https://ncats.nih.gov/
tox21. Accessed 11 May 2017

Huang R, Xia M, Nguyen D-T et al (2016) Editorial: Tox21 challenge to
build predictive models of nuclear receptor and stress response path-
ways as mediated by exposure to environmental toxicants and drugs.
Front Environ Sci 5:3. https://doi.org/10.3389/fenvs.201 7.00003

Huang R, Xia M, Nguyen D-T et al (2017) Tox21Challenge to build predic-
tive models of nuclear receptor and stress response pathways as medi-
ated by exposure to environmental chemicals and drugs. Frontiers Media,
Lausanne

MolVS: Molecule Validation and Standardization—MolVS 0.0.9 documen-
tation. https://molvs.readthedocs.io/en/latest/. Accessed 6 Feb 2018
Greg L RDKit: Open-source cheminformatics Software

Tropsha A, Gramatica P Gombar V (2003) The importance of being
Earnest: validation is the absolute essential for successful application and
interpretation of QSPR models. QSAR Comb Sci 22:69-77. https://doi.
org/10.1002/qsar.200390007

Stefaniak F (2015) Prediction of compounds activity in nuclear receptor
signaling and stress pathway assays using machine learning algorithms
and low-dimensional molecular descriptors. Front Environ Sci 3:77. https
//doi.org/10.3389/fenvs.2015.00077

Rogers D, Hahn M (2010) Extended-connectivity fingerprints. J ChemInf
Model 50:742-754. https://doi.org/10.1021/ci100050t

1. Seiffert C, Knoshgoftaar TM, Van Hulse J, Napolitano A (2010) RUSBoost:

a hybrid approach to alleviating class imbalance. IEEE Trans Syst Man,
Cybern Part ASyst Humans 40:185-197. https://doi.org/10.1109/TSMCA
.2009.2029559

Garcia V, Sanchez JS, Mollineda RA (2012) On the effectiveness of
preprocessing methods when dealing with different levels of class imbal-
ance. Knowl Based Syst. https://doi.org/10.1016/j.knosys.2011.06.013
Galar M, Fernandez A, Barrenechea E, Herrera F (2013) EUSBoost: enhanc-
ing ensembles for highly imbalanced data-sets by evolutionary under-
sampling. Pattern Recognit 46:3460-3471. https://doi.org/10.1016/J.
PATCOG.2013.05.006

Wilson DL (1972) Asymptotic Properties of Nearest Neighbor Rules Using
Edited Data. IEEE Trans Syst Man Cybern 3:408-421. doi.:https://doi.
org/10.1109/TSMC.1972.4309137

Breiman L (2001) Random forests. Mach Learn 45:5—32. https://doi.
org/10.1023/A:1010933404324

Han J, Kamber M, Pei J (2011) Data mining : concepts and techniques, 3rd
edn. Elsevier Science, Amsterdam

Altman N, Krzywinski M (2017) Ensemble methods: bagging and random
forests. Nat Methods 14:933—934. https://doi.org/10.1038/nmeth.4438
Khoshgoftaar TM, Van Hulse J, Napolitano A (2011) Comparing boosting
and bagging techniques with noisy and imbalanced data. IEEE Trans Syst
Man Cybern Part A Syst Humans 41:552-568. https://doi.org/10.1109/
TSMCA.2010.2084081

Laszczyski J, Stefanowski J, Idkowiak L (2013) Extending bagging for
imbalanced data. In: Burduk R., Jackowski K., Kurzynski M., Wozniak M.,
Zolnierek A. (eds) Proceedings of the 8th International Conference on
Computer Recognition Systems CORES 2013. Advances in Intelligent
Systems and Computing. Springer, Heidelberg, pp 269-278

Chawla NV, Lazarevic A, Hall LO, Bowyer KW (2003) SMOTEBoost: improv-
ing prediction of the minority class in boosting. Springer, Berlin, Heidel-
berg, pp 107-119

Pedregosa F, Varoquaux G, Gramfort A et al (2011) Scikit-learn: machine
learning in python. J Mach Learn Res 12:2825-2830

Lema‘itre G, Nogueira F, Aridas CK (2017) Imbalanced-learn: a python
toolbox to tackle the curse of imbalanced datasets in machine learning. J
Mach Learn Res 18:1-5

53.

54.

55.

56

57.

58.

59.

60.

61.

62.

63.

64.

65.

66.

67

68.

69.

70.

Page 19 of 19

Boughorbel S, Jarray F, El-Anbari M (2017) Optimal classifier for imbal-
anced data using Matthews Correlation Coefficient metric. PLoS ONE
12:e0177678. https://doi.org/10.1371/journal.pone.01 77678

Bergmann B, Hommel G (1988) Improvements of general multiple test
procedures for redundant systems of hypotheses. Springer, Berlin, Heidel-
berg, pp 100-115

Garcia S, Fernandez A, Luengo J, Herrera F (2010) Advanced nonparamet-
ric tests for multiple comparisons in the design of experiments in compu-
tational intelligence and data mining: experimental analysis of power.
InfSci (Ny) 180:2044—2064. https://doi.org/10.1016/J.INS.2009.12.010
Calvo B, Santafé G (2016) scmamp: Statistical comparison of multiple
algorithms in multiple problems. R J 8:248-256. https://doi.org/10.32614/
rj-2016-017

Hodges JL, Lehmann EL (2012) Rank methods for combination of
independent experiments in analysis of variance. In: Rojo J (ed) Selected
works of EL. Lehmann. Springer US, Boston, MA, pp 403-418

Barta G (2016) Identifying biological pathway interrupting toxins using
multi-tree ensembles. Front Environ Sci. https://doi.org/10.3389/fenvs
.2016.00052

Uesawa Y (2016) Rigorous selection of random forest models for identify-
ing compounds that activate toxicity-related pathways. Front Environ Sci
4:9. https://doi.org/10.3389/fenvs.2016.00009

Kuncheva LI, Whitaker CJ (2003) Measures of diversity in classifier ensem-
bles and their relationship with the ensemble accuracy. Mach Learn
51:181-207

Ferri C, Hernandez-Orallo J, Modroiu R (2009) An experimental compari-
son of performance measures for classification. Pattern Recognit Lett
30:27-38. https://doi.org/10.1016/J.PATREC.2008.08.010

Jeni LA, Cohn JF, De La Torre F (2013) Facing imbalanced data—recom-
mendations for the use of performance metrics. In: 2013 Humaine Asso-
ciation Conference on Affective Computing and Intelligent Interaction.
IEEE, New York, pp 245-251

Tong W, Hong H, Fang H et al (2003) Decision forest: combining the
predictions of multiple independent decision tree models. J ChemInf-
ComputSci 43:525-531. https://doi.org/10.1021/ci020058s

Sakkiah S, Selvaraj C, Gong P et al (2017) Development of estrogen
receptor beta binding prediction model using large sets of chemicals.
Oncotarget 8:92989-93000. https://doi.org/10.18632/oncotarget.21723
Cruz-Monteagudo M, Medina-Franco JL, Pé Rez-Castillo Y et al (2014)
Activity cliffs in drug discovery: Dr Jekyll or Mr Hyde? Drug Discov Today
19:1069-1080. https://doi.org/10.1016/j.drudis.2014.02.003

Stumpfe D, Hu H, Bajorath J (2019) Evolving concept of activity cliffs. ACS
Omega 4:14360

Yang Z, Gao D (2013) Classification for imbalanced and overlapping
classes using outlier detection and sampling techniques. NSP Natural
Sciences Publishing, New York

Abdelaziz A, Soahn-Langguth H, Schramm K-W, Tetko IV (2016) Consen-
sus modeling for HTS assays using in silico descriptors calculates the best
balanced accuracy in Tox21 challenge. Front Environ Sci 4:2. https://doi.
org/10.3389/fenvs.2016.00002

Zang Q, Rotroff DM, Judson RS (2013) Binary classification of a large
collection of environmental chemicals from estrogen receptor assays by
quantitative structure-activity relationship and machine learning meth-
ods. J Chem Inf Model 53:3244—3261. https://doi.org/10.1021/ci400527b
Idakwo G, Thangapandian S, Luttrell J et al (2019) Deep learning-based
structure-activity relationship modeling for multi-category toxicity
classification: a case study of 10KTox21 chemicals with high-throughput
cell-based androgen receptor bioassay data. Front Physiol 10:1044. https
//doi.org/10.3389/fphys.2019.01044

Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in pub-
lished maps and institutional affiliations.
