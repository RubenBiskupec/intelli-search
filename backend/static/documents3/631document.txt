Huang and Yang Journal of Statistical Distributions and ict]
Applications (2020) 7:10 Journal of Statistical

https://doi.org/10.1186/s40488-020-00111-y Distributions and Applications

RESEARCH Open Access

Affine-transformation invariant ®
clustering models

updates
Hsin-Hsiung Huang!” ® and Jie Yang?

 

 

*Correspondence:

hsin.huang@ucf.edu Abstract

‘Department of Statistics and Data We develop a cluster process which is invariant with respect to unknown affine
rence University of Central transformations of the feature space without knowing the number of clusters in
Full list of author information is advance. Specifically, our proposed method can identify clusters invariant under (|)

available at the end of the article orthogonal transformations, (Il) scaling-coordinate orthogonal transformations, and (Ill)
arbitrary nonsingular linear transformations corresponding to models |, Il, and Ill,
respectively and represent clusters with the proposed heatmap of the similarity matrix.
The proposed Metropolis-Hasting algorithm leads to an irreducible and aperiodic

Markov chain, which is also efficient at identifying clusters reasonably well for various
applications. Both the synthetic and real data examples show that the proposed
method could be widely applied in many fields, especially for finding the number of
clusters and identifying clusters of samples of interest in aerial photography and
genomic data.

Keywords: Dirichlet process, Ewens process, Metropolis-Hastings algorithm, Markov
chain Monte Carlo sampling, Unsupervised learning

 

1 Introduction
Clustering of objects invariant with respect to affine transformations of feature vectors
is an important research topic since objects may be recorded via different angles and
positions so that their coordinates may vary and their nearest neighbors may belong to
other clusters. For example, the longitude, latitude, and altitude coordinates of an object
which are recorded by devices equipped in aircrafts or satellites change across different
observational time. In this situation, distance-based clustering method including k-means
(MacQueen 1967), hierarchical clustering (Ward 1963), clustering based on principal
components, spectral clustering (Ng et al. 2001), and others (Jain and Dubes 1988; Ozawa
1985) may fail to identify the correct clusters by grouping nearest points. Another cat-
egory is distribution-based clustering methods (Banfield and Raftery 1993; Fraley and
Raftery 1998; Fraley and Raftery 2002; Fraley and Raftery 2007; McCullagh and Yang 2008;
Vogt et al. 2010) which may specify a partition as a parameter in a likelihood function and
estimate it under a Bayesian framework.

In certain areas of application, the goal is to cluster objects i = 1,...,” into dis-

joint subsets based on their feature vectors Y; € R®%. In this paper, we propose group

. © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
GQ) Springer O pen which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate
— credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were
made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your
intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 2 of 24

invariance by considering three cases of a cluster process that are invariant with respect
to three groups of affine transformations g: R¢ — R*® acting on the feature space.
The group invariance implies that the feature configurations Y and Y’ in R”*@ deter-
mine the same clustering, or probability distribution on clusterings, if they belong to
the same group orbit that is an equivalence class. For example, if the feature space
is Euclidean and G is the group of Euclidean isometries or congruences, the clus-
tering is a function only of the maximal invariant, which is the array of Euclidean
distances Dj = ||Y; — Y;j||. For example, image data such as the aerial photogra-
phy and three-dimensional protein structures are two motivating examples. The shape
and relative locations of data may vary due to the change of the viewer’s angle and
location.

Our goal is to develop a novel clustering method which can identify clusters of Y =
(Y1,..., Y,) even when all Y;’s are mapped by an unknown affine transformation Y; =
a-+AY;, where a = (a),...,d4q) € Ré andA € R% is nonsingular. Affine-invariant clus-
tering is important when the clusters are not well-separated in the observational space.
Although there are previous work on affine-invariant clustering methods (Fitzgibbon and
Zisserman 2002; Begelfor and Werman 2006; Shioda R. and Tungel 2007; Brubaker. S.C.
and Vempala 2008; Kumar and Orlin 2008; Garcia-Escudero et al. 2010; Lee et al. 2014),
these existing methods handle different problems from ours. These methods aim to clus-
ter the same item observed in different angles or mapped by different unknown affine
transformations. Instead, in our problem setting we consider only one unknown affine
transformation that is applied to all objects.

The affine transformations consist of three types: (1) index permutations, rotation, one-
scaling on all variables, and location-translation transformations that are under the first
type of covariance structures and named model I whose transformation and covariance
structure o7J7 were also adopted by Vogt et al. (2010); (2) each variable may have dif-
ferent scaling transformations that are under the second type of covariance structures
and named model II; (3) the variables are transformed by a nonsingular matrix that is
named model III, where the observed variables may be linear combinations of some latent
variables in model I. These models cover fairly general situations of clustering in nature.

McCullagh and Yang (2008) constructed a Dirichlet cluster process together with a ran-
dom partition representing the clustering. In this paper, we follow their setup and extend
their framework. We assume that the random partition of objects follows Ewens distribu-
tion (Ewens 1972), and we propose a likelihood of the responses which is invariant respect

to affine transformations.

2 Cluster process and prior distributions

In this paper, an R?-valued cluster process (Y, B) means a random partition B of the natu-
ral numbers, together with an infinite sequence Yj, Y2,... of random vectors in the state
space R@. The restriction of such a process to a finite sample [1] = {1,...,7} of units or
specimens consists of the restricted partition B[ 1] accompanied by the finite sequence
Y[n]= (%,...,Y,). A partition B[n]:[1] x[]— {0,1} is the partition of the sample
units expressed as a binary cluster-factor matrix of Bj; = 1 if Y; and Y; are of the same
cluster (denoted asi ~ j), and Bj; = 0 otherwise (McCullagh and Yang 2008). For exam-
ple, when 1 = 3, the partition {{1,2},3} and the cluster labels 112 correspond to an

equivalence relation
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 3 of 24

110
B=]110
001

Notice that the elements of B are transitional. i.e., if individuals i, j,k belong to the same
cluster, then Bj; = 1 and B;, = 1 imply B;x = 1.

The term cluster process implies infinite exchangeability, which means that the joint
distribution p, of (Y[],B[n]) is symmetric (McCullagh and Yang 2006) or invariant
under permutations of indices (Pitman 2002), and p, is the marginal distribution of py+1
under deletion of the (7 + 1)th unit from the sample.

Similar to (McCullagh and Yang 2008), we construct an exchangeable Gaussian mixture
as a simple example of clustering processes. First, B ~ p is some infinitely exchange-
able random partition. Secondly, the conditional distribution of the samples Y, which is
regarded as a matrix (Y;,) of order 1 x d given B (say the cluster label c/(Y;) = /) and 9, is

Gaussian with mean and variance as follows
E(¥i|B, m1) = m1, Cov (Yin, ¥js|B,0) = (6:7 + OBij) Zr,s,

where fly = (Mly)--->HMid) € R?@ is the centroid of cluster k, 5 is Kronecker’s delta, that
is, dj; = lifi =jandOifi 47,0 > Ois a ratio parameter connecting the within- and
between-cluster covariance matrices, and £ = (2,,s) is a positive definite matrix of order
d x d, known as the within-cluster covariance matrix. In our settings, the between-cluster
covariance matrix is simply 0%, the cluster centroids w,,...,, are iid from N (w,0%),
and the mean of Y given Band f11,..., My is

E (Y | B, 1 ee) Lx) = (Mey,) ce  Met(y,))

and the covariance of Y given B can also be represented by the covariance of its vector
form Vec(Y) = (Y11, wey Yid> wey Yui» wey Ynd)* as

Cov (Vec(Y) |B, 6) = Uy, + OB) ® &

which is an nd x nd matrix with “®” indicating the Kronecker product. &, the column
covariance of Y, is assumed identical for all clusters, J, + 0B is assumed an exchangeable
structure for the row covariance of Y, and 6 is the product of the standard deviations
of two rows. There exist competing algorithms that are affine-equivariant and do note
impose this requirement (Shioda R. and Tuncel 2007; Kumar and Orlin 2008; Garcia-
Escudero et al. 2010; Lee et al. 2014). The identity matrix itself is also a partition in which
each cluster consists of one element.

Given the number of clusters k, the cluster sizes (7,...,,) may follow a multinomial
distribution with category probabilities m = (71,...,7,), where w follows an exchange-
able Dirichlet distribution Dir(A/k,...,4/k). After integrating out wz, the partition B

follows a Dirichlet-multinomial prior

kt TA) [pep Pty + 4/4)

Pi Bio) = Cy Tat OPO

where #B < k denotes the number of clusters presented in the partition B and ny is the
size of cluster b (MacEachern 1994; Dahl 2005; McCullagh and Yang 2008). The limit as
k — oo is well defined and known as the Ewens’s sampling formula (ESF) with parameter
A> 0
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 4 of 24

raya*8
Pn (M4,.--5NK|A) = en I] I'(np),
beB

which is also known as Chinese restaurant process (CRP) (Ewens 1972; Neal 2000; Blei
and Jordan 2006; Crane 2016). McCullagh and Yang (2008) provided a framework with a
finite number of clusters and general covariance structures. In this paper, we adopt the
CRP prior for partition B which implies k = oo in the population with the proposed
Gaussian likelihood to get the affine-invariant clusters. Note that #B < n for any given
sample size n.

We choose a proper prior distribution for the variance ratio 0, the symmetric F-family

a—1

PON Ty om

with a > 0 allowing a range of reasonable choices (Chaloner 1987).

We propose a sampling procedure to estimate the partition B and the parameter 6 from
conditional probabilities. Since the conditional distribution of 0 does not have a recog-
nized form, we propose to use a discrete version { p(Gj) an , where J is a predetermined
moderately large integer. Based on our experience, J = 100 works reasonably well for the

real data examples that we have examined.

3 Affine-transformation invariant clustering

The affine-transformation invariant clustering identified in this manuscript is invariant
even when the objects are mapped by an unknown affine transformation. The conditional
distribution on partitions of [”] = {1,...,n} is determined by the finite sequence Y =
(Y1,..., Y,) regarded as a configuration of 1 labeled points in R¢. The exchangeability
condition implies that any permutation z of the sequence induces a corresponding per-
mutation in B, ie. py, (B” | Y = y”) = pn (B| Y = y), where y? = yy) and Bi = By i(i),2()):
In many cases, it is reasonable to assume additional symmetries involving transformations
in R4, for example p,, (B| Y) = pn(B| — Y). We are asking, in effect, whether two labeled
configurations Y and Y’ which are geometrically equivalent in IR@ should determine the
same conditional distribution on sample partitions.

If the state space R® is regarded as a d-dimensional Euclidean space with the standard
Euclidean inner product and Euclidean metric, the configurations Y and Y’ are congruent
if there exists a vector a = (d1,...,4q) € R% and an orthogonal matrix A € R?@*4 such
that Y’ = a+ AY; for each i. Equivalently, the 1 x n arrays of squared Euclidean distances

Di = } Yj — yj ||" and Di = | Y; - Yi | are equal. The configurations are geometrically
similar if Y; = a + bY; for b € Rand b ¢ 0, implying that the arrays of distances are
proportional D’ = b*D.

The geometric equivalence is defined by regarding the observation Y as a group orbit
rather than a point. In general, the group is the affine group GA (R) ,G = R? x LandL
is the collection of all d x d nonsingular matrices, with the operation (a1, A1) 0 (a2,A2) =
(a, + Aja, AA?) for a; € R?, A; € L withi = 1,2, which is consistent with compositions
of affine transformations. The orbit of an element Y = (Yj,..., Y,)7 € R”*% is defined as

Orb(Y) = x ER! : 4geGstX=ge | (1)
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 5 of 24

where the group action is that G acts on R’*“ as
(a,A)* Y =(a+AYj,...,a+AY,)' =1,a' + YAT (2)

where 1, is a length-m vector of 1’s. It can be verified that its vector form
Vec ((a4,A) * Y) = 1, ®a+ (I, ® A)Vec(Y). If

Vec(Y) ~ N (Ay ® #, Un + OB) ® X),
then an element in the same orbit

Vec ((a,A) * Y) ~ N (1, ® (4+ Ap), In + 0B) ® (AZAT))
More specifically,

Vec ((—, 1g) * Y) ~ N (0, dn + 6B) ® &))
Vec ((—T7 "pn, T~') « Y) ~ NO, Un + 9B) @ Iu)

where T isad x d nonsingular matrix satisfying © = TT’.

Theorem 1 if < d, then all Y € R"*@ of full rank n belong to the same orbit. If
n=d-+l,thenall Y € R"*4 satisfying rank(Y) = rank(Y — 1,1} Y/n) = d belong to the
same orbit.

The proof of Theorem 1 is relegated to the Appendix A. According to the proof, if
n = d +1, then rank(Y) = d implies that rank(Y — 1,1}, Y/n) is either d or d — 1. The
case of d — 1 only occupies a lower-dimensional subspace.

According to Theorem 1, for 1 < d + 1, the action is essentially transitive in the sense
that all configurations of 1 distinct points in R? belong to the same orbit: all other orbits
are negligible in that they have Lebesgue measure zero. As a result, the observation Y
regarded as a group orbit GY is uninformative for clustering unless n > d+ 1. We name
the orbit and group action defined above as model III.

In model I, which is the case considered in Vogt et al. (2010), the covariance between
features are proportional to an identity matrix. The group is G = R@ x R \ {0} with the
operation (41, bj) 0(d2, bz) = (a, + b1 a2, b1b2) for aj € R?,b; € R\ {0}, i = 1,2. The orbit
of an element Y € R”*@ and the group action are defined similarly as in (1) and (2) with
A replaced by b. Then (a, b) * Y = 1,a™ + bY and Vec ((a, b) « Y) = 1, ®a+ bVec(Y). If

Vec(Y) ~ N (In @ hs Un + OB) ® 07a),

then Vec((—w,1)*Y) ~ N(0,(,+6B) ® o7Iz) and Vec((—p/o,1/o)*Y) ~
N (0, 7, + 6B) ® Iz), which correspond to elements in Orb(Y).

In essence, the observation is not regarded as a point in R”*@ but is treated as a group
orbit generated by the group of rigid transformations, or similarity transformations if
scalar multiples are permitted. In statistical terms, this approach meshes with the sub-
model in which the matrix & in model I is a scaled identity matrix Jy . An equivalent
way of saying the same thing for nm > d is that the column-centered sample matrix
Y = Y — 1,1} Y/n determines the sample covariance matrix S = (YT Y) /(n — 1) and
hence the Mahalanobis metric ||x —* ||? = (x—x*)TS~1(~—.*) in the state space (Maha-

lanobis 1936; Gnanadesikan and Kettenring 1972). One implication is that the 1 x matrix
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 6 of 24

D = (Dj) = (| Y; - YI") of standardized inter-point Mahalanobis distances is maxi-
mal invariant, and the conditional distribution on sample partitions depends on Y only
through this matrix.

In practice, the d variables are sometimes measured on scales that are not commen-
surate with one another, so the state space seldom has a natural metric. In this case, we
assume that Y and Y’ as equivalent configurations for each feature Y.; if there are a; €¢ R
and b; < IR \ {0}, such that Y’ j= art b;Y.; . In model II, the group is the affine group
GA(R)4,G = R¢ x DandD = {diag{by,...,ba} | bi 40, i=1,...,d} with the opera-
tion (a, A1) 0 (a9, A2) = (a, + A1a2, A1A2) for a; € R4, A; € D with i = 1,2. The orbit
of an element Y € R”*@ and the group action are defined in (1) and (2) with A ¢€ D. If

Vec(Y) ~ N (In ® ft; Un + OB) ® diag{o?, ...,03})

then Vec((—pH,1g)*Y) ~ N(O,U, + 6B) @® diag {o?, eey o7}), and furthermore
Vec ((a4,A)*Y) ~ N(0,U, + 0B) ® Ig) with a = —(p4/04,...,ug/oq)' and A =
diag \o; Ve, a7! , which correspond to elements of the group orbit. No linear combi-
nations are permitted here, so that the integrity of the variables is preserved.

Moreover, in some cases, the location information or shapes of objects from aerial
photography applications may be distorted by the viewer's angle or position so that the
variables may be strongly correlated. A more extreme approach avoids the metric assump-
tion by regarding Y and Y’ as equivalent configurations if there exists a vector a € R®@
and a non-singular matrix A € R?*? such that Y; = a+ AY; with ATA is a positive defi-
nite matrix for all i. Consequently, models I, II, III specify the structures of the covariance
matrix between features, and the partition B of Y is affine invariant and the same as the
partition B of the group orbit GY C R”*4, which is independent of the mean.

3.1 Gaussian marginal probabilities
The distribution of the column-centered group orbit, GY, is assumed to be a Gaussian
distribution

N (0, I; + 0B) ® ATA)

which depends only on J, +0B and ATA. Actually, it can be verified that for any (4, A) € G,
the two distributions of group orbits induced by N (1, ® pw, UZ, + OB) ® X) and N(1, ®
(a+ Am), Uy, + OB) ® (AXA) respectively are the same.

McCullagh (2008) studied the d time series with an autocorrelation I and n observa-
tions in time or space following three Gaussian distribution models N(0O,l. ® &) under
different assumptions of © as follows:

Model I:= = oz (3)
Model Il:= = diag {o7,--- ,07} (4)
Model III: € PDg (5)

where PDj is the collection of d x d symmetric positive definite matrices. These three
models correspond to our three models of affine transformed equivalence classes which
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 7 of 24

we discussed in the previous section. In this paper, we set (J, + 0B) as and ATA as &.
Following (McCullagh 2008), the log-likelihood based on Y for all three models is:

1(V, D|Y)

1 1
—5 logdet(l @ E) — Str (yTr-tyx7")

d 1
—5 log det(I‘) — = log det(Z) — gt (yTr' y=") ,

Lemma 1 (J, + 6B)! = I, — 9WB, where W = diag{(1+0Ni)',...,(1+9Ny)*}
and Nj; is the ith diagonal element of N = B1,.

According to Lemma 1 and its proof, which is relegated to the Appendix A, T = I, +
6B is always nonsingular for 6 > 0 and its inverse P~! = (Uy + 0B)! = I, — O0WB
can be obtained explicitly. To ensure that YTT'~'Y is positive definite with probability 1
(McCullagh 2008), as well as informative group orbits (see Theorem 1 and its subsequent
discussion), we assume n > d +1.

After plugging in the maximum likelihood estimator of & which for model III is “r=
Ytl~1Y/n, for model II is diag (Sr), and for model | is tr (Sr) Iy/d (McCullagh 2008),
the profile likelihood of I’ is

det (P-1)% /tr (ytr-Ly) "4? (1)

Lp (CGY) = } det (P-1)"” /T]Z, (vary) (ID)
det (P-!)*? / det (Yt Y)””” (IID)
where Y(,) € R” is the rth column of Y,r = 1,...,d.

The conditional distribution on partitions of [1] depends on the group orbit and the
assumptions made regarding &. For group I, with & « Jg in the Gaussian model, the
likelihood depends only on the distance matrix D, so the likelihood is constant on the
orbits associated with the larger group of Euclidean similarities. Therefore, for model I,
the similarity transformation can be generalized as if Y/ = a + AY; for ATA = oy and
o # 0, implying that the arrays of distances are proportional D’ = «7D. Consequently,
there is a representative element of the group orbit with feature mean vector 0, so that
Vec(Y) ~ N (0, I, + OB) @ o7 ly).

For model II, the affine transformation can be generalized as Y/ = a+AY;, wherea € IRA
and A € R?*@ with ATA as a diagonal matrix with positive diagonal entries for all i. As a
result, there is a representative element of the group orbit with feature mean vector 0, so
that Vec(Y) ~ N (0, (I, + 0B) ® diag {o?,...,07}). This is to work with GA(R)@ which
is the general affine group acting independently on the d columns of Y. For model II, &
is an arbitrary matrix in PDg. The group is GA (R) and > d+ 1. These three models
are nested by model I C model II C model II.

Affine invariance in R?@ is a strong requirement, which comes at a small cost for mod-
erate d provided that d/n is small. When d/n < 1, YTT~'Y is positive definite with
probability one (McCullagh 2008), then model III works. However, while d/n < 1 is not
small, model III may be inefficient due to some eigenvalues of YTT~1Y and det (yT ro} Y)
close to zero (Dempster 1972; Stein 1975). Asa result, the profile likelihood of ! becomes
unstable. In contrast, model II is less computationally expensive than model II, and
model I is the most efficient one.
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 8 of 24

4 Markov chain Monte Carlo algorithm for sampling partitions

We use the prior and posterior distributions of 6 and B discussed in Section 2 through
a Markov chain Monte Carlo (MCMC) algorithm for sampling partitions. The iterative
0 is obtained by Gibbs sampling (Geman and Geman 1984) according to the conditional
distribution p, (6;|B, GY) « p(6j) x Lp (F~'|GY), where p(j)) « 0° -*/ (1+ 6)" for
j = 1,...,J. For instance, a = 1 and the discrete set {2-3,2°%, _ ., 210} for the range
of 6 are used as the default setting in our experiments. For updating B, the conditional

distribution on partitions is
Pn (BIO, GY) & pn(BlA) x Ly (T'NGY),

where p;,(B|A) is the Ewens distribution, and a Metropolis-Hastings algorithm (Metropo-
lis et al. 1953; Hastings 1970) is used to choose the iterative B. A is set as 1 in the following
applications. After burning in a certain number of the resulting Markov chain, we use the
average of the partition matrix as the similarity matrix to make inference on partition.
The proposal distribution q (BorD IB®,G Y) is proportional to exp(—a X dyc), where dy
is the distance between each point and the corresponding centroid of the clusters and a is
a scale hyperparameter which was set as 2 in our experiments. More specifically, a parti-
tion candidate B* is generated by re-assigning the label of each point with the probability
proportional to the reciprocal of the distance between each point and the corresponding
centroid.

Algorithm 1 Metropolis-Hastings algorithm
1: Initialize B and 0.
2: fori = 1: N dor N is the number of total iterations. Suppose that the current values
are 0 and BY.
3: Randomly sample 6“* from py (9B, GY), j =1,...,/.
4: Propose B* ~ g (B“*) |B®,GY).
5: Calculate
Pn (B*|A) Lp (TO, B*) GY) q(BO|B*, GY)
© pn(BO|ADLp (POF, BO)" GY) q(B*|BO, GY)

6: Accept B“+)) = B* with probability min{1, R}

7: Keep Bt) = B® with probability 1 — min{1, R}
s: end for

9: return all the B’s and 6's.

Since Algorithm 1 is a Metropolis-Hastings algorithm, it satisfies the detailed balance
condition, and therefore the generated Markov chain has a stationary distribution (Chib
and Greenberg 1995; Gamerman 1997; Robert and Casella 2010). Since we leave a small
but positive probability that the partition stays the same in the Gibbs sampling and the
discrete posterior of 0 stays positive always, then the transition probability

pn(OktY, BAY a®, BW) 0

where 04%+)) = 9 and B&t+)) = B®, and then the (6,B)-valued Markov chain
constructed by Algorithm 1 is aperiodic.
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 9 of 24

Lemma 2 [fn > d +1, the (0, B)-valued Markov chain constructed by Algorithm 1 is

aperiodic.

Since there is always a positive chance that the partition can be split further into the
simplest partition in which each element is a cluster, then all possible partitions communi-
cate with each other, so that the (0, B)-valued Markov chain constructed by Algorithm 1 is
irreducible. Given the sample size u, the size of the state space of B known as the Bell num-
ber (Bell 1934), and the size of the state space of 6 are all finite, then the irreducibility also
implies positive recurrence. Consequently, the (6, B)-valued Markov chain constructed
by Algorithm 1 is ergodic (Isaacson and Madsen 1976; Gilks et al. 1996). The properties
are summarized as the following lemma and theorem, whose proofs are relegated to the
Appendix A.

Lemma 3 [fn > d +1, the (0, B)-valued Markov chain constructed by Algorithm 1 is
irreducible, and thus is positive recurrent.

Theorem 2 (Ergodic theorem) Ifn > d+ 1, the (@,B)-valued Markov chain con-
structed by Algorithm 1 converges to its stationary distribution py (0,B\GY) « p(@) x
Pn(Bla) x Ly (C-1|GY). More specifically, for any real-valued function f satisfying
> 6.B) If (6, B)|pn(8, BIGY) < 00, we have

 

5 - oF (0°. B°) > S26, Bypn(O,BIGY)
i=0 (0,B)

almost surely for all initial value (0 (0), BO),

5 Analysis of simulated and real data

We test the proposed Bayesian cluster process with Algorithm 1 on both synthetic and
real data. Algorithm 1 with model I and point-wise updating is equivalent to the method
of (Vogt et al. 2010). If there is no prior information of the number of clusters, users
can set the initial partition B as J, in which each observation is a block. In practice, we
use a randomly sampled clusters from a discrete uniform distribution of a range chosen
by users. The clustering result is represented by the average of the estimated similarity

matrix
N Bw

S = ,
N- No
k=no+1

 

where No is the number of burn-in iterations. Furthermore, we also define a dissimilarity
matrix D as 1,1}, — S. The dissimilarity matrix, D, can be expressed by a heatmap which
represents a matrix with grayscale colors with white as 1, black as 0, and the spectrum
of gray as values between 0 and 1. The heatmap of the original similarity matrix cannot
be recognized with the naked eye and equivalence relation needs to be decoded from the
matrix B. However, in practice, users can identify clusters through including the names of
rows and columns of the similarity matrix to find which individuals are clustered together.
Additionally, the heatmap function of the stats R package can permute the order of indi-
viduals to have cluster blocks with hierarchical dendrograms. It is challenging to monitor
convergence of the Markov chain because the sampled clusters are random and may vary
Huang and Yang Journal of Statistical Distributions and Applications

(2020) 7:10

in each iteration. To determine convergence, we run Algorithm 1 ten times for each data
set and stop the chain when we observe the number of clusters remain the same in the
given chain length (Chang and Fisher 2013).

5.1 Illustrative simulated data
Four clusters on the vertices of a unit square data Three simulated data sets are gener-
ated for illustration. In the simulation study, 1000 initial burn-in iterations were discarded,
and 2000 Markov chains of B samples based on each model were used to calculate D.
We first applied the proposed cluster process with model I on the synthetic data for four
clusters centered at the four vertices of a unit square. For each vertex xz, we generate 20
points from N (x, (1/4)/2) for k = 1,...,4 (see Fig. 1, the left panel). We call the data
X7, and then apply model I to cluster X; with the average within- and between-cluster dis-
tances. The resulting heatmap successfully reveals the true clusters for most of the points
(not shown here).

3 0

01/3
to have two groups (see Fig. 1, the middle panel), clusters (1,2) and clusters (3,4). The

Then we transform the data by X77 = X7 x . The transformed features seem

cluster process with model I does not work well for this case, while the heatmap based on
model II without knowing the transformation can reveal the true clusters for most of the
points (not shown here).

4.1 2.1

1.9 1.1
tures are aligned in a straight line (see Fig. 1, the right panel). The transformed data Xj; is

Furthermore we transform the data by Xy = X7 x . The transformed fea-

more difficult to cluster than X7 and Xj, since the original four clusters are transformed
to be not well separated.

The resulting heatmap using model III with the initial clusters assigned randomly and
uniformly from {1, 2, 3,4} reveals the true four clusters for most of the points (see Fig. 2).

5.2 Applications to real data
Besides the synthetic data, we also evaluate the performance of the proposed approach
by using real data. We run 3000 MCMC iterations and burn in the first 1000 iterations,

 

 

 

 

2PRBZHA 2 3 3.2979 38,3 7 ae

st) a ty 44, 44 stfan 4

 

 

 

 

 

 

 

 

Fig. 1 The scatter plots for X), Xj, and Xj of the unit square synthetic data from the left to the right. The most
left panel is the original features which have four clusters at the vertices of the unit square with equal size 20;
the middle panel is the features which are transformed by scaling each dimension differently, clusters 1 and 2
are grouped as well as clusters 3 and 4 are grouped; the right panel shows the transformed features are
aligned as a straight line

 

 

 

Page 10 of 24
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 11 of 24

 

 

©
00 |
20 40 60 80

Fig. 2 The heatmap of the similarity matrix using model Ill reveals the true four clusters for most of the
transformed data Xj

 

 

 

and use the heatmap of matrix S to visualize the clusters. The accuracy rate is based
on the average proportion of identical elements of matrix B of the cluster and the true
matrix B, and compared the accuracy rates with k-means (MacQueen 1967) and Mclust
using R package ‘mclust’ with its default setting (Fraley and Raftery 2002). The reason
why we chose R package ‘mclust’ is that Mclust is a model-based clustering approach
using the Gaussian mixture model, which assumes a Gaussian distribution for each com-
ponent under one of the three types covariance structures (the argument of Mclust:
modelNames) 1. Spherical (EI), 2. Diagonal (VVI), and 3. General (VVV) for compar-
ing with our proposed model I, IJ, III, correspondingly. The main difference is that the
Mclust obtains clusters with an expectation—maximization (EM) algorithm (Dempster
1972; McLachlan and Peel 2000), but our method uses a Metropolis-Hasting algorithm
with the profile likelihood of C to sample clusters.

Model I: Gene expression data of Leukemia patients The gene expression microar-
ray data (Dua and Graff 2019) has been used to study genetic disorder such as identifying
diagnostic or prognostic biomarkers or clustering and classifying diseases (Dudoit et al.
2002). For example, (Golub et al. 1999) classified patients of acute leukemia into two sub
types, Acute Lymphoblastic Leukemia (ALL) and Acute Myeloid Leukemia (AML). For
illustration purpose, we use the training set of the leukemia data which consists of 3051
genes and 38 tumor mRNA samples. Pretending we do not know the label information,
we would like to cluster the 38 samples according to their 3051 features (gene expression
levels). The two clusters comprise 27 ALL cases and 11 AML cases. Since the number
of features is larger than the sample size, our approach is not applicable to this dataset
directly. Therefore, we first reduce the dimension by projecting the data on the subspace
which consists of the first twenty principal components (PC) (Jolliffe 1986). Note that
these PC scores are orthonormal which satisfies the assumption of model I. The resulting
heatmap based on model I (Fig. 3) reveal the cluster of the 11 AML cases. The accuracy
rate using the proposed model I with the initial clusters assigned randomly and uniformly
from {1,2} is 0.9164, while the accuracy rates of k-means and Mclust are 0.6994 and
0.5886, respectively. We noticed that Mclust resulted in only one cluster.
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10

 

Ss
ss a
Ss
ee ee
eee
a
i

© SEEEEEEEEEE SESSSSeeeeee
° -SEEEEEEEEEE SERRE
Ss

= veal BSR SSeS aaa
2s es
|
oOo
2
a
OOO
Cc SSSR Eee
<| -—SSSSEEEEEEs SESE
a
ooo
OCC
Ss

a a EES SES Sea
SRS Eee
Ss

a = See

oO B _ Sse

S - a _ Ss

a _ See

rl I BER SESS See

g _ Sse

Sg _ See

5 — Sse

i S Ss

g _ 2

_s 3 = Sees

10 20 30

Fig. 3 The heatmap of the similarity matrix using model | identifies the ALL group in the left upper corner

and the AML group in the right bottom corner

 

 

 

Model II: Geographic coordinate data of Denmark’s 3D Road Network

This three-dimensional road network dataset of geographic coordinates includes the
altitude, latitude, and longitude degrees of each road segments in North Jutland in north-
ern Denmark, which is publicly available at the UC Irvine Machine Learning Repository
(Kaul 2013; Dua and Graff 2019). Since three spatial dimensional features are orthogonal,
it satisfies the assumption of model II so that we use this dataset to demonstrate model
Il. Three subjects with the road maps OSM ID 144552912 (19 observations), 125829151
(13 observations), 145752974 (14 observations) are used for the clustering analysis. Note
that each objects may have several observations measured from different angles, and the
altitude values are extracted from NASA’s Shuttle Radar Topography Mission (SRTM)
data (Jarvis et al. 2008). The accuracy rate using model II with the initial clusters assigned
randomly and uniformly from {1, 2,3, 4,5} is 1, while the accuracy rates of k-means with
k = 3 and Mclust are 0.7486 and 0.9490, respectively. The resulting heatmap using model
II (Fig. 4) reveals 3 clusters correctly.

 

10
|
SREERERERERERER ee
SEEEREREREREREREeee
EREERERERERERERERE
SRREREREREREREREee
SEEEREREREREREREREe
EEEEREREREREREREREe
EREEREREREREREREREe
SEREREREREREREREeee
SEREREREREREREREeEe
EERERERERERERE RE
EREEREREREREREREeee
SERERERERERERER Eee
SEREREREREREREREeEe
SEREREREREREREREeee
SEEERERERERERER eee
SREEREREREREREREREe
SEEEREREREREREREREe
EERERERERERERER Ee
SERERERERERERERe

20
|

30
amnnnn
1
1
1
1

40

 

10 20 30 40

Fig. 4 The heapmat of the similarity matrix using model II correctly reveals three clusters corresponding to
the three buildings in the Denmark 3-D road map data

 

 

 

Page 12 of 24
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 13 of 24

Model III: Iris data

This iris dataset (Fisher 1936) contain three species—Setosa, Versicolor, and Virginica
with four features which are the measurements of the variables sepal length and width
and petal length and width in centimeters, respectively. Each species consists of 50 iris
flowers. The data points are clustered by their four features. Here, d = 4, 1 = 150,
k = 3. The heatmap of the similarity matrix using model III correctly reflects three clus-
ters corresponding to the three species of iris for most points (Fig. 5). The accuracy rate
using the proposed model III with the initial clusters assigned randomly and uniformly
from {1, 2,3} is 0.9087, while the accuracy rates of k-means with k = 3 and Mclust are
0.7740 and 0.7763, respectively. We noticed that both the k-means and Mclust result in
two clusters by grouping Versicolor and Virginica as a cluster.

6 Concluding discussion

The proposed clustering method is invariant under different groups of affine transfor-
mations and computationally efficient. It identifies clusters for most samples without
knowing the number of clusters in advance, and it may group a big cluster as several small
clusters. These problems are dealt with an exchangeable partition prior which avoids
label-switching problems and the partition valued in the MCMC algorithm is invariant
under linear transformations under three types of covariance structures. The advantage of
replacing the Dirichlet-multinomial prior with its limiting process is that we do not need
to know the number of clusters in advance. The disadvantage is that it may be less efficient
computationally if the number of clusters is known. Note that the proposed approach
does not target the partition maximizing the posterior distribution. Instead, it estimates
the expected partition or the similarity matrix.

The three clustering models are based on the covariance matrix between variables.
There are guidelines of telling which model work best in practice by the experimental
design or testing its sample covarinace matrix. If the features are othornormal or orthog-
onal, then model I and model II are applicable, respectively. Models I and II run faster
than model III due to the structure of the covariance matrix. Otherwise, model III can be

used in general. It works reasonably well across various applications.

 

100

 

= | | |
50 100 150

Fig. 5 The heatmap of the similarity matrix using model Ill correctly reveals three clusters corresponding to
the three species of iris for most points

 

 

 
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 14 of 24

Since we use the profile likelihood of [ in our model, we do not sample the covari-
ance matrix directly, and Lemma 1 and Theorem 1 implies as 1 > d+ 1, the proposed
Metropolis-Hasting algorithm can work. However, the maximum likelihood estimator
(MLE) of the general unstructured covariance matrix will be less efficient if the diagonal
covariance structure is actually correct because it will tend to have small eigenvalues and
a large determinant of the inverse covariance matrix (i.e. !~+). Indeed, when using model
II and even ifn > d+ 1, IT may be near singular. This may make the sampling less effi-
cient. ie. the acceptance rate may become small (Roberts and Rosenthal 2001). Although
the stationary distribution of the sampled clusters’ Markov chain using Algorithm 1 is
independent of the initial clusters according to Theorem (2), we practically suggest to set
the initial clusters sampled from a discrete uniform distribution of a range given by users
instead of setting each individual as a cluster in order to obtain convergent sampled clus-
ters without using a long Markov chain. This makes the proposed Algorithm 1 sample
more efficiently from a smaller collection of partition candidates.

The proposed clustering algorithm produces the desired clusters with 2000 iterations
after 1000 burn-in iterations in our experiments. The main contributions of our work
include: 1) The proposed three clustering models with three types of covariance struc-
tures can handle general cases of affine transformations. In contrast, (Vogt et al. 2010)
only dealt with the case of model I. 2) Algorithm 1 is efficient, since it updates all individ-
uals’ clusters instead of a single individual’s cluster per iteration. It also ensures that the
resulting partition-valued Markov chain is ergodic and convergent in distribution. 3) The
experiments show the advantages of our cluster process which successfully identifies the
true clusters using the proposed distance matrix. In particular if the clusters are not well
separated, the similarity matrix with probabilistic nature can still reveal the relationships
through hierarchical approaches. The proposed method could be used to extract interest-
ing information from aerial photography, genomic data, and data with attributes under
different scales, especially when the nearest neighbors may belong to different clusters in
the feature space. The proposed method can be improved in the further work by modeling

the mean of each cluster with regression on covariates or non-Gaussian distributions.

Appendix A
Proof of Theorem 1: For any Y € R”*4, denote Y = Y — 1,1), Y/n. Let Yi,_1) be the
(1 — 1) x d matrix consisting of the first n — 1 rows of Y. Since 1} Y = 0, then rank(Y) =
rank(¥(n—1)).

Ifn < d+ and rank(Y(,-1)) = n—1, that is, Y—1) is of full row rank, then there exists
an orthogonal matrix O € IR?*4 (column permutations), such that, Yin—1)O = (U,V),
where U € R“@-)*@-D jg of full rank, and V € R“~)*@+1-”) We let

1 UT 0 In1 0
a=~-Y'l,, A=O ,Z=( "7 .
i VT Ias1—n -17_,0

It can be verified that Y = (a4, A) * Z. That is, Y € Orb(Z), where Z is a constant matrix.
Ifm < dandrank(Y) = 4, then rank(¥(n—1)) = n — l, since Y(n—1) = WY, where
W=(y,-1- 1,-11)_,/n, —1,_1/n) is of full row rank n — 1.
Suppose 1 = d+ 1 and rank(Y) = d = n — 1. Without any loss of generality, we
assume rank(Y(,—1)) = 1 — 1, where Y(q_-1) consists of the first 1 — 1 rows of Y. Then
Y, = (1 Y, +--+ + Cy-1Yy_-1 for some cj,...,c,—-1 € Rand Yn—1) = DY(y-1), where
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 15 of 24

D=I,-1 - 1,-11)_,/n — 1,-1c'/n, and cc! = (cj,...,Cy_1). It can be verified that if
Cy +---+cy_-1 #1, then rank(D) = n—1 and rank(¥(4—1)) = n—l;ifcy +---+c,_-)] = 1,
then rank(D) = n—2 and rank(¥(n—1)) = n—2. Note that ifm = d+1 but rank(¥(n—1)) =
n—2,then Y ¢ Orb(Z). L
Proof of Lemma 1: Suppose the partition matrix B consists of k blocks with block sizes
N1,...,Np, wherek > 1,n; > Ofori=1,...,k,andnj +---+nyp =n.

We first assume that B = diag {1,,1/,,.--)1n,11,}, which is in its standard form. Then
B=LL'T with L = diag{1y,,...,1n,} € R’** and I, + 0B = 1, + EET with E = VOL.

According to the Sherman-Morrison-Woodbury formula (see, for example, Section
2.1.4 in Golub and Van Loan (2013)), for matrices A € R”*” and U,V € R"**,
(A+UVT) 1 =A 1A 1UC4+VTA'U) VTA if both A and I+ VTA~'U are non-
singular. In our case, A = J, is nonsingular, U = V = E,andI + VTA1U =i + ETE =
diag{1 + 0m1,...,1+ On x} is also nonsingular. Thus

(In + EET)
= I,—E(+ETE) ET
= I, —OL (Ik + OLTL) LT

= I, —@ - diag ton 1! ,..., ——-_1,, 1)

1+6n, ° ™ 1+0n, *
= IF — 6. diag | ppt 2) an | . diag {1 17,). . Any 1 f
= I, —OWB.

In general, by row-switching and column-switching transformations, we can always
transform B into its standard form. That is, there exists an orthogonal matrix O such
that B, = OBOT is in standard form. Let W, = OWOT. Then (J, + 6B)! = OT, +
6B,) 'O = OT, — 0 W,B,)O = In — 0 - OTW,O- OTB,O = I, — OWB. O
Proof of Lemma 3: In our case, the Markov chain built by Algorithm 1 is actually a dis-
crete chain. It is irreducible since p,(6 (K+1) BK+D 1g@® BY) s 0 for each pair of states.
As a direct conclusion of Theorem 4.1 in Gilks et al. (1996), our Markov chain is positive
recurrent. L
Proof of Theorem 2: Algorithm 1 is a Gibbs sampler plus a Metropolis-Hastings com-
ponent for sampling B“t)., Given BY and 6“*), the Metropolis-Hastings ratio with
proposal distribution q(B|B, GY) and target distribution p,,(0, B|GY) is

Pn (O°), B*|GY) - q(B|B*, GY)

Pn (0G), BO\GY) q (B*|BO, GY)

pO?) - py (BA) «Lp (POY, BY) GY) - q(BO|B*, GY)
pF) - py(BO|A) - Ly (PG, BO)-HGY) - g (B*|BO, GY)
Pn (B*|A) «Ly (FO"T), BY)" |GY) - q(BO |B*, GY)

~ py(BOA) «Lp (TOD, BO)—1GY) - q(B* BO, GY)

R(B”,B*) =

which is exactly R in Algorithm 1. Since Metropolis-Hastings algorithms satisfy detailed
balance condition, the target distribution p,,(6, B|GY) is a stationary distribution. By Lem-
mas 2 and 3, the convergence statements follow as a direct conclusion of Theorems 4.3
and 4.4 in Gilks et al. (1996). L
Huang and Yang Journal of Statistical Distributions and Applications

Page 16 of 24

Appendix B journal name abbreviations for use in Boundary-Layer meteorology

 

Journal Name

Abbreviation used in BLM

 

ACM Transactions of Mathematical Software

Acoustics Australia

Acta Geophysica

Acta Mechanica Synica

Acta Mechanica Supplement
Advances in Atmospheric Science
Advances in Ecological Research
Advances in Meteorology
Advances in Science and Research
Advances in Water Resources
Aeolian Research

Aerospace Science and Technology
Agricultural Meteorology
Agricultural and Forest Meteorology
Agricultural Water Management

American Institute of Aeronautics and
Astronautics

Annals of Glaciology

Annalen der Meteorologie

Annals of Statistics

Antarctic Science

Annual Review of Fluid Mechanics
Applied Energy

Applied Mechanics Review
Applied Numerical Mathematics
Applied Physics B

Applied Optics

Aquatic Botany

Archiv fur Meteorologie Geophysik und

Bioklimatologie Serie A-Meteorologie und
Geophysik

Archiv fur Hydrobiologie

Artificial Intelligence

Astronomy & Astrophysics

Atmospheric Measurement Techniques

Atmosphere-Ocean

ACM ‘Trans Math
Soft

Acoust Aust

Acta Geophys
Acta Mech Sinica
Acta Mech Suppl
Adv Atmos Sci
Adv Ecol Res

Adv Meteorol

Adv Sci Res

Adv Water Resour
Aeolian Res
Aerosp Sci Technol
Agric Meteorol
Agric For Meteorol
Agric Water Manag

Am Inst Aeronaut
Astronaut

Ann Glaciol

Ann Meteorol
Ann Stat

Antarct Sci

Annu Rev Fluid Mech
Appl Energy

App! Mech Rev
App! Numer Math
Appl Phys B

Appl Opt

Aquat Bot

Arch Meteorol Geo-
phys Bioklim Ser A

Arch Hydrobiol
Artif Intell
Astron Astrophys
Atmos Meas Tech

Atmos-Ocean

 
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10

Page 17 of 24

 

Journal Name

Abbreviation used in BLM

 

Atmospheric Research

Atmospheric Science Letters
Australian Journal of Physics
Australian Journal of Botany
Beitraege zur Physik der Atmosphaere
Biogeosciences

Biometrika

Biosystems Engineering

Boreal Environment Research

Boundary-Layer Meteorology

Building and Environment

Bulletin of the American Meteorological Society
Climate Research

Cold Regions Science and Technology

Communications in Agricultural and Applied
Biological Sciences

Communications in Mathematical Physics

Communications on Pure and Applied
Mathematics

Comptes Rendus Physique

Computers and Electronics in Agriculture
Computing and Informatics

Computer Methods in Applied Mechanical
Engineering

Computational Statistics and Data Analysis
Contributions to Atmospheric Physics
Crop Protection

Deep Sea Research Part II

Dynamics of Atmpsheres and Oceans

Earth System Science Data Discussions

Earth Surface Processes and Landforms

Ecological Applications
Ecological Indicators
Ecological Modelling
Ecology

Atmos Res

Atmos Sci Lett
Aust J Phys

Aust J Bot

Beitr Phys Atmos
Biogeosciences
Biometrika

Biosyst Eng

Boreal Environ Res

Boundary-Layer
Meteorol

Build Environ

Bull Am Meteorol Soc
Clim Res

Cold Reg Sci Technol

Commun Agric Appl
Biol Sci

Commun Math Phys

Commun Pure Appl
Math

C R Phys
Comput Electron Agric
Comput Inf

Comput Methods Appl
Mech Eng

Comput Stat Data Anal
Contr Atmos Phys
Crop Prot

Deep Sea Res II

Dyn Atmos Oceans

Earth Syst Sci Data
Discuss

Earth Surf Process
Landf

Ecol Appl
Ecol Indic
Ecol Model
Ecology

 
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10

Page 18 of 24

 

Journal Name

Abbreviation used in BLM

 

Electronic Journal of Operational Meteorology

Enerhies

Energy and Buildings

Energy Conversion and Management
Environmental Fluid Mechanics
Environmental Modeling and Software
Environmental Pollution
Environmental Research Letters
Environmental Science and Technology
Environmental Software

Eos, Transactions, American Geophysical Union
European Journal of Forest Research
Experiments in Fluids

Fisheries Research

Flow Turbulence and Combustion
Forestry

Freshwater Biology

Functional Ecology

Acta Geodaetica et Geophysica Hungarica
Geografiska Annaler Series A
Geography Compass

Geomorphology

Geophysical Research Letters

Geoscientific Instrumentation, Methods and
Data Systems

Geoscientific Model Development

Global Biogeochemical Sciences

Glocal Change Biology

Hydrology and Earth System Sciences
Hydrological Processes

IEEE Journal of Ocean Engineering

IEEE Transactions on Geoscience and Remote
Sensing

International Journal of Climatology
International Journal of Wildland Fire

International Journal of Heat and Fluid Flow

Electron J Oper
Meteorol

Energies

Energy Buil

Energy Convers Manag
Environ Fluid Mech
Environ Modell Softw
Environ Pollut
Environ Res Lett
Environ Sci Technol
Environ Softw

Eos Trans AGU

Eur J For Res

Exp Fluids

Fish Res

Flow Turbul Combust
Forestry

Freshwater Biol

Funct Ecol

Geod Geophys

Geogr Ann Ser A
Geogr Compass
Geomorphology
Geophys Res Lett

Geosci Instrum
Method Data Syst

Geosci Model Dev

Glob Biogeochem
Cycles

Glob Change Biol
Hydrol Earth Syst Sci
Hydrol Proc

IEEE J Ocean Eng

IEEE Trans Geosci
Remote

Int J Climatol
Int J Wildland Fire
Int J Heat Fluid Flow

 
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10

Page 19 of 24

 

Journal Name

Abbreviation used in BLM

 

International Journal of Numerical Methods for
Fluids

International Journal of Remote Sensing
Izvestiya, Atmospheric and Oceanic Physics
Journal of Advances in Modeling Earth Systems
Journal of Aerosol Science

Journal of Agricultural Engineering Research

Journal of the Air Pollution Control Association

Journal of Aircraft

Journal of Applied Meteorology and Climatology
Journal of Applied Meteorology

Journal of Aquatic Plant Management

Journal of Arid Environments

Journal of Atmospheric and Oceanic Technology
Journal of Atmospheric Science

Journal of Climate

Journal of Computational Physics

Journal of Earth Simulation

Journal of Earth System Science

Journal of Environmental Engineering

Journal of Experimental Botany

Journal of the Faculty of Science Hokkaido
University

Journal of Field Robotics

Journal of Fluid Mechanics
Journal of Geophysical Research
Journal of Geophysical Research-Atmospheres
Journal of Glaciology

Journal of Great Lakes Research
Journal of Hazardous Materials
Journal of Heat Transfer

Journal of Hydraulic Engineering
Journal of Hydrology

Journal of Hydrometeorology
Journal of Marine Research
Journal of Marine Systems

Journal de Mathematiques Pures et Appliquees

Int J Numer Methods
Fluids

Int J Remote Sens

Izv Atmos Ocean Phys
J Adv Model Earth Syst
J Aerosol Sci

J Agric Eng Res

J Air Pollut Control
Assoc

J Aircr

J Appl Meteorol Clim

J Appl Meteorol

J Aquat Plant Manag

J Arid Environ

J Atmos OceanTechnol
J Atmos Sci

J Clim

J Comput Phys

J Earth Simul

J Earth Syst Sci

J Environ Eng

) Exp Bot

J Fac Sci Hokkaido Univ

J Field Robot

J Fluid Mech

J Geophys Res

J Geophys Res Atmos
J Glaciol

J Great Lakes Res
J) Hazard Mater A
J Heat Transf

J Hydraul Eng

J Hydrol

J Hydrometeorol

J) Mar Res

J Mar Syst

J Math Pures Appl

 
Huang and Yang Journal of Statistical Distributions and Applications

(2020) 7:10

Page 20 of 24

 

Journal Name

Abbreviation used in BLM

 

Journal of Meteorology

Journal of the Meteorological Society of Japan
Journal of Oceanography

Journal of Operational Oceanography

Journal of the operational Research Society
Journal of the Optical Society of America
Journal of Plankton Research

Journal of Solar Energy Engineering

Journal of Quantitative Spectroscopy and Radiative
Transfer

Journal of Renewable and Sustainable Energy
Journal of Scientific Statistical Computing
Journal of Statistical Physics

Journal of Thermophysics and Heat Transfer
Journal of Tropical Ecology

Journal of Turbulence

Journal of Wind Engineering and Industrial
Aerodynamics
Landscape and Urban Planning

Limnology and Oceanography

Low Temperature Science

Machine Learning

Marine Chemistry

Mathematische Annalen
Meteorological Applications
Meteorology and Atmospheric Physics
Meteorologische Zeitschrift

Monthly Weather Review

Natural hazards and Earth System Sciences
Nature Climate Change

Nature Letters

Nature Geoscience

Neural Computation

Nonlinear Processes in Geophysics
New Zealand Journal of Science
Oceanography

Ocean Dynamics

Ocean Engineering Science

J Meteorol

J Meteorol Soc Jpn

J Oceanogr

J Oper Oceanogr

J Oper Res Soc

J Opt Soc Am

J Plankton Res

J Sol Energy Eng

J Quant Spectrosc Radiat Transf

J Renew Sust Energy

J Sci Stat Comput

J Stat Phys

J Thermophys Heat Transf
J Trop Ecol

J Turbul

J Wind Eng Ind Aerodyn

Landsc Urban Plan
Limnol Oceanogr

Low Temp Sci

Mach Learn

Mar Chem

Math Ann

Meteorol Appl
Meteorol Atmos Phys
Meteorol Z

Mon Weather Rev

Nat Hazards Earth Syst Sci
Nat Clim Change

Nat Clim Change

Nat Geosci

Neural Comput

Nonlin Process Geophys
N ZJ Sci

Oceanography

Ocean Dyn

Ocean Eng Sci

 
Huang and Yang Journal of Statistical Distributions and Applications

(2020) 7:10

Page 21 of 24

 

Journal Name

Abbreviation used in BLM

 

Ocean Modeling

Papers in Physical Oceanography and
Meteorology
Particle & Particle Systems Characterization

Particuology

Philosophical Transactions of the Royal Society
of London

Photogrammetric Engineering and Remote
Sensing

Physical Review Letters

Physical Review E
Physics and Chemistry of the Earth
Physics of Fluids

Physics A - Statistical Mechanics and its
Applications
Physica D

Plant Biosystems

PLOS One

Powder technology

Proceedings of the Royal Society
Progress in Aerospace Science
Progress in Heat and Mass Transfer
Progress in Physical Geography
Pure and Applied Geophysics

Quarterly Journal of the Royal Meteorological
Society
Remote Sensing

Remote Sensing of Environment
Renewable Energy

Reviews of Geophysics

Reviews of Geophysics and Space Physics
Review of Scientific Instruments

Science

Science of the Total Environment
Sedimentology

Siam Journal on Applied Mathematics
Tellus

Tellus Series B - Chemical and Physical
Meteorology

Ocean Model
Pap Phys Oceanogr Meteorol

Part Syst Charact
Particuology
Philos Trans R Soc

Photogramm Eng Remote Sens

Phys Rev Lett

Phys Rev E

Phys Chem Earth

Phys Fluids

Physica A Stat Mech Appl

Physica D

Plant Biosyst

PLOS One

Powder Technol

Proc Roy Soc

Prog Aerosp Sci

Prog Heat Mass Transf
Prog Phys Geogr

Pure Appl Geophys
QJ R Meteorol Soc

Remote Sens

Remote Sens Environ
Renew Energy

Rev Geophys

Rev Geophys Space Phys
Rev Sci Inst

Science

Sci Tot Environ
Sedimentol

SIAM J Appl Math
Tellus

Tellus Ser B Chem Phys Meteorol

 
Huang and Yang Journal of Statistical Distributions and Applications

(2020) 7:10

Page 22 of 24

 

Journal Name

Abbreviation used in BLM

 

Theoretical and Applied Climatology
Theoretical and Computational Fluid Dynamics
Theoretical Computational Fluid Dynamics
Thermal Science Engineering

Transactions of the American Society of Agricultural
Engineers

Tree Physiology

Trudy Geofizicheskogo Instituta, Akademiya Nauk
SSSR
Urban Climate

Water Air and Soil Pollution

Water Resources Research

Waterway Port Coastal and Ocean Engineering
Weather

Weather and Forecasting

Wind Energy

Wind Engineering

Zeitschrift fir Angewandte Mathematik und
Mechanik

Theor Appl Climatol
Theor Comput Fluid Dyn
Theor Comput Fluid Mech
Therm Sci Eng

Trans ASAE

Tree Physiol
Trudy Geofiz Inst AN SSSR

Urban Clim

Water Air Soil Pollut

Water Resour Res

Waterw Port Coast Ocean Eng
Weather

Weather Forecast

Wind Energy

Wind Eng

Z Agnew Math Mech

 

Abbreviations

ALL: Acute lymphoblastic leukemia; AML: Cute myeloid leukemia; MCMC: Markov chain Monte Carlo; PC: Principal

components PC; SRTM: NASA's shuttle radar topography mission

Acknowledgements

The authors thank Peter McCullagh for his insightful comments and suggestions on an early version of this paper. The
authors are grateful to the Editor-in-Chief, the Associate Editor and anonymous reviewers for their constructive
comments and suggestions which led to remarkable improvement of the paper.

Authors’ contributions

Hsin-Hsiung Huang wrote the draft of the manuscript, developed the algorithms, and conducted the experiments. Jie
Yang proposed the methods and the initial algorithms. Both authors read and approved the final manuscript.

Authors’ information

Hsin-Hsiung Huang, Ph.D., is an Associate Professor in the Department of Statistics and Data Science at the University of
Central Florida. Jie Yang, Ph.D., is an Associate Professor in the Department of Mathematics, Statistics, and Computer

Science at the University of Illinois at Chicago.

Funding

National Science Foundation grants (DMS-1924792, DMS-1924859), the LAS Award for Faculty of Science at the
University of Illinois at Chicago, and the In-House Award at the University of Central Florida.

Availability of data and materials

The datasets are from simulation and the UCI Machine Learning Repository and are available as per JSDA policy.

Competing interests
The authors declare that they have no competing interests.

Author details

'Department of Statistics and Data Science, University of Central Florida, Orlando, USA. 7Department of Mathematics,
Statistics, and Computer Science, University of Illinois at Chicago, Chicago, USA.

Received: 20 May 2020 Accepted: 5 October 2020
Published online: 28 October 2020
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 23 of 24

References

Banfield, J. D., Raftery, A. E.: Model-based Gaussian and non Gaussian Clustering. Biometrics. 49, 803-821 (1993)

Begelfor, E., Werman, M.: Affine Invariance Revisited. In: Proceedings of the 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pp. 2087-2094, (2006)

Bell, E. T.: Exponential polynomials. Ann. Math. 35, 258-277 (1934)

Blei, D., Jordan, M.: Variational inference for Dirichlet process mixtures. Bayesian Anal. 1, 121-144 (2006)

Brubaker. S.C., Vempala, S.: Isotropic PCA and affine-invariant clustering. In: Forty Ninth Annual IEEE Symposium on
Foundations of Computer Science, (2008)

Chaloner, K.: A Bayesian approach to the estimation of variance components in the unbalanced one-way random-effects
model. Technometrics. 29, 323-337 (1987)

Chang, J., Fisher, J. W.: Parallel sampling of DP mixture models using sub-clusters splits. ln: NIPS'13 Proceedings of the
26th International Conference on Neural Information Processing Systems, pp. 620-628, (2013)

Chib, S., Greenberg, E.: Understanding the Metropolis-Hastings Algorithm. Am. Stat. 49(4), 327-335 (1995)

Crane, H.: The ubiquitous Ewens sampling formula. Stat. Sci. 31, 1-19 (2016)

Dahl, D. B.: Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models
(2005). Technical Report, Department of Statistics, Texas A&M University

Dempster, A. P.: Covariance selection. Biometrics. 28(1), 157-175 (1972)

Dua, D., Graff, C: UCI Machine Learning Repository. University of California, School of Information and Computer Science,
Irvine (2019). http://archive.ics.uci.edu/ml|

Dudoit, S., Fridlyand, J., Soeed, T. P.: Comparison of discrimination methods for the classification of tumors using gene
expression data. J. Am. Stat. Assoc. 97(457), 77-87 (2002)

Ewens, W. J.: The sampling theory of selectively neutral alleles. Theor. Popul. Biol. 3, 87-112 (1972)

Fisher, R. A.. The use of multiple measurements in taxonomic problems. Ann. Eugenics. 7, 179-188 (1936)

Fitzgibbon, A., Zisserman, A.: On Affine Invariant Clustering and Automatic Cast Listing in Movies. In: European
Conference on Computer Vision 2002, pp. 304-320, (2002)

Fraley, C., Raftery, A. E.: How many clusters? Which clustering methods? Answers via model-based cluster analysis.
Comput. J. 41, 578-588 (1998)

Fraley, C., Raftery, A. E.: Model-based clustering, discriminant analysis, and density estimation. J. Am. Stat. Assoc. 97,
611-631 (2002)

Fraley, C., Raftery, A. E.: Bayesian regularization for normal mixture estimation and model-based clustering. J. Classif. 24,
155-181 (2007)

Gamerman, D.: Efficient Sampling from the Posterior Distribution in Generalized Linear Models. Stat. Comput. 7, 57-68
(1997)

Garcia-Escudero, L. A., Gordaliza, A., Matran, C., Mayo-lscar, A.: A review of robust clustering methods. ADAC. 4, 89-109
(2010)

Geman, S., Geman, D.: Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans.
Pattern Anal. Mach. Intell. 6(6), 721-741 (1984)

Gilks, W. R., Richardson, S., Spiegelhalter, D. J.: Markov Chain Monte Carlo in Practice. Chapman & Hall, New York (1996)

Gnanadesikan, R., Kettenring, J. R.: Robust Estimates, Residuals, and Outlier Detection with Multiresponse Data.
Biometrics. 28(1), 81-124 (1972)

Golub, T.R., Slonim, D. K., Tamayo, P., Huardm, C., Gaasenbeek, M., Mesirov, J. P., Coller, H., Loh, M. L., Downing, J. R.,
Caligiuri, M. A., Bloomfield, C. D., Lander, E. S.: Molecular Classification of Cancer: Class Discovery and Class Prediction
by Gene Expression Monitoring. Science. 286, 531-537 (1999)

Golub, H. G., Van Loan, C. F.: Matrix Computations. 4th edition. Johns Hopkins University Press, Baltimore (201 3)

Hastings, W. K.: Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika. 57(1), 97-109
(1970)

Isaacson, D. L., Madsen, R. W.: Markov Chains. Wiley, New York (1976)

Jain, A. K., Dubes, R. C.: Algorithms for clustering data. Prentice Hall, Upper Saddle River (1988)

Jarvis, A., Reuter, H. |, Nelson, A., Guevara, E.: JHole-filled seamless SRTM data V4, International Centre for Tropical
Agriculture (CIAT) (2008). http://srtm.csi.cgiar.org

Jolliffe, |. T.: Principal Component Analysis (1986)

Kaul, M.: Building Accurate 3D Spatial Networks to Enable Next Generation Intelligent Transportation Systems. In:
Proceedings of International Conference on Mobile Data Management (IEEE MDM), Vol 1., pp. 137-146. Milan, Italy,
(2013)

Kumar, M., Orlin, J. B.: Scale-invariant clustering with minimum volume ellipsoids. Comput. Oper. Res. 35, 1017-1029 (2008)

Lee, H., Yoo, J.-H., Park, D.: Data clustering method using a modified Gaussian kernel metric and kernel PCA. ETRI J. 36(3),
333-342 (2014)

MacEachern, S. N.: Estimating normal means with a conjugatestyle Dirichlet process prior. Commun. Stat. Simul. Comput.
23, 727-741 (1994)

MacQueen, J. B.: Some Methods for Classification and Analysis of Multivariate Observations. In: Proceedings of 5th Berkeley
Symposium on Mathematical Statistics and Probability, pp. 281-297. University of California Press, Berkeley, (1967)
Mahalanobis, P. C.: On the Generalized Distance in Statistics. In: Proceedings of the National Institute of Sciences of India,

(1936)

McCullagh, P.: Marginal likelihood for parallel series. Bernoulli. 14(3), 593-603 (2008)

McCullagh, P., Yang, J.: Stochastic classification models. In: Proceedings of the International Congress of Mathematicians,
vol. Ill, pp. 669-686, Madrid, (2006)

McCullagh, P., Yang, J.: How many clusters? Bayesian Anal. 3, 101-120 (2008)

McLachlan, G., Peel, D.: Finite Mixture Models. Wiley, New York (2000)

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., Teller, E.: Equation of state calculations by fast computing
machines. J. Chem. Phys. 21, 1087-1092 (1953)
Huang and Yang Journal of Statistical Distributions and Applications (2020) 7:10 Page 24 of 24

Neal, R. M.: Markov chain sampling methods for Dirichlet process mixture models. J. Comput. Graph. Stat. 9, 249-265
(2000)

Ng, A., Jordan, M., Weiss, Y.: On spectral clustering: Analysis and an algorithm. Adv. Neural Inform. Process. Syst. 14,
849-856 (2001)

Ozawa, K.: A stratificational overlapping cluster scheme. Pattern Recognit. 18, 279-286 (1985)

Pitman, J.. Combinatorial Stochastic Processes. (621). In: Ecole d’Ete de Probabilites de Saint-Flour XXXII-2002. Dept.
Statistics, U.C. Berkeley, (2002). Lecture notes for St. Flour course

Robert, C. P., Casella, G.: Introducing Monte Carlo Methods with R. Springer, (2010)

Roberts, G., Rosenthal, J.: Optimal Scaling for Various Metropolis-Hastings Algorithms. Stat. Sci. 16(4), 351-367 (2001)

Shioda R., Tuncel, L.: Clustering via minimum volume ellipsoids. Comput. Optim. Appl. 37, 247-295 (2007)

Stein, C.: Estimation of a covariance matrix. Reitz Lecture, IMS-ASA Annual Meeting in 1975 (1975)

Vogt, J. E., Prabhakaran, S., Fuchs, T. J., Roth, V.: The translation-invariant Wishart-Dirichlet process for clustering distance
data. Proceedings of the 27th International Conference on Machine Learning, 1111-1118, (2010)

Ward, J. H.: Hierarchical Grouping to Optimize an Objective Function. J. Am. Stat. Assoc. 58, 236-244 (1963)

Publisher’s Note

Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

 

Submit your manuscript to a SpringerOpen®
journal and benefit from:

> Convenient online submission

> Rigorous peer review

> Open access: articles freely available online
> High visibility within the field

> Retaining the copyright to your article

Submit your next manuscript at > springeropen.com

 

 

 
