np} Quantum Information

ARTICLE OPEN

www.nature.com/npjqi

® Check for updates

A neural network oracle for quantum nonlocality problems

in networks

Tamas Krivachy'™, Yu Cai(@®', Daniel Cavalcanti*, Arash Tavakoli@®*, Nicolas Gisin@' and Nicolas Brunner’

Characterizing quantum nonlocality in networks is a challenging, but important problem. Using quantum sources one can achieve
distributions which are unattainable classically. A key point in investigations is to decide whether an observed probability
distribution can be reproduced using only classical resources. This causal inference task is challenging even for simple networks,
both analytically and using standard numerical techniques. We propose to use neural networks as numerical tools to overcome
these challenges, by learning the classical strategies required to reproduce a distribution. As such, a neural network acts as an
oracle for an observed behavior, demonstrating that it is classical if it can be learned. We apply our method to several examples in
the triangle configuration. After demonstrating that the method is consistent with previously known results, we give solid evidence
that a quantum distribution recently proposed by Gisin is indeed nonlocal as conjectured. Finally we examine the genuinely
nonlocal distribution recently presented by Renou et al., and, guided by the findings of the neural network, conjecture nonlocality
in a new range of parameters in these distributions. The method allows us to get an estimate on the noise robustness of all

examined distributions.

npj Quantum Information (2020)6:70; https://doi.org/10.1038/s41534-020-00305-x

INTRODUCTION

The possibility of creating stronger than classical correlations
between distant parties has deep implications for both the
foundations and applications of quantum theory. These ideas
have been initiated by Bell', with subsequesnt research leading to
the theory of Bell nonlocality”. In the Bell scenario multiple parties
jointly share a single classical or quantum source, often referred to
as local and nonlocal sources, respectively. Recently, interest in
more decentralized causal structures, in which several indepen-
dent sources are shared among the parties over a network, has
been on the rise* °. Contrary to the Bell scenario, in even slightly
more complex networks the boundary between local and nonlocal
correlations becomes nonlinear and the local set non-convex,
greatly perplexing rigorous analysis. Though some progress has
been made’~7°, we still lack a robust set of tools to investigate
generic networks from an analytic and numerical perspective.

Here, we explore the use of machine learning in these
problems. In particular we tackle the membership problem for
causal structures, i.e., given a network and a distribution over the
observed outputs, we must decide whether it could have been
produced by using exclusively local resources. We encode the
causal structure into a neural network and ask the network to
reproduce the target distribution. By doing so, we approximate
the question “does a local causal model exist?” with “is a local
causal model learnable?”. Neural networks have proven to be
useful ansatze for generic nonlinear functions in terms of
expressivity, ease of learning and robustness, both in- and outside
the domain of physical sciences****. Machine learning has also
been used in the study of nonlocality?”*°. However, while the
techniques of ref. °° can only suggest if a distribution is local or
nonlocal, the method employed here is more generative in spirit
and provides a certificate that a distribution is local once it is
learned, by actually constructing the local model.

In our approach we exploit that both causal structures and
feedforward neural networks have their information flow deter-
mined by a directed acyclic graph. For any given distribution over
observed variables and an ansatz causal structure, we train a
neural network which respects that causal structure to reproduce
the target distribution, as in Fig. 1. This is equivalent to having a
neural network learn the local responses of the parties to their
inputs. With this constraint, if the target distribution is inside the
local set, then a sufficiently expressive neural network should be
able to learn the appropriate response functions and reproduce it.
For distributions outside the local set, we should see that the
machine can not approximate the given target. This gives us a
criterion for deciding whether a target distribution is inside the
local set or not. In particular, if a given distribution is truly outside
the local set, then by adding noise in a physically relevant way we
should see a clear transition in the machine’s behavior when
entering the set of local correlations.

We explore the strength of this method by examining a
notorious causal structure, the so-called “triangle” network, i.e., the
causal structure in Fig. 1. The triangle configuration is among the
simplest tripartite networks, yet it poses immense challenges
theoretically and numerically. We use the triangle with quaternary
outcomes as a test-bed for our neural network oracle. After
checking for the consistency of our method with known results,
we examine the so-called Elegant distribution, proposed in ref. *',
and the distribution proposed by Renou et al. in ref. 7°. Our
method gives solid evidence that the Elegant distribution is
outside the local set, as originally conjectured. The family of
distributions proposed by Renou et al. was shown to be nonlocal
in a certain regime of parameters. When examining the full range
of parameters we not only recover the nonlocality in the already
known regime, but also get a conjecture of nonlocality from the
machine in another range of the parameters. Finally, we use our

 

‘Department of Applied Physics, University of Geneva, CH-1211 Geneva, Switzerland. *ICFO, The Institute of Photonic Sciences, 08860 Castelldefels, Barcelona, Spain. *Dyson
School of Design Engineering, Imperial College London, London SW7 2AZ, UK. “email: tamas.krivachy@gmail.com

Published in partnership with The University of New South Wales

np} nature partner

journals
np}

T. Krivachy et al.

 

(a)

 

a

OF —o
Y B
Fig. 1 Triangle network and its neural network encoding. a

Triangle network configuration. b Neural network which reproduces
distributions compatible with the triangle configuration.

method to get estimates of the noise robustness of these nonlocal
distributions, and to gain insight into the learned strategies.

RESULTS
Encoding causal structures into neural networks

The methods developed in this work are in principle applicable to
any causal structure. Here, we demonstrate how to encode a
network nonlocality configuration into a neural network on the
highly nontrivial example of the triangle network with quaternary
outputs and no inputs. In this scenario three sources, a, B, y, send
information through either a classical or a quantum channel to
three parties, Alice, Bob, and Charlie. Flow of information is
constrained such that the sources are independent from each
other, and each one only sends information to two parties of the
three, as depicted in Fig. 1. Alice, Bob, and Charlie process their
inputs with arbitrary local response functions, and they each
output a number a, b, c € {0, 1, 2, 3}, respectively. Under the
assumption that each source is independent and _ identically
distributed from round to round, and that the local response
functions are fixed (though possibly stochastic), such a scenario is
well characterized by the probability distribution p(abc) over the
random variables of the outputs.

If quantum channels are permitted from the sources to the
parties then the set of distributions is larger than that achievable
classically. Due to the nonlocal nature of quantum theory, these
correlations are often referred to as nonlocal ones, as opposed to
local behaviors arising from only using classical channels. In the
classical case, the scenario is equivalent to a causal structure,
otherwise known as a Bayesian network*~”>.

For the classical setup we can assume without loss of generality
that the sources each send a random variable drawn from a
uniform distribution on the continuous interval between 0 and 1
(any other distribution can be reabsorbed by the parties’ response
functions, e.g., via the inverse transform sampling method). Given
the network constraint, the probability distribution over the
parties’ outputs can be written as

1
p(abe) = / _dadBay pa(alBy)p(blva)Pe(claB). (1)

npj Quantum Information (2020) 70

where the conditional probability p,y(x| - , -) is the response
function of party X.

We now construct a neural network which is able to
approximate a distribution of the form (1). We use a feedforward
neural network, since it is described by a directed acyclic graph,
similarly to a causal structure** °**. This allows for a seamless
transfer from the causal structure to the neural network model. On
a practical level, we represent each party's response function by a
fully connected multilayer perceptron, one of the simplest artificial
neural network architectures**. In our case, the inputs to the three
perceptrons are the hidden variables, i.e., uniformly drawn
random numbers a, B, y. So as to respect the communication
constraints of the triangle, inputs are routed to the three
perceptrons in a restricted manner, as shown in Fig. 1. The
outputs are the conditional probabilities conditioned on the
respective inputs, pa(alBy), pp(blya), and pc(claB), ie. three
normalized vectors, each of length 4. This restructuring can also
be viewed as having one large, not fully connected multilayer
perceptron, outputting the three probability vectors pa(a|By),
Pp(blya), pc(claB) for a given input a, B, y. Due to the restricted
architecture, the output conditional probabilities will obey the
causal network constraints, i.e., by construction only local models
can be generated by such a neural network.

We evaluate the neural network for Npatch values of a, B, y in
order to approximate the joint probability distribution (1) with a
Monte Carlo approximation,

N batch

» Pa(Q|B,y;) Pp (bly ;ai)Pc(c|aiB;). (2)

abc) =
Pl ) Nbatch i=1

 

Note that before summing over the batch, we take the Cartesian
product of the conditional probability vectors. In our implementa-
tion each of these three conditional probability functions is
modeled by a multilayer perceptron, with rectified linear or
tangent hyperbolic activations, except at the last layer, where we
have a softmax layer to impose normalization. Note, however, that
any feedforward network can be used to model these conditional
probabilities. The loss function can be any differentiable measure
of discrepancy between the target distribution p, and the neural
network's output py, such as the Kullback—Leibler divergence of
one relative to the other, namely

=)‘ p,(abc)log (ee ae) (3)

abc

In order to train the neural network we synthetically generate
uniform random numbers for the hidden variables, the inputs. We
then adjust the weights of the network after evaluating the loss
function on a minibatch of size Npatch, using conventional neural
network optimization methods**. The minibatch size is chosen
arbitrarily and can be increased in order to increase the neural
network's precision. For the triangle with quaternary outputs an
Npatch Of several thousands is typically satisfactory.

By encoding the causal structure in a neural network like this,
we can train the neural network to try to reproduce a given target
distribution. The procedure generalizes in a_ straight-forward
manner to any causal structure, and is thus in principle applicable
to any quantum nonlocality network problem. We provide specific
code online for the triangle configuration, as well as for the
standard Bell scenario, which has inputs as well (see Section “Code
availability”). After finishing this work we realized that related
ideas have been investigated in causal inference, though in a
different context, where network architectures and weights are
simultaneously optimized to reproduce a given target distribution
over continuous outputs, as opposed to discrete ones examined
here’. In addition, due to the strict constraint of having a single
fixed causal structure we evaluate results differently, by examining
transitions in compatibility with the causal structure at hand, as we
will soon demonstrate.

Published in partnership with The University of New South Wales
Evaluating the output of the neural network

Given a target distribution p,, the neural network provides an
explicit model for a distribution py, which is, according to the
machine, the closest local distribution to p,. The distribution pay is
guaranteed to be from the local set by construction. When can we
confidently deduce that the target distribution is local (i.e., if we
see P; ~ Pm), Or nonlocal (p;# py)? At first sight the question is
difficult, since the neural network will almost never exactly
reproduce the target distribution since py is evaluated by
sampling the model a finite number of times, and additionally
the learning techniques do not guarantee convergence to the
global optimum. A first approach could be to define some
confidence level for the similarity between py and p,. This would,
however, be somewhat arbitrary, and would give only limited
insight into the problem. A central notion in this work is to search
for qualitative changes in the machine’s behavior when transition-
ing from the local set to the nonlocal one. We believe this to be
much more robust and informative for deciding nonlocality than a
confidence level approach.

In order to find such a “phase transition”, we typically define a
family of target distributions p,(v) by taking a distribution which is
believed to be nonlocal and by adding some noise controlled by
the parameter v, with p,(v=0) being the completely noisy (local)
distribution and p,(v = 1) being the noiseless, “most nonlocal” one.
By adding noise in a physically meaningful way we guarantee that
at some parameter value, v*, we will enter the local set and stay in
it for v<v*. For each noisy target distribution we retrain the neural
network and obtain a family of learned distributions pyy(v) (see Fig.
2 for an illustration). Observing a qualitative change in the
machine’s performance at some point is an indication of
traversing the local set’s boundary. In this work we extract
information from the learned model through

@ the distance between the target and the learned distribution,

d(p,Pm) =, S— [p,(abc) — py(abc)]’,

abc

@ the learned distributions py(v), in particular by examining the
local response functions of Alice, Bob, and Charlie.

Observing a clear liftoff of the distance dy(v) := d(p,(v), pu(v)) at
some point is a signal that we are leaving the local set. Somewhat
surprisingly, we can deduce even more from the distance dy(v).
Though the shape of the local set and the threshold value v* are
unknown, in some cases, under mild assumptions, we can extract
from dy(v) not only v*, but also the angle at which the curve p,(v)
exits the local set, and in addition gain confidence in the proper

(a)

v=1
d,sin(®)

    
   
 

     

Local set Local set

Fig.2 Geometric considerations when evaluating neural network
results. Visualization of target distributions p,(v) leaving the local set
at an angle @ for a a generic noisy distribution and for b the specific
case of the Fritz distribution with a two-qubit Werner state shared
between Alice and Bob. The gray dots depict the target distribu-
tions, while the red dots depict the distributions which the neural
network would find. In the generic case we additionally depict the
distance d, :=d(p,(v), p,(v*)) introduced in Eq. (4), for the special case
of v= 1, as well as d, sin @. Given an estimate for v*, the distance d,
can be evaluated analytically, which (for an appropriate 0) allows us
to compare d, sin@ with the distance that the machine perceives.

Published in partnership with The University of New South Wales

T. Krivachy et al.

 

functioning of the algorithm. To do this, let us first assume that the
local set is flat near p,(v*) and that p,(v) is a straight curve. Then
the true distance from the local set is

dv) = 0 ifv <v (4)
w= d(p,(v),p,(v*)) sin(@) — ifv>v*,

where @ is the angle between the curve p,(v) and the local set’s
hyperplane (see Fig. 2 for an illustration). In the more general
setting Eq. (4) is still approximately correct even for v>v’, if p,(v)
is almost straight and the local set is almost flat near v*. We
denote this analytic approximation of the true distance form the
local set as d(v). We use Eq. (4) to calculate it but keep in mind
that it is only an approximation. After having trained the
machine, we fit d(v) to dy(v) by adjusting v* and @. Finding a
good fit of the two distance functions gives us strong evidence
that indeed the curve p,(v) exits the local set at v’ at an angle @,
where the hat is used to signify the obtained estimates.
Acquiring such a fit gives us more confidence in the machine
since now we do not just observe a qualitative phase transition,
but we can also model it quantitatively with just two free
parameters, v* and @.

In addition, we get information out of the learned model by
looking at the local responses of Alice, Bob and Charlie. Recall that
the shared random variables, the sources, are uniformly dis-
tributed, hence the response functions encode the whole
problem. We can visualize, for example, Bob’s response function
pa(bla, y) by sampling several thousand values of {a, y} € [0, 1]. In
order to capture the stochastic nature of the responses, for each
pair a, y we sample from pa,(bla, y) 30 times and color-code the
results b € {red, blue, green, and yellow}. By scatter plotting these
points with a finite opacity we gain an impression of the response
function, such as in Fig. 3b.

These figures are already interesting in themselves and can
guide us towards analytic guesses of the ideal response
functions. However, they can also be used to verify our results
in some special cases. For example, if @ = 90° and the local set is
sufficiently flat, then the response functions should be the same
for all v>v*, as it is in Fig. 3b. On the other hand if 8 < 90° then
we are in a scenario similar to that of panel (a) in Fig. 2 and the
response functions should differ for different values of v. Finally,
note that for any target distribution there is no unique closest
local response function, so the visualized response functions
could vary greatly. As a result, in order to have visually more
similar response functions and to smooth the results, after
running the algorithm for the full range of v, for each v we check
whether the models at other v’ values perform better for p,(v)
(after allowing for small adjustments) and update the model for v
accordingly.

Fritz distribution

In order to benchmark the method, we first consider the quantum
distribution proposed by Fritz’, which can be viewed as a Bell
scenario wrapped into the triangle topology, and its nonlocality is
thus well understood. Alice and Bob share a singlet, i.e.
lY) ng = |W) = 3 (01) —|10)), while Bob and Charlie share

either a maximally entangled or a classically correlated state with
Charlie, such as Pgc = 5 (|00) (00| + |11)(11|) and similarly for Pac.
Alice measures the shared state with Charlie in the computational
basis and, depending on this random bit, she measures either the
Pauli X or Z observable. Bob does the same with his shared state
with Charlie and measures either As or ei They then both
output the measurement result and the bit which they used to
decide the measurement. Charlie measures both sources in the
computational basis and announces the two bits. As a noise
model we introduce a finite visibility for the singlet shared by Alice

npj Quantum Information (2020) 70
T. Krivachy et al.

 

Analytic v*

Machine

0.04

©
oO
Ww

distance

0.02

0.01

em es seers ey ys

0.00

 

0.0 0.5
Qa

1.0 0.0 0.5 1.0
a

Fig. 3 Fritz distribution results. a Plot of the distance perceived by the machine, dy(v) and the analytic distance d(v) for 7* = 1/,/2 and
€@ = 90°. b Visualization of response functions of Bob as a function of a, y for v=0, 0.44, 0.71, 1, from top left to bottom right, respectively.

Note how the responses for v>v* are the same.

(a)

Visibility: analytic V* =0.8,6=50°
--- Visibility: machine

Detector eff.: analytic V* = 0.86, 6 = 60°
--- Detector eff.: machine

0.05

0.04

0.03

distance

0.02

>
”
3
©
2
Oo
oc
Q

0.01
outcome

ee aaa

0.0 0.2 0.4 0.6 0.8

Vv

 

(b) 1.0
~- 0.5
0.0

1.0

~- 0.5
0.0

0.0 0.5 1.0 0.0 0.5 1.0
a Qa

Fig. 4 Elegant distribution results. a Comparison of the distance perceived by the machine, dy(v) and the analytic distance d(v). Both
visibility and detector efficiency model results are shown. Inset: The target (gray) and learned (red) distributions visualized by plotting the
probability of each of the 64 possible outcomes, for detector efficiency v= 1 and v= 0.84. Note that for v= 0.84 most gray dots are almost
fully covered by the corresponding red dots. b Responses of Charlie illustrated as a function of a, 6. Detector efficiency values (top left to

bottom right): v= 0.5, 0.72, 0.76, 1.

and Bob, thus we examine a Werner state,

a I

pv) =v w+ (I —v)z, (5)

where I/4 denotes the maximally mixed state of two qubits. For

such a state we expect to find a local model below the threshold
of v* = FF

In Fig. 3a we plot the learned dy(v) and analytic d(v) distances

discussed previously, for @ = 90° and v* = He The coincidence of

the two curves is already good evidence that the machine finds
the closest local distributions to the target distributions. Upon
examining the response functions of Alice, Bob and Charlie, in
Fig. 3b, we see that they do not change above v", which means
that the machine finds the same distributions for target
distributions outside the local set. This is in line with our

npj Quantum Information (2020) 70

expectations. Due to the connection with the standard Bell
scenario (where the local set is actually a polytope), we believe the
curve p,(v) exits the local set perpendicularly, as it is depicted on
panel (b) in Fig. 2. These results confirm that our algorithm
functions well.

Elegant distribution

Next we turn our attention to a more demanding distribution, as
neither its locality or nonlocality has been proven to date, and is
lacking a proper numerical analysis due to the intractability of
conventional optimization over local models (see the “Discussion”
section). Compared to the Fritz distribution, it is also more native
to the triangle structure, as it combines entangled states and
entangled measurements. We examine the Elegant distribution,
which is conjectured in ref. °' to be outside the local set. The three

Published in partnership with The University of New South Wales
0.010

a
0.008 a

Qa
e
Qa

S 0.006

Cc

oO

a

os outcome

0.004

0.002

0.000
0.5 0.6 0.7 0.8 0.9
u2

 

T. Krivachy et al.

np)

 

Visibility: analytic V* = 0.89, 6 =6°
--- Visibility: machine

Detector eff.: analytic v* =0.91,0=6°
=-=-- Detector eff.: machine

1.0 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

Fig. 5 Renou et al. distribution results. a The distance perceived by the machine, dy, as a function of u*, with no added noise. Inset:
The target (gray) and learned (red) distributions visualized by plotting the probability of each of the 64 possible outcomes, for u? = 0.63 and
u* = 0.85. These u’ values approximately correspond to the two peaks in the scan. Note that most gray dots are almost fully covered by the
corresponding red dots. This is an excellent confirmation that searching for transitions is more informative than using predefined thresholds
for locality, as the learned and target distribution are nearly indistinguishable for a human eye for u* = 0.85, even though we know that the

target is nonlocal here. b Noise scans, i.e., the analytic d(v v) (see Eq. (4)) and the learned dy(v), for the target distribution of u* = 0.85, with v

being visibility (green and blue) or detector efficiency (orange and red).

parties share singlets and each perform a measurement on their
two qubits, the eigenstates of which are

3 3-1)
jo) = f3Jm,—m) + 5): (6)

where the |m;) are the pure qubit states with unit length Bloch
vectors pointing at the four vertices of the tetrahedron for j = 1, 2,
3, 4, and |—m,) are the same for the inverted tetrahedron.

We examine two noise models—one at the sources and one
at the detectors. First we introduce a visibility to the singlets
such that all three shared quantum states have the form (5).
Second, we examine detector noise, in which each detector
defaults independently with probability 1—v and gives a
random output as a result. This is equivalent to adding white
noise to the quantum measurements performed by the parties,
i.e., the positive operator-valued measure elements are
Mj = v|®;)(j| + (1 =v) 5

For both noise models we see a transition in the distance dy(v),
depicted in Fig. 4a, giving us strong evidence that the conjectured
distribution is indeed nonlocal. Through this examination we gain
insight into the noise robustness of the Elegant distribution as
well. It seems that for visibilities above Vv" ~ 0.80, or for detector
efficiency above ”* = 0.86, the distribution is still nonlocal. The
curves exit the local set at approximately 0 ~ 50° and @ = 60°,
respectively. Note that for both distribution families, by looking at
the unit tangent vector, one can analytically verify that the curves
are almost straight for values of v above the observed threshold.
This gives us even more confidence that it is legitimate to use the
analytic distance d(v) as a reference (see Eq. (4)). In Fig. 4b, we
illustrate how the response function of Charlie changes when
adding detector noise. It is peculiar how the machine often prefers
horizontal and vertical separations of the latent variable space,
with very clean, deterministic responses, similarly to how we
would do it intuitively, especially for noiseless target distributions.

 

Renou et al. distribution

The authors of ref. *° recently introduced the first distribution in
the triangle scenario which is not directly inspired by the Bell
scenario and is proven to be nonlocal. To generate the distribution
take all three shared states to be the entangled states
lpr) = J (00) + |11)). Each party performs the same measure-

ment, characterized by a single parameter u € a I), with

eigenstates |01),|10),u|00) + V1 — u?|11), V1 — u2|00) — u]11).

Published in partnership with The University of New South Wales

The authors prove that for u2,.<u2<1 this distribution is
nonlocal, where u2,,.. & 0.785 and also show that there exist local
models for u* € {0.5, u2,,., 1}. Though they argue that there must
be some noise tolerance of the distribution, they lack a proper
estimation of it.

First we examine these distributions as a function of u*, without
any added noise. The results are depicted in Fig. 5a. To start with,
note how the distances are numerically much smaller than in the
previous examples, i.e., the machine finds distributions which are
extremely close to the targets. See the inset in Fig. 5a for examples
which exhibit how close the learned distributions are to the
targets even for the points which have large distances (u? = 0.63,
0.85). We observe, consistently with analytic findings, that for

ur 4. <u? <1, the machine finds a nonzero distance from the local
set. It also recovers the local models at u* € {0.5,u2.,,, 1}, with
minor difficulties around Umax; Astonishingly, the machine finds
that for some values of 0.5 <u? <u2,., the distance from the local
set is even larger than in the provenly nonlocal regime. This is a
somewhat surprising finding, as one might naively assume that
between 0.5 and u2... distributions are local, especially when one
looks at the nonlocality proof used in the other regime. However,
this is not what the machine finds. Instead it gives us a nontrivial
conjecture about nonlocality in a new range of parameters u*.
Though extracting precise boundaries in terms of u* for the new
nonlocal regime would be difficult from the results in Fig. 5a
alone, they strongly suggest that there is some nonlocality in this
regime.

Finally, we have a look at the noise robustness of the
distribution with u*=0.85, which is approximately the most
distant distribution from the local set, from within the provenly
nonlocal regime. For the detector efficiency and visibility noise
models we recover V° = 0.91, ”° = 0.89 respectively, and 0 = 6°
for both. Note that these estimates are much more crude than
those obtained for the Elegant distributions, primarily due to the
target distributions being so much closer to the local set and
the neural network getting stuck in local optima. This increases
the variations in independent runs of the learning algorithm.
E.g. in Fig. 5a, at u* = 0.85 the distance is about 0.0034, whereas in
Fig. 5b, in an independent run, the distance for this same
point (v=1) is around 0.0055. The absolute difference is small,
however the relative changes can have an impact in extracting
noise thresholds. Given that the local set is so close to the target
distributions (exemplified in the inset in Fig. 5a), it is easily

npj Quantum Information (2020) 70
np}

T. Krivachy et al.

 

possible that the noise tolerance is smaller than that

obtained here.

DISCUSSION

Let us contrast the presented method to known techniques.
Among analytic methods, the technique of inflation, introduced
in'’, is known to converge toward a perfect oracle'*. This method
consists of a hierarchy of linear programs, which can be
implemented on a computer. However, so far it could not witness
nonlocality of the Elegant distribution, and can only witness the
nonlocality of the distribution presented by Renou et al. in the
provenly nonlocal regime by a small margin, making it difficult to
extract noise tolerance results. Another approach that has been
taken is to consider the entropy of the distribution, which makes
the independence condition a linear one”'’. However, such
techniques are typically relatively weak at detecting nonlocality
and are not particularly useful for examining the distributions
studied here.

The analytical difficulties in proving nonlocality and extracting
noise robustness motivate us to look at numerical techniques. The
standard method for tackling the membership problem _ in
network nonlocality is nonlinear numerical optimization. For a
fixed number of possible outputs per party, o, without loss of
generality one can take the hidden variables to be discrete with a
finite alphabet size, and the response functions to be determinis-
tic. In fact the cardinality of the hidden variables can be upper
bounded as a function of o'°. Specifically for the triangle this
upper bound is o° — o. This results in a straightforward optimiza-
tion over the probabilities of each hidden variable symbol and the
deterministic responses of the observers, giving 3(0° — 0 — 1)
continuous parameters and a discrete configuration space of size
12(0? — 0) to optimize over jointly. Note that this is a non-convex
optimization space, making it a terribly difficult task. For binary
outputs, i.e., o= 2, this means only 15 continuous variables and a
discrete configuration space of 432 possibilities, and is feasible.
However, already for the case of quaternary outputs, o = 4, this
optimization is a computational nightmare on standard CPUs with
a looming 177 continuous parameters and a discrete configura-
tion space of size 43,200. Even when constraining the response
functions to be the same for the three parties, pb, = Pg = Pc, and
the latent variables to have the same distributions, Ppa = pg = Py,
the problem becomes intractable around a hidden variable
cardinality of 8, which is still much lower than the current upper
bound of 60 that needs to be examined. Standard numerical
optimization tools quickly become infeasible even for the triangle
configuration—not to mention larger networks!

The causal modeling and Bayesian network communities
examine scenarios similar to those relevant for quantum informa-
tion’?°*. The core of both lines of research are directed acyclic
graphs and probability distributions generated by them. In these
communities there exist methods for this so-called “structure
recovery” or “structure learning” task. However, these methods are
either not applicable to our particular scenarios or are also
approximate learning methods which make many assumptions on
the hidden variables, including that the hidden variables are
discrete. Hence, even if these learning methods are quicker than
Standard optimization for current scenarios of interest, they will
run into the scaling problem of the latent variable cardinality.

The method demonstrated in this paper attacks the problem
from a different angle. It relaxes both the discrete hidden variable
and deterministic response function assumptions which are made
by the numerical methods mentioned previously. The complexity
of the problem now boils down to the response function of the
observers —each of which is represented by a feedforward neural
network. Though our method is an approximate one, one can
increase its precision by increasing the size of the neural network,
the number of samples we sum over (Npatch) and the amount of

npj Quantum Information (2020) 70

time provided for learning. Due to universal approximation
theorems we are guaranteed to be able to represent essentially
any function with arbitrary precision®°**®. For the first two
distributions examined here we find that there is no significant
change in the learned distributions after increasing the neural
network's width and depth above some moderate level, i.e., we
have reached a plateau in performance. Regarding the Elegant
distribution, for example, we used depth 5 and width 30 per party.
However, we did not do a rigorous analysis in the minimum
required size, perhaps an even smaller network would have
worked. We were satisfied with the current complexity, since
getting a local model for a single target distribution takes a few
minutes on a standard computer, using a mini-batch size of Npatch
~8000. For the Renou et al. distribution there is still space for
improvement in terms of the neural network architecture and the
training procedure. The question of what the minimal required
complexity of the response functions for a given target distribu-
tion is, is in itself interesting enough for a separate study, and can
become a tedious task since the amount of time that the machine
needs to learn typically increases with network size.

We have demonstrated how, by adding noise to a distribution
and examining a family of distributions with the neural network,
we can deduce information about the membership problem. For a
single target distribution the machine finds only an upper bound
to the distance from the local set. By examining families of target
distributions, however, we get a robust signature of nonlocality
due to the clear transitions in the distance function, which match
very well with the approximately expected distances.

In conclusion, we provide a method for testing whether a
distribution is classically reproducible over a directed acyclic
graph, relying on a fundamental connection to neural networks.
The simple, yet the effective method can be used for arbitrary
causal structures, even in cases where current analytic tools are
unavailable and numerical methods are futile, allowing quantum
information scientist to test their conjectured quantum, or post-
quantum, distributions to see whether they are locally reprodu-
cible or not, hopefully paving the way to a deeper understanding
of quantum nonlocality in networks.

To illustrate the relevance of the method, we have applied it to
two open problems, giving firm numerical evidence that the
Elegant distribution is nonlocal on the triangle network, and
getting estimates for the noise robustness of both the Elegant and
the Renou et al. distribution, under physically relevant noise
models. Additionally, we conjecture nonlocality in a surprising
range of the Renou et al. distribution. Our work motivates finding
proofs of the nonlocality for both these distributions.

The obtained results on nonlocality are insightful and convin-
cing, but are nonetheless only numerical evidence. Examining
whether a certificate of nonlocality can be obtained from machine
learning techniques would be an interesting further research
direction. In particular, it would be fascinating if a machine could
derive, or at least give a good guess for a (nonlinear) Bell-type
inequality which is violated by the Elegant or Renou et al.
distribution. In general, seeing what insight can be gained about
the boundary of the local set from machine learning would be
interesting. Perhaps a step in this direction would be to
understand better what the machine learned, for example by
somehow extracting an interpretable model from the neural
network analytically, instead of by sampling from it. A different
direction for further research would be to apply similar ideas to
networks with quantum sources, allowing a machine to learn
quantum strategies for some target distributions. Moreover, the
method introduced here could be straightforwardly applied to
other networks, such as the Bell scenario with more inputs,
outputs and/or parties, or to bilocality’.

Published in partnership with The University of New South Wales
DATA AVAILABILITY

The authors declare that the data supporting the findings of this study are available
within the paper.

CODE AVAILABILITY

Our implementation of the method for the triangle network and for the two-party
Bell scenario can be found at https://www.github.com/tkrivachy/neural-network-for-
nonlocality-in-networks.

Received: 13 March 2020; Accepted: 15 July 2020;
Published online: 21 August 2020

REFERENCES

1. Bell, J. S. On the Einstein Podolsky Rosen paradox. Phys. Phys. Fiz. 1, 195-200
(1964).

2. Brunner, N., Cavalcanti, D., Pironio, S., Scarani, V. & Wehner, S. Bell nonlocality. Rev.
Mod. Phys. 86, 419-478 (2014).

3. Branciard, C., Gisin, N. & Pironio, S. Characterizing the nonlocal correlations cre-
ated via entanglement swapping. Phys. Rev. Lett. 104, 170401 (2010).

4. Branciard, C., Rosset, D., Gisin, N. & Pironio, S. Bilocal versus nonbilocal cor-
relations in entanglement-swapping experiments. Phys. Rev. A 85, 032119
(2012).

5. Fritz, T. Beyond Bells theorem: correlation scenarios. New J. Phys. 14, 103001
(2012).

6. Pusey, M. F. Viewpoint: quantum correlations take a new shape. Physics 12, 106
(2019).

7. Henson, J., Lal, R. & Pusey, M. F. Theory-independent limits on correlations from
generalized Bayesian networks. New J. Phys. 16, 113043 (2014).

8. Tavakoli, A., Skrzypczyk, P., Cavalcanti, D. & Acin, A. Nonlocal correlations in the
star-network configuration. Phys. Rev. A 90, 062109 (2014).

9. Chaves, R., Luft, L. & Gross, D. Causal structures from entropic information:
geometry and novel scenarios. New J. Phys. 16, 043001 (2014).

10. Chaves, R. et al. Inferring latent structures via information inequalities. Proc. 30th
Conference on Uncertainty in Artificial Intelligence, 112-121, Quebec, Canada
(2014).

11. Chaves, R., Majenz, C. & Gross, D. Information-theoretic implications of quantum
causal structures. Nat. Commun. 6, 5766 (2015).

12. Rosset, D. et al. Nonlinear Bell inequalities tailored for quantum networks. Phys.
Rev. Lett. 116, 010403 (2016).

13. Chaves, R. Polynomial Bell inequalities. Phys. Rev. Lett. 116, 010402 (2016).

14. Navascues, M. & Wolfe, E. The Inflation Technique Completely Solves the Causal
Compatibility Problem. Preprintat http://arxiv.org/abs/1707.06476 (2017).

15. Rosset, D., Gisin, N. & Wolfe, E. Universal bound on the cardinality of local hidden
variables in networks. Quantum Inform. Comp. 18, 0910-0926 (2017).

16. Fraser, T. C. & Wolfe, E. Causal compatibility inequalities admitting quantum
violations in the triangle structure. Phys. Rev. A 98, 022113 (2018).

17. Weilenmann, M. & Colbeck, R. Non-Shannon inequalities in the entropy vector
approach to causal structures. Quantum 2, 57 (2018).

18. Luo, M.-X. Computationally efficient nonlinear Bell inequalities for quantum
networks. Phys. Rev. Lett. 120, 140402 (2018).

19. Wolfe, E., Spekkens, R. W. & Fritz, T. The inflation technique for causal inference
with latent variables. J. Causal Inference 7, 20170020 (2019).

20. Renou, M.-O. et al. Genuine quantum nonlocality in the triangle network. Phys.
Rev. Lett. 123, 140401 (2019).

21. Gisin, N. et al. Constraints on nonlocality in networks from no-signaling and
independence. Nat. Commun. 11, 2378 (2020).

22. Renou, M.-O. et al. Limits on correlations in networks for quantum and no-
signaling resources. Phys. Rev. Lett. 123, 070403 (2019).

23. Pozas-Kerstjens, A. et al. Bounding the sets of classical and quantum correlations
in networks. Phys. Rev. Lett. 123, 140503 (2019).

24. Melko, R. G., Carleo, G., Carrasquilla, J. & Cirac, J. |. Restricted Boltzmann machines
in quantum physics. Nat. Phys. 15, 887-892 (2019).

25. Iten, R., Metger, T., Wilming, H., del Rio, L. & Renner, R. Discovering physical
concepts with neural networks. Phys. Rev. Lett. 124, 010508 (2020).

26. Melnikov, A. A. et al. Active learning machine learns to create new quantum
experiments. Proc. Nat. Acad. Sci. USA 115, 1221-1226 (2018).

27. van Nieuwenburg, E. P. L., Liu, Y.-H. & Huber, S. D. Learning phase transitions by
confusion. Nat. Phys. 13, 435-439 (2017).

Published in partnership with The University of New South Wales

T. Krivachy et al.

Np}

 

28. Carrasquilla, J. & Melko, R. G. Machine learning phases of matter. Nat. Phys. 13,
431-434 (2017).

29. Deng, D.-L. Machine learning detection of bell nonlocality in quantum many-
body systems. Phys. Rev. Lett. 120, 240402 (2018).

30. Canabarro, A., Brito, S. & Chaves, R. Machine learning nonlocal correlations. Phys.
Rev. Lett. 122, 200401 (2019).

31. Gisin, N. Entanglement 25 years after quantum teleportation: testing joint mea-
surements in quantum networks. Entropy 21, 325 (2019).

32. Pearl, J. Causality: Models Reasoning and Inference (Cambridge University Press,
2000).

33. Koller, D. & Friedman, N. Probabilistic Graphical Models: Principles and Techniques
(MIT Press, 2009).

34. Goodfellow, |., Bengio, Y. & Courville, A. Deep Learning (MIT Press, 2016).

35. Goudet, O. et al. Learning functional causal models with generative neural net-
works. Explainable and Interpretable Models in Computer Vision and Machine
Learning, 39-80 (Springer International Publishing, Cham, 2018).

36. Cybenko, G. Approximation by superpositions of a sigmoidal function. Math.
Control! Signal. 2, 303-314 (1989).

37. Hornik, K. Approximation capabilities of multilayer feedforward networks. Neural
Netw. 4, 251-257 (1991).

38. Lu, Z., Pu, H., Wang, F., Hu, Z. & Wang, L. The expressive power of neural networks:
a view from the width. Adv. Neural Inf. Process. Syst. 30 6231-6239 (2017).

ACKNOWLEDGEMENTS

The authors thank Raban Iten, Tony Metger, Elisa Baumer, Marc-Olivier Renou, Elie
Wolfe, and Askery Canabarro for discussions. T.K., Y.C., N.G., and N.B. acknowledge
financial support from the Swiss National Science Foundation (Starting grant DIAQ
and QSIT), and the European Research Council (ERC MEC). D.C. acknowledges support
from the Ramon y Cajal fellowship, Spanish MINECO (QIBEQI, Project No. FIS2016-
80773-P, and Severo Ochoa SEV-2015-0522) and Fundacidé Cellex, Generalitat de
Catalunya (SGR875 and CERCA Program). A.T. acknowledges financial support from
the UK Engineering and Physical Sciences Research Council (EPSRC DTP).

AUTHOR CONTRIBUTIONS

N.B. and T.K. had the idea to connect the nonlocality with neural networks. T.K.
developed the concept in detail, wrote and ran the code. A.T. and T.K. explored other
approaches to the problem which turned out to be less efficient and were not
included. All authors discussed and analyzed the results extensively and contributed
to writing the paper.

COMPETING INTERESTS

The authors declare no competing interests.

ADDITIONAL INFORMATION

Correspondence and requests for materials should be addressed to T.K.

Reprints and permission information is available at http://www.nature.com/
reprints

Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims
in published maps and institutional affiliations.

Open Access This article is licensed under a Creative Commons

7 Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you give
appropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this license, visit http://creativecommons.
org/licenses/by/4.0/.

© The Author(s) 2020

npj Quantum Information (2020) 70
