Algorithmica (2020) 82:3676-3706
https://doi.org/10.1007/s00453-020-00743-1

m)

Check for
updates

On the Benefits of Populations for the Exploitation Speed
of Standard Steady-State Genetic Algorithms

Dogan Corus'® - Pietro S. Oliveto’

Received: 21 October 2019 / Accepted: 23 June 2020 / Published online: 15 July 2020
© The Author(s) 2020

Abstract

It is generally accepted that populations are useful for the global exploration of
multi-modal optimisation problems. Indeed, several theoretical results are available
showing such advantages over single-trajectory search heuristics. In this paper we
provide evidence that evolving populations via crossover and mutation may also
benefit the optimisation time for hillclimbing unimodal functions. In particular, we
prove bounds on the expected runtime of the standard (uv + 1) GA for OneMax that
are lower than its unary black box complexity and decrease in the leading constant

with the population size up to vw = o(v log n), Our analysis suggests that the opti-

mal mutation strategy is to flip two bits most of the time. To achieve the results we
provide two interesting contributions to the theory of randomised search heuristics:
(1) A novel application of drift analysis which compares absorption times of differ-
ent Markov chains without defining an explicit potential function. (2) The inversion
of fundamental matrices to calculate the absorption times of the Markov chains. The
latter strategy was previously proposed in the literature but to the best of our knowl-
edge this is the first time is has been used to show non-trivial bounds on expected
runtimes.

Keywords Genetic algorithms - Crossover - Recombination - Population-based
algorithms - Theory - Runtime analysis

An extended abstract of this paper was published in [3].

b<] Dogan Corus
d.corus @ sheffield.ac.uk

Pietro S. Oliveto
p.oliveto @ sheffield.ac.uk

| The University of Sheffield, Sheffield, UK

Q) Springer
Algorithmica (2020) 82:3676-3706 3677

1 Introduction

Populations in evolutionary and genetic algorithms are considered crucial for the
effective global optimisation of multi-modal problems. For this to be the case,
the population should be sufficiently diverse such that it can explore multiple
regions of the search space at the same time [10]. Also, if the population has suf-
ficient diversity, then it considerably enhances the effectiveness of crossover for
escaping from local optima. Indeed the first proof that crossover can consider-
ably improve the performance of GAs relied on either enforcing diversity by not
allowing genotypic duplicates or by using unrealistically small crossover rates for
the so-called Jump function [14]. Since then, it has been shown several times that
crossover is useful to GAs using the same, or similar, diversity enhancing mecha-
nisms for a range of optimisation problems including shortest path problems [5],
vertex cover [23], colouring problems inspired by the Ising model [32] and com-
puting input output sequences in finite state machines [17].

These examples provide considerable evidence that, by enforcing the necessary
diversity, crossover makes GAs effective and often superior to applying muta-
tion alone. However, rarely it has been proven that the diversity mechanisms are
actually necessary for GAs, or to what extent they are beneficial to outperform
their mutation-only counterparts rather than being applied to simplify the analy-
sis. Recently, some light has been shed on the power of standard genetic algo-
rithms without diversity over the same algorithms using mutation alone. Dang
et al. showed that the plain (#7 + 1) GA is at least a linear factor faster than its
(u + 1) EA counterpart at escaping the local optimum of Jump [4]. Sutton showed
that the same algorithm with crossover if run sufficiently many times is a fixed
parameter tractable (FPT) algorithm for the closest string problem while without
crossover it is not [33]. Lengler provided an example of a class of unimodal func-
tions to highlight the robustness of the crossover based version with respect to the
mutation rate compared to the mutation-only version 1.e., the (uv + 1) GA its effi-
cient for any mutation rate c/n, c a constant, while the (“+ 1) EA requires expo-
nential time as soon as approx. c > 2.13 [19]. In all three examples the population
size has to be large enough for the results to hold, thus providing evidence of the
importance of populations in combination with crossover. A follow-up work by
Lengler and Zou also established that for some monotone functions increasing
the population size can result in superpolynomial runtimes when crossover is not
implemented [20].

Recombination has also been shown to be very helpful at exploitation if the
necessary diversity is enforced through some mechanism. In the (1+(A, 4)) GA
such diversity is achieved through large mutation rates. The algorithm can opti-
mise the well-known ONEMax function in o( logn) expected time with static off-
spring population sizes A [7], and in linear time with self-adaptive values of A [6].
Although using a recombination operator, the algorithm is still basically a single-
trajectory one (i.e., there is no population). More realistic steady-state GAs that
actually create offspring by recombining parents have also been analysed for ONE-
Max. Sudholt showed that (4+ A) GAs are twice as fast as their mutation-only

Q) Springer
3678 Algorithmica (2020) 82:3676-3706

version (i.e., no recombination) for ONEMAx if diversity is enforced artificially
1.e., genotype duplicates are preferred for deletion [31]. He proved a runtime of
(e/2)nInn + O(n) versus the enInn + O(n) function evaluations required by any
standard bit mutation-only evolutionary algorithm for ONEMAx and any other lin-
ear function [30, 35]. If offspring are identical to their parents it is not neces-
sary to evaluate the quality of their solution. When the unnecessary queries are
avoided, the expected runtime of the GA using artificial diversity from [31] is
bounded above by (1 + 0(1))0.850953n Inn [28]. Hence, it is faster than any unary
(i.e., mutation-only) unbiased! black-box search heuristic [16].

On one hand, the enforced artificiality in the last two results considerably sim-
plifies the analysis. On the other hand, the power of evolving populations for
effective optimisation cannot be appreciated. Since the required diversity to make
crossover effective is artificially enforced, the optimal population size is 2 and
larger populations provide no benefits. Corus and Oliveto showed that the stand-
ard (uw +1) GA without diversity is still faster for ONEMAx than mutation-only
ones by proving an upper bound on the runtime of (3/4)enInn + O(n) for any
3 < uw < o(logn/ loglogn) [2]. A result of enforcing the diversity in [31] was that
the best GA for the problem only used a population of size 2. However, even though
this artificiality was removed in [2], a population of size 3 was sufficient to get the
best upper bound on the runtime achievable with their analysis. Overall, their anal-
ysis does not indicate any tangible benefit towards using a population larger than
y= 3. Thus, rigorously showing that populations are beneficial for GAs in the
exploitation phase has proved to be a non-trivial task.

In this paper we provide a more precise analysis of the behaviour of the popula-
tion of the (w+ 1) GA for ONEMAx. We prove that the standard (4 + 1) GA with
H = 0(\/logn) is at least 60% faster than the same algorithm using only mutation.
We also prove that the GA is faster than any unary unbiased black-box search heu-
ristic if offspring with identical genotypes to their parents are not evaluated. More
importantly, our upper bounds on the expected runtime decrease with the popula-
tion size up to “w = o(1/logn), thus providing for the first time a natural example
where populations evolved via recombination and mutation optimise faster than
any unary unbiased search heuristic. The mutation rate that minimises our upper
bounds is approximately 1.4/n. With such rates all population sizes yw > 5 have an
expected runtime that is smaller than 1.675n In n(1 + o0(1)) in the standard case when
all offspring are evaluated. A recent analysis that proves that the expected runtime
of the (2+1) GA is at least 2.18417n Inn + O(n) for any mutation rate c/n, for any
c < 1.422 [27] combined with our previously proven upper bounds for yp > 3 [2],
have allowed the first rigorous proof that any population size greater than yw = 2
makes the (“+ 1) GA faster than the same algorithm using only 2 individuals (..e.,
populations are provably beneficial in the exploitation phase—for hillclimbing ONE-
Max). In this paper we provide guarantees of larger speed-ups. These guarantees
increase with the population size y.

' The probability of a bit being flipped by an unbiased operator is the same for each bit-position and bit
value.

Q) Springer
Algorithmica (2020) 82:3676-3706 3679

2 Problem Definition and Our Results
2.1 The Genetic Algorithm

The (4+ 1) GA is a standard steady-state GA which samples a single new solu-
tion at every generation [9, 29]. It keeps a population of the yw best solutions sam-
pled so far and at every iteration selects two solutions from the current population
uniformly at random with replacement as the parents. The recombination opera-
tor then picks building blocks from the parents to create the offspring solution.
For the case of pseudo-Boolean functions f : {0,1}” — R, the most frequently
used recombination operator 1s uniform crossover which picks the value of each
bit position 7 € [n] from one parent or the other uniformly at random (1.e., from
each parent with probability 1/2) [9]. Then, an unbiased unary variation operator,
which is called the mutation operator, is applied to the offspring solution before
it is added to the population. The most common mutation operator is standard bit
mutation which independently flips each bit of the offspring solution with some
probability c/n [35]. Finally, before moving to the next iteration, one of the solu-
tions with the worst fitness value is removed from the population. For the case
of maximisation the (w+ 1) GA is defined in Algorithm |. The runtime of Algo-
rithm | is the number of function evaluations until a solution which maximises
the function fis sampled for the first time. If every offspring is evaluated, then the
runtime is equal to the value of the variable tin Alg. 1 when the optimal solution
is sampled. However, if the fitness of offspring which are identical to their parents
is not evaluated, then the runtime is smaller than t. We will first analyze the for-
mer scheme and then adapt the result to the latter.

Algorithm 1: (u+1) GA [9,28]

1 P, + p individuals, uniformly at random from {0,1}";

2t—p;

3 repeat

4 Select x,y € P; uniformly at random with replacement ;

5 z< uniform crossover(x, y);

6 z «+ mutate(z);

7 Przt + PU {2};

8 Remove the element with lowest fitness from P;41, breaking ties at
random;

9 t<¢t+1,;

10 until optimum is found;

2.2 The Optimisation Problem

Given an unknown bitstring z € {0,1}”, OneMax,(x) := |{i € [n] | z; = x;}| returns
the number of bits on which a candidate solution x € {0, 1}” matches z [35]. W.l.o.g.
we will assume that the target string z of the OneMax, function to be identified is the
bitstring of all one-bits since all the operators in the (uv + 1) GA are invariant to the
bit-value (have no bias towards Os or Is).

Q) Springer
3680 Algorithmica (2020) 82:3676-3706

Leading constant

 

 

Fig.1 The leading constant for the (u + 1) GA which does not evaluate the fitness of copies versus the
population size with py = Q(1), py = QC), p. & 1 and py +p; + py = 1. The best leading constant
achievable by any unary unbiased algorithm is 1. Note that the vertical axis starts at 0.916

2.3 Our Results
In this paper we prove the following results.

Informal The expected runtime E[T] for the (u+ 1) GA with unbiased mutations
and population size 4 = o(1/logn) to optimise the ONEMAx function is

1. ELT] < (1 +o0(1))nInn- y, (4, po, P}, Pr), f offspring identical to their parents are
not evaluated and po, P;, and p, are respectively the probabilities that zero, one
or two bits are flipped, [Theorem 1, Sect. 3]

2. E[T]) <1 +o0())nInn- y,(u, c), if the quality of each offspring is evaluated and
standard bit mutation with rate c/n, c € O(1), is used, [Corollary 1, Sect. 3]

where y, and y, are decreasing functions of the population size wl.

The above two statements are very general as they provide upper bounds on
the expected runtime of the (u+ 1) GA for each value of the population size up
to w = o(\/logn) and any unbiased mutation operator. The leading constants y, and
Y> in Statements 1 and 2 are plotted respectively in Fig. | and in Fig. 3 for differ-
ent population sizes using the po, Pp), Pp» and c values which minimise the upper
bounds. The result is significant particularly for the following three reasons (in order
of increasing importance).

(1) The first statement shows how the genetic algorithm outperforms any unbi-
ased mutation-only heuristic since the best expected runtime achievable by any algo-
rithm belonging to such class is at least n Inn — cn + o(n) [8]. The leading constants
in the expected runtime for different population sizes using the po, p, and p, values

Q) Springer
Algorithmica (2020) 82:3676-3706 3681

 

 

 

 

e T T T T T
0.9990 | « |
e@
€ } ?
S 0.9985 + ° |
no e
Cc r e |
Oo
oO e
2 [ '
® 0.9980 + le 4
oO L ° |
e
bk me, 4
0.9975 + yee, |
L e. J
ee
Co.
To eee...
ott ec eee]
10 20 30 40 50

Fig.2 The leading constant when SBM is used without evaluating offspring duplicates versus the popu-
lation size. The best leading constant achievable by any unary unbiased algorithm is 1. Note that the
vertical axis starts at 0.9966

 

1.670 | ]
1.665 | |
1.660 | |

1.655 | ° |

Leading Constant
e

1.650 | e |

1.645 } Se, |

t Totes...
PP eee,

®ee6e

teaobE PETE EEE EEE TEEPE EET TEEPE ETT TEEPE EL TTL ttt ecco!
10 20 30 40 50

y

 

 

 

Fig.3 The leading constant when SBM is used and offspring duplicates are evaluated versus the popula-
tion size. For each yp value, the corresponding mutation rate minimising the upper bound is used. The
best mutation-only variant has a the leading constant of e © 2.71. Note that the vertical axis starts at
1.639

which minimise the upper bounds (i.e., local mutations) are plotted in Fig. 1 while
Fig. 2 shows the leading constants for the standard bit mutation rates c which mini-
mise them. We have assumed p, = | to obtain the leading constants in Fig. | since
the leading constants in the upper bounds improve as p, approaches | and we can
pick p, to be arbitrarily close to 1 (Figs. 3, 4). For the SBM variant, the mutation

D) Springer
3682 Algorithmica (2020) 82:3676-3706

0.440 |
0.435 f .°
0.430 f ?

0.425 | °

&2

0.420 }
0.415 f

0.410 | «

 

10 20 30 40 50
[Ll

Fig.4 The value of €, with respect to the population size uw > 7

0.14 -

Mutation rate

 

 

50 100 150 200
jl

Fig.5 The mutation rates minimising the upper bounds when SBM is used and the offspring duplicates
are not evaluated versus the population size. Note that the x-axis starts at a population size of “= 7

rates that minimise the upper bounds when the duplicates are not evaluated are plot-
ted for each y in Fig. 5. It can be appreciated how in both cases all unbiased muta-
tion-only heuristics are outperformed.

Given that the best expected runtime achievable by any search heuristic using
only standard bit mutation is (1 — o(1))en Inn [30, 35], the second statement shows
how by adding recombination a speed-up of at least 60% is achieved for the ONE-
Max problem for any population size up to vw = o(4/logn). The leading constants, in
this case where offspring are always evaluated, are provided in Fig. 3 and the muta-
tion rates that minimise the upper bound are plotted in Fig. 6.

Q) Springer
Algorithmica (2020) 82:3676-3706 3683

1.445

 

 

 

1.440 |
oO
aS)
@C L
1.435
cq L
Oo
= [
4 1.430
>
=
1.425 [
1.420 Ls L 1 L | ! ! ! ! | 1 1 1 \ | \ \ ! ! |
50 100 150 200
Ll

Fig.6 The mutation rates minimising the upper bounds when SBM is used and the offspring duplicates
are evaluated versus the population size. Note that the x-axis starts at a population size of 4“ = 7

(2) Very few results are available proving constants in the leading terms of the
expected runtime for randomised algorithms due to the considerable technical dif-
ficulties in deriving them. Exceptions exist such as the analyses of [8, 35] without
which our comparative results would not have been achievable. While such precise
results are gaining increasing importance in the theoretical computer science com-
munity, the available ones are related to more simple algorithms. This is the first
time similar results are achieved concerning a much more complicated to analyse
standard genetic algorithm using realistic population sizes and recombination.

(3) The preciseness of the analysis allows for the first time an appreciation of the
surprising importance of the population for optimising unimodal functions” as our
upper bounds on the expected runtime decrease as the population size increases. In
particular as the problem size increases, so does the optimal size of the population®
(the best known runtime available for the (u + 1) GA was of (1 + 0(1))3/4enInn
independent of the population size as long as it is greater than yw = 31.e., there were
no evident advantages in using a larger population [2]). This result is in contrast to
all previous analyses of simplified evolutionary algorithms for unimodal functions
where the algorithmic simplifications, made for the purpose of making the analy-
sis more accessible, caused the use of populations to be either ineffective or detri-
mental [28, 31, 34]. In particular, since a (2+1) GA has an expected runtime of at
least 2.18417n Inn for any mutation rate c < 1.422 [27], any population size greater
than 2 is provably faster (unless higher mutation rates turn out to be beneficial to
the (2+1) GA which is unlikely because the optimal mutation rate for the algorithm
within the range is approximately c = 1.2122). Our upper bound of uv = o(1/logn)

? Populations are traditionally thought to be useful for solving multi-modal problems.
> The population size that minimises the upper bound we obtain.

Q) Springer
3684 Algorithmica (2020) 82:3676-3706

is very close to the at most logarithmic population sizes typically recommended for
monotone functions to achieve asymptotically optimal runtimes [1, 34]. We conjec-
ture that the optimal population size is O((log n)'~°) for any constant e > 0, which
cannot be proven with our mathematical methods for technical reasons.

2.4 Proof Strategy

Our aim is to provide a precise analysis of the expected runtime of the (w+ 1) GA
for optimising ONEMAx with arbitrary problem size n. Deriving the exact transition
probabilities of the algorithm from all possible configurations of its population to all
others is prohibitive. We will instead devise a set of n Markov chains, one for each
improvement the algorithm has to make pessimistically to reach the global opti-
mum, which will be easier to analyse. Then we will prove that the Markov chains
are slower to reach their absorbing state than the (w+ 1) GA 1s in finding the cor-
responding improvement.

In essence, our proof strategy is: (1) to identify suitable Markov chains, (2) to
prove that the absorbing times of the Markov chains are larger than the expected
improving times of the actual algorithm and (3) to bound the absorbing times of
each Markov chain.

In particular, concerning point (2) we will first define a potential function which
monotonically increases with the number of copies of the genotype with most dupli-
cates in the population and then bound the expected change in the potential func-
tion at every iteration (1.e., the drift) from below. Using the maximum value of the
potential function and the minimum drift, we will bound the expected time until the
potential function value drops to its minimum value for the first time. This part of
the analysis is a novel application of drift analysis techniques [18, 36]. In particular,
rather than using an explicit distance function as traditionally occurs, we define the
potential function to be equal to the conditional expected absorption time of the cor-
responding states of each Markov chain.

Concerning point (3) of our proof strategy, we will calculate the absorbing times
of the Markov chains M! by identifying their fundamental matrices. This requires the
inversion of tridiagonal matrices. Similar matrix manipulation strategies to bound
the runtime of evolutionary algorithms have been previously suggested in the litera-
ture [11, 26]. However, all previous applications of the approach proved results that
could be trivially achieved via simpler standard methods such as the artificial fitness
levels method and random walks [13, 15]. To the best of our knowledge, this is the
first time that the power of this long abandoned approach has finally been shown by
proving non-trivial bounds on the expected runtime.

3 Main Result Statement

Our main result is the following theorem. The transition probabilities p;, for
(i,k) € [m}* and m := [y/2]are defined in Definition 1 (Sect. 4.1) and are depicted
in Fig. 9.

Q) Springer
Algorithmica (2020) 82:3676-3706 3685

Theorem 1 The expected runtime E[T] for the (u+1) GA with yu = o(v/logn)
using the uniform crossover and an unbiased mutation operator mutate(x) that flips i
bits with probability p; with py € (1) and p, € (1) to optimise the ONEMax func-
tion IS:

1. E[T] <d+o0(1))nIn NaH if the quality of each offspring is evaluated,
1Tr2 (itl)

2. E[T])<U+o0(1))nIn n— if the quality of offspring identical to their par-

POP? (ut)
ents is not evaluated; where,

a Pi-1i-2
© Pint + Pint i-2 + Pi-1i = Sin)
Pm-1,m-2
Cm *

Pin-1,m + Pim-1,m-2

The recombination operator of the GA is effective only if individuals with dif-
ferent genotypes are picked as parents (i.e., recombination cannot produce any
improvements if two identical individuals are recombined). However, more often
than not, the population of the (4+ 1) GA consists only of copies of a single indi-
vidual. When diversity is created via mutation (1.e., a new genotype is added to the
population), it either quickly leads to an improvement or it quickly disappears. The
bound on the runtime reflects this behaviour as it is simply a waiting time until one
of two event happens; either the current individual is mutated to a better one or
diversity emerges and leads to an improvement before it is lost.

The &, term in the runtime is the conditional probability that once diversity is
created by mutation, it will be lost before reaching the next fitness level (an improve-
ment). Naturally, (1 — €,) is the probability that a successful crossover will occur
before losing diversity. The (1 — &,) factor increases with the population size yw and
it is independent from the mutation operator used by the algorithms, which implies
that larger populations have a higher capacity to maintain diversity long enough to
be exploited by the recombination operator. In Fig. 4, how &, changes with respect to
the population size pis depicted.

Note that setting p, := 0 for alli > 2 minimises the upper bound on the expected
runtime in the second statement of Theorem | and reduces the bound to:

E[T] < (1 + 0(0))nInn(p, + p,)/ (p, +p) ) Now, we can see the critical role

that €*(u4) = (1 — €,)u/(u + 1) plays in the expected runtime. For any population size
which yields €*(u) < 1/2, flipping only one bit per mutation becomes advantageous.
The best upper bound achievable from the above expression is then (1 + o(1))nInn
by assigning an arbitrarily small constant to po, p; = 1 — po (which implies that
P> = 0). As long as py = £2(1), when an improvement occurs, the superior genotype
takes over the population quickly relative to the time between improvements. Since
there are only one-bit flips, the crossover operator becomes virtually useless (..e.,
crossover requires a Hamming distance of 2 between parents to create an improving
offspring) and the resulting algorithm is a stochastic local search algorithm with a

Q) Springer
3686 Algorithmica (2020) 82:3676-3706

population. However, when &*() > 1/2, which is the case for all « > 5, setting p, as
large as possible provides the best upper bound. For yw > 5, by setting p, := e/2 and
Po =: €/2 to an arbitrarily small constant e and setting p, = 1 — e, we get the upper
bound E[T] < (1 + o(1))(1 + e)nInn(u + 1)/(2u(1 - €,)), which is plotted for dif-
ferent population sizes in Fig. 1. We have assumed p, = 1 to obtain the leading con-
stants since the leading constants improve as p, approaches | and we can pick p, to
be arbitrarily close to 1.

A direct corollary to the main result is the upper bound for the classical (w+ 1
) GA commonly used in evolutionary computation which applies standard bit muta-
tion with mutation rate c/n for which py = (1 — 0(1))/e°, p, = 1 — o(1))c/e® and
py = (1 — o(l))c?/(2e°).

Corollary 1 Let €, be as defined in Theorem |. The expected runtime E[T] for the
(u + 1) GA with np = o(1/logn) using standard bit mutation with mutation rate c/n,
c = O(1) to optimise the ONEMax function is:

1. E[T] <d+o0(1))nIn na if the quality of each offspring is evaluated,
. ytd oe °

2. E[T] <1 +o0(1))nIn no if the quality of offspring identical to their par-
c+—

 

(u+l SP
ents is not evaluated.

By calculating €*(u) := (1 — &,)u/(u + 1) for fixed values of ~ we can deter-
mine values of c (i.e., mutation rate) which minimise the leading constant of the
runtime bound in Corollary 1. In Fig. 3 we plot the leading constants in the first
statement, minimised by picking the appropriate c values (1.e., the ones that mini-
mise the upper bounds) for w ranging from 5 to 50. All the presented values improve
upon the upper bound on the runtime of 1.96n Inn given in [2] for any yw > 3 and
Hu = o(logn/loglogn). All the upper bounds are smaller than 1.7n Inn and clearly
decrease with the population size, signifying an at least 60% increase in speed com-
pared to the en In n(1 — o(1)) lower bound for the same algorithm without the recom-
bination operator [30, 35].

Considering the leading constants in the second statement of Corollary 1, for all
population sizes larger than 5, the upper bound for the optimal mutation rate is smaller
than the theoretical lower bound on the runtime of unary unbiased black-box algo-
rithms. For population sizes of 3 and 4, €* = 1/3 and the expression to be minimised
is (1 — e~°)e°/(c +c? /3). For c > 0, this expression has no minimum and is always
larger than one. Thus, at least with our technique, a population of size 5 or larger is nec-
essary to prove that the (uv + 1) GA outperforms stochastic local search and any other
unary unbiased optimisation heuristic. The mutation rates which minimise the upper
bounds on the expected runtime are provided in Fig. 6 for the variant which evaluates
the duplicate offspring and in Fig. 5 for the variant which does not. It can be appreci-
ated that a mutation rate of approximately 1.44/n provides better upper bounds than the
standard recommended rate of 1/n, which instead is known to be optimal for the (1+1)
EA on ONEMax [30, 35]. For comparison the leading constants of the upper bounds for

Q) Springer
Algorithmica (2020) 82:3676-3706 3687

1.125

 

» e
So
g
S e
a °
© e
1.120 °
oO
= e
so e
@ e
o e
4H .
e
1.115 ’ °.
ee,
See,
ee
To eee...
\ ! | ! 1 1 \ \ \ 1 1 1 1 1 \ 1 naae eee
10 20 30 40 50

[Ll

Fig.7 The leading constant of the upper bound on the runtime when the standard mutation rate 1/n is
used and the offspring duplicates are not evaluated versus the population size

1.785

1.780

1.775 ?

1.770 t

Leading constant

1.765 °.

1.760

 

Fig.8 The leading constant of the upper bound on the runtime when the standard mutation rate 1/n is
used and the offspring duplicates are evaluated versus the population size

the (uv + 1) GA with the standard mutation rate of 1/n are depicted in Fig. 7 for the vari-
ant which does not evaluate duplicate offspring and in Fig. 8 for the variant which does.

D) Springer
3688 Algorithmica (2020) 82:3676-3706

4 Analysis

Our main aim is to provide an upper bound on the expected runtime (£[7]) of the
(u + 1) GA defined in Algorithm | to maximise the ONEMax function. We will pro-
vide upper bounds on the expected value E[T’], where T’ is the time until an indi-
vidual with at least 7 + 1 one-bits is sampled for the first time given that the initial
population consists of individuals with j one-bits (1.e., the population is at level 7).
Then, by summing up the values of E[T’] and the expected times for the whole pop-
ulation to reach j + 1 one-bits for 7 € {1...,,2— 1} we achieve a valid upper bound
on the expected runtime of the (uv + 1) GA. Similarly to the analysis in [2], we will
pessimistically assume that the algorithm is initialised with all individuals having
just zero-bits, and that throughout the optimisation process at most one extra one-bit
is discovered at a time.

Although we divide the optimisation process into phases 7! according to the fit-
ness of the population, the widely used artificial fitness levels (AFL) method is not
adequate to observe the potential advantages of the recombination operator. The
AFL method relies on the minimum probability of improvement given a fitness level
and uses its reciprocal to bound the expected time to leave the level from above.
Using this minimal probability can give tight results when the probability of
improvement is approximately the same for all possible population configurations at
a given level. However, when uniform crossover is implemented, the improvement
probability does not only depend on the current fitness but it is also heavily depend-
ent on the diversity of the population. Moreover, the above-mentioned diversity is
not a scalar value but consists of all pairwise Hamming distances among the indi-

viduals in the population, 1.e., a 5 dimensional variable. In order to avoid work-

ing with a multi-dimensional state space we will define the diversity
D, € {0,..., [5] — 1}, as the number of non-majority individuals in the population
at time ft, where the majority genotype is the genotype that has the most copies in the
population. While the improvement probability increases with D, and can be lower
bounded in a way that captures the contribution of crossover, it is no longer straight-
forward to convert these improvement probabilities to expected runtime values as it
is done in the artificial fitness level method. The levels of diversity are different
compared to the levels of fitness since while an elitist algorithm never returns to a
fitness level it leaves, diversity levels can be visited multiple times and the total time
to traverse all the diversity levels cannot be seperated into independent phases.

Fortunately, since the (4 + 1) GA adds a single solution to the population in every
generation, the diversity can change by at most one unless the algorithm finds an
improvement. Markov chains of similar structure where the state transition is lim-
ited to either moving one state forward or backward, staying put, or moving to the
absorbing state can be analysed with existing tools in the literature. However, when
we use the possible values for D, to represent the state of the algorithm we cannot
build an explicit Markov chain because, as we discussed above, the transition prob-
abilities would not only depend on D, but depend on the precise Hamming distances
among individuals. This prevents us from having a single transition probability
between any two states.

Q) Springer
Algorithmica (2020) 82:3676-3706 3689

PO,m

   

1— po,1 — PO,m

1 — P1,0 — P1,2 — Pi,m

1 — Pm—1,m—2 — Pm—1,m

Fig.9 The topology of Markov Chain M/

In order to overcome the above-mentioned difficulties in analysing the (uv + 1) GA
on OnEMax, we will devise a Markov chain M’ for each j € {0,...,2—1} with
States that correspond to different levels of diversity in the population. Then, we
will analyse the expected absorbing time EF [T ] initialised at its state S, by applying
Markov chain specific methods (See Definition 1 and Fig. 9). Afterwards, we will
prove that M/ is in expectation slower at reaching its absorbing state than the (u + 1
) GA is at finding an improvement given an initial population at level 7. In particular,
we will define a non-negative potential function on the domain of all possible con-
figurations of a population at level 7 or above. Our potential function will be mono-
tonically decreasing with the diversity D,. Moreover, we will assign to the potential
function a value of zero for all populations with at least one solution which has more
than j one-bits. Then, we will bound the expected change in the potential function at
every iteration (1.e., the drift) from below. Using the maximum value of the poten-
tial function and the minimum drift, we will derive a bound on the expected time
until an improvement is found starting from a population at level 7 with no diversity
(i.e., all the solutions in the population are identical). While this upper bound will
not provide an explicit runtime as a function of the problem size, it will allow us to
conclude that (1 + o(1))E [To / ] > E[T’]. Thus, all that remains will be to bound the
expected absorbing time of Mi initialised at state Si). We will obtain this bound by
identifying the fundamental matrix of M’. After establishing that the inverse of the
fundamental matrix is a strongly diagonally dominant tridiagonal matrix, we will
make use of existing tools in the literature for inverting such matrices and complete
our proof.

4.1 Markov Chain Definition

In this subsection we present the Markov chains which we will use to analyse the
behaviour of the (4 + 1) GA. We should emphasise again that these Markov chains
do not represent the exact behaviour of the algorithm and we will later prove that
their expected absorbing times can be used to bound the expected time for the actual
algorithm to improve the best fitness from j € MY — 1] to 7+ 1. Each Markov chain
M! has m := [y/2] transient states (Sp, Si,...,S/,_,) and one absorbing state (s',)
with the topology depicted in Fig. 9. The Swot state S, represents a population

Q) Springer
3690 Algorithmica (2020) 82:3676-3706

with at least one improved individual with more than j one-bits. The states s! repre-
sent the amount of diversity in the population. In particular, fori € {0,1,...,m— 2},
the state My denotes populations where all the individuals have j one-bits and all but i
of them share the same genotype. More diverse populations at fitness level 7, which
have at most y — (m — 1) identical individuals, are all denoted by the last transient
state s _, We have picked m := [4/2] because increasing the number of minor-
ity individuals above half of the population cannot be accomplished by copying
the existing individuals and these higher diversity levels have significantly smaller
probabilities to be observed. Compared to the analysis presented in [2] that used
Markov chains of only three states (i.e., no diversity, diversity, increase in one-bits),
M’ allows to control the diversity in the population more precisely, thus to show that
larger populations are beneficial to the optimisation process.

Definition ] Let M’ be a Markov chain with m := [y/2] transient states

(S, S\, St S’ _,). and one absorbing state (S, ) with transition probabilities Piz from

state S to state Ss as follows:

 

Mw 2y(n-Jj)po Po, i= (n-j)pP;
us I) ne 7 no”
— ae

Po. +=

ifi > 0,

 

Pim - S205
, anf (2) #2 poiuatinal)
12 ul aie wow 4u
(5) oe ieee

Pio: _

Hu Hu

l if—-ilp-i
i =I 4)+2- —
Pit. + =n ( ) mi n (4 /4) ui ot)

1

 

 

 

 

 

Ul
ifm—-1>i>1,
2 . \2 .
roveen(() ayer (5) ie)
ifm>i> 1,
Pnm*=1, Poo *= 1— Pot — Pom
Pm-1tym—1 *= 1 = Pin-tm—2 — Pm-1ym>

Pig *=1-Dii-t — Pitt ~ Pim =MO<i<m—1,
Piz +=9 otherwise.

Now, we will point out the important characteristics of these transition probabili-
ties. The transition probabilities, p;,, are set to be equal to provable bounds on the
probabilities of the (4 + 1) GA with a population consisting of solutions with 7 bits
of gaining/losing diversity (p;;,/p;;-)) and sampling a solution with more than j
one-bits (p;,,). In particular, upper bounds are used for the transition probabilities

Q) Springer
Algorithmica (2020) 82:3676-3706 3691

Pp; where i < k and lower bounds are used for the transition probabilities p;, where
i > k which will be shown to be precise up to a (1 + 0(1)) factor in Lemma 3. Note
that greater diversity corresponds to a higher probability of two distinct individu-
als with j one-bits being selected as parents and improved via recombination (..e.,
Pim Monotonically increases with i and recombination is ineffective if i = 0 and the
improvement probability po,, is simply the probability of increasing the number of
one-bits by mutation only. Thus, po ,, = O(n —j)/n) while p;,, = O@i(u — 1)/ HW’)
when i > 0. The first forward transition probability po, denotes the probability of
the mutation operator of creating a different individual with 7 one-bits and the selec-
tion operator removing one of the majority individuals from the population. The
other transition probabilities, p;;,; and p;;_, bound the probability that a copy of
the minority solution or the majority solution is added to the population and that a
member of the other species (minority/majority) is removed in the subsequent selec-
tion phase. All transition probabilities except po, and po,, are independent of j and
referred to in the theorem statements without specifying /.

4.2 Validity of the Markov Chain Model

In this subsection we will present how we establish that M/ is a pessimistic represen-
tation of Algorithm | initialised with a population of y identical individuals at level
j. In particular, we will first show that E [T)], the expected absorbing time starting
from state So: is larger than E[T’]. Consequently, this result will allow us to bound
the leading constant of the expected runtime of (4 + 1) GA from above by the lead-

ing constant of Yeo E[T,] in the following lemma:

Lemma 1 Let E[T] be the expected runtime until the (u+1) GA with
H = o(logn/ log log n) and {po, P|, P2} = QU) optimises the ONEMAx function and
let

E [T’ ] (or E[T;] wherever j is prespecified) be the expected absorbing time of Mi
starting from state S.. Then, E[T] < o(nlogn) + (1 + o(1)) Yeo E(T)).

The sum Ye E [T)] excludes the fitness evaluations between when the first solu-
tion at level 7 is created and when the whole population is at level j or better. The
expectation of the number of omitted evaluations is O(u log yw) for each fitness level
using standard take-over arguments [2]. The restriction of the population size to
H = o(logn/ log log n) allows to bound the expectation of the total number of omit-
ted evaluations by the o(n logn) term which does not affect the leading constant

We use drift analysis [18], a frequently used tool in the runtime analysis of evolu-
tionary algorithms, to prove the above result. In particular, we will make use of the
additive drift theorem from [12], using its formulation from [18].

Theorem 2 (Additive Drift Theorem [12, 18]) Let (X,),59 be a sequence of non-
negative random variables with a finite state space S C Rj such that 0 € S. Let
T :=inf{t > 0 |X, = 0}. Jf there exists 6 > 0 such that for all s € S \ {0} and for
all t > 0,

Q) Springer
3692 Algorithmica (2020) 82:3676-3706

E[A(s)] := ELX, — X41, | X, = s] = 6,
then

airy < HX

We will start by defining a potential function over the state space of Alg. | that
maps a state of the population to the conditional expected absorbing time of M’ that
is initialised at the corresponding state in the Markov chain. Note that the drift anal-
ysis will be applied to the stochastic process that represents the actual algorithm,
not on the Markov chain itself. The Markov chain will only be used to define the
potential function that will be used for the drift analysis. The minimum of the poten-
tial function will correspond to the state of Algorithm | which has sampled a solu-
tion with more than j one-bits and we will explicitly prove that the maximum of
the potential function is E [T)]. Then, we will show that the drift, 1.e., the expected
decrease in the potential function value in a single time unit (from time ¢ tof + 1), is
at least 1 — o(1). Using the maximum value of the potential function and the mini-
mum drift, we will bound the expected time until the algorithm leaves fitness level /,
by the absorbing time of the Markov chain M’.

We will define our potential function over the domain of all possible population
diversities at level 7. We will refer to the genotype with the most copies in the popu-
lation as the majority genotype and recall that the diversity, D, € {0,...,4—1}, ofa
population P, is defined as the number of non-majority individuals in the population.

Definition 2 The potential function value for level j, g/ (or g wherever j is prespeci-
fied), is defined as follows:

E{T!, ] 0<D,<fu/2]-1
g(D,) :=g t= EIT, jo] [u/2] -1<D,<y-1
0 dx EP, s.t. OneMax(x) > j,

where E [T! ] (denoted as E[T;] wherever j is prespecified) is the expected absorbing
time of the Markov chain M’ starting from state S’.

The absorbing state of the Markov chain corresponds to a population with
at least one individual with more than j one-bits, thus having potential function
value (and expected absorbing time) equal to zero. The state So corresponds to
a population with no diversity. This potential function is quite similar to the so-
called canonical potential function [18], which maps each state S, to the condi-
tional expected runtime given that the process is initialised at S;. The drift of the
canonical potential function is always equal to 1 due to the law of total expecta-
tion. The potential function g/ similarly maps states to conditional expectations,
however the referred conditional expectations belong to a different but related
stochastic process, the Markov chain M’. Moreover, when D, > 0, the exact tran-
sition probabilities for the algorithm cannot be expressed as a function that only

Q) Springer
Algorithmica (2020) 82:3676-3706 3693

depends on D,. When diversity is present in the population the transition prob-
abilities depend also on the pairwise Hamming distance between all pairs of indi-
viduals in the population. However, bounds on the transition probabilities that
hold for any configuration of population for a given D, can be obtained and these
bounds are used to design the Markov chain M’, which in turn provides us with
the conditional expectations that define our potential function. The following
lemma formalises that the expected absorbing time gets larger as the initial states
get further away from [4/2] — 1. The main observation behind this result is that
the probability of directly jumping to the absorbing state increases as the process
approaches the end of the chain. This property implies that the expected absorb-
ing time from state S constitutes an upper bound for the potential function g¢’.

Lemma 2 Let E [T! ] be the expected absorbing time of the Markov chain Mi condi-
tional on its initial state being S’. Then, Vj € {0,...,n—1}, E[T/]SE[T!_,] for all
1<i<[p/2landg < E[T,|Vt > 0.

Proof We are interested in max (E[7)], E[T,], ...,E[T,,,]) since these are the val-
ues that the potential function g can have. According to the transition probabilities
in Definition 1, P;4) _ 2 Pim for all i. Using this observation and the law of total
expectation we will show that not only max (E[7T 9], E[T,],..., E[T,,]) = E[To] but
also E[T,_,] = E[T,] for all i. First, we will prove that E[T,,_,] = E[T,,,_,] by con-
tradiction. Then, we will prove by induction that E[T,_,] > E[T;,] for all i. For this
induction we will use E[T,,_,] > E[T,,_,;] as our basic step and we will prove by
contradiction that if for all A > i, E[T,_,] = E[T,] holds, then E[T,_,] = E[7;] must
also hold.

If we use the law of total expectation for the absorbing time starting from state
0 <i<™m, we obtain:

E[T;) =pii41(ElTi41] + D + pi FU] + )
+ Pim + Cl = Pitt — Pitt — Pim) (ELT;] + 1).

This equation can be rearranged as follows:
L = py (ElT) — ELT) + 0,-1.4) -— EIT) + Pi ETI.
For the special case of i = m — 1, we have:
L = Ppt mE nD) + Pin—tm—2 ELD ni) — ELT n_2)).

If we introduce the allegedly contradictory assumption E[T,,_.] < E[T,,,_;], the
above equation implies:

] > ]

m—2,m Pm-1,m

> E[T,,_)] > ElT,-»] => > E[T,,_9].

 

 

l > Pmt mE nD =

 

m—2,m

Q) Springer
3694 Algorithmica (2020) 82:3676-3706

Given that -— > E[T,] and E[T,,,] > E[T;] the law of total expectation for i implies:

l= Pix (FLT — E[T,,.,]) + p; (ELT; ] — E|T,_,]) + Pj ELT; ]

1 < pi) (ElT] — ELT 4) + pi (FIZ) — EIT) + 1

0 < p;,_-\(ELT;) — ELT_,)
Loa

i—l,m i,m

E[T,_,] < EIT] =>

 

> E[T,] > E[T,_)].

Thus, the allegedly contradictory claim E[T,,_,] > E[T,,,_.] induces over i such that
it implies E[T,] > E[T,] and 1 Pow > E[T,]. We can now write the total law of
expectation fori = 0.

1 = po (Elo) — EIT, )) + PomElT]

1 < po (E[To] - EIT, ]) + 1

0 < po (ElTo] — ELT) ))

O> Po
The last statement is a contradiction since a probability cannot be negative. This
contradiction proves the initial claim E[T,,_,] > E[T,,_;1-

We will now follow a similar route to prove that E[7;_,] > E[T;] for all i. Given

that for all k > i, E[T,_,] = E[T,] holds, we will show that E[T, > E[T,_,] creates a
contradiction. We start with the law of total expectation for E[7,]:

1 = pj j4, (EIT) — ETD + pi) (EIT) — EID) + PETZ

Our assumption “Vk>i: E[T,) < E[T,_,|” implies that E[7;] — E[T;,,] = 0,
thus we obtain:

With our allegedly contradictory assumption E[T;] — E[T,_,] > 0 we obtain:
]

 

> | 5 er) > EIT].

i-—l,m im

1 > PimEIT;] =>

We have already shown above that 1/p;_) 2 1/Pim > EIT] > E[T;_;] can be
induced over i and implies E[T,] > E[T] and 1/po,, > E[To]. Then we can conclude
that:

E(T;] = E[T;,;] VO<i< [y/2]-1. (1)
The above conclusion implies that E[T)] is the largest value that our potential func-

tion can have and E[T;] — E[T7;,,]is non-negative for all 7. O

Now that the potential function is bounded from above, we will bound the drift
Elg, — 8141 | D; = i]. Due to the law of total expectation, the expected absorbing
time, E[T,] satisfies »  Pis(ELT] — E[T,]) = 1 for any absorbing Markov chain at a
transient state i. Since E[T;] and E [7] are the respective potentials of the states S;

Q) Springer
Algorithmica (2020) 82:3676-3706 3695

and S;, the left hand side of the equation closely resembles the drift. In particular,
the actual drift at state S, is also a linear combination of the terms (E[T,] — E [7;]),
although the precise weights (1.e., transition probabilities) of these terms can-
not be expressed as functions of i and j alone. However, these weights can be
bounded and using these bounds the drift can be compared with the expression
Dj D; jElTi — E|T;]) in order to determine how close it is to 1. Since the prob-
abilities for M’ are pessimistically set to underestimate the drift, we can formally
prove the following lemma.

Lemma 3 For a population at level j, El¢’ — g | D, =i] = 1-01) for all t > 0
andi € {0,1,..., 4-1}.

Proof When there is no diversity in the population (i.e., D, = O and g, = g(0) = E[T))
the only way to increase the diversity is to introduce it during a mutation operation. A
non-majority individual is obtained when one of the n —j zero-bits and one of the j
one-bits are flipped while no other bits are touched. Then, one of the majority individ-
uals must be removed from the population during the selection phase. This event has

probability at least p, -2- “244 =p, , and decreases the potential to g(1) = E[T, ].
2 nn pti 0,1 Pp 1

Another way to change the potential function value is to create an improved individual
with the mutation operator. In order to improve a solution it is sufficient to pick one of
n —j one-bits and flip no other bits. This event has probability at least p, - OD) = Pom
and reduces the potential to 0 since an improvement has been found. Thus, we can
conclude that when D,=0, the conditional drift E[A,|D,=0] is at least
Pom(8() — 9) + Po.1(g) — gC) = PomEITo] + Po (EITo] — EIT, )). The law of
total expectation for the state S$) of Markov chain M’ similarly states
» Pij(EITo1 — E[T,]) = Po m(EITo] — ELT, 1) + Po (ElTo] -— EIT\]) = 1. Since the
state 5’, is the absorbing state, E[T,,,] = 0, and therefore,

 

E[A, | D, = 0] > PoynZ(To] + Po ElTo] — ELT) = 1.

For D, > 0, we will condition the drift on whether the picked parents are both
majority individuals €,, are both minority individuals with the same genotype €,, are
a pair that consists of one majority and one minority individual €3, or they are both
minority individuals with different genotypes €,.

Let E* be the event that the population P, consists of two genotypes with Ham-
ming distance two. Then,

 

| _\2
P(E, | &"} = PE, | E} = (4 ‘)

.\ 2
P{E, | E°} = P{E, | E°} + P{E, | E} = (4) (2)

 

P(E |E'} = P(E |B} = 2, PLE, | E"} =0.

Q) Springer
3696 Algorithmica (2020) 82:3676-3706

Let E [a7 be the drift conditional oni > O. The law of total expectation states:

4
E[ AP} =P{E"} Y° PLE, | E}ELAP® | EE)
k=1
4 _ | _
+ (1—P{E*}) }) PLE, | EJ ELAP? | EE]
k=1

.\ 2 .\ 2
(o) ear? |e) (4) min (a?? | £1 614>° | €)
+25 ELA? | €3],
HL

where the last inequality is obtained by substituting the probabilities from (2) and
rearranging the terms. Due to the Definition 2, the conditional drifts in the above
expression depend on the values of (E[T,;] — E [T;]) for (1,7) € {0,...,m}. Therefore,
in order to prove a lower bound on the drift, we will make use of another equality
where the (E[7,] — E [7;]) terms appear. We will now write the law of total expecta-
tion for state i for our Markov chain M’ and then replace the probabilities in the law
of total expectation with the values from Definition | and rearrange it into additive
terms with the probabilities of events €; as multiplicative factors.

1 = pjiniElT] — oD + Pa (EIT) — ELT) + PimEIT)])

 

\ 2
= (4*) Po—— (EIT; | ~ ELT 1)

 

Hu
2
i _fu-i UE|T;] — E[T;_,])
+(4) sm eae)
ip-i tH a E(T;_;)) FELT]

(3)
In particular, we will prove that the following three inequalities hold in order to
prove that E[A’°] > 1 — o(1).

 

E[A™® | E)] 2 (1 = 0(1)) “Pox pelt - E[T,_;)) (4)

min (E[A?° | €,], E[A?’® | €,]) = (1 — 0(1)):

; url 1
Po ( min (4 1/4) (E[T;] — E[T,,,]) + —

i (5)
16yn+1

      

— E[T;_ »)

Q) Springer
Algorithmica (2020) 82:3676-3706 3697

E[A?® | €3] > (1 - 0(1))-

      

—E[T,_

     

ly 1 i E{T;] (6)
po( 4S cetr - EIT}41)) +7 Ant 1

We will start with (4).When two majority individuals are selected as parents (€,),
we pessimistically assume that improving to the next level and increasing the diver-
sity has zero probability. Losing the diversity requires that no bits are flipped during
mutation (with probability p,)) and that a minority individual will be removed from
the population with probability Tar Therefore, E [ai°® |é,] => PoE [T,] — E[T;_ )—
which proves that (4) holds.

Next, we will prove the inequality (5). When two minority individuals are
selected as parents (€, or €,), if they are identical (€,) then it is sufficient that the
mutation does not flip any bits and that a majority individual is removed from the
population. Thus, given €,, the probability of increasing the diversity is at least
Po: (4 —1)/(u + I) and the probability of creating a majority individual is O(1 / n)
since it is necessary to flip at least two particular bit positions. The resulting lower
bound is,

H+1

 

i>0 Mol
FLAP? | E) > pot EIT | - EIT) + O(= ; (ELT - E(T,,))

(7)

 

LH —_—
> (1 -—o(1))- E[T,| — E[T,
= (1 — o(1)) Poe [T;] — ElT;41).
If the two minority individuals have a Hamming distance of 2d > 2 (i.e., €,), then
in order to create another minority individual at the end of the crossover operation it
is sufficient that crossover picks exactly d one-bits and d zero-bits among 2d bit

positions where they differ. There are ad different ways that this can happen and

d

the probability that any particular outcome of crossover is realised is 2-4. One of
those outcomes though, might be the majority individual and if that is the case the
diversity can decrease afterwards. However, while the Hamming distance between
the minority individuals can be 2d = 2, obtaining a majority individual by recom-
bining two minority individuals requires at least four specific bit positions to be
picked correctly during the crossover with probability at most 1/16 or at least one
specific bit must be flipped by the mutation operator to obtain the majority individ-
ual with probability at most O(1 /n). On the other hand, when two different minority

individuals are selected as parents, there is at least a | e b probability that the

2
crossover will result in an individual with more one-bits and then with probability

Po the mutation will not flip any bits. The probability | tb monotonically

2
increases with d and has its minimum value 1/4 for d = 1. Hence,

Q) Springer
3698 Algorithmica (2020) 82:3676-3706

2d _
(( i )-1)2 “oT (EIT) — ElT a)

i 1
+ (min (=. 2- )s +O(1 /n)) EIT) - E[T,_,]) + rer .

E[A?® | E,] = po

 

 

1 E[T;]
TH 5 (EIT | ~ ELT; 7 |

]
16 yu

    

> (0 — o(1)) Po) kt

= (1 — o(1)) Po) et To Eid - E[T;_;]) + FEIT — etr.D)
Where in the last inequality we used the fact that E[T,;] > E[T;] — E[T,,,]. The ine-
qualities (7) and (8) establish that (5) holds. Finally, we will prove that (6) holds.
We will consider the drift conditional on event €3, the case when one minority and
one majority individual are selected as parents. We will further divide this event into
two subcases. In the first case the Hamming distance 2d between the minority and
the majority individual is exactly two (d = 1). Then, the probabilities that crossover
creates a copy of the minority individual, a copy of the majority individual or a
new individual with more one-bits are all equal to 1/4 unless the mutation operator
flips at least one of the bit positions where the minority and majority individuals
differ which happens with probability at most O(1/n) which we can absorb in the
(1 — o(1)) multiplier. Thus, the conditional drift is:

E(A?® | €3,d= 1]

      

> 1 = oy 2e (— S J EIT.) + SEIT] - FUT.) + EIT)

(9)

On the other hand, when d > 1, the drift is more similar to the case of €, where

the probabilities of creating copies of either the minority or the majority individ-

uals diminish with larger d while larger d increases the probability of creating an
improved individual. More precisely,

meee 1) (E{T)) = ElT iD

2d \,_
CG)
——___ er!)

+ (2°44 O(1 /") 5

       

E|T,_,]) +

We will now show that E[A’?° | €;,d > 1] > E[A?’® | €;,d = 1] which together with
(9) will imply that (6) holds. Since (E[T,] — E om is negative, for d > 1,

 

      

(2-4 + O11 /) (EIT;| - EIT) 2 i — E[T,_,)).

Q) Springer
Algorithmica (2020) 82:3676-3706 3699

As the multiplier of the negative (E[7;] — E[T;_,]) term decreases when d > 1,
showing that the sum of the remaining two positive terms increases when d > 1,
establishes that E[A’° | €;,d > 1] > E[A’?° | €;,d = 1]. Between the cases of d = 1
and d > 1, the probability of creating a copy of the minority individual decreases

from 1/4 to (2) — 1) while the probability of creating an improvement
2d |_.,
+
EIT] - E|T,,,]) < E[T,]. Thus, if,

(2) -1)-"-1) + eG) ne

then, E[A'*® | €3,d > 1] > E[A’® | €3,d = 1]. We can factorise the above expres-
Sion as,

((( 77) -1)2""- 4) 4 eG)

where we can see that it is positive as long as ( e > 2, which holds for all d > 1

and proves that E[A'*° | €,,d > 1] > E[A’’® | €;,d = 1]. This in turn shows that (6)
holds and establishes our claim.

increases from 1/4 to . A trivial but crucial observation here is that

le
S
T.
NO
Q
aN
aN
a2
NWS
|
N
NWS

O

Now, we can prove Lemma | using the above results.

Proof of Lemma 1 Since Elg’ - g |D,=i]>1-o0(1) from Lemma 3
and g < E[T A from Lemma 2, we can apply Theorem 2 to obtain
E[T’|<(1+o0(0)E [T! ]. Given that there are k individuals with at least 7 + 1 one-
bits (improved individuals) in the population, in every generation with probability at
least k/, at least one improved individual is selected as parent. If we pessimistically
assume that the other parent has only 7 1-bits, the Hamming distance between the
genotypes is equal to 2d + 1 for some integer d > 0. Let @ be an arbitrarily picked
bit-position which was set to 1 in the improved individuals and to 0 in the other par-
ents. With probability 1/2, this bit-position will be set to 1 in the offspring as well.
For the remaining 2d bit-positions, the probability that at least d 1-bits are inherited
by the offspring is at least 1/2 due to the symmetry of the actual outcome around d.
Thus, given that there are k improved individuals in the population, the expected

Q) Springer
3700 Algorithmica (2020) 82:3676-3706

time until a new improved individual is at most u/k-8-(1—1/n)~" = O(u/k).
Summing over k € [yu] we obtain, ve O(u/k) = O(log pw) expected iterations
after sampling the first improved solution, as an upper bound on the expected time
until all individuals in the population have at least 7+ 1 1-bits. If the population
size is in the order of o(logn/ log logn), then the total number of iterations where
there are individuals with different fitness values in the population is in the order of
o(nlogn). Since j € {0, 1, ...,2— 1}, we can establish that

n-1 n—-1
E(T] < o(nlogn) + ¥ Ef] < omlogn) + (1 + 0(1)) ¥ ELT].
j=0 j=0

4.3 Markov Chain Absorption Time Analysis

In the previous subsection we stated in Lemma | that we can bound the absorbing times
of the Markov chains M! to derive an upper bound on the runtime of Algorithm 1. In
this subsection we use mathematical tools developed for the analysis of Markov chains
to provide such bounds on the absorbing times.

The absorbing time of a Markov chain starting from any initial state i can be
derived by identifying its fundamental matrix. Let the matrix Q denote the transition
probabilities between the transient states of the Markov chain M’. The fundamental
matrix of M/ is defined as N := (I — Q)~! where J is the identity matrix. The most
important characteristic of the fundamental matrix is that, when it is multiplied by
a column vector of ones, the product is a vector holding E [T! ], the expected absorb-
ing times conditional on the initial state 7 of the Markov chain. Since, Lemma | only
involves Ti, we are only interested in the entries of the first row of N = [n,,]. How-
ever, inverting the matrix / — Q is not always a straightforward task. Fortunately,
I —Q=[a,,| has characteristics that allow bounds on the entries of its inverse. Its
entries are related to the transition probabilities of M’ as follows:

41; =1—Poo = Poi + Pom (10)
aum > l — Pm-1,m-1 = Pm-1,m-2 F Pm-1m (11)

ai = 1 — Diy j-) = Pi-ti-2 + Pit + Pi-tm (12)
Vi € {2,...,m—1}

in = —Pi-tp_-) Vik € {1,...,m}Ai#K (13)

Observe that J — Q is a tridiagonal matrix, in the sense that all non-zero ele-
ments of J — Q are either on the diagonal or adjacent to it. Moreover, the diagonal
entries a,; of /— Q are in the form | — p;_;;_;, which is equal to the sum of all
transition probabilities out of state i— 1. Since the other entries on row 7 are

Q) Springer
Algorithmica (2020) 82:3676-3706 3701

transition probabilities from state i—1 to adjacent states, we can see that

lai] > Dien 4x]. The matrices where |a;;| > ))j4, |a,| holds are called strongly

diagonally dominant (SDD). Since I — Q is SDD, according to [21, Lemma 2.1],

it holds for the fundamental matrix WN _ for all iFk _ that,
Der lial \ \7 -]

nicl < IMexl S (ae — Halu )) < (lagal -— Diee level) -

lax x
In our particular case, the above inequality implies that |n,,| < 1/py_1,)- For

any population with diversity, there is a probability in the order of Q(1/y) of
selecting one minority and one majority individual and a constant probability
that their offspring will have more one-bits than the current level. Considering
m= O(n), we obtain:

 

Eri} = Ying <m + ¥ —— sm +O), (14)
k=l k=2 Pk-1,m

We note here that the O( yu) factor in the above expression creates the con-
dition 4 = o(1/logn) on the population size for our main results. We will now
bound the term n, , from above to establish our upper bound using the following
theorem which follows from [21, Corollary 3.2]:

Theorem 3 Let A be an mxXm tridiagonal non-singular SDD matrix such
that a,, <0 for all i#k, A7' =[nj,,] exists and n,,>0 for all i, k. Then,
Ny = 1/(4,) + 4) 26), where ¢; = aj; /(Gij + Gi,i416i41)> ANd On = Gnm—1/ 4mm:

In order to use Theorem 3, we need to satisfy its conditions. We can easily see that
non-diagonal entries of the original matrix J — Q are non-positive and use [21, Theo-
rem 3.1] to show that N = (J — Q)~'has no negative entries:

Theorem 4 (Theorem 3.1 in [21]) Jf A is a tridiagonal non-singular SDD matrix
and a; ; > 0, then Al= [n;,] exists and

— sign(n;;) = 1 .
— sign(n;,) = (-1)'** Tews Ajj. > k
— sign(n,,) = (-1)'™* TTL, aut < k.

Since the diagonal entries of /—@Q are strictly positive, according to Theo-
rem 4 the diagonal entries of N are also positive. The non-diagonal entries of J — Q
are all negative thus the series multiplication from Theorem 4 for i > k reduces to
(—1)tht-k — (—1)* = 1. Similarly for the case i < k, the multiplication reduces to
(—1)it*k-? = (—1)** = 1. Hence, N does not have any negative entries.

Lemma 4 With an initial population of size wp = o(/logn) at level j, the expected

time E[T’] until an individual with j + 1 one-bits is sampled by the (u + 1) GA for
the first time is bounded from above as follows:

Q) Springer
3702 Algorithmica (2020) 82:3676-3706

 

 

E{T!) < n —= = + o(logn), where,
n—- 2d, _
Pm-1,m-2
En = OE
Pm-1,m + Pm—1m-2
Pi-1,i-2

E, = — ._ V1 K<ixm= [pe /2).
Pi-tm + Pi-1i-2 + Pi-s i — Sin)

where p;, are the transition probabilities of the Markov chain M.
Proof Starting from Inequality 14 and applying Theorem 3 we obtain:

ij 2 | 2
E[T)] Sm. + O(u") < a1 4 aoe + O(y’)
]

= 4 FO,"
Pom + PorG — €2) (1 )

1
< ——________ +
Pom + PosG — €2)

]
tt O(n")
(n—=))P 4 SHO DP 2 ( — &)

n (u+1) n
~7 |g Grog ny,

a, 2
” Jp, + AAA (1 - $5)

O(n")

 

where we substitute py ; and po, with their values declared in Definition |. The defi-
nitions of €, and &,, are obtained by simply substituting the matrix entries in Theo-
rem 3 with their respective values from Eqs. 10 to 13. O

In Lemma 4, we can now see that the bound depends on p, and p, of the mutation
operator and €,. The term €, represents the conditional probability of returning to the
state (i — 1) given that you initialise at state 7. It only depends on the probabilities of
creating either a copy of the minority/majority solution or an improvement. Since
we pessimistically assume that we can only improve by crossover when diversity is
present, the leading term of all these probabilities include py. Thus, when the condi-
tional probabilities €, are calculated, the common factor p, disappears and we obtain
the upper bound that only depends on p,, p, and yw.

The above bound on E [T)], together with Lemma 1, yields Theorem 1, our main
result.

Proof of Theorem 1 Combining Lemmas | and 4 we obtain:

Q) Springer
Algorithmica (2020) 82:3676-3706 3703

n—-1
E[T] < o(mlogn) + (1 + 0(1)) )Y EIT]
j=0
ol (15)
< o(nlogn) + (1 + 0(1)) ))

_; 2j
j=0 | Jp, + mp — &5)

n 1

 

 

We will now divide the sum into two smaller sums:

= n 1

 

—j 2)
jl” ~I py + AT =F (1 — $)

n—n//logn

n 1

 

 

_; 2
jl OS py t ae — &)

 

n

]
* 2 7 + 4 2(] _ &,)
j=n—n/vVlogn+1 P\ (utl) on 2

< O(nylogn ) + ———*______ » :

2 1 nn
Pit aap (1 ~~ a) ~ ¢) j=n—n/v/logn+1 J

< O(nvlogn) + inn

2P2"

We conclude the proof by substituting the sum in Eq. 15 with the above expression.

 

 

 

 

 

E[T] < o(nlogn)

ninn

+1-+0(1)] 0(nyiogn) + p+ Bi-Lo )a-e

 

 

1” utd

< o(nlogn)

ninn

2P2H _
Pit Gap oy)

+ (1 +0(1)) O(nvlog n) +(1 +0(1))

1

2P2H _
Pit Gap f)

= (1+o0(1))nInn

 

The expressions for €, and €,, come from Lemma 4 and prove the first statement.

For the second statement, we adapt the result for the variant of the (w+ 1) GA
which does not evaluate copies of either parents. When there is no diversity in the
population the offspring is identical to the parent with probability py. Then, given
that a fitness evaluation occurs, the probability of improvement via mutation is

Q) Springer
3704 Algorithmica (2020) 82:3676-3706

Pom/U — po) and the probability that diversity is introduced is pg ,/(1 — po). The
proof is identical to the proof of the first statement, except for using probabilities
Pom = Pom/( = Po) and po, = Poi /C — Po) instead of po; and po, from Defini-
tion 1. Even if we pessimistically assume that a function evaluation occurs at every
iteration when there is diversity in the population, we still get a (1 — py) decrease in
the leading constant. O

Whether the copies of the parents are evaluated or not does not affect the algo-
rithm’s trajectory in the search space. Its effect on the expected runtime in turn can
be estimated in a straightforward manner if the probability of producing a copy is
known. In general this probability is a function of the mutation operator and the
Hamming distances between each pair of individuals in the population. For the
special case of monotypic populations (where the Hamming distance between all
pairs is zero) it is equal to the probability that the mutation operator does not flip
any bits, i.e., Py. The probability of creating a copy decreases with the diversity of
the population. However, for population sizes of o(1/logn), the expected number
of iterations with a diverse population is asymptotically smaller than the number of
iterations with a monotypic population. This property allows us to adapt the leading
constant of the upper bound on the expected runtime of the standard algorithm by
multiplying it with (1 — po).

5 Conclusion

In this work, we have shown that the steady-state (u + 1) GA optimises ONEMaAx
faster than any unary unbiased search heuristic. Providing precise asymptotic
bounds on the expected runtime of standard GAs without artificial mechanisms
that simplify the analysis has been a long standing open problem [22, 31]. We have
derived bounds up to the leading term constants of the expected runtime. To achieve
this result we show that a simplified Markov chain pessimistically represents the
behaviour of the GA for ONEMax. This insight about the algorithm/problem pair
allows the derivation of runtime bounds for a complex multi-dimensional stochastic
process. The analysis shows that as the number of states in the Markov chain (the
population size) increases, so does the probability that diversity in the population
is kept. Thus, larger populations increase the probability that recombination finds
improved solutions quickly, hence reduce the expected runtime. Recent work has
provided lower bounds on the expected runtime of the (2 + 1) GA with mutation
rates up to 1.422/n [27]. Future work should focus on deriving corresponding lower
bounds for different population sizes to fill in the gaps left in this paper regarding
the power of populations for hill-climbing OneMax-like functions. The only other
lower bounds available for standard genetic algorithms are those derived to show
that generational GAs are inefficient even for easy problems if fitness proportional
selection is used [24, 25].

Acknowledgements This work was supported by EPSRC under Grant EP/M004252/1.

Q) Springer
Algorithmica (2020) 82:3676-3706 3705

Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as
you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-
mons licence, and indicate if changes were made. The images or other third party material in this article
are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the article’s Creative Commons licence and your intended use is
not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission
directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licen
ses/by/4.0/.

References

1. Corus, D., Dang, D.C., Eremeev, A.V., Lehre, P.K.: Level-based analysis of genetic algorithms
and other search processes. IEEE Trans. Evolut. Comput. 22(5), 707-719 (2018)

2. Corus, D., Oliveto, P.S.: Standard steady state genetic algorithms can hillclimb faster than muta-
tion-only evolutionary algorithms. IEEE Trans. Evolut. Comput. 22(5), 720-732 (2018)

3. Corus, D., Oliveto, P.S.: On the benefits of populations for the exploitation speed of standard
steady-state genetic algorithms. In: Proceedings of the Genetic and Evolutionary Computation
Conference, Proc. of GECCO’ 19, pp. 1452-1460 (2019)

4. Dang, D.C., Friedrich, T., Kotzing, T., Krejca, M.S., Lehre, P.K., Oliveto, P.S., Sudholt, D., Sut-
ton, A.M.: Escaping local optima using crossover with emergent diversity. IEEE Trans. Evolut.
Comput. 22(3), 484-497 (2018)

5. Doerr, B., Happ, E., Klein, C.: Crossover can provably be useful in evolutionary computation.
Theor. Comput. Sci. 425, 17-33 (2012)

6. Doerr, B., Doerr, C.: Optimal parameter choices through self-adjustment: applying the 1/5-th
rule in discrete settings. In: Proceedings of GECCO’15, pp. 1335-1342 (2015)

7. Doerr, B., Doerr, C.: A tight runtime analysis of the (1+(A, 4)) genetic algorithm on OneMax. In:
Proceedings of GECCO’15, pp. 1423-1430 (2015)

8. Doerr, B., Doerr, C., Yang, J.: Optimal parameter choices via precise black-box analysis. In: Pro-
ceedings of GECCO’ 16, pp. 1123-1130 (2016)

9. Eiben, A.E., Smith, J.E.: Introduction to Evolutionary Computing. Springer, Berlin (2003)

10. Friedrich, T., Oliveto, P.S., Sudholt, D., Witt, C.: Analysis of diversity-preserving mechanisms
for global exploration. Evolut. Comput. 17(4), 455-476 (2009)

11. He, J., Yao, X.: Towards an analytic framework for analysing the computation time of evolution-
ary algorithms. Artif. Intell. 145(1—2), 59-97 (2003)

12. He, J., Yao, X.: A study of drift analysis for estimating computation time of evolutionary algo-
rithms. Nat. Comput. 3, 21-35 (2004)

13. Jansen, T.: Analyzing Evolutionary Algorithms: The Computer Science Perspective. Springer,
Berlin (2013)

14. Jansen, T., Wegener, I.: The analysis of evolutionary algorithms-a proof that crossover really can
help. Algorithmica 34(1), 47-66 (2002)

15. Lehre, P.K., Oliveto, P.S.: Theoretical analysis of stochastic search algorithms. In: Resende,
R.M.M.G.C., Pardalos, P.M. (eds.) Handbook of Heuristics. Springer, Berlin (2018)

16. Lehre, P.K., Witt, C.: Black-box search by unbiased variation. Algorithmica 64(4), 623-642
(2012)

17. Lehre, P.K., Yao, X.: Crossover can be constructive when computing unique input-output sequences.
Soft Comput. 15(9), 1675-1687 (2011)

18. Lengler, J.: Drift analysis. In: Doerr, B., Neumann, F. (eds.) Theory of Evolutionary Computation:
Recent Developments in Discrete Optimization, pp. 89-131. Springer, Berlin (2020)

19. Lengler, J.: A general dichotomy of evolutionary algorithms on monotone functions. In: Proceed-
ings of PPSN XV, pp. 3-15. Springer (2018)

20. Lengler, J., Zou, X.: Exponential slowdown for larger populations: The (u + 1)-ea on monotone
functions. In: Proceedings of FOGA, pp. 87—101 (2019)

21. Li, H.B., Huang, T.Z., Liu, X.P., Li, H.: On the inverses of general tridiagonal matrices. Linear
Algebra Appl. 433(5), 965-983 (2010)

Q) Springer
3706 Algorithmica (2020) 82:3676-3706

22. Mitchell, M., Holland, J.H., Forrest, S.: When will a genetic algorithm outperform hill climbing. In:
Cowan, J.D., Tesauro, G., Alspector, J. (eds.) Advances in Neural Information Processing Systems,
vol. 6, pp. 51-58. Morgan Kaufmann Publishers, Burlington (1994)

23. Neumann, F., Oliveto, P.S., Rudolph, G., Sudholt, D.: On the effectiveness of crossover for migra-
tion in parallel evolutionary algorithms. In: Proceedings of GECCO’ 11, pp. 1587-1594 (2011)

24. Oliveto, P.S., Witt, C.: On the runtime analysis of the simple genetic algorithm. Theor. Comput. Sci.
545, 2-19 (2014)

25. Oliveto, P.S., Witt, C.: Improved time complexity analysis of the simple genetic algorithm. Theor.
Comput. Sci. 605, 21-41 (2015)

26. Oliveto, P.S., Yao, X.: Runtime analysis of evolutionary algorithms for discrete optimization. In:
Doerr, B., Auger, A. (eds.) Theory of Randomized Search Heuristics: Foundations and Recent
Developments, p. 21. World Scientific, Singapore (2011)

27. Oliveto, P.S., Sudholt, D., Witt, C.: A tight lower bound on the expected runtime ofstandard steady
state genetic algorithms. In: Proceedings of GECCO’ 20, p. 1323-1331 (2020)

28. Pinto, E.C., Doerr, C.: A simple proof for the usefulness of crossover in black-box optimization. In:
Proceedings of PPSN XV, pp. 29-41 (2018)

29. Sarma, J., Jong, K.D.: Generation gap methods. In: Back, T., Fogel, D.B., Michalewicz, Z. (eds.)
Handbook of Evolutionary Computation. IOP Publishing Ltd, Bristol (1997)

30. Sudholt, D.: A new method for lower bounds on the running time of evolutionary algorithms. IEEE
Trans. Evolut. Comput. 17(3), 418-435 (2013)

31. Sudholt, D.: How crossover speeds up building block assembly in genetic algorithms. Evolut. Com-
put. 25(2), 237-274 (2017)

32. Sudholt, D.: Crossover is provably essential for the ising model on trees. In: Proceedings of
GECCO’ 11, pp. 1161-1167. New York, New York, USA (2005)

33. Sutton, A.: Crossover can simulate bounded tree search on a fixed-parameter tractable optimization
problem. In: Proceedings of GECCO’ 18, pp. 1531-1538 (2018)

34. Witt, C.: Runtime analysis of the (u + 1) ea on simple pseudo-boolean functions. Evolut. Comput.
14(1), 65-86 (2006)

35. Witt, C.: Tight bounds on the optimization time of a randomized search heuristic on linear func-
tions. Combin. Probab. Comput. 22(2), 294-318 (2013)

36. Yu, Y., Qian, C., Zhou, Z.H.: Switch analysis for running time analysis of evolutionary algorithms.
IEEE Trans. Evolut. Comput. 19(6), 777-792 (2015)

Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published
maps and institutional affiliations.

Q) Springer
